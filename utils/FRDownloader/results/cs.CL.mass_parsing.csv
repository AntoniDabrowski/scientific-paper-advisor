,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract
7,"With insight into these trends, we hope
                                        decoder then utilizes this representation to gener-   to further research on multilingual translation by
                                        ate a sequence of words, which is the translation.",The     models.,"Attention allows the decoder to weight individual         1Among other features, including positional encoding and
                                        tokens in the source sequence depending on their      layer normalization
                                        importance to the word being generated.",2021-12-31 23:28:28+00:00,How do lexical semantics affect translation? An empirical study,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Vivek Subramanian'), arxiv.Result.Author('Dhanasekar Sundararaman')]","Neural machine translation (NMT) systems aim to map text from one language
into another. While there are a wide variety of applications of NMT, one of the
most important is translation of natural language. A distinguishing factor of
natural language is that words are typically ordered according to the rules of
the grammar of a given language. Although many advances have been made in
developing NMT systems for translating natural language, little research has
been done on understanding how the word ordering of and lexical similarity
between the source and target language affect translation performance. Here, we
investigate these relationships on a variety of low-resource language pairs
from the OpenSubtitles2016 database, where the source language is English, and
find that the more similar the target language is to English, the greater the
translation performance. In addition, we study the impact of providing NMT
models with part of speech of words (POS) in the English sequence and find
that, for Transformer-based models, the more dissimilar the target language is
from English, the greater the benefit provided by POS."
31,"In
particular, further research into candidate generation and how the algorithm
perceives the context of both the abstract and the knowledge base is needed.","The method is robust,
modular base which can be iteratively improved, to achieve better accuracy.","Regarding classiﬁcation speed, substantial improvements could be made by
parallelizing the code.",2022-01-02 16:15:05+00:00,Topical Classification of Food Safety Publications with a Knowledge Base,cs.CL,['cs.CL'],"[arxiv.Result.Author('Piotr Sowiński'), arxiv.Result.Author('Katarzyna Wasielewska-Michniewska'), arxiv.Result.Author('Maria Ganzha'), arxiv.Result.Author('Marcin Paprzycki')]","The vast body of scientific publications presents an increasing challenge of
finding those that are relevant to a given research question, and making
informed decisions on their basis. This becomes extremely difficult without the
use of automated tools. Here, one possible area for improvement is automatic
classification of publication abstracts according to their topic. This work
introduces a novel, knowledge base-oriented publication classifier. The
proposed method focuses on achieving scalability and easy adaptability to other
domains. Classification speed and accuracy are shown to be satisfactory, in the
very demanding field of food safety. Further development and evaluation of the
method is needed, as the proposed approach shows much potential."
32,"In
particular, further research into candidate generation and how the algorithm
perceives the context of both the abstract and the knowledge base is needed.","The method is robust,
modular base which can be iteratively improved, to achieve better accuracy.","Regarding classiﬁcation speed, substantial improvements could be made by
parallelizing the code.",2022-01-02 16:15:05+00:00,Topical Classification of Food Safety Publications with a Knowledge Base,cs.CL,['cs.CL'],"[arxiv.Result.Author('Piotr Sowinski'), arxiv.Result.Author('Katarzyna Wasielewska-Michniewska'), arxiv.Result.Author('Maria Ganzha'), arxiv.Result.Author('Marcin Paprzycki')]","The vast body of scientific publications presents an increasing challenge of
finding those that are relevant to a given research question, and making
informed decisions on their basis. This becomes extremely difficult without the
use of automated tools. Here, one possible area for improvement is automatic
classification of publication abstracts according to their topic. This work
introduces a novel, knowledge base-oriented publication classifier. The
proposed method focuses on achieving scalability and easy adaptability to other
domains. Classification speed and accuracy are shown to be satisfactory, in the
very demanding field of food safety. Further development and evaluation of the
method is needed, as the proposed approach shows much potential."
162,"Of course, this is a very simpliﬁed case of analysis, where
the eﬀect of overﬁtting may occur, so this approach requires further study.","The results show that an intelligent agent can ﬁnd the an opti-
mal proﬁtable strategy.","The
main goal is to show that, using reinforced learning and an environment model
based on historical ﬁnancial data and quantitative characteristics of tweets, it
is possible to build a model in which an intelligent agent can ﬁnd an optimal
strategy that optimizes the reward function in episodes of interaction of learning
agent with the environment.",2022-01-06 13:25:57+00:00,Forming Predictive Features of Tweets for Decision-Making Support,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']",[arxiv.Result.Author('Bohdan M. Pavlyshenko')],"The article describes the approaches for forming different predictive
features of tweet data sets and using them in the predictive analysis for
decision-making support. The graph theory as well as frequent itemsets and
association rules theory is used for forming and retrieving different features
from these datasests. The use of these approaches makes it possible to reveal a
semantic structure in tweets related to a specified entity. It is shown that
quantitative characteristics of semantic frequent itemsets can be used in
predictive regression models with specified target variables."
349,cases with entitlement (E) and non-factive verbs - remain an open issue for further research.,"Complex cases in the phenomenon -
                                                   e.g.","Keywords Natural Language Inference · Factivity · HerBERT, Low-resource Languages

                                        1 Introduction

                                        Semantics is still one of the biggest problems of Natural Language Processing (NLP)1.",2022-01-10 18:32:55+00:00,Polish Natural Language Inference and Factivity -- an Expert-based Dataset and Benchmarks,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Daniel Ziembicki'), arxiv.Result.Author('Anna Wróblewska'), arxiv.Result.Author('Karolina Seweryn')]","Despite recent breakthroughs in Machine Learning for Natural Language
Processing, the Natural Language Inference (NLI) problems still constitute a
challenge. To this purpose we contribute a new dataset that focuses exclusively
on the factivity phenomenon; however, our task remains the same as other NLI
tasks, i.e. prediction of entailment, contradiction or neutral (ECN). The
dataset contains entirely natural language utterances in Polish and gathers
2,432 verb-complement pairs and 309 unique verbs. The dataset is based on the
National Corpus of Polish (NKJP) and is a representative sample in regards to
frequency of main verbs and other linguistic features (e.g. occurrence of
internal negation). We found that transformer BERT-based models working on
sentences obtained relatively good results ($\approx89\%$ F1 score). Even
though better results were achieved using linguistic features ($\approx91\%$ F1
score), this model requires more human labour (humans in the loop) because
features were prepared manually by expert linguists. BERT-based models
consuming only the input sentences show that they capture most of the
complexity of NLI/factivity. Complex cases in the phenomenon - e.g. cases with
entitlement (E) and non-factive verbs - remain an open issue for further
research."
350,"They performed
experiments on similar datasets and thus inﬂuenced our further research.","Also, we are grateful for many students from the Faculty of Mathematics and Information Science at Warsaw University
of Technology, working under Anna Wróblewska’s guidance in Natural Language Processing course.","References

Kyle Richardson, Hai Hu, Lawrence Moss, and Ashish Sabharwal.",2022-01-10 18:32:55+00:00,Polish Natural Language Inference and Factivity -- an Expert-based Dataset and Benchmarks,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Daniel Ziembicki'), arxiv.Result.Author('Anna Wróblewska'), arxiv.Result.Author('Karolina Seweryn')]","Despite recent breakthroughs in Machine Learning for Natural Language
Processing, the Natural Language Inference (NLI) problems still constitute a
challenge. To this purpose we contribute a new dataset that focuses exclusively
on the factivity phenomenon; however, our task remains the same as other NLI
tasks, i.e. prediction of entailment, contradiction or neutral (ECN). The
dataset contains entirely natural language utterances in Polish and gathers
2,432 verb-complement pairs and 309 unique verbs. The dataset is based on the
National Corpus of Polish (NKJP) and is a representative sample in regards to
frequency of main verbs and other linguistic features (e.g. occurrence of
internal negation). We found that transformer BERT-based models working on
sentences obtained relatively good results ($\approx89\%$ F1 score). Even
though better results were achieved using linguistic features ($\approx91\%$ F1
score), this model requires more human labour (humans in the loop) because
features were prepared manually by expert linguists. BERT-based models
consuming only the input sentences show that they capture most of the
complexity of NLI/factivity. Complex cases in the phenomenon - e.g. cases with
entitlement (E) and non-factive verbs - remain an open issue for further
research."
365,"For example, pre-      with the expectation of stimulating further research
vious works on explaining uncertainty estimates       on explaining predictive uncertainty in NLP.","In this work, we focus on extracting
   There is limited work on studying the source of    uncertain words from existing explanation methods,
predictive uncertainty in NLP.","mainly focus on tabular and image data (Antorán
et al., 2020; Ley et al., 2021; Perez et al., 2022).",2022-01-11 02:04:50+00:00,Explaining Predictive Uncertainty by Looking Back at Model Explanations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hanjie Chen'), arxiv.Result.Author('Wanyu Du'), arxiv.Result.Author('Yangfeng Ji')]","Predictive uncertainty estimation of pre-trained language models is an
important measure of how likely people can trust their predictions. However,
little is known about what makes a model prediction uncertain. Explaining
predictive uncertainty is an important complement to explaining prediction
labels in helping users understand model decision making and gaining their
trust on model predictions, while has been largely ignored in prior works. In
this work, we propose to explain the predictive uncertainty of pre-trained
language models by extracting uncertain words from existing model explanations.
We find the uncertain words are those identified as making negative
contributions to prediction labels, while actually explaining the predictive
uncertainty. Experiments show that uncertainty explanations are indispensable
to explaining models and helping humans understand model prediction behavior."
556,"It calls for further research to establish global normalization
based on PLMs to ensure that text generation can be controlled locally and globally at the same
time.","For example, it is hard to keep long-range coherency in both semantic logic and controlled
condition for long text generation.","Fourth, the construction of large scale pre-trained language models are typically data-driven,
which allows the models to learn the primary logic and common sense contained in the training
corpus.",2022-01-14 08:32:20+00:00,A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hanqing Zhang'), arxiv.Result.Author('Haolin Song'), arxiv.Result.Author('Shaoyu Li'), arxiv.Result.Author('Ming Zhou'), arxiv.Result.Author('Dawei Song')]","Controllable Text Generation (CTG) is emerging area in the field of natural
language generation (NLG). It is regarded as crucial for the development of
advanced text generation technologies that are more natural and better meet the
specific constraints in practical applications. In recent years, methods using
large-scale pre-trained language models (PLMs), in particular the widely used
transformer-based PLMs, have become a new paradigm of NLG, allowing generation
of more diverse and fluent text. However, due to the lower level of
interpretability of deep neural networks, the controllability of these methods
need to be guaranteed. To this end, controllable text generation using
transformer-based PLMs has become a rapidly growing yet challenging new
research hotspot. A diverse range of approaches have emerged in the recent 3-4
years, targeting different CTG tasks which may require different types of
controlled constraints. In this paper, we present a systematic critical review
on the common tasks, main approaches and evaluation methods in this area.
Finally, we discuss the challenges that the field is facing, and put forward
various promising future directions. To the best of our knowledge, this is the
first survey paper to summarize CTG techniques from the perspective of PLMs. We
hope it can help researchers in related fields to quickly track the academic
frontier, providing them with a landscape of the area and a roadmap for future
research."
557,"It calls for further research to establish global normalization based on PLMs to ensure
that text generation can be controlled locally and globally at the same time.","For example, it is hard to keep long-range coherency in both semantic logic and controlled
condition.","Fourth, the construction of large-scale pre-trained language models is typically data-driven,
which allows the models to learn the primary logic and common sense contained in the training
corpus.",2022-01-14 08:32:20+00:00,A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hanqing Zhang'), arxiv.Result.Author('Haolin Song'), arxiv.Result.Author('Shaoyu Li'), arxiv.Result.Author('Ming Zhou'), arxiv.Result.Author('Dawei Song')]","Controllable Text Generation (CTG) is emerging area in the field of natural
language generation (NLG). It is regarded as crucial for the development of
advanced text generation technologies that are more natural and better meet the
specific constraints in practical applications. In recent years, methods using
large-scale pre-trained language models (PLMs), in particular the widely used
transformer-based PLMs, have become a new paradigm of NLG, allowing generation
of more diverse and fluent text. However, due to the lower level of
interpretability of deep neural networks, the controllability of these methods
need to be guaranteed. To this end, controllable text generation using
transformer-based PLMs has become a rapidly growing yet challenging new
research hotspot. A diverse range of approaches have emerged in the recent 3-4
years, targeting different CTG tasks which may require different types of
controlled constraints. In this paper, we present a systematic critical review
on the common tasks, main approaches and evaluation methods in this area.
Finally, we discuss the challenges that the field is facing, and put forward
various promising future directions. To the best of our knowledge, this is the
first survey paper to summarize CTG techniques from the perspective of PLMs. We
hope it can help researchers in related fields to quickly track the academic
frontier, providing them with a landscape of the area and a roadmap for future
research."
562,"(3) Responses show that
To further study the effects of inquiry strategies,                          the chatbots do not know / remember the answers.","It does
                                                                             not have enough meaningful information because
5.3 Effects of Inquiry Strategies                                            the questionnaire only cares about the recent situa-
                                                                             tions of the participants.","we plot the averaged score of 50 experiments under                           For example, the chatbot respond “I don’t know”
each question in Figure 4.",2022-01-14 10:38:59+00:00,Mental Health Assessment for the Chatbots,cs.CL,"['cs.CL', 'cs.HC']","[arxiv.Result.Author('Yong Shan'), arxiv.Result.Author('Jinchao Zhang'), arxiv.Result.Author('Zekang Li'), arxiv.Result.Author('Yang Feng'), arxiv.Result.Author('Jie Zhou')]","Previous researches on dialogue system assessment usually focus on the
quality evaluation (e.g. fluency, relevance, etc) of responses generated by the
chatbots, which are local and technical metrics. For a chatbot which responds
to millions of online users including minors, we argue that it should have a
healthy mental tendency in order to avoid the negative psychological impact on
them. In this paper, we establish several mental health assessment dimensions
for chatbots (depression, anxiety, alcohol addiction, empathy) and introduce
the questionnaire-based mental health assessment methods. We conduct
assessments on some well-known open-domain chatbots and find that there are
severe mental health issues for all these chatbots. We consider that it is due
to the neglect of the mental health risks during the dataset building and the
model training procedures. We expect to attract researchers' attention to the
serious mental health problems of chatbots and improve the chatbots' ability in
positive emotional interaction."
590,"We further study the impact
store can improve the model’s performance.","As shown in section 3.1, retrieving neighbors from the knowledge       knowledge store with a query triple.","When facing emerging        of different pre-trained language models with different pre-trained
entities, the translation-based model (such as TransE) have to add     tasks on the performance of 𝑘NN-KGE.",2022-01-14 17:35:16+00:00,Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ningyu Zhang'), arxiv.Result.Author('Xin Xie'), arxiv.Result.Author('Xiang Chen'), arxiv.Result.Author('Xu Cheng'), arxiv.Result.Author('Huajun Chen')]","Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they struggle to reason rare or emerging unseen entities. In this paper, we
propose kNN-KGE, a new knowledge graph embedding approach with pre-trained
language models, by linearly interpolating its entity distribution with
k-nearest neighbors. We compute the nearest neighbors based on the distance in
the entity embedding space from the knowledge store. Our approach can allow
rare or emerging entities to be memorized explicitly rather than implicitly in
model parameters. Experimental results demonstrate that our approach can
improve inductive and transductive link prediction results and yield better
performance for low-resource settings with only a few triples, which might be
easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG."
625,and code open-source for further study.,We regardless make our data           is possible to rank the candidates accordingly.,"Language model feature:
 Total Dataset Training Data Test Data                      A substitution candidate should ﬁt in the context of the
 3k Sentences 2650 Sentences 350 Sentences                  words that come before and after the original term.",2022-01-15 15:58:44+00:00,Automatic Lexical Simplification for Turkish,cs.CL,['cs.CL'],[arxiv.Result.Author('Ahmet Yavuz Uluslu')],"In this paper, we present the first automatic lexical simplification system
for the Turkish language. Recent text simplification efforts rely on manually
crafted simplified corpora and comprehensive NLP tools that can analyse the
target text both in word and sentence levels. Turkish is a morphologically rich
agglutinative language that requires unique considerations such as the proper
handling of inflectional cases. Being a low-resource language in terms of
available resources and industrial-strength tools, it makes the text
simplification task harder to approach. We present a new text simplification
pipeline based on pretrained representation model BERT together with
morphological features to generate grammatically correct and semantically
appropriate word-level simplifications."
626,"The common assumption among lin-            ing a general-purpose lexical simpliﬁcation pipeline for
                                        guists is that those who are familiar with the vocabu-      Turkish to lay the foundations of further research.","Therefore, we simply focus on build-
                                        et al., 2013a).","lary of a text can often understand the meaning even if
                                        they have problems with grammatical structures.",2022-01-15 15:58:44+00:00,Automatic Lexical Simplification for Turkish,cs.CL,['cs.CL'],[arxiv.Result.Author('Ahmet Yavuz Uluslu')],"In this paper, we present the first automatic lexical simplification system
for the Turkish language. Recent text simplification efforts rely on manually
crafted simplified corpora and comprehensive NLP tools that can analyse the
target text both in word and sentence levels. Turkish is a morphologically rich
agglutinative language that requires unique considerations such as the proper
handling of inflectional cases. Being a low-resource language in terms of
available resources and industrial-strength tools, it makes the text
simplification task harder to approach. We present a new text simplification
pipeline based on pretrained representation model BERT together with
morphological features to generate grammatically correct and semantically
appropriate word-level simplifications."
627,"We regardless make our
                                                           data and code open-source for further study.","This is not a complete study or dataset, as CWI
                                                           Shared Task included multiple annotators with differ-
                                                           ent assumed roles to construct a corpus of 90.000 sen-
                                                           tences (Yimam et al., 2018).","Total Dataset Training Data Test Data
                                                            3k Sentences 2650 Sentences 350 Sentences

                                                                         Table 1: Turkish CWI Dataset

Figure 3: Dependency Visualisation After Simpliﬁca-        A sequence labelling based word-level BiLSTM model
tion                                                       was trained to predict the binary complexity of words
                                                           annotated in the dataset.",2022-01-15 15:58:44+00:00,Automatic Lexical Simplification for Turkish,cs.CL,['cs.CL'],[arxiv.Result.Author('Ahmet Yavuz Uluslu')],"In this paper, we present the first automatic lexical simplification system
for the Turkish language. Recent text simplification efforts rely on manually
crafted simplified corpora and comprehensive NLP tools that can analyse the
target text both in word and sentence levels. Turkish is a morphologically rich
agglutinative language that requires unique considerations such as the proper
handling of inflectional cases. Being a low-resource language in terms of
available resources and industrial-strength tools, it makes the text
simplification task harder to approach. We present a new text simplification
pipeline based on pretrained representation model BERT together with
morphological features to generate grammatically correct and semantically
appropriate word-level simplifications."
636,"Stakeholders The researchers engaged in the
Chinese offensive language study will be the direct     B.3 Model-in-the-loop Collection
stakeholders, and proposed COLDATASET will ef-
fectively support their further research.","automatically selected and labeled in the model-in-
                                                        the-loop setup.","The man-      We adopt the model-in-the-loop setup to discover
agers of social platforms can use this dataset to       the target data and optimize the classiﬁer perfor-
optimize their detector, contributing to better offen-  mance.",2022-01-16 11:47:23+00:00,COLD: A Benchmark for Chinese Offensive Language Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Jingyan Zhou'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Helen Meng'), arxiv.Result.Author('Minlie Huang')]","Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier."
637,"We call for further research to explore the inter-
nal knowledge of language models to facilitate this
task, and the following tips can be considered.","Bert-
base-chinese is taken as the model to predict the
scores of [MASK] token and we take the scores
of candidate words of 可(yes) and 否(no) as the
results of self-detection.","The
ﬁrst is exploring appropriate word pairs.",2022-01-16 11:47:23+00:00,COLD: A Benchmark for Chinese Offensive Language Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Jingyan Zhou'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Helen Meng'), arxiv.Result.Author('Minlie Huang')]","Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier."
648,"It needs further research to investigate
Moving on to the examination of the ranking capabili-                   whether such metrics are suitable for mass application.","produce estimates of the HTR (or OCR) and identify
                                                                        quality issues.","ties of our metrics, we ﬁnd that most metrics’ rankings
exhibit strong correlations to the reference rankings, as                           8.",2022-01-17 01:26:09+00:00,Evaluation of HTR models without Ground Truth Material,cs.CL,['cs.CL'],"[arxiv.Result.Author('Phillip Benjamin Ströbel'), arxiv.Result.Author('Simon Clematide'), arxiv.Result.Author('Martin Volk'), arxiv.Result.Author('Raphael Schwitter'), arxiv.Result.Author('Tobias Hodel'), arxiv.Result.Author('David Schoch')]","The evaluation of Handwritten Text Recognition (HTR) models during their
development is straightforward: because HTR is a supervised problem, the usual
data split into training, validation, and test data sets allows the evaluation
of models in terms of accuracy or error rates. However, the evaluation process
becomes tricky as soon as we switch from development to application. A
compilation of a new (and forcibly smaller) ground truth (GT) from a sample of
the data that we want to apply the model on and the subsequent evaluation of
models thereon only provides hints about the quality of the recognised text, as
do confidence scores (if available) the models return. Moreover, if we have
several models at hand, we face a model selection problem since we want to
obtain the best possible result during the application phase. This calls for
GT-free metrics to select the best model, which is why we (re-)introduce and
compare different metrics, from simple, lexicon-based to more elaborate ones
using standard language models and masked language models (MLM). We show that
MLM-based evaluation can compete with lexicon-based methods, with the advantage
that large and multilingual transformers are readily available, thus making
compiling lexical resources for other metrics superfluous."
649,"It needs further research to investigate
Lastly, we take a look at the Top-N evaluation in Table                 whether such metrics are suitable for mass application.","produce estimates of the HTR (or OCR) and identify
                                                                        quality issues.",4.,2022-01-17 01:26:09+00:00,Evaluation of HTR models without Ground Truth Material,cs.CL,['cs.CL'],"[arxiv.Result.Author('Phillip Benjamin Ströbel'), arxiv.Result.Author('Simon Clematide'), arxiv.Result.Author('Martin Volk'), arxiv.Result.Author('Raphael Schwitter'), arxiv.Result.Author('Tobias Hodel'), arxiv.Result.Author('David Schoch')]","The evaluation of Handwritten Text Recognition (HTR) models during their
development is straightforward: because HTR is a supervised problem, the usual
data split into training, validation, and test data sets allows the evaluation
of models in terms of accuracy or error rates. However, the evaluation process
becomes tricky as soon as we switch from development to application. A
compilation of a new (and forcibly smaller) ground truth (GT) from a sample of
the data that we want to apply the model on and the subsequent evaluation of
models thereon only provides hints about the quality of the recognised text, as
do confidence scores (if available) the models return. Moreover, if we have
several models at hand, we face a model selection problem since we want to
obtain the best possible result during the application phase. This calls for
GT-free metrics to select the best model, which is why we (re-)introduce and
compare different metrics, from simple, lexicon-based to more elaborate ones
using standard language models and masked language models (MLM). We show that
MLM-based evaluation can compete with lexicon-based methods, with the advantage
that large and multilingual transformers are readily available, thus making
compiling lexical resources for other metrics superfluous."
652,"We further study how the Transformer         training helps the overall performance of the model;
model in SQUIRE infers the evidential path by at-       (b) Rule-guided searching improves the quality of
tention visualization.",4.6).,"The visualization result sug-    training paths, resulting in a more stable conver-
gests that the Transformer model has memorized          gence.",2022-01-17 04:22:54+00:00,SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning,cs.CL,"['cs.CL', 'cs.SI']","[arxiv.Result.Author('Yushi Bai'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Yincen Qu'), arxiv.Result.Author('Zelin Dai'), arxiv.Result.Author('Feiyu Xiong')]","Multi-hop knowledge graph (KG) reasoning has been widely studied in recent
years to provide interpretable predictions on missing links with evidential
paths. Most previous works use reinforcement learning (RL) based methods that
learn to navigate the path towards the target entity. However, these methods
suffer from slow and poor convergence, and they may fail to infer a certain
path when there is a missing edge along the path. Here we present SQUIRE, the
first Sequence-to-sequence based multi-hop reasoning framework, which utilizes
an encoder-decoder Transformer structure to translate the query to a path. Our
framework brings about two benefits: (1) It can learn and predict in an
end-to-end fashion, which gives better and faster convergence; (2) Our
Transformer model does not rely on existing edges to generate the path, and has
the flexibility to complete missing edges along the path, especially in sparse
KGs. Experiments on standard and sparse KGs show that our approach yields
significant improvement over prior methods, while converging 4x-7x faster."
653,"We further study how the Transformer               sis in Appendix B): (a) New aggregated data during
model in SQUIRE infers the evidential path by at-             training helps the overall performance of the model;
tention visualization.",4.6).,"The visualization result sug-          (b) Rule-guided searching improves the quality of
gests that the Transformer model has memorized                training paths, resulting in a more stable conver-
the graph during training, and in generation phase            gence.",2022-01-17 04:22:54+00:00,SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning,cs.CL,"['cs.CL', 'cs.SI']","[arxiv.Result.Author('Yushi Bai'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Yincen Qu'), arxiv.Result.Author('Zelin Dai'), arxiv.Result.Author('Feiyu Xiong')]","Multi-hop knowledge graph (KG) reasoning has been widely studied in recent
years to provide interpretable predictions on missing links with evidential
paths. Most previous works use reinforcement learning (RL) based methods that
learn to navigate the path towards the target entity. However, these methods
suffer from slow and poor convergence, and they may fail to infer a certain
path when there is a missing edge along the path. Here we present SQUIRE, the
first Sequence-to-sequence based multi-hop reasoning framework, which utilizes
an encoder-decoder Transformer structure to translate the query to a path. Our
framework brings about two benefits: (1) It can learn and predict in an
end-to-end fashion, which gives better and faster convergence; (2) Our
Transformer model does not rely on existing edges to generate the path, and has
the flexibility to complete missing edges along the path, especially in sparse
KGs. Experiments on standard and sparse KGs show that our approach yields
significant improvement over prior methods, while converging 4x-7x faster."
694,"Eliza was a landmark system that stimulated
                        further research in the ﬁeld.","It searches for an appropriate transformation rule to reformulate the input
                        and provide an output, i.e., an answer to the user.","Nonetheless, ELIZA’s scope of knowledge was limited because
                        it depended on minimal context identiﬁcation and, generally, pattern matching rules are not
                        ﬂexible to be easily implemented in new domains [15–17].",2022-01-17 23:08:58+00:00,A Literature Survey of Recent Advances in Chatbots,cs.CL,['cs.CL'],"[arxiv.Result.Author('Guendalina Caldarini'), arxiv.Result.Author('Sardar Jaf'), arxiv.Result.Author('Kenneth McGarry')]","Chatbots are intelligent conversational computer systems designed to mimic
human conversation to enable automated online guidance and support. The
increased benefits of chatbots led to their wide adoption by many industries in
order to provide virtual assistance to customers. Chatbots utilise methods and
algorithms from two Artificial Intelligence domains: Natural Language
Processing and Machine Learning. However, there are many challenges and
limitations in their application. In this survey we review recent advances on
chatbots, where Artificial Intelligence and Natural Language processing are
used. We highlight the main challenges and limitations of current work and make
recommendations for future research investigation."
695,"The authors concluded that
                        further research is needed on existing chatbots platforms and ethical issues related to chatbots.","Finally, they discussed the general architecture
                        of modern chatbots and the primary platforms for their creation.","The study by [89] aimed at resolving the critical issue of identifying suitable deep learning
                        techniques.",2022-01-17 23:08:58+00:00,A Literature Survey of Recent Advances in Chatbots,cs.CL,['cs.CL'],"[arxiv.Result.Author('Guendalina Caldarini'), arxiv.Result.Author('Sardar Jaf'), arxiv.Result.Author('Kenneth McGarry')]","Chatbots are intelligent conversational computer systems designed to mimic
human conversation to enable automated online guidance and support. The
increased benefits of chatbots led to their wide adoption by many industries in
order to provide virtual assistance to customers. Chatbots utilise methods and
algorithms from two Artificial Intelligence domains: Natural Language
Processing and Machine Learning. However, there are many challenges and
limitations in their application. In this survey we review recent advances on
chatbots, where Artificial Intelligence and Natural Language processing are
used. We highlight the main challenges and limitations of current work and make
recommendations for future research investigation."
753,"To resolve this privacy                         When a new
                                        issue, we further study the under-explored problem of privacy-                            patient Bob with
                                        preserving domain adaptation and propose a method with a novel                            diabetes visited
                                        differential privacy training strategy to protect the source data                         this hospital on
                                        privacy.","However, sharing only the
                                        source feature distribution may still suffer from the membership             Feature                          Target client
                                        inference attack who can infer an individual’s membership by                 extractor
                                        the black-box access to the source model.",We model the source feature distribution by Gaussian                             8/27/2020.,2022-01-18 21:23:28+00:00,A Privacy-Preserving Unsupervised Domain Adaptation Framework for Clinical Text Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Qiyuan An'), arxiv.Result.Author('Ruijiang Li'), arxiv.Result.Author('Lin Gu'), arxiv.Result.Author('Hao Zhang'), arxiv.Result.Author('Qingyu Chen'), arxiv.Result.Author('Zhiyong Lu'), arxiv.Result.Author('Fei Wang'), arxiv.Result.Author('Yingying Zhu')]","Unsupervised domain adaptation (UDA) generally aligns the unlabeled target
domain data to the distribution of the source domain to mitigate the
distribution shift problem. The standard UDA requires sharing the source data
with the target, having potential data privacy leaking risks. To protect the
source data's privacy, we first propose to share the source feature
distribution instead of the source data. However, sharing only the source
feature distribution may still suffer from the membership inference attack who
can infer an individual's membership by the black-box access to the source
model. To resolve this privacy issue, we further study the under-explored
problem of privacy-preserving domain adaptation and propose a method with a
novel differential privacy training strategy to protect the source data
privacy. We model the source feature distribution by Gaussian Mixture Models
(GMMs) under the differential privacy setting and send it to the target client
for adaptation. The target client resamples differentially private source
features from GMMs and adapts on target data with several state-of-art UDA
backbones. With our proposed method, the source data provider could avoid
leaking source data privacy during domain adaptation as well as reserve the
utility. To evaluate our proposed method's utility and privacy loss, we apply
our model on a medical report disease label classification task using two noisy
challenging clinical text datasets. The results show that our proposed method
can preserve source data's privacy with a minor performance influence on the
text classification task."
760,"Additionally ,
Our work instead focus on a realistic threat model to deter-            Around ﬁfty plagiarisms of the original solutions are made,
mine that there is a real risk that needs further study, though      designed using techniques from and classiﬁed according to,
we suspect many of the best solutions would be non-technical         the taxonomy presented in Faidhi and Robinson [1987].",introductory assignments without assistance).,"Fol-
(See section 5).",2022-01-19 04:00:46+00:00,Neural Language Models are Effective Plagiarists,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Stella Biderman'), arxiv.Result.Author('Edward Raff')]","As artificial intelligence (AI) technologies become increasingly powerful and
prominent in society, their misuse is a growing concern. In educational
settings, AI technologies could be used by students to cheat on assignments and
exams. In this paper we explore whether transformers can be used to solve
introductory level programming assignments while bypassing commonly used AI
tools to detect plagiarism. We find that a student using GPT-J [Wang and
Komatsuzaki, 2021] can complete introductory level programming assignments
without triggering suspicion from MOSS [Aiken, 2000], a widely used plagiarism
detection tool. This holds despite the fact that GPT-J was not trained on the
problems in question and is not provided with any examples to work from. We
further find that the code written by GPT-J is diverse in structure, lacking
any particular tells that future plagiarism detection techniques may use to try
to identify algorithmically generated code. We conclude with a discussion of
the ethical and educational implications of large language models and
directions for future research."
761,"Our work instead
there exist more powerful transformers that are publicly available,             focus on a realistic threat model to determine that there is a real
they are either incapable of generating code [50, 52] or lack free              risk that needs further study, though we suspect many of the best
public APIs and require substantial expertise to deploy [6, 66].",Although               introductory assignments without assistance).,solutions would be non-technical (See subsection 6.3).,2022-01-19 04:00:46+00:00,Fooling MOSS Detection with Pretrained Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Stella Biderman'), arxiv.Result.Author('Edward Raff')]","As artificial intelligence (AI) technologies become increasingly powerful and
prominent in society, their misuse is a growing concern. In educational
settings, AI technologies could be used by students to cheat on assignments and
exams. In this paper we explore whether transformers can be used to solve
introductory level programming assignments while bypassing commonly used AI
tools to detect similarities between pieces of software. We find that a student
using GPT-J [Wang and Komatsuzaki, 2021] can complete introductory level
programming assignments without triggering suspicion from MOSS [Aiken, 2000], a
widely used software similarity and plagiarism detection tool. This holds
despite the fact that GPT-J was not trained on the problems in question and is
not provided with any examples to work from. We further find that the code
written by GPT-J is diverse in structure, lacking any particular tells that
future plagiarism detection techniques may use to try to identify
algorithmically generated code. We conclude with a discussion of the ethical
and educational implications of large language models and directions for future
research."
767,"We further study the selected num-         there are speciﬁc properties which are learned more
ber of neurons across different properties, to
see how speciﬁc linguistic properties are encoded     on higher layers than lower layers and vice versa.","These properties differ in their linguis-
tic roles.",in the network.,2022-01-19 06:32:25+00:00,Interpreting Arabic Transformer Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ahmed Abdelali'), arxiv.Result.Author('Nadir Durrani'), arxiv.Result.Author('Fahim Dalvi'), arxiv.Result.Author('Hassan Sajjad')]","Arabic is a Semitic language which is widely spoken with many dialects. Given
the success of pre-trained language models, many transformer models trained on
Arabic and its dialects have surfaced. While these models have been compared
with respect to downstream NLP tasks, no evaluation has been carried out to
directly compare the internal representations. We probe how linguistic
information is encoded in Arabic pretrained models, trained on different
varieties of Arabic language. We perform a layer and neuron analysis on the
models using three intrinsic tasks: two morphological tagging tasks based on
MSA (modern standard Arabic) and dialectal POS-tagging and a dialectal
identification task. Our analysis enlightens interesting findings such as: i)
word morphology is learned at the lower and middle layers ii) dialectal
identification necessitate more knowledge and hence preserved even in the final
layers, iii) despite a large overlap in their vocabulary, the MSA-based models
fail to capture the nuances of Arabic dialects, iv) we found that neurons in
embedding layers are polysemous in nature, while the neurons in middle layers
are exclusive to specific properties."
833,"Our progress on this has been limited to simple questions of fact, and
more complex reasoning remains open for further study (see example dialogs 15)).","Fine-tuning can improve output groundedness, but the model can still generate responses that do not accurately reﬂect
the contents of authoritative external sources.","Similarly, while the model generates
responses that make sense most of the time, it can still suffer from subtler quality issues.",2022-01-20 15:44:37+00:00,LaMDA: Language Models for Dialog Applications,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Romal Thoppilan'), arxiv.Result.Author('Daniel De Freitas'), arxiv.Result.Author('Jamie Hall'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('Apoorv Kulshreshtha'), arxiv.Result.Author('Heng-Tze Cheng'), arxiv.Result.Author('Alicia Jin'), arxiv.Result.Author('Taylor Bos'), arxiv.Result.Author('Leslie Baker'), arxiv.Result.Author('Yu Du'), arxiv.Result.Author('YaGuang Li'), arxiv.Result.Author('Hongrae Lee'), arxiv.Result.Author('Huaixiu Steven Zheng'), arxiv.Result.Author('Amin Ghafouri'), arxiv.Result.Author('Marcelo Menegali'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Maxim Krikun'), arxiv.Result.Author('Dmitry Lepikhin'), arxiv.Result.Author('James Qin'), arxiv.Result.Author('Dehao Chen'), arxiv.Result.Author('Yuanzhong Xu'), arxiv.Result.Author('Zhifeng Chen'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Yanqi Zhou'), arxiv.Result.Author('Chung-Ching Chang'), arxiv.Result.Author('Igor Krivokon'), arxiv.Result.Author('Will Rusch'), arxiv.Result.Author('Marc Pickett'), arxiv.Result.Author('Kathleen Meier-Hellstern'), arxiv.Result.Author('Meredith Ringel Morris'), arxiv.Result.Author('Tulsee Doshi'), arxiv.Result.Author('Renelito Delos Santos'), arxiv.Result.Author('Toju Duke'), arxiv.Result.Author('Johnny Soraker'), arxiv.Result.Author('Ben Zevenbergen'), arxiv.Result.Author('Vinodkumar Prabhakaran'), arxiv.Result.Author('Mark Diaz'), arxiv.Result.Author('Ben Hutchinson'), arxiv.Result.Author('Kristen Olson'), arxiv.Result.Author('Alejandra Molina'), arxiv.Result.Author('Erin Hoffman-John'), arxiv.Result.Author('Josh Lee'), arxiv.Result.Author('Lora Aroyo'), arxiv.Result.Author('Ravi Rajakumar'), arxiv.Result.Author('Alena Butryna'), arxiv.Result.Author('Matthew Lamm'), arxiv.Result.Author('Viktoriya Kuzmina'), arxiv.Result.Author('Joe Fenton'), arxiv.Result.Author('Aaron Cohen'), arxiv.Result.Author('Rachel Bernstein'), arxiv.Result.Author('Ray Kurzweil'), arxiv.Result.Author('Blaise Aguera-Arcas'), arxiv.Result.Author('Claire Cui'), arxiv.Result.Author('Marian Croak'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Quoc Le')]","We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency."
834,We hope that this work encourages further research in this area.,"LaMDA is a step closer to practical and safe open-ended dialog systems, which can in turn unlock a wide range of
useful applications.","Acknowledgements

We thank Javier Alberca, Thushan Amarasiriwardena, Martin Baeuml, Jonas Bragagnolo, Bill Byrne, Eli Collins,
Andrew Dai, Dipanjan Das, Jeff Dean, Rajat Dewan, Doug Eck, Noah Fiedel, Christian Frueh, Harish Ganapathy,
Saravanan Ganesh, Kourosh Gharachorloo, Zoubin Ghahramani, Daphne Ippolito, Thomas Jurdi, Ashwin Kakarla,
Nand Kishore, Karthik Krishnamoorthi, Vivek Kwatra, Katherine Lee, Max Lee, David Luan, Daphne Luong, Laichee
Man, Muqthar Mohammad, Erica Moreira, Maysam Moussalem, Tyler Mullen, Alexander Passos, Fernando Pereira,
Slav Petrov, Roberto Pieraccini, Christian Plagemann, Sahitya Potluri, Andy Pratt, RJ Skerry-Ryan, Grigori Somin,
Pranesh Srinivasan, Amarnag Subramanya, Mustafa Suleyman, Song Wang, Chris Wassman, Denny Zhou, and Hao
Zhou for their help with the paper and the project.",2022-01-20 15:44:37+00:00,LaMDA: Language Models for Dialog Applications,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Romal Thoppilan'), arxiv.Result.Author('Daniel De Freitas'), arxiv.Result.Author('Jamie Hall'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('Apoorv Kulshreshtha'), arxiv.Result.Author('Heng-Tze Cheng'), arxiv.Result.Author('Alicia Jin'), arxiv.Result.Author('Taylor Bos'), arxiv.Result.Author('Leslie Baker'), arxiv.Result.Author('Yu Du'), arxiv.Result.Author('YaGuang Li'), arxiv.Result.Author('Hongrae Lee'), arxiv.Result.Author('Huaixiu Steven Zheng'), arxiv.Result.Author('Amin Ghafouri'), arxiv.Result.Author('Marcelo Menegali'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Maxim Krikun'), arxiv.Result.Author('Dmitry Lepikhin'), arxiv.Result.Author('James Qin'), arxiv.Result.Author('Dehao Chen'), arxiv.Result.Author('Yuanzhong Xu'), arxiv.Result.Author('Zhifeng Chen'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Yanqi Zhou'), arxiv.Result.Author('Chung-Ching Chang'), arxiv.Result.Author('Igor Krivokon'), arxiv.Result.Author('Will Rusch'), arxiv.Result.Author('Marc Pickett'), arxiv.Result.Author('Kathleen Meier-Hellstern'), arxiv.Result.Author('Meredith Ringel Morris'), arxiv.Result.Author('Tulsee Doshi'), arxiv.Result.Author('Renelito Delos Santos'), arxiv.Result.Author('Toju Duke'), arxiv.Result.Author('Johnny Soraker'), arxiv.Result.Author('Ben Zevenbergen'), arxiv.Result.Author('Vinodkumar Prabhakaran'), arxiv.Result.Author('Mark Diaz'), arxiv.Result.Author('Ben Hutchinson'), arxiv.Result.Author('Kristen Olson'), arxiv.Result.Author('Alejandra Molina'), arxiv.Result.Author('Erin Hoffman-John'), arxiv.Result.Author('Josh Lee'), arxiv.Result.Author('Lora Aroyo'), arxiv.Result.Author('Ravi Rajakumar'), arxiv.Result.Author('Alena Butryna'), arxiv.Result.Author('Matthew Lamm'), arxiv.Result.Author('Viktoriya Kuzmina'), arxiv.Result.Author('Joe Fenton'), arxiv.Result.Author('Aaron Cohen'), arxiv.Result.Author('Rachel Bernstein'), arxiv.Result.Author('Ray Kurzweil'), arxiv.Result.Author('Blaise Aguera-Arcas'), arxiv.Result.Author('Claire Cui'), arxiv.Result.Author('Marian Croak'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Quoc Le')]","We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency."
835,"Our progress on this has been limited to simple questions of fact, and
more complex reasoning remains open for further study (see example dialogs 15)).","Fine-tuning can improve output groundedness, but the model can still generate responses that do not accurately reﬂect
the contents of authoritative external sources.","Similarly, while the model generates
responses that make sense most of the time, it can still suffer from subtler quality issues.",2022-01-20 15:44:37+00:00,LaMDA: Language Models for Dialog Applications,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Romal Thoppilan'), arxiv.Result.Author('Daniel De Freitas'), arxiv.Result.Author('Jamie Hall'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('Apoorv Kulshreshtha'), arxiv.Result.Author('Heng-Tze Cheng'), arxiv.Result.Author('Alicia Jin'), arxiv.Result.Author('Taylor Bos'), arxiv.Result.Author('Leslie Baker'), arxiv.Result.Author('Yu Du'), arxiv.Result.Author('YaGuang Li'), arxiv.Result.Author('Hongrae Lee'), arxiv.Result.Author('Huaixiu Steven Zheng'), arxiv.Result.Author('Amin Ghafouri'), arxiv.Result.Author('Marcelo Menegali'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Maxim Krikun'), arxiv.Result.Author('Dmitry Lepikhin'), arxiv.Result.Author('James Qin'), arxiv.Result.Author('Dehao Chen'), arxiv.Result.Author('Yuanzhong Xu'), arxiv.Result.Author('Zhifeng Chen'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Yanqi Zhou'), arxiv.Result.Author('Chung-Ching Chang'), arxiv.Result.Author('Igor Krivokon'), arxiv.Result.Author('Will Rusch'), arxiv.Result.Author('Marc Pickett'), arxiv.Result.Author('Kathleen Meier-Hellstern'), arxiv.Result.Author('Meredith Ringel Morris'), arxiv.Result.Author('Tulsee Doshi'), arxiv.Result.Author('Renelito Delos Santos'), arxiv.Result.Author('Toju Duke'), arxiv.Result.Author('Johnny Soraker'), arxiv.Result.Author('Ben Zevenbergen'), arxiv.Result.Author('Vinodkumar Prabhakaran'), arxiv.Result.Author('Mark Diaz'), arxiv.Result.Author('Ben Hutchinson'), arxiv.Result.Author('Kristen Olson'), arxiv.Result.Author('Alejandra Molina'), arxiv.Result.Author('Erin Hoffman-John'), arxiv.Result.Author('Josh Lee'), arxiv.Result.Author('Lora Aroyo'), arxiv.Result.Author('Ravi Rajakumar'), arxiv.Result.Author('Alena Butryna'), arxiv.Result.Author('Matthew Lamm'), arxiv.Result.Author('Viktoriya Kuzmina'), arxiv.Result.Author('Joe Fenton'), arxiv.Result.Author('Aaron Cohen'), arxiv.Result.Author('Rachel Bernstein'), arxiv.Result.Author('Ray Kurzweil'), arxiv.Result.Author('Blaise Aguera-Arcas'), arxiv.Result.Author('Claire Cui'), arxiv.Result.Author('Marian Croak'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Quoc Le')]","We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency."
836,We hope that this work encourages further research in this area.,"LaMDA is a step closer to practical and safe open-ended dialog systems, which can in turn unlock a wide range of
useful applications.","Acknowledgements

We thank Javier Alberca, Thushan Amarasiriwardena, Martin Baeuml, Jonas Bragagnolo, Bill Byrne, Eli Collins,
Andrew Dai, Dipanjan Das, Jeff Dean, Rajat Dewan, Doug Eck, Noah Fiedel, Christian Frueh, Harish Ganapathy,
Saravanan Ganesh, Kourosh Gharachorloo, Zoubin Ghahramani, Sissie Hsiao, Daphne Ippolito, Thomas Jurdi, Ashwin
Kakarla, Nand Kishore, Karthik Krishnamoorthi, Vivek Kwatra, Katherine Lee, Max Lee, David Luan, Daphne Luong,
Laichee Man, Jianchang (JC) Mao, Yossi Matias, Muqthar Mohammad, Erica Moreira, Maysam Moussalem, Tyler
Mullen, Eric Ni, Alexander Passos, Fernando Pereira, Slav Petrov, Roberto Pieraccini, Christian Plagemann, Sahitya
Potluri, Andy Pratt, RJ Skerry-Ryan, Grigori Somin, Pranesh Srinivasan, Amarnag Subramanya, Mustafa Suleyman,
Song Wang, Chris Wassman, Denny Zhou, and Hao Zhou for their help with the paper and the project.",2022-01-20 15:44:37+00:00,LaMDA: Language Models for Dialog Applications,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Romal Thoppilan'), arxiv.Result.Author('Daniel De Freitas'), arxiv.Result.Author('Jamie Hall'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('Apoorv Kulshreshtha'), arxiv.Result.Author('Heng-Tze Cheng'), arxiv.Result.Author('Alicia Jin'), arxiv.Result.Author('Taylor Bos'), arxiv.Result.Author('Leslie Baker'), arxiv.Result.Author('Yu Du'), arxiv.Result.Author('YaGuang Li'), arxiv.Result.Author('Hongrae Lee'), arxiv.Result.Author('Huaixiu Steven Zheng'), arxiv.Result.Author('Amin Ghafouri'), arxiv.Result.Author('Marcelo Menegali'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Maxim Krikun'), arxiv.Result.Author('Dmitry Lepikhin'), arxiv.Result.Author('James Qin'), arxiv.Result.Author('Dehao Chen'), arxiv.Result.Author('Yuanzhong Xu'), arxiv.Result.Author('Zhifeng Chen'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Yanqi Zhou'), arxiv.Result.Author('Chung-Ching Chang'), arxiv.Result.Author('Igor Krivokon'), arxiv.Result.Author('Will Rusch'), arxiv.Result.Author('Marc Pickett'), arxiv.Result.Author('Kathleen Meier-Hellstern'), arxiv.Result.Author('Meredith Ringel Morris'), arxiv.Result.Author('Tulsee Doshi'), arxiv.Result.Author('Renelito Delos Santos'), arxiv.Result.Author('Toju Duke'), arxiv.Result.Author('Johnny Soraker'), arxiv.Result.Author('Ben Zevenbergen'), arxiv.Result.Author('Vinodkumar Prabhakaran'), arxiv.Result.Author('Mark Diaz'), arxiv.Result.Author('Ben Hutchinson'), arxiv.Result.Author('Kristen Olson'), arxiv.Result.Author('Alejandra Molina'), arxiv.Result.Author('Erin Hoffman-John'), arxiv.Result.Author('Josh Lee'), arxiv.Result.Author('Lora Aroyo'), arxiv.Result.Author('Ravi Rajakumar'), arxiv.Result.Author('Alena Butryna'), arxiv.Result.Author('Matthew Lamm'), arxiv.Result.Author('Viktoriya Kuzmina'), arxiv.Result.Author('Joe Fenton'), arxiv.Result.Author('Aaron Cohen'), arxiv.Result.Author('Rachel Bernstein'), arxiv.Result.Author('Ray Kurzweil'), arxiv.Result.Author('Blaise Aguera-Arcas'), arxiv.Result.Author('Claire Cui'), arxiv.Result.Author('Marian Croak'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Quoc Le')]","We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency."
837,"Our progress on this has been limited to simple questions of fact, and
more complex reasoning remains open for further study (see example dialogs 15)).","Fine-tuning can improve output groundedness, but the model can still generate responses that do not accurately reﬂect
the contents of authoritative external sources.","Similarly, while the model generates
responses that make sense most of the time, it can still suffer from subtler quality issues.",2022-01-20 15:44:37+00:00,LaMDA: Language Models for Dialog Applications,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Romal Thoppilan'), arxiv.Result.Author('Daniel De Freitas'), arxiv.Result.Author('Jamie Hall'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('Apoorv Kulshreshtha'), arxiv.Result.Author('Heng-Tze Cheng'), arxiv.Result.Author('Alicia Jin'), arxiv.Result.Author('Taylor Bos'), arxiv.Result.Author('Leslie Baker'), arxiv.Result.Author('Yu Du'), arxiv.Result.Author('YaGuang Li'), arxiv.Result.Author('Hongrae Lee'), arxiv.Result.Author('Huaixiu Steven Zheng'), arxiv.Result.Author('Amin Ghafouri'), arxiv.Result.Author('Marcelo Menegali'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Maxim Krikun'), arxiv.Result.Author('Dmitry Lepikhin'), arxiv.Result.Author('James Qin'), arxiv.Result.Author('Dehao Chen'), arxiv.Result.Author('Yuanzhong Xu'), arxiv.Result.Author('Zhifeng Chen'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Vincent Zhao'), arxiv.Result.Author('Yanqi Zhou'), arxiv.Result.Author('Chung-Ching Chang'), arxiv.Result.Author('Igor Krivokon'), arxiv.Result.Author('Will Rusch'), arxiv.Result.Author('Marc Pickett'), arxiv.Result.Author('Pranesh Srinivasan'), arxiv.Result.Author('Laichee Man'), arxiv.Result.Author('Kathleen Meier-Hellstern'), arxiv.Result.Author('Meredith Ringel Morris'), arxiv.Result.Author('Tulsee Doshi'), arxiv.Result.Author('Renelito Delos Santos'), arxiv.Result.Author('Toju Duke'), arxiv.Result.Author('Johnny Soraker'), arxiv.Result.Author('Ben Zevenbergen'), arxiv.Result.Author('Vinodkumar Prabhakaran'), arxiv.Result.Author('Mark Diaz'), arxiv.Result.Author('Ben Hutchinson'), arxiv.Result.Author('Kristen Olson'), arxiv.Result.Author('Alejandra Molina'), arxiv.Result.Author('Erin Hoffman-John'), arxiv.Result.Author('Josh Lee'), arxiv.Result.Author('Lora Aroyo'), arxiv.Result.Author('Ravi Rajakumar'), arxiv.Result.Author('Alena Butryna'), arxiv.Result.Author('Matthew Lamm'), arxiv.Result.Author('Viktoriya Kuzmina'), arxiv.Result.Author('Joe Fenton'), arxiv.Result.Author('Aaron Cohen'), arxiv.Result.Author('Rachel Bernstein'), arxiv.Result.Author('Ray Kurzweil'), arxiv.Result.Author('Blaise Aguera-Arcas'), arxiv.Result.Author('Claire Cui'), arxiv.Result.Author('Marian Croak'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Quoc Le')]","We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency."
838,We hope that this work encourages further research in this area.,"LaMDA is a step closer to practical and safe open-ended dialog systems, which can in turn unlock a wide range of
useful applications.","Acknowledgements

We thank Javier Alberca, Thushan Amarasiriwardena, Martin Baeuml, Jonas Bragagnolo, Bill Byrne, Eli Collins,
Andrew Dai, Dipanjan Das, Jeff Dean, Rajat Dewan, Doug Eck, Noah Fiedel, Christian Frueh, Harish Ganapathy,
Saravanan Ganesh, Kourosh Gharachorloo, Zoubin Ghahramani, Sissie Hsiao, Daphne Ippolito, Thomas Jurdi, Ashwin
Kakarla, Nand Kishore, Karthik Krishnamoorthi, Vivek Kwatra, Katherine Lee, Max Lee, David Luan, Daphne Luong,
Laichee Man, Jianchang (JC) Mao, Yossi Matias, Muqthar Mohammad, Erica Moreira, Maysam Moussalem, Tyler
Mullen, Eric Ni, Alexander Passos, Fernando Pereira, Slav Petrov, Roberto Pieraccini, Christian Plagemann, Sahitya
Potluri, Andy Pratt, RJ Skerry-Ryan, Grigori Somin, Pranesh Srinivasan, Amarnag Subramanya, Mustafa Suleyman,
Song Wang, Chris Wassman, Denny Zhou, and Hao Zhou for their help with the paper and the project.",2022-01-20 15:44:37+00:00,LaMDA: Language Models for Dialog Applications,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Romal Thoppilan'), arxiv.Result.Author('Daniel De Freitas'), arxiv.Result.Author('Jamie Hall'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('Apoorv Kulshreshtha'), arxiv.Result.Author('Heng-Tze Cheng'), arxiv.Result.Author('Alicia Jin'), arxiv.Result.Author('Taylor Bos'), arxiv.Result.Author('Leslie Baker'), arxiv.Result.Author('Yu Du'), arxiv.Result.Author('YaGuang Li'), arxiv.Result.Author('Hongrae Lee'), arxiv.Result.Author('Huaixiu Steven Zheng'), arxiv.Result.Author('Amin Ghafouri'), arxiv.Result.Author('Marcelo Menegali'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Maxim Krikun'), arxiv.Result.Author('Dmitry Lepikhin'), arxiv.Result.Author('James Qin'), arxiv.Result.Author('Dehao Chen'), arxiv.Result.Author('Yuanzhong Xu'), arxiv.Result.Author('Zhifeng Chen'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Vincent Zhao'), arxiv.Result.Author('Yanqi Zhou'), arxiv.Result.Author('Chung-Ching Chang'), arxiv.Result.Author('Igor Krivokon'), arxiv.Result.Author('Will Rusch'), arxiv.Result.Author('Marc Pickett'), arxiv.Result.Author('Pranesh Srinivasan'), arxiv.Result.Author('Laichee Man'), arxiv.Result.Author('Kathleen Meier-Hellstern'), arxiv.Result.Author('Meredith Ringel Morris'), arxiv.Result.Author('Tulsee Doshi'), arxiv.Result.Author('Renelito Delos Santos'), arxiv.Result.Author('Toju Duke'), arxiv.Result.Author('Johnny Soraker'), arxiv.Result.Author('Ben Zevenbergen'), arxiv.Result.Author('Vinodkumar Prabhakaran'), arxiv.Result.Author('Mark Diaz'), arxiv.Result.Author('Ben Hutchinson'), arxiv.Result.Author('Kristen Olson'), arxiv.Result.Author('Alejandra Molina'), arxiv.Result.Author('Erin Hoffman-John'), arxiv.Result.Author('Josh Lee'), arxiv.Result.Author('Lora Aroyo'), arxiv.Result.Author('Ravi Rajakumar'), arxiv.Result.Author('Alena Butryna'), arxiv.Result.Author('Matthew Lamm'), arxiv.Result.Author('Viktoriya Kuzmina'), arxiv.Result.Author('Joe Fenton'), arxiv.Result.Author('Aaron Cohen'), arxiv.Result.Author('Rachel Bernstein'), arxiv.Result.Author('Ray Kurzweil'), arxiv.Result.Author('Blaise Aguera-Arcas'), arxiv.Result.Author('Claire Cui'), arxiv.Result.Author('Marian Croak'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Quoc Le')]","We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency."
840,"(2020) analysed Igbo emotion words
      foster further research in the NLP community.",Umoh et al.,using Interval Type-2 Fuzzy Logic.,2022-01-20 16:28:06+00:00,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Shamsuddeen Hassan Muhammad'), arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Ibrahim Said Ahmad'), arxiv.Result.Author('Idris Abdulmumin'), arxiv.Result.Author('Bello Shehu Bello'), arxiv.Result.Author('Monojit Choudhury'), arxiv.Result.Author('Chris Chinenye Emezue'), arxiv.Result.Author('Anuoluwapo Aremu'), arxiv.Result.Author('Saheed Abdul'), arxiv.Result.Author('Pavel Brazdil')]","Sentiment analysis is one of the most widely studied applications in NLP, but
most work focuses on languages with large amounts of data. We introduce the
first large-scale human-annotated Twitter sentiment dataset for the four most
widely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and Yoruba)
consisting of around 30,000 annotated tweets per language (except for
Nigerian-Pidgin), including a significant fraction of code-mixed tweets. We
propose text collection, filtering, processing, and labelling methods that
enable us to create datasets for these low-resource languages. We evaluate a
range of pre-trained models and transfer strategies on the dataset. We find
that language-specific models and language-adaptive fine-tuning generally
perform best. We release the datasets, trained models, sentiment lexicons, and
code to incentivize research on sentiment analysis in under-represented
languages."
841,"Igbo Ogbuju and Onyesolu (2019) translated an En-
                                                           glish sentiment lexicon (Hu and Liu, 2004) and man-
 [5] We make the datasets and code freely available to     ually added Igbo native words to create IgboSen-
      foster further research in the NLP community.","sentiment analysis in Hausa, Igbo, Yorùbá, and
      Pidgin languages.",tiLex.,2022-01-20 16:28:06+00:00,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Shamsuddeen Hassan Muhammad'), arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Sebastian Ruder'), arxiv.Result.Author('Ibrahim Said Ahmad'), arxiv.Result.Author('Idris Abdulmumin'), arxiv.Result.Author('Bello Shehu Bello'), arxiv.Result.Author('Monojit Choudhury'), arxiv.Result.Author('Chris Chinenye Emezue'), arxiv.Result.Author('Saheed Salahudeen Abdullahi'), arxiv.Result.Author('Anuoluwapo Aremu'), arxiv.Result.Author('Alipio Jeorge'), arxiv.Result.Author('Pavel Brazdil')]","Sentiment analysis is one of the most widely studied applications in NLP, but
most work focuses on languages with large amounts of data. We introduce the
first large-scale human-annotated Twitter sentiment dataset for the four most
widely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and
Yor\`ub\'a ) consisting of around 30,000 annotated tweets per language (and
14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed
tweets. We propose text collection, filtering, processing and labeling methods
that enable us to create datasets for these low-resource languages. We evaluate
a rangeof pre-trained models and transfer strategies on the dataset. We find
that language-specific models and language-adaptivefine-tuning generally
perform best. We release the datasets, trained models, sentiment lexicons, and
code to incentivizeresearch on sentiment analysis in under-represented
languages."
842,"(2020) analysed Igbo emotion words
      foster further research in the NLP community.",Umoh et al.,using Interval Type-2 Fuzzy Logic.,2022-01-20 16:28:06+00:00,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Shamsuddeen Hassan Muhammad'), arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Sebastian Ruder'), arxiv.Result.Author('Ibrahim Said Ahmad'), arxiv.Result.Author('Idris Abdulmumin'), arxiv.Result.Author('Bello Shehu Bello'), arxiv.Result.Author('Monojit Choudhury'), arxiv.Result.Author('Chris Chinenye Emezue'), arxiv.Result.Author('Saheed Salahudeen Abdullahi'), arxiv.Result.Author('Anuoluwapo Aremu'), arxiv.Result.Author('Alipio Jeorge'), arxiv.Result.Author('Pavel Brazdil')]","Sentiment analysis is one of the most widely studied applications in NLP, but
most work focuses on languages with large amounts of data. We introduce the
first large-scale human-annotated Twitter sentiment dataset for the four most
widely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and
Yor\`ub\'a ) consisting of around 30,000 annotated tweets per language (and
14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed
tweets. We propose text collection, filtering, processing and labeling methods
that enable us to create datasets for these low-resource languages. We evaluate
a rangeof pre-trained models and transfer strategies on the dataset. We find
that language-specific models and language-adaptivefine-tuning generally
perform best. We release the datasets, trained models, sentiment lexicons, and
code to incentivizeresearch on sentiment analysis in under-represented
languages."
911,"In
   We leave a lot of questions for further research—here we           Proc.",dimensional vector spaces and weighted register automata.,"LICS, pages 1–13.",2022-01-22 14:35:30+00:00,Solvability of orbit-finite systems of linear equations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Arka Ghosh'), arxiv.Result.Author('Piotr Hofman'), arxiv.Result.Author('Sławomir Lasota')]","We study orbit-finite systems of linear equations, in the setting of sets
with atoms. Our principal contribution is a decision procedure for solvability
of such systems. The procedure works for every field (and even commutative
ring) under mild effectiveness assumptions, and reduces a given orbit-finite
system to a number of finite ones: exponentially many in general, but
polynomially many when atom dimension of input systems is fixed. Towards
obtaining the procedure we push further the theory of vector spaces generated
by orbit-finite sets, and show that each such vector space admits an
orbit-finite basis. This fundamental property is a key tool in our development,
but should be also of wider interest."
912,"We leave a lot of questions for further research—here we
Proof.",S (A) = F S (Ã).,W.l.o.g.,2022-01-22 14:35:30+00:00,Solvability of orbit-finite systems of linear equations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Arka Ghosh'), arxiv.Result.Author('Piotr Hofman'), arxiv.Result.Author('Sławomir Lasota')]","We study orbit-finite systems of linear equations, in the setting of sets
with atoms. Our principal contribution is a decision procedure for solvability
of such systems. The procedure works for every field (and even commutative
ring) under mild effectiveness assumptions, and reduces a given orbit-finite
system to a number of finite ones: exponentially many in general, but
polynomially many when atom dimension of input systems is fixed. Towards
obtaining the procedure we push further the theory of vector spaces generated
by orbit-finite sets, and show that each such vector space admits an
orbit-finite basis. This fundamental property is a key tool in our development,
but should be also of wider interest."
922,"These works highlight the limitations of
existing metrics and oﬀer various resources for conducting further research on
the evaluation task.","Using these character-
istics, recent works have attempted to quantify and compare the performance
of existing evaluation metrics [1,10].","One such work is the SummEval dataset [10] that provides
human annotation scores for - coherence, consistency, ﬂuency and relevance.",2022-01-23 14:40:42+00:00,WIDAR -- Weighted Input Document Augmented ROUGE,cs.CL,['cs.CL'],"[arxiv.Result.Author('Raghav Jain'), arxiv.Result.Author('Vaibhav Mavi'), arxiv.Result.Author('Anubhav Jangra'), arxiv.Result.Author('Sriparna Saha')]","The task of automatic text summarization has gained a lot of traction due to
the recent advancements in machine learning techniques. However, evaluating the
quality of a generated summary remains to be an open problem. The literature
has widely adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as
the standard evaluation metric for summarization. However, ROUGE has some
long-established limitations; a major one being its dependence on the
availability of good quality reference summary. In this work, we propose the
metric WIDAR which in addition to utilizing the reference summary uses also the
input document in order to evaluate the quality of the generated summary. The
proposed metric is versatile, since it is designed to adapt the evaluation
score according to the quality of the reference summary. The proposed metric
correlates better than ROUGE by 26%, 76%, 82%, and 15%, respectively, in
coherence, consistency, fluency, and relevance on human judgement scores
provided in the SummEval dataset. The proposed metric is able to obtain
comparable results with other state-of-the-art metrics while requiring a
relatively short computational time."
974,"Therefore, we encourage further research on two research
Large-scale text search based on dense embeddings and                           agendas: (a) developing robust evaluation methodologies
neural information retrieval (neural IR) have the poten-                        for multiple classes of bias in training data and pre-trained
tial to generalize better than keyword matching in classic                      models, and (b) developing and improving methods for
IR systems.",perform notably worse than supervised sentence encoders.,"Neural IR systems encode documents at the                           mitigating encoded bias, including ﬁne-tuning to reduce
indexing stage and then perform nearest neighbor search                         bias in pre-trained models (Caliskan et al., 2017; May et al.,
(Johnson et al., 2019) at query time (Lin et al., 2021).",2022-01-24 23:36:20+00:00,Text and Code Embeddings by Contrastive Pre-Training,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Arvind Neelakantan'), arxiv.Result.Author('Tao Xu'), arxiv.Result.Author('Raul Puri'), arxiv.Result.Author('Alec Radford'), arxiv.Result.Author('Jesse Michael Han'), arxiv.Result.Author('Jerry Tworek'), arxiv.Result.Author('Qiming Yuan'), arxiv.Result.Author('Nikolas Tezak'), arxiv.Result.Author('Jong Wook Kim'), arxiv.Result.Author('Chris Hallacy'), arxiv.Result.Author('Johannes Heidecke'), arxiv.Result.Author('Pranav Shyam'), arxiv.Result.Author('Boris Power'), arxiv.Result.Author('Tyna Eloundou Nekoul'), arxiv.Result.Author('Girish Sastry'), arxiv.Result.Author('Gretchen Krueger'), arxiv.Result.Author('David Schnurr'), arxiv.Result.Author('Felipe Petroski Such'), arxiv.Result.Author('Kenny Hsu'), arxiv.Result.Author('Madeleine Thompson'), arxiv.Result.Author('Tabarak Khan'), arxiv.Result.Author('Toki Sherbakov'), arxiv.Result.Author('Joanne Jang'), arxiv.Result.Author('Peter Welinder'), arxiv.Result.Author('Lilian Weng')]","Text embeddings are useful features in many applications such as semantic
search and computing text similarity. Previous work typically trains models
customized for different use cases, varying in dataset choice, training
objective and model architecture. In this work, we show that contrastive
pre-training on unsupervised data at scale leads to high quality vector
representations of text and code. The same unsupervised text embeddings that
achieve new state-of-the-art results in linear-probe classification also
display impressive semantic search capabilities and sometimes even perform
competitively with fine-tuned models. On linear-probe classification accuracy
averaging over 7 tasks, our best unsupervised model achieves a relative
improvement of 4% and 1.8% over previous best unsupervised and supervised text
embedding models respectively. The same text embeddings when evaluated on
large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and
10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and
TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code
embedding models on (text, code) pairs, obtaining a 20.8% relative improvement
over prior best work on code search."
975,"We encourage further research          ian, M., Winter, C., Tillet, P., Such, F. P., Cummings,
and implementation efforts in these areas.","For example, safe public         Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
access to large pre-trained language models, and efﬁcient          M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,
training pipelines that leverage improved model architec-          S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-
tures and training schemes.","D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss,
                                                                   A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,
6.",2022-01-24 23:36:20+00:00,Text and Code Embeddings by Contrastive Pre-Training,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Arvind Neelakantan'), arxiv.Result.Author('Tao Xu'), arxiv.Result.Author('Raul Puri'), arxiv.Result.Author('Alec Radford'), arxiv.Result.Author('Jesse Michael Han'), arxiv.Result.Author('Jerry Tworek'), arxiv.Result.Author('Qiming Yuan'), arxiv.Result.Author('Nikolas Tezak'), arxiv.Result.Author('Jong Wook Kim'), arxiv.Result.Author('Chris Hallacy'), arxiv.Result.Author('Johannes Heidecke'), arxiv.Result.Author('Pranav Shyam'), arxiv.Result.Author('Boris Power'), arxiv.Result.Author('Tyna Eloundou Nekoul'), arxiv.Result.Author('Girish Sastry'), arxiv.Result.Author('Gretchen Krueger'), arxiv.Result.Author('David Schnurr'), arxiv.Result.Author('Felipe Petroski Such'), arxiv.Result.Author('Kenny Hsu'), arxiv.Result.Author('Madeleine Thompson'), arxiv.Result.Author('Tabarak Khan'), arxiv.Result.Author('Toki Sherbakov'), arxiv.Result.Author('Joanne Jang'), arxiv.Result.Author('Peter Welinder'), arxiv.Result.Author('Lilian Weng')]","Text embeddings are useful features in many applications such as semantic
search and computing text similarity. Previous work typically trains models
customized for different use cases, varying in dataset choice, training
objective and model architecture. In this work, we show that contrastive
pre-training on unsupervised data at scale leads to high quality vector
representations of text and code. The same unsupervised text embeddings that
achieve new state-of-the-art results in linear-probe classification also
display impressive semantic search capabilities and sometimes even perform
competitively with fine-tuned models. On linear-probe classification accuracy
averaging over 7 tasks, our best unsupervised model achieves a relative
improvement of 4% and 1.8% over previous best unsupervised and supervised text
embedding models respectively. The same text embeddings when evaluated on
large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and
10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and
TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code
embedding models on (text, code) pairs, obtaining a 20.8% relative improvement
over prior best work on code search."
1017,"This review 24 studies investigating the
                                         feasibility of social media usage for suicidal ideation detection is intended to facilitate further research in
                                         the ﬁeld and will be a beneﬁcial resource for researchers engaged in suicidal text classiﬁcation.","This paper presents a comprehensive summary of current research efforts to detect suicidal
                                         ideation using machine learning algorithms on social media.","INTRODUCTION

                                        Millions of individuals regularly use social media such as chat rooms, blogging websites, and social
                                        networking platforms, with 3.96 billion people actively utilizing the internet [1].",2022-01-25 18:23:47+00:00,Suicidal Ideation Detection on Social Media: A Review of Machine Learning Methods,cs.CL,['cs.CL'],"[arxiv.Result.Author('Asma Abdulsalam'), arxiv.Result.Author('Areej Alhothali')]","Social media platforms have transformed traditional communication methods by
allowing users worldwide to communicate instantly, openly, and frequently.
People use social media to express their opinion and share their personal
stories and struggles. Negative feelings that express hardship, thoughts of
death, and self-harm are widespread in social media, especially among young
generations. Therefore, using social media to detect and identify suicidal
ideation will help provide proper intervention that will eventually dissuade
others from self-harming and committing suicide and prevent the spread of
suicidal ideations on social media. Many studies have been carried out to
identify suicidal ideation and behaviors in social media. This paper presents a
comprehensive summary of current research efforts to detect suicidal ideation
using machine learning algorithms on social media. This review 24 studies
investigating the feasibility of social media usage for suicidal ideation
detection is intended to facilitate further research in the field and will be a
beneficial resource for researchers engaged in suicidal text classification."
1046,"As such, it enables further research, de-
                                                            velopment and application of ASR not only for Nor-
Table 3: WER(%) per dialect area in Module 3 of NB          wegian, but also for many other languages affected
Tale for the three combinations of models tested on         by the phenomena we discussed here.",(%)  9.97 10.89 8.94 9.81                     Norwegian.,"Nevertheless,
these data, see Table 2.",2022-01-26 11:41:55+00:00,The Norwegian Parliamentary Speech Corpus,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Per Erik Solberg'), arxiv.Result.Author('Pablo Ortiz')]","The Norwegian Parliamentary Speech Corpus (NPSC) is a speech dataset with
recordings of meetings from Stortinget, the Norwegian parliament. It is the
first, publicly available dataset containing unscripted, Norwegian speech
designed for training of automatic speech recognition (ASR) systems. The
recordings are manually transcribed and annotated with language codes and
speakers, and there are detailed metadata about the speakers. The
transcriptions exist in both normalized and non-normalized form, and
non-standardized words are explicitly marked and annotated with standardized
equivalents. To test the usefulness of this dataset, we have compared an ASR
system trained on the NPSC with a baseline system trained on only
manuscript-read speech. These systems were tested on an independent dataset
containing spontaneous, dialectal speech. The NPSC-trained system performed
significantly better, with a 22.9% relative improvement in word error rate
(WER). Moreover, training on the NPSC is shown to have a ""democratizing"" effect
in terms of dialects, as improvements are generally larger for dialects with
higher WER from the baseline system."
1066,"We claim the results to be testifying to the via-
bility of the task in our setting and encouraging for further research on our
data.","In a No Score Evidence setting, where the gold evi-
dence requirement is removed, we scored 55.40% and 61.24% on CsFEVER
and CTKFacts, respectively.","7.1 Future work

   • The fact-checking pipeline is to also be augmented by the check-worthiness
      estimation [50], that is, a model that classiﬁes which sentences of given
      text in Czech are appropriate for the fact veriﬁcation.",2022-01-26 18:48:42+00:00,CsFEVER and CTKFacts: Czech Datasets for Fact Verification,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jan Drchal'), arxiv.Result.Author('Herbert Ullrich'), arxiv.Result.Author('Martin Rýpar'), arxiv.Result.Author('Hana Vincourová'), arxiv.Result.Author('Václav Moravec')]","In this paper we present two Czech datasets aimed for training automated
fact-checking machine learning models. Specifically we deal with the task of
assessment of a textual claim veracity w.r.t. to a (presumably) verified
corpus. The output of the system is the claim classification SUPPORTS or
REFUTES complemented with evidence documents or NEI (Not Enough Info) alone. In
the first place we publish CsFEVER of approximately 112k claims which is an
automatically generated Czech version of the well-known Wikipedia-based FEVER
dataset. We took a hybrid approach of machine translation and language
alignment, where the same method (and tools we provide) can be easily applied
to other languages. The second dataset CTKFacts of 3,097 claims is built on the
corpus of approximately two million Czech News Agency news reports. We present
an extended methodology based on the FEVER approach. Most notably, we describe
a method to automatically generate wider claim contexts (dictionaries) for
non-hyperlinked corpora. The datasets are analyzed for spurious cues, which are
annotation patterns leading to model overfitting. CTKFacts is further examined
for inter-annotator agreement, and a typology of common annotator errors is
extracted. Finally, we provide baseline models for all stages of the
fact-checking pipeline."
1079,"To support further research, we also pro-
train data.","Although our North Korean
strings between the train and evaluation data leads to an   MT datasets have some limitations with regards to size
overestimation of the North Korean models, we eval-         and diversity of sentences, the ﬁndings of our study are
uate the models by deleting the sentences of dev and        useful for the development of a North Korean transla-
test that duplicate more than ten substrings with the       tion system.",Table 7 shows the BLEU scores obtained us-      vide the data and code used in our experiments.,2022-01-27 01:21:29+00:00,Learning How to Translate North Korean through South Korean,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hwichan Kim'), arxiv.Result.Author('Sangwhan Moon'), arxiv.Result.Author('Naoaki Okazaki'), arxiv.Result.Author('Mamoru Komachi')]","South and North Korea both use the Korean language. However, Korean NLP
research has focused on South Korean only, and existing NLP systems of the
Korean language, such as neural machine translation (NMT) models, cannot
properly handle North Korean inputs. Training a model using North Korean data
is the most straightforward approach to solving this problem, but there is
insufficient data to train NMT models. In this study, we create data for North
Korean NMT models using a comparable corpus. First, we manually create
evaluation data for automatic alignment and machine translation. Then, we
investigate automatic alignment methods suitable for North Korean. Finally, we
verify that a model trained by North Korean bilingual data without human
annotation can significantly boost North Korean translation accuracy compared
to existing South Korean models in zero-shot settings."
1089,"Another interesting ﬁnding is                   To further study how our methods make sense, we
that adding extra data for retrieval (100+600k,                study how the relevance of the retrieved evidences
300+400k, 500+200k) in our methods can outper-                 and ground-truth response can inﬂuence the gener-
form the baselines (700k) with extra data added via            ation performance.","We be-           4.5.5 Relevance of Evidence and
lieve larger retrieval sets can introduce more rel-                     Ground-truth
evant evidences, which brings performance gain
for the model.","For each instance (ci, ri) which
direct training.",2022-01-27 08:02:59+00:00,Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yihe Wang'), arxiv.Result.Author('Yitong Li'), arxiv.Result.Author('Yasheng Wang'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Pingyi Zhou'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Jin Liu'), arxiv.Result.Author('Qun Liu'), arxiv.Result.Author('Xin Jiang')]","Real human conversation data are complicated, heterogeneous, and noisy, from
whom building open-domain dialogue systems remains a challenging task. In fact,
such dialogue data can still contain a wealth of information and knowledge,
however, they are not fully explored. In this paper, we show existing
open-domain dialogue generation methods by memorizing context-response paired
data with causal or encode-decode language models underutilize the training
data. Different from current approaches, using external knowledge, we explore a
retrieval-generation training framework that can increase the usage of training
data by directly considering the heterogeneous and noisy training data as the
""evidence"". Experiments over publicly available datasets demonstrate that our
method can help models generate better responses, even such training data are
usually impressed as low-quality data. Such performance gain is comparable with
those improved by enlarging the training set, even better. We also found that
the model performance has a positive correlation with the relevance of the
retrieved evidence. Moreover, our method performed well on zero-shot
experiments, which indicates that our method can be more robust to real-world
data."
1090,"which indicates that open-domain dialogue gen-
                                                              eration is still a difﬁcult task, and better retrieval
Relevance of Evidence and Ground-truth To                     methods are required to further improve our gener-
further study how our methods make sense, we                  ation performance.","Also, there
rectly training these responses, and our method has           are low-relevant evidences left in the retrieval step,
good generalization over the retrieval evidences.","study how the relevance of the retrieved evidences
5 Conclusion                                                  Askell, Sandhini Agarwal, Ariel Herbert-Voss,
                                                              Gretchen Krueger, T. J. Henighan, Rewon Child,
In this paper, we propose a self-retrieval train-             Aditya Ramesh, Daniel M. Ziegler, Jeff Wu,
ing framework for open-domain dialogue gener-                 Clemens Winter, Christopher Hesse, Mark Chen,
ation.",2022-01-27 08:02:59+00:00,Pan More Gold from the Sand: Refining Open-domain Dialogue Training with Noisy Self-Retrieval Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yihe Wang'), arxiv.Result.Author('Yitong Li'), arxiv.Result.Author('Yasheng Wang'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Pingyi Zhou'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Jin Liu'), arxiv.Result.Author('Xin Jiang'), arxiv.Result.Author('Qun Liu')]","Real human conversation data are complicated, heterogeneous, and noisy, from
which building open-domain dialogue systems remains a challenging task. In
fact, such dialogue data still contains a wealth of information and knowledge,
however, they are not fully explored. In this paper, we show existing
open-domain dialogue generation methods that memorize context-response paired
data with autoregressive or encode-decode language models underutilize the
training data. Different from current approaches, using external knowledge, we
explore a retrieval-generation training framework that can take advantage of
the heterogeneous and noisy training data by considering them as ""evidence"". In
particular, we use BERTScore for retrieval, which gives better qualities of the
evidence and generation. Experiments over publicly available datasets
demonstrate that our method can help models generate better responses, even
such training data are usually impressed as low-quality data. Such performance
gain is comparable with those improved by enlarging the training set, even
better. We also found that the model performance has a positive correlation
with the relevance of the retrieved evidence. Moreover, our method performed
well on zero-shot experiments, which indicates that our method can be more
robust to real-world data."
1120,"Our findings inform future
design of saliency visualizations towards closing the gap between communicated and interpreted saliency explanations,
and call for further research in the human factors in interpretation methods of AI, that study not only how the AI
operates, but how humans perceive the communicated information.","We present two bias correction methods and demonstrate
their ability to compensate the distorting influence of word length and repeated exposure.","19We hypothesize that belief biases (such as sentiment polarity) exhibit more distinct expression across indiviuals, which requires subject-adaptive
correction methods and should be addressed by online estimation of individual participant slopes and intercepts within our GAMM model in future work.",2022-01-27 15:20:32+00:00,Human Interpretation of Saliency-based Explanation Over Text,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'cs.LG']","[arxiv.Result.Author('Hendrik Schuff'), arxiv.Result.Author('Alon Jacovi'), arxiv.Result.Author('Heike Adel'), arxiv.Result.Author('Yoav Goldberg'), arxiv.Result.Author('Ngoc Thang Vu')]","While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation."
1121,"visualizations towards closing the gap between communicated and
interpreted saliency explanations, and call for further research in                          [4] Nadia Burkart and Marco F. Huber.","Our findings inform future design of saliency                                  Library of Science San Francisco, CA USA.",2021.,2022-01-27 15:20:32+00:00,Human Interpretation of Saliency-based Explanation Over Text,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'cs.LG']","[arxiv.Result.Author('Hendrik Schuff'), arxiv.Result.Author('Alon Jacovi'), arxiv.Result.Author('Heike Adel'), arxiv.Result.Author('Yoav Goldberg'), arxiv.Result.Author('Ngoc Thang Vu')]","While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation."
1151,"Finally,
the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in
real-world applications; further research could explore how to induce reasoning in smaller models.","Third, there is no guarantee of correct reasoning paths, which can lead to both correct
and incorrect answers; improving factual generations of language models is an open direction for
future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia).","7 Related Work

This work is inspired by many research areas, which we detail in an extended related work section
(Appendix C).",2022-01-28 02:33:07+00:00,Chain of Thought Prompting Elicits Reasoning in Large Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Xuezhi Wang'), arxiv.Result.Author('Dale Schuurmans'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Brian Ichter'), arxiv.Result.Author('Fei Xia'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Quoc Le'), arxiv.Result.Author('Denny Zhou')]","We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier."
1171,"The
                                                                        further research of the residual unit will also be included in
Figure 5.","What we actually do is that using the hidden states
                                     S                                  sufﬁciently by the residual unit make classiﬁcation.",The accuracy of the validation set of RTE when the dif-       our plan.,2022-01-28 13:34:30+00:00,"Protum: A New Method For Prompt Tuning Based on ""[MASK]""",cs.CL,['cs.CL'],"[arxiv.Result.Author('Pan He'), arxiv.Result.Author('Yuxi Chen'), arxiv.Result.Author('Yan Wang'), arxiv.Result.Author('Yanru Zhang')]","Recently, prompt tuning \cite{lester2021power} has gradually become a new
paradigm for NLP, which only depends on the representation of the words by
freezing the parameters of pre-trained language models (PLMs) to obtain
remarkable performance on downstream tasks. It maintains the consistency of
Masked Language Model (MLM) \cite{devlin2018bert} task in the process of
pre-training, and avoids some issues that may happened during fine-tuning.
Naturally, we consider that the ""[MASK]"" tokens carry more useful information
than other tokens because the model combines with context to predict the masked
tokens. Among the current prompt tuning methods, there will be a serious
problem of random composition of the answer tokens in prediction when they
predict multiple words so that they have to map tokens to labels with the help
verbalizer. In response to the above issue, we propose a new \textbf{Pro}mpt
\textbf{Tu}ning based on ""[\textbf{M}ASK]"" (\textbf{Protum}) method in this
paper, which constructs a classification task through the information carried
by the hidden layer of ""[MASK]"" tokens and then predicts the labels directly
rather than the answer tokens. At the same time, we explore how different
hidden layers under ""[MASK]"" impact on our classification model on many
different data sets. Finally, we find that our \textbf{Protum} can achieve much
better performance than fine-tuning after continuous pre-training with less
time consumption. Our model facilitates the practical application of large
models in NLP."
1188,"In this paper, we take the ﬁrst steps towards a sys-
                                        First of all, in this framework the model ingests the        tematic investigation of Context Tracking and lay the
                                        messages of a conversation turn by turn and updates a        groundwork for further research.","This formulation extends and complements existing re-
                                        search in three key areas.","The most impor-
                                        growing repository of detected entity references in each     tant part of this effort is the new Contrack dataset,
                                        turn.",2022-01-28 20:38:13+00:00,A Unified Approach to Entity-Centric Context Tracking in Social Conversations,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ulrich Rückert'), arxiv.Result.Author('Srinivas Sunkara'), arxiv.Result.Author('Abhinav Rastogi'), arxiv.Result.Author('Sushant Prakash'), arxiv.Result.Author('Pranav Khaitan')]","In human-human conversations, Context Tracking deals with identifying
important entities and keeping track of their properties and relationships.
This is a challenging problem that encompasses several subtasks such as slot
tagging, coreference resolution, resolving plural mentions and entity linking.
We approach this problem as an end-to-end modeling task where the
conversational context is represented by an entity repository containing the
entity references mentioned so far, their properties and the relationships
between them. The repository is updated turn-by-turn, thus making training and
inference computationally efficient even for long conversations. This paper
lays the groundwork for an investigation of this framework in two ways. First,
we release Contrack, a large scale human-human conversation corpus for context
tracking with people and location annotations. It contains over 7000
conversations with an average of 11.8 turns, 5.8 entities and 15.2 references
per conversation. Second, we open-source a neural network architecture for
context tracking. Finally we compare this network to state-of-the-art
approaches for the subtasks it subsumes and report results on the involved
tradeoffs."
1189,"In this paper, we take the ﬁrst steps towards a sys-
                                        First of all, in this framework the model ingests the        tematic investigation of Context Tracking and lay the
                                        messages of a conversation turn by turn and updates a        groundwork for further research.","This formulation extends and complements existing re-
                                        search in three key areas.","The most impor-
                                        growing repository of detected entity references in each     tant part of this effort is the new Contrack dataset,
                                        turn.",2022-01-28 20:38:13+00:00,A Unified Approach to Entity-Centric Context Tracking in Social Conversations,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ulrich Rückert'), arxiv.Result.Author('Srinivas Sunkara'), arxiv.Result.Author('Abhinav Rastogi'), arxiv.Result.Author('Sushant Prakash'), arxiv.Result.Author('Pranav Khaitan')]","In human-human conversations, Context Tracking deals with identifying
important entities and keeping track of their properties and relationships.
This is a challenging problem that encompasses several subtasks such as slot
tagging, coreference resolution, resolving plural mentions and entity linking.
We approach this problem as an end-to-end modeling task where the
conversational context is represented by an entity repository containing the
entity references mentioned so far, their properties and the relationships
between them. The repository is updated turn-by-turn, thus making training and
inference computationally efficient even for long conversations. This paper
lays the groundwork for an investigation of this framework in two ways. First,
we release Contrack, a large scale human-human conversation corpus for context
tracking with people and location annotations. It contains over 7000
conversations with an average of 11.8 turns, 5.8 entities and 15.2 references
per conversation. Second, we open-source a neural network architecture for
context tracking. Finally we compare this network to state-of-the-art
approaches for the subtasks it subsumes and report results on the involved
tradeoffs."
1206,"Case Study In this section, we further study
                                                      some cases to sufﬁciently illustrate our model’s
                                                      effectiveness qualitatively.","However,       In practice, simply setting ρ = 0.5 can observe a
since ATE is a sub-task, improving ATE does not       fairly competitive performance.","With comparison to
                                                      BERT-Base baseline, we calculate the two terms
                                                      of mutual information (i.e.",2022-01-29 10:18:07+00:00,A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xiang Chen'), arxiv.Result.Author('Xiaojun Wan')]","Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task which aims to extract the aspects from sentences and identify their
corresponding sentiments. Aspect term extraction (ATE) is the crucial step for
ABSA. Due to the expensive annotation for aspect terms, we often lack labeled
target domain data for fine-tuning. To address this problem, many approaches
have been proposed recently to transfer common knowledge in an unsupervised
way, but such methods have too many modules and require expensive multi-stage
preprocessing. In this paper, we propose a simple but effective technique based
on mutual information maximization, which can serve as an additional component
to enhance any kind of model for cross-domain ABSA and ATE. Furthermore, we
provide some analysis of this approach. Experiment results show that our
proposed method outperforms the state-of-the-art methods for cross-domain ABSA
by 4.32% Micro-F1 on average over 10 different domain pairs. Apart from that,
our method can be extended to other sequence labeling tasks, such as named
entity recognition (NER)."
1207,"Error Analysis We further study the errors that
                                                      our approach still makes to provide some sugges-
tions for future research.","This leads
                                                      to the correct ﬁnal predictions.","As shown in Table 10,        Ting Chen, Simon Kornblith, Mohammad Norouzi,
there are three main error types.",2022-01-29 10:18:07+00:00,A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xiang Chen'), arxiv.Result.Author('Xiaojun Wan')]","Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task which aims to extract the aspects from sentences and identify their
corresponding sentiments. Aspect term extraction (ATE) is the crucial step for
ABSA. Due to the expensive annotation for aspect terms, we often lack labeled
target domain data for fine-tuning. To address this problem, many approaches
have been proposed recently to transfer common knowledge in an unsupervised
way, but such methods have too many modules and require expensive multi-stage
preprocessing. In this paper, we propose a simple but effective technique based
on mutual information maximization, which can serve as an additional component
to enhance any kind of model for cross-domain ABSA and ATE. Furthermore, we
provide some analysis of this approach. Experiment results show that our
proposed method outperforms the state-of-the-art methods for cross-domain ABSA
by 4.32% Micro-F1 on average over 10 different domain pairs. Apart from that,
our method can be extended to other sequence labeling tasks, such as named
entity recognition (NER)."
1350,"and dataset publicly available1, and hope that this     We prune out (1) other language sentences using
will help advance further research in this critical     Polyglot language detector3, (2) sentences with less
area.","We make our code            ters, sentence delimiters and non-breaking preﬁxes.","than 5 words or more than 100 words, (3) sentences
                                                        which could potentially have no factual information
3 Data Collection & Pre-processing                      (sentences with no noun or verb4).",2022-02-01 09:41:59+00:00,XAlign: Cross-lingual Fact-to-Text Alignment and Generation for Low-Resource Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tushar Abhishek'), arxiv.Result.Author('Shivprasad Sagare'), arxiv.Result.Author('Bhavyajeet Singh'), arxiv.Result.Author('Anubhav Sharma'), arxiv.Result.Author('Manish Gupta'), arxiv.Result.Author('Vasudeva Varma')]","Multiple critical scenarios (like Wikipedia text generation given English
Infoboxes) need automated generation of descriptive text in low resource (LR)
languages from English fact triples. Previous work has focused on English
fact-to-text (F2T) generation. To the best of our knowledge, there has been no
previous attempt on cross-lingual alignment or generation for LR languages.
Building an effective cross-lingual F2T (XF2T) system requires alignment
between English structured facts and LR sentences. We propose two unsupervised
methods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with
0.45M pairs across 8 languages, of which 5402 pairs have been manually
annotated. We also train strong baseline XF2T generation models on the XAlign
dataset."
1376,"We hope that this study will inspire
the community to investigate further research into cross-domain aspect-based sentiment analysis to
greatly reduce the time and economic cost of retraining.","However, currently our model can only process a single target as input, and
we will conduct further work on multiple targets in the future.","References

[Alhuzali and Ananiadou, 2021] Alhuzali, H. and Ananiadou, S. (2021).",2022-01-05 04:23:29+00:00,Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis,cs.CL,"['cs.CL', 'cs.AI']",[arxiv.Result.Author('Teng Wang')],"After transformer is proposed, lots of pre-trained language models have been
come up with and sentiment analysis (SA) task has been improved. In this paper,
we proposed a method that uses an auxiliary sentence about aspects that the
sentence contains to help sentiment prediction. The first is aspect detection,
which uses a multi-aspects detection model to predict all aspects that the
sentence has. Combining the predicted aspects and the original sentence as
Sentiment Analysis (SA) model's input. The second is to do out-of-domain
aspect-based sentiment analysis(ABSA), train sentiment classification model
with one kind of dataset and validate it with another kind of dataset. Finally,
we created two baselines, they use no aspect and all aspects as sentiment
classification model's input, respectively. Compare two baselines performance
to our method, found that our method really makes sense."
1377,"We hope that this study
will inspire the community to investigate further research into cross-domain aspect-based sentiment
analysis to greatly reduce the time and economic cost of retraining.","However, currently our model can only process a single target as
input, and we will conduct further work on multiple targets in the future.","References

[Alhuzali and Ananiadou, 2021] Alhuzali, H. and Ananiadou, S. (2021).",2022-01-05 04:23:29+00:00,Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Teng Wang'), arxiv.Result.Author('Bolun Sun'), arxiv.Result.Author('Yijie Tong')]","After transformer is proposed, lots of pre-trained language models have been
come up with and sentiment analysis (SA) task has been improved. In this paper,
we proposed a method that uses an auxiliary sentence about aspects that the
sentence contains to help sentiment prediction. The first is aspect detection,
which uses a multi-aspects detection model to predict all aspects that the
sentence has. Combining the predicted aspects and the original sentence as
Sentiment Analysis (SA) model's input. The second is to do out-of-domain
aspect-based sentiment analysis(ABSA), train sentiment classification model
with one kind of dataset and validate it with another kind of dataset. Finally,
we created two baselines, they use no aspect and all aspects as sentiment
classification model's input, respectively. Compare two baselines performance
to our method, found that our method really makes sense."
1384,"Such hybrid methods are not currently
well-studied and we believe that our encouraging results will stimulate further research on similar active learning
strategies.","However, in the second phase, later stages of selection, taking
the model uncertainty into the account can improve the selection performance.","7 Ethics Statement

Our proposed active learning approach has the potential to mitigate the difﬁculties associated with the annotation, and
classiﬁcation of textual content, e.g.",2022-01-28 19:19:03+00:00,Dominant Set-based Active Learning for Text Classification and its Application to Online Social Media,cs.CL,['cs.CL'],"[arxiv.Result.Author('Toktam A. Oghaz'), arxiv.Result.Author('Ivan Garibay')]","Recent advances in natural language processing (NLP) in online social media
are evidently owed to large-scale datasets. However, labeling, storing, and
processing a large number of textual data points, e.g., tweets, has remained
challenging. On top of that, in applications such as hate speech detection,
labeling a sufficiently large dataset containing offensive content can be
mentally and emotionally taxing for human annotators. Thus, NLP methods that
can make the best use of significantly less labeled data points are of great
interest. In this paper, we present a novel pool-based active learning method
that can be used for the training of large unlabeled corpus with minimum
annotation cost. For that, we propose to find the dominant sets of local
clusters in the feature space. These sets represent maximally cohesive
structures in the data. Then, the samples that do not belong to any of the
dominant sets are selected to be used to train the model, as they represent the
boundaries of the local clusters and are more challenging to classify. Our
proposed method does not have any parameters to be tuned, making it
dataset-independent, and it can approximately achieve the same classification
accuracy as full training data, with significantly fewer data points.
Additionally, our method achieves a higher performance in comparison to the
state-of-the-art active learning strategies. Furthermore, our proposed
algorithm is able to incorporate conventional active learning scores, such as
uncertainty-based scores, into its selection criteria. We show the
effectiveness of our method on different datasets and using different neural
network architectures."
1439,"The dataset and resources are              Another data set for Marathi NER was introduced
publicly shared to facilitate further research in          in (Murthy et al., 2018).",Marathi Corpus.,"It consists of 5591 sen-
Marathi NLP.",2022-02-02 17:35:52+00:00,"L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources",cs.CL,"['cs.CL', 'cs.LG']",[arxiv.Result.Author('Raviraj Joshi')],"We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from
different internet sources. We expand the existing Marathi monolingual corpus
with 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT,
and MahaRoBerta all BERT-based masked language models, and MahaFT, the fast
text word embeddings both trained on full Marathi corpus with 752M tokens. We
show the effectiveness of these resources on downstream Marathi sentiment
analysis, text classification, and named entity recognition (NER) tasks. We
also release MahaGPT, a generative Marathi GPT model trained on Marathi corpus.
Marathi is a popular language in India but still lacks these resources. This
work is a step forward in building open resources for the Marathi language. The
data and models are available at https://github.com/l3cube-pune/MarathiNLP ."
1455,"further research towards improving and understanding uni-
versal, multi-modal pre-trained models.",We hope that this work catalyzes        https://aclanthology.org/W19-5301.,"Barrault, L., Biesialska, M., Bojar, O., Costa-jussa`, M. R.,
                                                                  Federmann, C., Graham, Y., Grundkiewicz, R., Had-
References                                                        dow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P.,
                                                                  Lo, C.-k., Ljubesˇic´, N., Monz, C., Morishita, M., Nagata,
Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler,         M., Nakazawa, T., Pal, S., Post, M., and Zampieri, M.
   M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M.,         Findings of the 2020 conference on machine translation
   and Weber, G. Common voice: A massively-multilingual          (WMT20).",2022-02-03 02:26:40+00:00,mSLAM: Massively multilingual joint pre-training for speech and text,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Ankur Bapna'), arxiv.Result.Author('Colin Cherry'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Ye Jia'), arxiv.Result.Author('Melvin Johnson'), arxiv.Result.Author('Yong Cheng'), arxiv.Result.Author('Simran Khanuja'), arxiv.Result.Author('Jason Riesa'), arxiv.Result.Author('Alexis Conneau')]","We present mSLAM, a multilingual Speech and LAnguage Model that learns
cross-lingual cross-modal representations of speech and text by pre-training
jointly on large amounts of unlabeled speech and text in multiple languages.
mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on
character-level text, along with Connectionist Temporal Classification (CTC)
losses on paired speech and transcript data, to learn a single model capable of
learning from and representing both speech and text signals in a shared
representation space. We evaluate mSLAM on several downstream speech
understanding tasks and find that joint pre-training with text improves quality
on speech translation, speech intent classification and speech language-ID
while being competitive on multilingual ASR, when compared against speech-only
pre-training. Our speech translation model demonstrates zero-shot text
translation without seeing any text translation data, providing evidence for
cross-modal alignment of representations. mSLAM also benefits from multi-modal
fine-tuning, further improving the quality of speech translation by directly
leveraging text translation data during the fine-tuning process. Our empirical
analysis highlights several opportunities and challenges arising from
large-scale multimodal pre-training, suggesting directions for future research."
1538,"models to the community for further research.1          (2020) used a simple nearest-neighbors-based ap-

    1https://github.com/guyrosin/temporal_
attention
proach to detect semantically-changed words.",Gonen et al.,"Oth-      Figure 2: Illustration of our proposed temporal atten-
ers learned time-aware embeddings simultaneously       tion mechanism.",2022-02-04 11:55:34+00:00,Temporal Attention for Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Guy D. Rosin'), arxiv.Result.Author('Kira Radinsky')]","Pretrained language models based on the transformer architecture have shown
great success in NLP. Textual training data often comes from the web and is
thus tagged with time-specific information, but most language models ignore
this information. They are trained on the textual data alone, limiting their
ability to generalize temporally. In this work, we extend the key component of
the transformer architecture, i.e., the self-attention mechanism, and propose
temporal attention - a time-aware self-attention mechanism. Temporal attention
can be applied to any transformer model and requires the input texts to be
accompanied with their relevant time points. It allows the transformer to
capture this temporal information and create time-specific contextualized word
representations. We leverage these representations for the task of semantic
change detection; we apply our proposed mechanism to BERT and experiment on
three datasets in different languages (English, German, and Latin) that also
vary in time, size, and genre. Our proposed model achieves state-of-the-art
results on all the datasets."
1539,"This task is
diverse datasets in terms of time, language, size,      often addressed using time-aware word representa-
and genre; (3) We contribute our code and trained       tions that are learned from time-annotated corpora
models to the community for further research.1

    1https://github.com/guyrosin/temporal_
attention
and then compared between different time points        Figure 2: Illustration of our proposed temporal atten-
(Jatowt and Duh, 2014; Kim et al., 2014; Kulkarni      tion mechanism.","The time
is considered during the computation of attention       2.2 Semantic Change Detection
scores, thus allowing to create time-speciﬁc con-
textualized word representations; (2) We conduct        Semantic change detection is the task of identify-
evaluations on the task of semantic change detec-       ing words that change meaning over time (Kutuzov
tion and reach state-of-the-art performance on three    et al., 2018; Tahmasebi et al., 2018).","et al., 2015; Hamilton et al., 2016; Dubossarsky
et al., 2019; Del Tredici et al., 2019).",2022-02-04 11:55:34+00:00,Temporal Attention for Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Guy D. Rosin'), arxiv.Result.Author('Kira Radinsky')]","Pretrained language models based on the transformer architecture have shown
great success in NLP. Textual training data often comes from the web and is
thus tagged with time-specific information, but most language models ignore
this information. They are trained on the textual data alone, limiting their
ability to generalize temporally. In this work, we extend the key component of
the transformer architecture, i.e., the self-attention mechanism, and propose
temporal attention - a time-aware self-attention mechanism. Temporal attention
can be applied to any transformer model and requires the input texts to be
accompanied with their relevant time points. It allows the transformer to
capture this temporal information and create time-specific contextualized word
representations. We leverage these representations for the task of semantic
change detection; we apply our proposed mechanism to BERT and experiment on
three datasets in different languages (English, German, and Latin) that also
vary in time, size, and genre. Our proposed model achieves state-of-the-art
results on all the datasets."
1608,"We further study the relation of NFIR and depen-
dency relation for two directly connected words.","We can see that in all kinds of model updates,       the golden dependency tree (i.e., the dependency distance
                          Measuring and Reducing Model Update Regression in Structured Prediction for NLP
is 1.).","The results
of stackptr⇒stackptr are shown in Table 9.",2022-02-07 07:04:54+00:00,Measuring and Reducing Model Update Regression in Structured Prediction for NLP,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Elman Mansimov'), arxiv.Result.Author('Yi-An Lai'), arxiv.Result.Author('Yixuan Su'), arxiv.Result.Author('Lei Shu'), arxiv.Result.Author('Yi Zhang')]","Recent advance in deep learning has led to rapid adoption of machine learning
based NLP models in a wide range of applications. Despite the continuous gain
in accuracy, backward compatibility is also an important aspect for industrial
applications, yet it received little research attention. Backward compatibility
requires that the new model does not regress on cases that were correctly
handled by its predecessor. This work studies model update regression in
structured prediction tasks. We choose syntactic dependency parsing and
conversational semantic parsing as representative examples of structured
prediction tasks in NLP. First, we measure and analyze model update regression
in different model update settings. Next, we explore and benchmark existing
techniques for reducing model update regression including model ensemble and
knowledge distillation. We further propose a simple and effective method,
Backward-Congruent Re-ranking (BCR), by taking into account the characteristics
of structured output. Experiments show that BCR can better mitigate model
update regression than model ensemble and knowledge distillation approaches."
1620,"possible beneﬁts of further studying them and learn-
ing or iterating over the hyperparameter α.","Indeed, on the specialized datasets
dataset, alpha embeddings perform best for the stan-   the variants that take word frequencies into account
dard size of 300 embeddings, which may signal the      show good results.","6 CONCLUSIONS

    For embeddings of dimension ﬁve on the IMDB        This position paper conducts a series of experiments
dataset, Poincare embeddings perform the best.",2022-02-07 12:56:32+00:00,Moving Other Way: Exploring Word Mover Distance Extensions,cs.CL,"['cs.CL', '49Q22', 'I.2.7']","[arxiv.Result.Author('Ilya Smirnov'), arxiv.Result.Author('Ivan P. Yamshchikov')]","The word mover's distance (WMD) is a popular semantic similarity metric for
two texts. This position paper studies several possible extensions of WMD. We
experiment with the frequency of words in the corpus as a weighting factor and
the geometry of the word vector space. We validate possible extensions of WMD
on six document classification datasets. Some proposed extensions show better
results in terms of the k-nearest neighbor classification error than WMD."
1621,"We can also notice that on TWITTER the classi-
    It seems that taking into account the frequency of               in neural information processing systems, 30:6338–
words and improving the mechanism of optimal trans-                  6347.
port in application to semantics could be promising
directions for further research.","dimensional spaces better than other embeddings’
types.","However, additional           Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.",2022-02-07 12:56:32+00:00,Moving Other Way: Exploring Word Mover Distance Extensions,cs.CL,"['cs.CL', '49Q22', 'I.2.7']","[arxiv.Result.Author('Ilya Smirnov'), arxiv.Result.Author('Ivan P. Yamshchikov')]","The word mover's distance (WMD) is a popular semantic similarity metric for
two texts. This position paper studies several possible extensions of WMD. We
experiment with the frequency of words in the corpus as a weighting factor and
the geometry of the word vector space. We validate possible extensions of WMD
on six document classification datasets. Some proposed extensions show better
results in terms of the k-nearest neighbor classification error than WMD."
1622,"dard standard dimension of 300, which may signal the
possible beneﬁts of further studying them and learn-    6 CONCLUSIONS
ing or iterating over the hyperparameter α.","On the TWITTER        the variants that take word frequencies into account
dataset, alpha embeddings perform best for the stan-    show good results.","This position paper conducts a series of experiments
    For embeddings of dimension 5 on the IMDB           to calculate Word Mover’s Distance in different em-
dataset, Poincare embeddings perform the best.",2022-02-07 12:56:32+00:00,Moving Other Way: Exploring Word Mover Distance Extensions,cs.CL,"['cs.CL', '49Q22', 'I.2.7']","[arxiv.Result.Author('Ilya Smirnov'), arxiv.Result.Author('Ivan P. Yamshchikov')]","The word mover's distance (WMD) is a popular semantic similarity metric for
two texts. This position paper studies several possible extensions of WMD. We
experiment with the frequency of words in the corpus as a weighting factor and
the geometry of the word vector space. We validate possible extensions of WMD
on six document classification datasets. Some proposed extensions show better
results in terms of the k-nearest neighbor classification error than WMD."
1623,"words and improving the mechanism of optimal trans-
                                                        port in application to semantics could be promising
    We can also notice that on TWITTER the classi-      directions for further research.","one could suggest that they capture semantics in low-
dimensional spaces better than other embeddings’            It seems that taking into account the frequency of
types.","However, additional
ﬁcation error is almost the same, whereas on IMDB       work on this problem is necessary.",2022-02-07 12:56:32+00:00,Moving Other Way: Exploring Word Mover Distance Extensions,cs.CL,"['cs.CL', '49Q22', 'I.2.7']","[arxiv.Result.Author('Ilya Smirnov'), arxiv.Result.Author('Ivan P. Yamshchikov')]","The word mover's distance (WMD) is a popular semantic similarity metric for
two texts. This position paper studies several possible extensions of WMD. We
experiment with the frequency of words in the corpus as a weighting factor and
the geometry of the word vector space. We validate possible extensions of WMD
on six document classification datasets. Some proposed extensions show better
results in terms of the k-nearest neighbor classification error than WMD."
1629,"The recent survey by Chaves and Gerrosa154 reports on further research
endeavors to elucidate the aspects that account for the perception of suc-
cessful and engaging interactions with CAs.","also attempted to ﬁnd a correlation between human judg-
ment and automatic metrics, but only small correlations were found.","These aspects include, for ex-
ample, conscientiousness: how aware of and attentive to the context the
CA appears to be; communicability: how transparent the CA’s interaction
capabilities are, e.g., for the user to know how to best query the agent;155
damage control : how capable the CA is to recover from failure, handle un-
known concepts; etc.156 Some apparent qualities can actually be perceived
to be negative, when excessive.",2022-02-07 13:48:14+00:00,Conversational Agents: Theory and Applications,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Mattias Wahde'), arxiv.Result.Author('Marco Virgolin')]","In this chapter, we provide a review of conversational agents (CAs),
discussing chatbots, intended for casual conversation with a user, as well as
task-oriented agents that generally engage in discussions intended to reach one
or several specific goals, often (but not always) within a specific domain. We
also consider the concept of embodied conversational agents, briefly reviewing
aspects such as character animation and speech processing. The many different
approaches for representing dialogue in CAs are discussed in some detail, along
with methods for evaluating such agents, emphasizing the important topics of
accountability and interpretability. A brief historical overview is given,
followed by an extensive overview of various applications, especially in the
fields of health and education. We end the chapter by discussing benefits and
potential risks regarding the societal impact of current and future CA
technology."
1639,"Furthermore, on some occasions they might provide a starting point
for further research into behavioural traits as observed on social media.","Such features constitute proxies, albeit imperfect, for signiﬁcant ﬁndings
in the mental health literature that may be manifested and quantiﬁed on online

    6See http://empath.stanford.edu/

                                                     9
settings.","As an
example, Reece et al.",2022-02-07 15:29:01+00:00,Mental Disorders on Online Social Media Through the Lens of Language and Behaviour: Analysis and Visualisation,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Esteban A. Ríssola'), arxiv.Result.Author('Mohammad Aliannejadi'), arxiv.Result.Author('Fabio Crestani')]","Due to the worldwide accessibility to the Internet along with the continuous
advances in mobile technologies, physical and digital worlds have become
completely blended, and the proliferation of social media platforms has taken a
leading role over this evolution. In this paper, we undertake a thorough
analysis towards better visualising and understanding the factors that
characterise and differentiate social media users affected by mental disorders.
We perform different experiments studying multiple dimensions of language,
including vocabulary uniqueness, word usage, linguistic style, psychometric
attributes, emotions' co-occurrence patterns, and online behavioural traits,
including social engagement and posting trends. Our findings reveal significant
differences on the use of function words, such as adverbs and verb tense, and
topic-specific vocabulary, such as biological processes. As for emotional
expression, we observe that affected users tend to share emotions more
regularly than control individuals on average. Overall, the monthly posting
variance of the affected groups is higher than the control groups. Moreover, we
found evidence suggesting that language use on micro-blogging platforms is less
distinguishable for users who have a mental disorder than other less
restrictive platforms. In particular, we observe on Twitter less quantifiable
differences between affected and control groups compared to Reddit."
1672,"The limitations we ﬁnd do not invalidate this larger conclusion,
though they do suggest the importance of further research in this area.","We have demonstrated here that the third option is a feasi-
ble explanation: three language models that contain no explicit linguistic biases
regarding possible position-role mappings nevertheless successfully demonstrate
knowledge of position-role mappings that largely generalizes across verbs and
syntactic structures.","We have shown that pretrained language models (BERT, RoBERTa, and Dis-
tilBERT) recognize distributional differences between THEME- and RECIPIENT-
expecting positions.",2022-02-08 02:50:53+00:00,Do Language Models Learn Position-Role Mappings?,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jackson Petty'), arxiv.Result.Author('Michael Wilson'), arxiv.Result.Author('Robert Frank')]","How is knowledge of position-role mappings in natural language learned? We
explore this question in a computational setting, testing whether a variety of
well-performing pertained language models (BERT, RoBERTa, and DistilBERT)
exhibit knowledge of these mappings, and whether this knowledge persists across
alternations in syntactic, structural, and lexical alternations. In Experiment
1, we show that these neural models do indeed recognize distinctions between
theme and recipient roles in ditransitive constructions, and that these
distinct patterns are shared across construction type. We strengthen this
finding in Experiment 2 by showing that fine-tuning these language models on
novel theme- and recipient-like tokens in one paradigm allows the models to
make correct predictions about their placement in other paradigms, suggesting
that the knowledge of these mappings is shared rather than independently
learned. We do, however, observe some limitations of this generalization when
tasks involve constructions with novel ditransitive verbs, hinting at a degree
of lexical specificity which underlies model performance."
1713,"Applying the objectives on different pre-
trained language models and conducting test on different cat-
egories of summarization datasets need further study.","Additionally, BART was pre-
trained on Wikipedia and books corpus, and we only investi-
gate our method on news summarization datasets which have
moderate text length.","In this paper, we propose two differentiable N -gram ob-
jectives based on probabilistic sub-sequence matching that no
longer ceil the matched number of sub-sequences and can
value the matched probabilistic sub-sequences equally and
produce fairly signiﬁcant improvement on abstractive summa-
rization.",2022-02-08 17:19:23+00:00,Differentiable N-gram Objective on Abstractive Summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yunqi Zhu'), arxiv.Result.Author('Xuebing Yang'), arxiv.Result.Author('Yuanyuan Wu'), arxiv.Result.Author('Mingjin Zhu'), arxiv.Result.Author('Wensheng Zhang')]","ROUGE is a standard automatic evaluation metric based on n-grams for
sequence-to-sequence tasks, while cross-entropy loss is an essential objective
of neural network language model that optimizes at a unigram level. We present
differentiable n-gram objectives, attempting to alleviate the discrepancy
between training criterion and evaluating criterion. The objective maximizes
the probabilistic weight of matched sub-sequences, and the novelty of our work
is the objective weights the matched sub-sequences equally and does not ceil
the number of matched sub-sequences by the ground truth count of n-grams in
reference sequence. We jointly optimize cross-entropy loss and the proposed
objective, providing decent ROUGE score enhancement over abstractive
summarization dataset CNN/DM and XSum, outperforming alternative n-gram
objectives."
1752,"Overall, this evaluation demonstrates that the
the importance of carefully designing the projection layer         MLP-Mixer is a weight-efﬁcient architecture for processing
and justiﬁes an effort for further research on projection al-      the projection output, i.e., it reaches a higher performance
gorithms.",This difference of over 3% highlights            quadratic.,"Given these results, in the remaining experiments        than the alternatives with a smaller amount of parameters.",2022-02-09 09:01:29+00:00,pNLP-Mixer: an Efficient all-MLP Architecture for Language,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Francesco Fusco'), arxiv.Result.Author('Damian Pascual'), arxiv.Result.Author('Peter Staar')]","Large pre-trained language models drastically changed the natural language
processing(NLP) landscape. Nowadays, they represent the go-to framework to
tackle diverse NLP tasks, even with a limited number of annotations. However,
using those models in production, either in the cloud or at the edge, remains a
challenge due to the memory footprint and/or inference costs. As an
alternative, recent work on efficient NLP has shown that small weight-efficient
models can reach competitive performance at a fraction of the costs. Here, we
introduce pNLP-Mixer, an embbedding-free model based on the MLP-Mixer
architecture that achieves high weight-efficiency thanks to a novel
linguistically informed projection layer. We evaluate our model on two
multi-lingual semantic parsing datasets, MTOP and multiATIS. On MTOP our
pNLP-Mixer almost matches the performance of mBERT, which has 38 times more
parameters, and outperforms the state-of-the-art of tiny models (pQRNN) with 3
times fewer parameters. On a long-sequence classification task (Hyperpartisan)
our pNLP-Mixer without pretraining outperforms RoBERTa, which has 100 times
more parameters, demonstrating the potential of this architecture."
1852,"As shown in Table 6, we                         6.5 XLDS Difﬁculties
ﬁnd that the performance of pre-trained model with
only AcI outperforms that with only TeI (row 2 vs.                    To further study the speciﬁc challenges of end-to-
row 3).","also conduct experiments on XMediaSum40k to
compare AcI with TeI.","Besides, we replace the AcI with TeI in our                   end XLDS and give multiple promising directions
mDIALBART, the results show that mDIALBART                            for future research, we take a closer look at CLID-
with AcI outperforms that with TeI (row 4 vs. row                     SUM and model generation errors.",2022-02-11 13:32:14+00:00,ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiaan Wang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Ziyao Lu'), arxiv.Result.Author('Duo Zheng'), arxiv.Result.Author('Zhixu Li'), arxiv.Result.Author('Jianfeng Qu'), arxiv.Result.Author('Jie Zhou')]","We present ClidSum, a benchmark dataset for building cross-lingual
summarization systems on dialogue documents. It consists of 67k+ dialogue
documents from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated
summaries in different target languages. Based on the proposed ClidSum, we
introduce two benchmark settings for supervised and semi-supervised scenarios,
respectively. We then build various baseline systems in different paradigms
(pipeline and end-to-end) and conduct extensive experiments on ClidSum to
provide deeper analyses. Furthermore, we propose mDialBART which extends
mBART-50 (a multi-lingual BART) via further pre-training. The multiple
objectives used in the further pre-training stage help the pre-trained model
capture the structural characteristics as well as important content in
dialogues and the transformation from source to the target language.
Experimental results show the superiority of mDialBART, as an end-to-end model,
outperforms strong pipeline models on ClidSum. Finally, we discuss specific
challenges that current approaches faced with this task and give multiple
promising directions for future research. We have released the dataset and code
at https://github.com/krystalan/ClidSum."
1853,"We also ﬁnd
that our mDIALBART outperforms mBART+DA                          To further study the speciﬁc challenges of end-to-
(row 24 vs. row 23), indicating the effectiveness                end XLDS and give multiple promising directions
of the second stage pre-training.","XLDS samples, mBART-50 and mDIALBART sig-
niﬁcantly improve their performance on XMedia-                   6.4 XLDS Difﬁculties
Sum40k (row 23/25 vs. row 22/24).","Through carefully              for future research, we take a closer look at CLID-
designed pre-training objectives, the pre-trained                SUM and model generation errors.",2022-02-11 13:32:14+00:00,ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiaan Wang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Ziyao Lu'), arxiv.Result.Author('Duo Zheng'), arxiv.Result.Author('Zhixu Li'), arxiv.Result.Author('Jianfeng Qu'), arxiv.Result.Author('Jie Zhou')]","We present ClidSum, a benchmark dataset for building cross-lingual
summarization systems on dialogue documents. It consists of 67k+ dialogue
documents from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated
summaries in different target languages. Based on the proposed ClidSum, we
introduce two benchmark settings for supervised and semi-supervised scenarios,
respectively. We then build various baseline systems in different paradigms
(pipeline and end-to-end) and conduct extensive experiments on ClidSum to
provide deeper analyses. Furthermore, we propose mDialBART which extends
mBART-50 (a multi-lingual BART) via further pre-training. The multiple
objectives used in the further pre-training stage help the pre-trained model
capture the structural characteristics as well as important content in
dialogues and the transformation from source to the target language.
Experimental results show the superiority of mDialBART, as an end-to-end model,
outperforms strong pipeline models on ClidSum. Finally, we discuss specific
challenges that current approaches faced with this task and give multiple
promising directions for future research. We have released the dataset and code
at https://github.com/krystalan/ClidSum."
1867,"The work done here opens up               Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
further study into the use of randomness in defense           Kristina Toutanova.","Journal of Medical Systems, 42(12):1–
range of NLP adversarial attacks, presenting evi-             10.
dence for the effectiveness of randomness in NLP
defense methods.",2019.,2022-02-11 16:50:17+00:00,Using Random Perturbations to Mitigate Adversarial Attacks on Sentiment Analysis Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Abigail Swenor'), arxiv.Result.Author('Jugal Kalita')]","Attacks on deep learning models are often difficult to identify and therefore
are difficult to protect against. This problem is exacerbated by the use of
public datasets that typically are not manually inspected before use. In this
paper, we offer a solution to this vulnerability by using, during testing,
random perturbations such as spelling correction if necessary, substitution by
random synonym, or simply dropping the word. These perturbations are applied to
random words in random sentences to defend NLP models against adversarial
attacks. Our Random Perturbations Defense and Increased Randomness Defense
methods are successful in returning attacked models to similar accuracy of
models before attacks. The original accuracy of the model used in this work is
80% for sentiment classification. After undergoing attacks, the accuracy drops
to accuracy between 0% and 44%. After applying our defense methods, the
accuracy of the model is returned to the original accuracy within statistical
significance."
1880,[68] further study        ent meta-paths/meta-graphs within the MICoL framework.,Zhang et al.,"MICoL
metadata-aware LMTC.",2022-02-11 23:22:17+00:00,Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Zhihong Shen'), arxiv.Result.Author('Chieh-Han Wu'), arxiv.Result.Author('Boya Xie'), arxiv.Result.Author('Junheng Hao'), arxiv.Result.Author('Ye-Yi Wang'), arxiv.Result.Author('Kuansan Wang'), arxiv.Result.Author('Jiawei Han')]","Large-scale multi-label text classification (LMTC) aims to associate a
document with its relevant labels from a large candidate set. Most existing
LMTC approaches rely on massive human-annotated training data, which are often
costly to obtain and suffer from a long-tailed label distribution (i.e., many
labels occur only a few times in the training set). In this paper, we study
LMTC under the zero-shot setting, which does not require any annotated
documents with labels and only relies on label surface names and descriptions.
To train a classifier that calculates the similarity score between a document
and a label, we propose a novel metadata-induced contrastive learning (MICoL)
method. Different from previous text-based contrastive learning techniques,
MICoL exploits document metadata (e.g., authors, venues, and references of
research papers), which are widely available on the Web, to derive similar
document-document pairs. Experimental results on two large-scale datasets show
that: (1) MICoL significantly outperforms strong zero-shot text classification
and contrastive learning baselines; (2) MICoL is on par with the
state-of-the-art supervised metadata-aware LMTC method trained on 10K-200K
labeled documents; and (3) MICoL tends to predict more infrequent labels than
supervised methods, thus alleviates the deteriorated performance on long-tailed
labels."
1881,[68] further study              ent meta-paths/meta-graphs within the MICoL framework.,Zhang et al.,"MICoL
metadata-aware LMTC.",2022-02-11 23:22:17+00:00,Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Zhihong Shen'), arxiv.Result.Author('Chieh-Han Wu'), arxiv.Result.Author('Boya Xie'), arxiv.Result.Author('Junheng Hao'), arxiv.Result.Author('Ye-Yi Wang'), arxiv.Result.Author('Kuansan Wang'), arxiv.Result.Author('Jiawei Han')]","Large-scale multi-label text classification (LMTC) aims to associate a
document with its relevant labels from a large candidate set. Most existing
LMTC approaches rely on massive human-annotated training data, which are often
costly to obtain and suffer from a long-tailed label distribution (i.e., many
labels occur only a few times in the training set). In this paper, we study
LMTC under the zero-shot setting, which does not require any annotated
documents with labels and only relies on label surface names and descriptions.
To train a classifier that calculates the similarity score between a document
and a label, we propose a novel metadata-induced contrastive learning (MICoL)
method. Different from previous text-based contrastive learning techniques,
MICoL exploits document metadata (e.g., authors, venues, and references of
research papers), which are widely available on the Web, to derive similar
document-document pairs. Experimental results on two large-scale datasets show
that: (1) MICoL significantly outperforms strong zero-shot text classification
and contrastive learning baselines; (2) MICoL is on par with the
state-of-the-art supervised metadata-aware LMTC method trained on 10K-200K
labeled documents; and (3) MICoL tends to predict more infrequent labels than
supervised methods, thus alleviates the deteriorated performance on long-tailed
labels."
1899,"Our experiments on diﬀerent state-of-the-art
                                                pre-trained contextualized language models shows 74.8% Exact Match
                                                (EM) and 87.6% F1-score that can be used as the baseline results for
                                                further research on Persian QA.","By releasing this dataset, we aim to ease research
                                                on Persian reading comprehension and development of persian ques-
                                                tion answering systems.","Keywords: Machine Reading Comprehension - Natural Lan-
                                                guage Processing - Persian Dataset - Question Answering

                                        1 Introduction

                                        Machine Reading Comprehension (MRC) is one of the central tasks in nat-
                                        ural language understanding which requires a system to read a passage and
                                        then answer the given questions from the passage.",2022-02-13 05:42:55+00:00,PQuAD: A Persian Question Answering Dataset,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Kasra Darvishi'), arxiv.Result.Author('Newsha Shahbodagh'), arxiv.Result.Author('Zahra Abbasiantaeb'), arxiv.Result.Author('Saeedeh Momtazi')]","We present Persian Question Answering Dataset (PQuAD), a crowdsourced reading
comprehension dataset on Persian Wikipedia articles. It includes 80,000
questions along with their answers, with 25% of the questions being
adversarially unanswerable. We examine various properties of the dataset to
show the diversity and the level of its difficulty as an MRC benchmark. By
releasing this dataset, we aim to ease research on Persian reading
comprehension and development of Persian question answering systems. Our
experiments on different state-of-the-art pre-trained contextualized language
models show 74.8% Exact Match (EM) and 87.6% F1-score that can be used as the
baseline results for further research on Persian QA."
1947,"QASPER (Dasigi
The examined dialogue agent poorly performs on ArgS-       et al., 2021) is a recent question-answering dataset on
icChat, motivating further research on argumentative       scientiﬁc papers.","man dialogue (Reddy et al., 2019).","Unlike QASPER, ArgSciChat consists
dialogue generation on scientiﬁc papers.",2022-02-14 13:27:19+00:00,ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Federico Ruggeri'), arxiv.Result.Author('Mohsen Mesgar'), arxiv.Result.Author('Iryna Gurevych')]","The applications of conversational agents for scientific disciplines (as
expert domains) are understudied due to the lack of dialogue data to train such
agents. While most data collection frameworks, such as Amazon Mechanical Turk,
foster data collection for generic domains by connecting crowd workers and task
designers, these frameworks are not much optimized for data collection in
expert domains. Scientists are rarely present in these frameworks due to their
limited time budget. Therefore, we introduce a novel framework to collect
dialogues between scientists as domain experts on scientific papers. Our
framework lets scientists present their scientific papers as groundings for
dialogues and participate in dialogue they like its paper title. We use our
framework to collect a novel argumentative dialogue dataset, ArgSciChat. It
consists of 498 messages collected from 41 dialogues on 20 scientific papers.
Alongside extensive analysis on ArgSciChat, we evaluate a recent conversational
agent on our dataset. Experimental results show that this agent poorly performs
on ArgSciChat, motivating further research on argumentative scientific agents.
We release our framework and the dataset."
1948,"This difﬁculty moti-
terms of Fact-F1 is comparable with the baseline models     vates further research on building argumentative con-
and far from the human performance, conﬁrming the           versational agents on scientiﬁc papers.","In summary, the results on both tasks conﬁrm the dif-
                                                            ﬁculty of our dataset for LED as an advanced document-
   We observe that the performance of the LED model in      grounded conversational agent.","The ArgSciChat
difﬁculty of the fact selection task on the ArgSciChat      dataset is a stepping stone in this research direction.",2022-02-14 13:27:19+00:00,ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Federico Ruggeri'), arxiv.Result.Author('Mohsen Mesgar'), arxiv.Result.Author('Iryna Gurevych')]","The applications of conversational agents for scientific disciplines (as
expert domains) are understudied due to the lack of dialogue data to train such
agents. While most data collection frameworks, such as Amazon Mechanical Turk,
foster data collection for generic domains by connecting crowd workers and task
designers, these frameworks are not much optimized for data collection in
expert domains. Scientists are rarely present in these frameworks due to their
limited time budget. Therefore, we introduce a novel framework to collect
dialogues between scientists as domain experts on scientific papers. Our
framework lets scientists present their scientific papers as groundings for
dialogues and participate in dialogue they like its paper title. We use our
framework to collect a novel argumentative dialogue dataset, ArgSciChat. It
consists of 498 messages collected from 41 dialogues on 20 scientific papers.
Alongside extensive analysis on ArgSciChat, we evaluate a recent conversational
agent on our dataset. Experimental results show that this agent poorly performs
on ArgSciChat, motivating further research on argumentative scientific agents.
We release our framework and the dataset."
1949,"QASPER (Dasigi
The examined dialogue agent poorly performs on ArgS-       et al., 2021) is a recent question-answering dataset on
icChat, motivating further research on argumentative       scientiﬁc papers.","man dialogue (Reddy et al., 2019).","Unlike QASPER, ArgSciChat consists
dialogue generation on scientiﬁc papers.",2022-02-14 13:27:19+00:00,ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Federico Ruggeri'), arxiv.Result.Author('Mohsen Mesgar'), arxiv.Result.Author('Iryna Gurevych')]","The applications of conversational agents for scientific disciplines (as
expert domains) are understudied due to the lack of dialogue data to train such
agents. While most data collection frameworks, such as Amazon Mechanical Turk,
foster data collection for generic domains by connecting crowd workers and task
designers, these frameworks are not much optimized for data collection in
expert domains. Scientists are rarely present in these frameworks due to their
limited time budget. Therefore, we introduce a novel framework to collect
dialogues between scientists as domain experts on scientific papers. Our
framework lets scientists present their scientific papers as groundings for
dialogues and participate in dialogue they like its paper title. We use our
framework to collect a novel argumentative dialogue dataset, ArgSciChat. It
consists of 498 messages collected from 41 dialogues on 20 scientific papers.
Alongside extensive analysis on ArgSciChat, we evaluate a recent conversational
agent on our dataset. Experimental results show that this agent poorly performs
on ArgSciChat, motivating further research on argumentative scientific agents.
We release our framework and the dataset."
1950,"This difﬁculty moti-
terms of Fact-F1 is comparable with the baseline models     vates further research on building argumentative con-
and far from the human performance, conﬁrming the           versational agents on scientiﬁc papers.","In summary, the results on both tasks conﬁrm the dif-
                                                            ﬁculty of our dataset for LED as an advanced document-
   We observe that the performance of the LED model in      grounded conversational agent.","The ArgSciChat
difﬁculty of the fact selection task on the ArgSciChat      dataset is a stepping stone in this research direction.",2022-02-14 13:27:19+00:00,ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Federico Ruggeri'), arxiv.Result.Author('Mohsen Mesgar'), arxiv.Result.Author('Iryna Gurevych')]","The applications of conversational agents for scientific disciplines (as
expert domains) are understudied due to the lack of dialogue data to train such
agents. While most data collection frameworks, such as Amazon Mechanical Turk,
foster data collection for generic domains by connecting crowd workers and task
designers, these frameworks are not much optimized for data collection in
expert domains. Scientists are rarely present in these frameworks due to their
limited time budget. Therefore, we introduce a novel framework to collect
dialogues between scientists as domain experts on scientific papers. Our
framework lets scientists present their scientific papers as groundings for
dialogues and participate in dialogue they like its paper title. We use our
framework to collect a novel argumentative dialogue dataset, ArgSciChat. It
consists of 498 messages collected from 41 dialogues on 20 scientific papers.
Alongside extensive analysis on ArgSciChat, we evaluate a recent conversational
agent on our dataset. Experimental results show that this agent poorly performs
on ArgSciChat, motivating further research on argumentative scientific agents.
We release our framework and the dataset."
1951,"This score of LED(Q,F) conﬁrms      these characteristics motivate further research on
the importance of supportive facts for generating a  building conversational agents in expert domains.","In particular,
level F1 points.",high-quality response.,2022-02-14 13:27:19+00:00,ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Federico Ruggeri'), arxiv.Result.Author('Mohsen Mesgar'), arxiv.Result.Author('Iryna Gurevych')]","The applications of conversational agents for scientific disciplines (as
expert domains) are understudied due to the lack of dialogue data to train such
agents. While most data collection frameworks, such as Amazon Mechanical Turk,
foster data collection for generic domains by connecting crowd workers and task
designers, these frameworks are not much optimized for data collection in
expert domains. Scientists are rarely present in these frameworks due to their
limited time budget. Therefore, we introduce a novel framework to collect
dialogues between scientists as domain experts on scientific papers. Our
framework lets scientists present their scientific papers as groundings for
dialogues and participate in dialogue they like its paper title. We use our
framework to collect a novel argumentative dialogue dataset, ArgSciChat. It
consists of 498 messages collected from 41 dialogues on 20 scientific papers.
Alongside extensive analysis on ArgSciChat, we evaluate a recent conversational
agent on our dataset. Experimental results show that this agent poorly performs
on ArgSciChat, motivating further research on argumentative scientific agents.
We release our framework and the dataset."
1982,"Studying the Size of Language Models To further study
Time-Unit Conversion The performance gap evaluated             the impact of language models sizes on the performance
on all the time unit conversion experiments is provided in     gap caused by the instance frequencies, we perform the
                        Impact of Pretraining Term Frequencies on Few-Shot Reasoning

Table 3.",based on the pretraining data.,"GPT-J-6B results on Time-Unit Conversion: ∆1,2, ∆1,2,3 and ∆1,2,y represent the performance gap over the frequency

distributions of ω{x1,x2}, ω{x1,x2,x3} and ω{x1,x2,y} respectively, where x1 is the number operand, x2 is the source time unit, x3 is the
second implicit number operand needed for performing the conversion and the y is the true answer.",2022-02-15 05:43:54+00:00,Impact of Pretraining Term Frequencies on Few-Shot Reasoning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yasaman Razeghi'), arxiv.Result.Author('Robert L. Logan IV'), arxiv.Result.Author('Matt Gardner'), arxiv.Result.Author('Sameer Singh')]","Pretrained Language Models (LMs) have demonstrated ability to perform
numerical reasoning by extrapolating from a few examples in few-shot settings.
However, the extent to which this extrapolation relies on robust reasoning is
unclear. In this paper, we investigate how well these models reason with terms
that are less frequent in the pretraining data. In particular, we examine the
correlations between the model performance on test instances and the frequency
of terms from those instances in the pretraining data. We measure the strength
of this correlation for a number of GPT-based language models (pretrained on
the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and
unit conversion). Our results consistently demonstrate that models are more
accurate on instances whose terms are more prevalent, in some cases above
$70\%$ (absolute) more accurate on the top 10\% frequent terms in comparison to
the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot
numerical reasoning tasks, our results raise the question of how much models
actually generalize beyond pretraining data, and we encourage researchers to
take the pretraining data into account when interpreting evaluation results."
1983,"We recommend further research             GPT-Neo: Large Scale Autoregressive Language Model-
in investigating methods in causal inference and interven-         ing with Mesh-Tensorﬂow, March 2021.","We are not making a causal claim here,
and in general, there may be confounders that we have not       Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S.
eliminated in our study.","URL https:
tions during training to provide ﬁner-grained analysis of the     //doi.org/10.5281/zenodo.5297715.",2022-02-15 05:43:54+00:00,Impact of Pretraining Term Frequencies on Few-Shot Reasoning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yasaman Razeghi'), arxiv.Result.Author('Robert L. Logan IV'), arxiv.Result.Author('Matt Gardner'), arxiv.Result.Author('Sameer Singh')]","Pretrained Language Models (LMs) have demonstrated ability to perform
numerical reasoning by extrapolating from a few examples in few-shot settings.
However, the extent to which this extrapolation relies on robust reasoning is
unclear. In this paper, we investigate how well these models reason with terms
that are less frequent in the pretraining data. In particular, we examine the
correlations between the model performance on test instances and the frequency
of terms from those instances in the pretraining data. We measure the strength
of this correlation for a number of GPT-based language models (pretrained on
the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and
unit conversion). Our results consistently demonstrate that models are more
accurate on instances whose terms are more prevalent, in some cases above
$70\%$ (absolute) more accurate on the top 10\% frequent terms in comparison to
the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot
numerical reasoning tasks, our results raise the question of how much models
actually generalize beyond pretraining data, and we encourage researchers to
take the pretraining data into account when interpreting evaluation results."
1984,"Studying the Size of Language Models To further study
Time-Unit Conversion The performance gap evaluated             the impact of language models sizes on the performance
on all the time unit conversion experiments is provided in     gap caused by the instance frequencies, we perform the
                        Impact of Pretraining Term Frequencies on Few-Shot Reasoning

Table 3.",based on the pretraining data.,"GPT-J-6B results on Time-Unit Conversion: ∆1,2, ∆1,2,3 and ∆1,2,y represent the performance gap over the frequency

distributions of ω{x1,x2}, ω{x1,x2,x3} and ω{x1,x2,y} respectively, where x1 is the number operand, x2 is the source time unit, x3 is the
second implicit number operand needed for performing the conversion and the y is the true answer.",2022-02-15 05:43:54+00:00,Impact of Pretraining Term Frequencies on Few-Shot Reasoning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yasaman Razeghi'), arxiv.Result.Author('Robert L. Logan IV'), arxiv.Result.Author('Matt Gardner'), arxiv.Result.Author('Sameer Singh')]","Pretrained Language Models (LMs) have demonstrated ability to perform
numerical reasoning by extrapolating from a few examples in few-shot settings.
However, the extent to which this extrapolation relies on robust reasoning is
unclear. In this paper, we investigate how well these models reason with terms
that are less frequent in the pretraining data. In particular, we examine the
correlations between the model performance on test instances and the frequency
of terms from those instances in the pretraining data. We measure the strength
of this correlation for a number of GPT-based language models (pretrained on
the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and
unit conversion). Our results consistently demonstrate that models are more
accurate on instances whose terms are more prevalent, in some cases above
$70\%$ (absolute) more accurate on the top 10\% frequent terms in comparison to
the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot
numerical reasoning tasks, our results raise the question of how much models
actually generalize beyond pretraining data, and we encourage researchers to
take the pretraining data into account when interpreting evaluation results."
1985,"We recommend further research
co-occurrence statistics of numbers and dates of documents         in investigating methods in causal inference and interven-
appearing in the Pile dataset.",Our work documents                eliminated in our study.,"tions during training to provide ﬁner-grained analysis of the
                                                                   effect of pretraining.",2022-02-15 05:43:54+00:00,Impact of Pretraining Term Frequencies on Few-Shot Reasoning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yasaman Razeghi'), arxiv.Result.Author('Robert L. Logan IV'), arxiv.Result.Author('Matt Gardner'), arxiv.Result.Author('Sameer Singh')]","Pretrained Language Models (LMs) have demonstrated ability to perform
numerical reasoning by extrapolating from a few examples in few-shot settings.
However, the extent to which this extrapolation relies on robust reasoning is
unclear. In this paper, we investigate how well these models reason with terms
that are less frequent in the pretraining data. In particular, we examine the
correlations between the model performance on test instances and the frequency
of terms from those instances in the pretraining data. We measure the strength
of this correlation for a number of GPT-based language models (pretrained on
the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and
unit conversion). Our results consistently demonstrate that models are more
accurate on instances whose terms are more prevalent, in some cases above
$70\%$ (absolute) more accurate on the top 10\% frequent terms in comparison to
the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot
numerical reasoning tasks, our results raise the question of how much models
actually generalize beyond pretraining data, and we encourage researchers to
take the pretraining data into account when interpreting evaluation results."
1997,"We release the data
                                        and code for baselines to encourage further research on efﬁcient NLP models.","Furthermore, by evaluating both regular
                                        and efﬁcient transformers, we show that models with increased context length are better able to solve the tasks presented,
                                        suggesting that future improvements in these models are vital for solving similar long document problems.","Keywords: Long Documents, Benchmark, Multitask learning, NLP

                                                        1.",2022-02-15 12:42:55+00:00,MuLD: The Multitask Long Document Benchmark,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('G Thomas Hudson'), arxiv.Result.Author('Noura Al Moubayed')]","The impressive progress in NLP techniques has been driven by the development
of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks
focus on tasks for one or two input sentences, there has been exciting work in
designing efficient techniques for processing much longer inputs. In this
paper, we present MuLD: a new long document benchmark consisting of only
documents over 10,000 tokens. By modifying existing NLP tasks, we create a
diverse benchmark which requires models to successfully model long-term
dependencies in the text. We evaluate how existing models perform, and find
that our benchmark is much more challenging than their `short document'
equivalents. Furthermore, by evaluating both regular and efficient
transformers, we show that models with increased context length are better able
to solve the tasks presented, suggesting that future improvements in these
models are vital for solving similar long document problems. We release the
data and code for baselines to encourage further research on efficient NLP
models."
1998,"Glge: A new general lan-
this further research into efﬁcient models for long doc-     guage generation evaluation benchmark.","Chen, J., Jiang, D., Lv, J., Zhang, R., Wu, W., Zhou,
We hope that the MuLD benchmark will encourage               M., and Duan, N. (2021).","In FIND-
ument NLP.",2022-02-15 12:42:55+00:00,MuLD: The Multitask Long Document Benchmark,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('G Thomas Hudson'), arxiv.Result.Author('Noura Al Moubayed')]","The impressive progress in NLP techniques has been driven by the development
of multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks
focus on tasks for one or two input sentences, there has been exciting work in
designing efficient techniques for processing much longer inputs. In this
paper, we present MuLD: a new long document benchmark consisting of only
documents over 10,000 tokens. By modifying existing NLP tasks, we create a
diverse benchmark which requires models to successfully model long-term
dependencies in the text. We evaluate how existing models perform, and find
that our benchmark is much more challenging than their `short document'
equivalents. Furthermore, by evaluating both regular and efficient
transformers, we show that models with increased context length are better able
to solve the tasks presented, suggesting that future improvements in these
models are vital for solving similar long document problems. We release the
data and code for baselines to encourage further research on efficient NLP
models."
2039,"For future work, we plan to further study load-balanced parameterization for parameter-efﬁcient
models, which is an interesting and new but seemingly profound machine learning research problem:
instead of naively assuming that all the parameters are equal in this preliminary study, we suspect that
parameters in different modules (e.g., parameters in the self-attn and FFN; or parameters in different
layers) should be under different amounts of load.","The cost-effective parameterization
and layer adaptation innovations in EDGEFORMER prove effective to improve the results with
negligible computation and memory cost, achieving state-of-the-art results in the on-device seq2seq
generation setting.","We look forward to in-depth research on this
problem, which might be helpful to deepen our understanding of neural networks.",2022-02-16 10:10:00+00:00,EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tao Ge'), arxiv.Result.Author('Furu Wei')]","We propose EdgeFormer -- a parameter-efficient Transformer of the
encoder-decoder architecture for on-device seq2seq generation, which is
customized under the strict computation and memory constraints. EdgeFormer
proposes two novel principles for cost-effective parameterization and further
enhance the model with efficient layer adaptation. We conduct extensive
experiments on two practical on-device seq2seq tasks: Machine Translation and
Grammatical Error Correction, and show that EdgeFormer can effectively
outperform previous parameter-efficient Transformer baselines and achieve very
competitive results with knowledge distillation under both the computation and
memory constraints."
2040,"For future work, we plan to further study load-balanced parameterization for parameter-efﬁcient
models, which is an interesting and new but seemingly profound machine learning research problem:
instead of naively assuming that all the parameters are equal in this preliminary study, we suspect that
parameters in different modules (e.g., parameters in the self-attn and FFN; or parameters in different
layers) should be under different amounts of load.","Our released pretrained EDGEFORMER checkpoint can be easily ﬁne-tuned for
English seq2seq tasks, largely facilitating on-device seq2seq generation in practice.","We look forward to in-depth research on this
problem, which might be helpful to deepen our understanding of neural networks.",2022-02-16 10:10:00+00:00,EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tao Ge'), arxiv.Result.Author('Furu Wei')]","We propose EdgeFormer -- a parameter-efficient Transformer of the
encoder-decoder architecture for on-device seq2seq generation, which is
customized under strict computation and memory constraints. EdgeFormer proposes
two novel principles for cost-effective parameterization and further enhance
the model with efficient layer adaptation. We conduct extensive experiments on
two practical on-device seq2seq tasks: Machine Translation and Grammatical
Error Correction, and show that EdgeFormer can effectively outperform previous
parameter-efficient Transformer baselines and achieve very competitive results
with knowledge distillation under both the computation and memory constraints.
Moreover, we release the pretrained EdgeFormer -- the first publicly available
pretrained model that can be easily fine-tuned for English seq2seq tasks with
strong results, largely facilitating on-device seq2seq generation in practice."
2047,"We believe that such data can ﬂourish
                                                          further research, such as training more socially re-
                                                          sponsible systems by directing them to generate
                                                          anti-biased responses.","We ﬁnd that anti-bias
                                                          discourses are widespread in human-human con-
                                                          versations.","To be more speciﬁc, in DIAL-BIAS FRAME, the
                                                          implications of the conversational data can be con-
                                                          veying (i) anti-bias or positive opinions, (ii) neu-
                                                          tral, objective opinions, or (iii) biased or negative
                                                          opinions.",2022-02-16 11:59:29+00:00,"Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks",cs.CL,['cs.CL'],"[arxiv.Result.Author('Jingyan Zhou'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Yitong Li'), arxiv.Result.Author('Yasheng Wang'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Xin Jiang'), arxiv.Result.Author('Qun Liu'), arxiv.Result.Author('Helen Meng')]","The research of open-domain dialog systems has been greatly prospered by
neural models trained on large-scale corpora, however, such corpora often
introduce various safety problems (e.g., offensive languages, biases, and toxic
behaviors) that significantly hinder the deployment of dialog systems in
practice. Among all these unsafe issues, addressing social bias is more complex
as its negative impact on marginalized populations is usually expressed
implicitly, thus requiring normative reasoning and rigorous analysis. In this
paper, we focus our investigation on social bias detection of dialog safety
problems. We first propose a novel Dial-Bias Frame for analyzing the social
bias in conversations pragmatically, which considers more comprehensive
bias-related analyses rather than simple dichotomy annotations. Based on the
proposed framework, we further introduce CDail-Bias Dataset that, to our
knowledge, is the first well-annotated Chinese social bias dialog dataset. In
addition, we establish several dialog bias detection benchmarks at different
label granularities and input types (utterance-level and context-level). We
show that the proposed in-depth analyses together with these benchmarks in our
Dial-Bias Frame are necessary and essential to bias detection tasks and can
benefit building safe dialog systems in practice."
2048,"86.0 71.0 77.8       70.0 85.4 76.9                    Motivated by the observation above that different
                                                          topics have varying bias characteristics and difﬁcul-
Table 6: Results of Bias Relevance Prediction (Task 1)    ties, we further study the average performance of
at utterance-level on CDIAL-BIAS-UTT.","TS 83.5 79.4 81.3         75.8 80.1 77.8
Comb.",Fine-grained        topic-speciﬁc classiﬁers (avg.),2022-02-16 11:59:29+00:00,"Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks",cs.CL,['cs.CL'],"[arxiv.Result.Author('Jingyan Zhou'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Yitong Li'), arxiv.Result.Author('Yasheng Wang'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Xin Jiang'), arxiv.Result.Author('Qun Liu'), arxiv.Result.Author('Helen Meng')]","The research of open-domain dialog systems has been greatly prospered by
neural models trained on large-scale corpora, however, such corpora often
introduce various safety problems (e.g., offensive languages, biases, and toxic
behaviors) that significantly hinder the deployment of dialog systems in
practice. Among all these unsafe issues, addressing social bias is more complex
as its negative impact on marginalized populations is usually expressed
implicitly, thus requiring normative reasoning and rigorous analysis. In this
paper, we focus our investigation on social bias detection of dialog safety
problems. We first propose a novel Dial-Bias Frame for analyzing the social
bias in conversations pragmatically, which considers more comprehensive
bias-related analyses rather than simple dichotomy annotations. Based on the
proposed framework, we further introduce CDail-Bias Dataset that, to our
knowledge, is the first well-annotated Chinese social bias dialog dataset. In
addition, we establish several dialog bias detection benchmarks at different
label granularities and input types (utterance-level and context-level). We
show that the proposed in-depth analyses together with these benchmarks in our
Dial-Bias Frame are necessary and essential to bias detection tasks and can
benefit building safe dialog systems in practice."
2110,"To investigate the        text, the task is worth further research and AISHELL-NER
                                                                   could be a proper benchmark.","Since there is still a gap between the
Since there is a gap between NER performances from the             performances of NER from the speech and the ground-truth
speech signal and the ground-truth text.",6.,2022-02-17 09:18:48+00:00,AISHELL-NER: Named Entity Recognition from Chinese Speech,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Boli Chen'), arxiv.Result.Author('Guangwei Xu'), arxiv.Result.Author('Xiaobin Wang'), arxiv.Result.Author('Pengjun Xie'), arxiv.Result.Author('Meishan Zhang'), arxiv.Result.Author('Fei Huang')]","Named Entity Recognition (NER) from speech is among Spoken Language
Understanding (SLU) tasks, aiming to extract semantic information from the
speech signal. NER from speech is usually made through a two-step pipeline that
consists of (1) processing the audio using an Automatic Speech Recognition
(ASR) system and (2) applying an NER tagger to the ASR outputs. Recent works
have shown the capability of the End-to-End (E2E) approach for NER from English
and French speech, which is essentially entity-aware ASR. However, due to the
many homophones and polyphones that exist in Chinese, NER from Chinese speech
is effectively a more challenging task. In this paper, we introduce a new
dataset AISEHLL-NER for NER from Chinese speech. Extensive experiments are
conducted to explore the performance of several state-of-the-art methods. The
results demonstrate that the performance could be improved by combining
entity-aware ASR and pretrained NER tagger, which can be easily applied to the
modern SLU pipeline. The dataset is publicly available at
github.com/Alibaba-NLP/AISHELL-NER."
2148,"In Appendix C, we further study the quality impact of adding new multiplicative interactions in
expert layers.","(2021) found adding a learned multiplicative scalar to the residual connection in Transformers
made them much more unstable.","We ﬁnd that this operation yields quality improvements with virtually no slow-down
in model step time.",2022-02-17 21:39:10+00:00,Designing Effective Sparse Expert Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Barret Zoph'), arxiv.Result.Author('Irwan Bello'), arxiv.Result.Author('Sameer Kumar'), arxiv.Result.Author('Nan Du'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Jeff Dean'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('William Fedus')]","Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3)."
2149,"ing objective invites further research on better leveraging sparsity and expert specialization in the
decoder.","Kenneth may become Ken, ne, th.","Alternatively, future work could study simply removing the experts in the decoder layer,
which also confers beneﬁts during autoregressive decoding (Kudugunta et al., 2021a).",2022-02-17 21:39:10+00:00,Designing Effective Sparse Expert Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Barret Zoph'), arxiv.Result.Author('Irwan Bello'), arxiv.Result.Author('Sameer Kumar'), arxiv.Result.Author('Nan Du'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Jeff Dean'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('William Fedus')]","Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3)."
2150,"In Appendix C, we further study the quality impact of adding new multiplicative interactions in
expert layers.","(2021) found adding a learned multiplicative scalar to the residual connection in Transformers
made them much more unstable.","We ﬁnd that this operation yields quality improvements with virtually no slow-down
in model step time.",2022-02-17 21:39:10+00:00,ST-MoE: Designing Stable and Transferable Sparse Expert Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Barret Zoph'), arxiv.Result.Author('Irwan Bello'), arxiv.Result.Author('Sameer Kumar'), arxiv.Result.Author('Nan Du'), arxiv.Result.Author('Yanping Huang'), arxiv.Result.Author('Jeff Dean'), arxiv.Result.Author('Noam Shazeer'), arxiv.Result.Author('William Fedus')]","Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3)."
2213,"For instance, initial letters
                                             show that AE in different languages and dif-       of the words in the phrases are commonly used to
                                             ferent learning settings has unique challenges,    form acronyms in scientiﬁc English; however, in le-
                                             emphasizing the necessity of further research      gal English or Danish documents, the use of initial
                                             on multilingual and multi-domain AE.","Our ex-      styles might be exerted to shorten a longer phrase
                                             tensive experiments on the proposed dataset        to produce acronyms.","letters for acronym detection is less effective (see
                                                                                                Section 3).",2022-02-19 23:08:38+00:00,MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Amir Pouran Ben Veyseh'), arxiv.Result.Author('Nicole Meister'), arxiv.Result.Author('Seunghyun Yoon'), arxiv.Result.Author('Rajiv Jain'), arxiv.Result.Author('Franck Dernoncourt'), arxiv.Result.Author('Thien Huu Nguyen')]","Acronym extraction is the task of identifying acronyms and their expanded
forms in texts that is necessary for various NLP applications. Despite major
progress for this task in recent years, one limitation of existing AE research
is that they are limited to the English language and certain domains (i.e.,
scientific and biomedical). As such, challenges of AE in other languages and
domains is mainly unexplored. Lacking annotated datasets in multiple languages
and domains has been a major issue to hinder research in this area. To address
this limitation, we propose a new dataset for multilingual multi-domain AE.
Specifically, 27,200 sentences in 6 typologically different languages and 2
domains, i.e., Legal and Scientific, is manually annotated for AE. Our
extensive experiments on the proposed dataset show that AE in different
languages and different learning settings has unique challenges, emphasizing
the necessity of further research on multilingual and multi-domain AE."
2284,"We ﬁnd that the parametric model performs worse

than CBR-SUBG on all the query types reaching an average performance of 13% point below

    1We will release the code, dataset and data generation pipeline for reproducibility and further research

                                                    13
CBR-SUBG.","Row 2 of Table 1 shows the

performance of GNN + TransE model.","This shows that a semiparametric model with a nonparametric component that
retrieves similar queries at inference can make it easier for the model to reason effectively.",2022-02-22 01:34:35+00:00,Knowledge Base Question Answering by Case-based Reasoning over Subgraphs,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Rajarshi Das'), arxiv.Result.Author('Ameya Godbole'), arxiv.Result.Author('Ankita Naik'), arxiv.Result.Author('Elliot Tower'), arxiv.Result.Author('Robin Jia'), arxiv.Result.Author('Manzil Zaheer'), arxiv.Result.Author('Hannaneh Hajishirzi'), arxiv.Result.Author('Andrew McCallum')]","Question answering (QA) over real-world knowledge bases (KBs) is challenging
because of the diverse (essentially unbounded) types of reasoning patterns
needed. However, we hypothesize in a large KB, reasoning patterns required to
answer a query type reoccur for various entities in their respective subgraph
neighborhoods. Leveraging this structural similarity between local
neighborhoods of different subgraphs, we introduce a semiparametric model with
(i) a nonparametric component that for each query, dynamically retrieves other
similar $k$-nearest neighbor (KNN) training queries along with query-specific
subgraphs and (ii) a parametric component that is trained to identify the
(latent) reasoning patterns from the subgraphs of KNN queries and then apply it
to the subgraph of the target query. We also propose a novel algorithm to
select a query-specific compact subgraph from within the massive knowledge
graph (KG), allowing us to scale to full Freebase KG containing billions of
edges. We show that our model answers queries requiring complex reasoning
patterns more effectively than existing KG completion algorithms. The proposed
model outperforms or performs competitively with state-of-the-art models on
several KBQA benchmarks."
2397,"4 Conclusion
   To further study the impact of NoisyTune on
model ﬁnetuning, we show the relative changes of                                   In this paper, we propose a very simple but effec-
the L1-norms of different kinds of parameters in                                   tive method named NoisyTune, which adds a little
the BERT model during training on MRPC and                                         noise to PLMs before ﬁnetuning for better trans-
STS-B in Fig.",and have better generalization abilities.,"3.3 Since the noise we added is zero-                                ferability from pretraining tasks to downstream
mean, the absolute parameter L1-norms will not                                     tasks.",2022-02-24 11:08:02+00:00,NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chuhan Wu'), arxiv.Result.Author('Fangzhao Wu'), arxiv.Result.Author('Tao Qi'), arxiv.Result.Author('Yongfeng Huang'), arxiv.Result.Author('Xing Xie')]","Effectively finetuning pretrained language models (PLMs) is critical for
their success in downstream tasks. However, PLMs may have risks in overfitting
pretraining signals, and there are some gaps between downstream tasks and the
pretraining tasks. It can be difficult for vanilla finetuning methods to
overcome the barrier between pretraining and downstream tasks, which leads to
suboptimal performance. In this paper, we propose a very simple yet effective
method named NoisyTune which can help better finetune PLMs in downstream tasks
by adding some noise to the parameters of PLMs before finetuning. More
specifically, we propose a matrix-wise perturbing method by adding different
uniform noises according to the standard deviations of different parameter
matrices, which can consider the varied characteristics of different types of
parameters in PLMs. Extensive experiments on the GLUE English benchmark and the
XTREME multilingual benchmark show that NoisyTune can consistently improve the
performance of different PLMs in many downstream tasks."
2398,"with different percentage of samples on the MRPC                            To further study the impact of NoisyTune on
dataset.2 The results are shown in Fig.","We compare the accuracy
of BERT with and without NoisyTune ﬁnetuned                              PLMs on downstream tasks with limited data.",4.,2022-02-24 11:08:02+00:00,NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chuhan Wu'), arxiv.Result.Author('Fangzhao Wu'), arxiv.Result.Author('Tao Qi'), arxiv.Result.Author('Yongfeng Huang'), arxiv.Result.Author('Xing Xie')]","Effectively finetuning pretrained language models (PLMs) is critical for
their success in downstream tasks. However, PLMs may have risks in overfitting
the pretraining tasks and data, which usually have gap with the target
downstream tasks. Such gap may be difficult for existing PLM finetuning methods
to overcome and lead to suboptimal performance. In this paper, we propose a
very simple yet effective method named NoisyTune to help better finetune PLMs
on downstream tasks by adding some noise to the parameters of PLMs before
fine-tuning. More specifically, we propose a matrix-wise perturbing method
which adds different uniform noises to different parameter matrices based on
their standard deviations. In this way, the varied characteristics of different
types of parameters in PLMs can be considered. Extensive experiments on both
GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can
consistently empower the finetuning of different PLMs on different downstream
tasks."
2451,"Our new

                                        corpus, named JParaCrawl v3.0, will be publicly avail-

                                        able through our website1 for further researches.","We experimentally show how the new crawled

                                        corpus increases the accuracy of machine translation

                                        for English-Japanese and Japanese-English.","Our contributions can be summarized:

                                        • We constructed a large-scale English-Japanese

                                            1http://www.kecl.ntt.co.jp/icl/lirg/
                                        jparacrawl/
Version  # sentences         # words                        We ignored the data released before March 2019
                                                            because they were already analyzed by the pre-
v1.0      4, 817, 172  125, 216, 523                        vious JParaCrawl project and focused more on
v2.0      8, 809, 771  234, 393, 978                        the latest Common Crawl archive.",2022-02-25 10:52:00+00:00,JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,cs.CL,['cs.CL'],"[arxiv.Result.Author('Makoto Morishita'), arxiv.Result.Author('Chousa Katsuki'), arxiv.Result.Author('Jun Suzuki'), arxiv.Result.Author('Masaaki Nagata')]","Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes."
2452,will be available on our website for further research.,"The new JParaCrawl v3.0
and reported the BLEU scores (Papineni et al., 2002).","For the evaluations, we NFKC-normalized all the             We expect that JParaCrawl v3.0 will support future re-
test sets for consistency with our previous experi-         search and products.",2022-02-25 10:52:00+00:00,JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,cs.CL,['cs.CL'],"[arxiv.Result.Author('Makoto Morishita'), arxiv.Result.Author('Chousa Katsuki'), arxiv.Result.Author('Jun Suzuki'), arxiv.Result.Author('Masaaki Nagata')]","Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes."
2453,"Our new

                                        corpus, named JParaCrawl v3.0, will be publicly avail-

                                        able through our website1 for further researches.","We experimentally show how the new crawled

                                        corpus increases the accuracy of machine translation

                                        for English-Japanese and Japanese-English.","Our contributions can be summarized:

                                        • We constructed a large-scale English-Japanese

                                            1http://www.kecl.ntt.co.jp/icl/lirg/
                                        jparacrawl/
Version  # sentences         # words                        We ignored the data released before March 2019
                                                            because they were already analyzed by the pre-
v1.0      4, 817, 172  125, 216, 523                        vious JParaCrawl project and focused more on
v2.0      8, 809, 771  234, 393, 978                        the latest Common Crawl archive.",2022-02-25 10:52:00+00:00,JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,cs.CL,['cs.CL'],"[arxiv.Result.Author('Makoto Morishita'), arxiv.Result.Author('Katsuki Chousa'), arxiv.Result.Author('Jun Suzuki'), arxiv.Result.Author('Masaaki Nagata')]","Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes."
2454,will be available on our website for further research.,"The new JParaCrawl v3.0
and reported the BLEU scores (Papineni et al., 2002).","For the evaluations, we NFKC-normalized all the             We expect that JParaCrawl v3.0 will support future re-
test sets for consistency with our previous experi-         search and products.",2022-02-25 10:52:00+00:00,JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,cs.CL,['cs.CL'],"[arxiv.Result.Author('Makoto Morishita'), arxiv.Result.Author('Katsuki Chousa'), arxiv.Result.Author('Jun Suzuki'), arxiv.Result.Author('Masaaki Nagata')]","Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes."
2474,"Attributing improvements
to wrong reasons is inevitably going to slow down further research.",We see these unnoticed changes as highly undesirable.,"Such a mistake
in explanation effectively creates a dead-end path that many of us might pursue.",2022-02-25 16:44:06+00:00,The Reality of Multi-Lingual Machine Translation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tom Kocmi'), arxiv.Result.Author('Dominik Macháček'), arxiv.Result.Author('Ondřej Bojar')]","Our book ""The Reality of Multi-Lingual Machine Translation"" discusses the
benefits and perils of using more than two languages in machine translation
systems. While focused on the particular task of sequence-to-sequence
processing and multi-task learning, the book targets somewhat beyond the area
of natural language processing. Machine translation is for us a prime example
of deep learning applications where human skills and learning capabilities are
taken as a benchmark that many try to match and surpass. We document that some
of the gains observed in multi-lingual translation may result from simpler
effects than the assumed cross-lingual transfer of knowledge.
  In the first, rather general part, the book will lead you through the
motivation for multi-linguality, the versatility of deep neural networks
especially in sequence-to-sequence tasks to complications of this learning. We
conclude the general part with warnings against too optimistic and unjustified
explanations of the gains that neural networks demonstrate.
  In the second part, we fully delve into multi-lingual models, with a
particularly careful examination of transfer learning as one of the more
straightforward approaches utilizing additional languages. The recent
multi-lingual techniques, including massive models, are surveyed and practical
aspects of deploying systems for many languages are discussed. The conclusion
highlights the open problem of machine understanding and reminds of two ethical
aspects of building large-scale models: the inclusivity of research and its
ecological trace."
2475,"In conclusion, deep learning is energy-intensive, and further research is needed
to find the best ways to minimize the impact.",(2019).,"However, restrictions on research are
not the solution.",2022-02-25 16:44:06+00:00,The Reality of Multi-Lingual Machine Translation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tom Kocmi'), arxiv.Result.Author('Dominik Macháček'), arxiv.Result.Author('Ondřej Bojar')]","Our book ""The Reality of Multi-Lingual Machine Translation"" discusses the
benefits and perils of using more than two languages in machine translation
systems. While focused on the particular task of sequence-to-sequence
processing and multi-task learning, the book targets somewhat beyond the area
of natural language processing. Machine translation is for us a prime example
of deep learning applications where human skills and learning capabilities are
taken as a benchmark that many try to match and surpass. We document that some
of the gains observed in multi-lingual translation may result from simpler
effects than the assumed cross-lingual transfer of knowledge.
  In the first, rather general part, the book will lead you through the
motivation for multi-linguality, the versatility of deep neural networks
especially in sequence-to-sequence tasks to complications of this learning. We
conclude the general part with warnings against too optimistic and unjustified
explanations of the gains that neural networks demonstrate.
  In the second part, we fully delve into multi-lingual models, with a
particularly careful examination of transfer learning as one of the more
straightforward approaches utilizing additional languages. The recent
multi-lingual techniques, including massive models, are surveyed and practical
aspects of deploying systems for many languages are discussed. The conclusion
highlights the open problem of machine understanding and reminds of two ethical
aspects of building large-scale models: the inclusivity of research and its
ecological trace."
2552,"All of the above call for a real-life
                                        Liang et al., 2021a) mainly focus on investigating              multimodal bilingual conversational data resource
                                                                                                        that can encourage further research in chat transla-
                                             ∗Equal contribution.",1 (c)).,"Work was done when Yunlong were
                                        interning at Pattern Recognition Center, WeChat AI, Tencent
                                        Inc, China.",2022-02-28 09:40:46+00:00,MSCTD: A Multimodal Sentiment Chat Translation Dataset,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yunlong Liang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Jinan Xu'), arxiv.Result.Author('Yufeng Chen'), arxiv.Result.Author('Jie Zhou')]","Multimodal machine translation and textual chat translation have received
considerable attention in recent years. Although the conversation in its
natural form is usually multimodal, there still lacks work on multimodal
machine translation in conversations. In this work, we introduce a new task
named Multimodal Chat Translation (MCT), aiming to generate more accurate
translations with the help of the associated dialogue history and visual
context. To this end, we firstly construct a Multimodal Sentiment Chat
Translation Dataset (MSCTD) containing 142,871 English-Chinese utterance pairs
in 14,762 bilingual dialogues and 30,370 English-German utterance pairs in
3,079 bilingual dialogues. Each utterance pair, corresponding to the visual
context that reflects the current conversational scene, is annotated with a
sentiment label. Then, we benchmark the task by establishing multiple baseline
systems that incorporate multimodal and sentiment features for MCT. Preliminary
experiments on four language directions (English-Chinese and English-German)
verify the potential of contextual and multimodal information fusion and the
positive impact of sentiment on the MCT task. Additionally, as a by-product of
the MSCTD, it also provides two new benchmarks on multimodal dialogue sentiment
analysis. Our work can facilitate research on both multimodal chat translation
and multimodal dialogue sentiment analysis."
2583,"The release of this resource can help enable multi-
                                                      faceted research in names, including name transla-
   However, it can be difﬁcult to differentiate the   tion/transliteration and further research in named
use of nicknames from ordinary transliteration of     entity recognition and linking, especially in lower-
the full name, which may show the affects of phono-   resourced languages.",of Joseph.,logical adaptation or morphological simpliﬁcation.,2022-02-28 18:58:06+00:00,ParaNames: A Massively Multilingual Entity Name Corpus,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jonne Sälevä'), arxiv.Result.Author('Constantine Lignos')]","This preprint describes work in progress on ParaNames, a multilingual
parallel name resource consisting of names for approximately 14 million
entities. The included names span over 400 languages, and almost all entities
are mapped to standardized entity types (PER/LOC/ORG). Using Wikidata as a
source, we create the largest resource of this type to-date. We describe our
approach to filtering and standardizing the data to provide the best quality
possible. ParaNames is useful for multilingual language processing, both in
defining tasks for name translation/transliteration and as supplementary data
for tasks such as named entity recognition and linking. Our resource is
released on GitHub (https://github.com/bltlab/paranames) under a Creative
Commons license (CC BY 4.0)."
2584,"Transliterating from all lan-
tion/transliteration and further research in named        guages.",2010.,"In Proceedings of the 9th Conference of
entity recognition and linking, especially in lower-      the Association for Machine Translation in the
resourced languages.",2022-02-28 18:58:06+00:00,ParaNames: A Massively Multilingual Entity Name Corpus,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jonne Sälevä'), arxiv.Result.Author('Constantine Lignos')]","This preprint describes work in progress on ParaNames, a multilingual
parallel name resource consisting of names for approximately 14 million
entities. The included names span over 400 languages, and almost all entities
are mapped to standardized entity types (PER/LOC/ORG). Using Wikidata as a
source, we create the largest resource of this type to-date. We describe our
approach to filtering and standardizing the data to provide the best quality
possible. ParaNames is useful for multilingual language processing, both in
defining tasks for name translation/transliteration and as supplementary data
for tasks such as named entity recognition and linking. We demonstrate an
application of ParaNames by training a multilingual model for canonical name
translation to and from English. Our resource is released at
\url{https://github.com/bltlab/paranames} under a Creative Commons license (CC
BY 4.0)."
2645,"or further research into efﬁcient methods for proba-
bility estimation.","are needed requires accepting more computation,                                    Association for Computational Linguistics.","Taken more broadly, our ﬁndings                               Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
emphasize the ongoing importance of ﬁnding better                                   togi, Raghav Gupta, Christopher D. Manning,
and scalable ways to encourage language models                                      and Christopher Potts.",2022-03-01 17:22:31+00:00,Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale,cs.CL,['cs.CL'],"[arxiv.Result.Author('Laurent Sartran'), arxiv.Result.Author('Samuel Barrett'), arxiv.Result.Author('Adhiguna Kuncoro'), arxiv.Result.Author('Miloš Stanojević'), arxiv.Result.Author('Phil Blunsom'), arxiv.Result.Author('Chris Dyer')]","We introduce Transformer Grammars (TGs), a novel class of Transformer
language models that combine (i) the expressive power, scalability, and strong
performance of Transformers and (ii) recursive syntactic compositions, which
here are implemented through a special attention mask and deterministic
transformation of the linearized tree. We find that TGs outperform various
strong baselines on sentence-level language modeling perplexity, as well as on
multiple syntax-sensitive language modeling evaluation metrics. Additionally,
we find that the recursive syntactic composition bottleneck which represents
each sentence as a single vector harms perplexity on document-level language
modeling, providing evidence that a different kind of memory mechanism -- one
that is independent of composed syntactic representations -- plays an important
role in current successful models of long text."
2656,"Our WOW++ dataset is          tion that only a single utterance from a knowledge
                                            open-sourced to further research in the area of  source is relevant.","These datasets suffer from a number of lim-
                                            evaluation and extrinsic measures of response    itations including the fact that they either do not ex-
                                            quality, showing that neural rerankers that use  plicitly annotate relevant knowledge sentences per
                                            WOW++ can outperform rankers trained on          turn in the conversation or make the strict assump-
                                            standard datasets.","Without sufﬁcient information
                                            knowledge selection.",2022-03-01 22:07:05+00:00,Multi-Sentence Knowledge Selection in Open-Domain Dialogue,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mihail Eric'), arxiv.Result.Author('Nicole Chartier'), arxiv.Result.Author('Behnam Hedayatnia'), arxiv.Result.Author('Karthik Gopalakrishnan'), arxiv.Result.Author('Pankaj Rajan'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Dilek Hakkani-Tur')]","Incorporating external knowledge sources effectively in conversations is a
longstanding problem in open-domain dialogue research. The existing literature
on open-domain knowledge selection is limited and makes certain brittle
assumptions on knowledge sources to simplify the overall task (Dinan et al.,
2019), such as the existence of a single relevant knowledge sentence per
context. In this work, we evaluate the existing state of open-domain
conversation knowledge selection, showing where the existing methodologies
regarding data and evaluation are flawed. We then improve on them by proposing
a new framework for collecting relevant knowledge, and create an augmented
dataset based on the Wizard of Wikipedia (WOW) corpus, which we call WOW++.
WOW++ averages 8 relevant knowledge sentences per dialogue context, embracing
the inherent ambiguity of open-domain dialogue knowledge selection. We then
benchmark various knowledge ranking algorithms on this augmented dataset with
both intrinsic evaluation and extrinsic measures of response quality, showing
that neural rerankers that use WOW++ can outperform rankers trained on standard
datasets."
2684,"To facilitate
further research on the topic, our code is publicly                  function from non-terminals to natural num-
available online.1                                                   bers; each non-terminal N is encoding a tuple
                                                                     of strings of ﬁxed arity d(N) and the maximal
2 Background                                                         arity of N decides the grammar’s multiplicity
                                                                  • C is a mapping that associates each non-
2.1 Context freeness of natural languages                            terminal N to a (possibly empty) set of el-
                                                                     ements from the d(N)-ary cartesian product
There has been a long debate, since the introduction                (A∗)d(N); put simply, the set of constants CN
of the Chomsky hierarchy (Chomsky, 1956), on                         prescribes all the possible ways of initializing
whether all string patterns in natural language can                  the non-terminal N
be encompassed by the class of context-free gram-                 • R a set of rewriting rules; rules are functions
mars.","Our              An m-multiple MCFG can be thought of as a tuple
experimental results convey a rapidly declining per-            A, N , d, C, R, S0 , where:
formance in the presence of discontinuous syntax,
suggesting that the Dutch models investigated do                  • A is the terminal alphabet
not automatically learn to resolve the complex de-                • N is a set of non-terminals and d : N → N a
pendencies occurring in the language.","The dispute often makes a distinction be-                      N × · · · × N → N that provide recipes on
tween weak and strong context-freeness, whereby                      how to combine a number of non-terminals
the question shifts between generating all strings                   into a single non-terminal by rearranging and
or all constituent expressions of a language.",2022-03-02 12:30:21+00:00,Discontinuous Constituency and BERT: A Case Study of Dutch,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Konstantinos Kogkalidis'), arxiv.Result.Author('Gijs Winholds')]","In this paper, we set out to quantify the syntactic capacity of BERT in the
evaluation regime of non-context free patterns, as occurring in Dutch. We
devise a test suite based on a mildly context-sensitive formalism, from which
we derive grammars that capture the linguistic phenomena of control verb
nesting and verb raising. The grammars, paired with a small lexicon, provide us
with a large collection of naturalistic utterances, annotated with verb-subject
pairings, that serve as the evaluation test bed for an attention-based span
selection probe. Our results, backed by extensive analysis, suggest that the
models investigated fail in the implicit acquisition of the dependencies
examined."
2685,"To facilitate
further research on the topic, our code is publicly                  function from non-terminals to natural num-
available online.1                                                   bers; each non-terminal N is encoding a tuple
                                                                     of strings of ﬁxed arity d(N) and the maximal
2 Background                                                         arity of N decides the grammar’s multiplicity
                                                                  • C is a mapping that associates each non-
2.1 Context freeness of natural languages                            terminal N to a (possibly empty) set of el-
                                                                     ements from the d(N)-ary cartesian product
There has been a long debate, since the introduction                (A∗)d(N); put simply, the set of constants CN
of the Chomsky hierarchy (Chomsky, 1956), on                         prescribes all the possible ways of initializing
whether all string patterns in natural language can                  the non-terminal N
be encompassed by the class of context-free gram-                 • R a set of rewriting rules; rules are functions
mars.","Our              An m-multiple MCFG can be thought of as a tuple
experimental results convey a rapidly declining per-            A, N , d, C, R, S0 , where:
formance in the presence of discontinuous syntax,
suggesting that the Dutch models investigated do                  • A is the terminal alphabet
not automatically learn to resolve the complex de-                • N is a set of non-terminals and d : N → N a
pendencies occurring in the language.","The dispute often makes a distinction be-                      N × · · · × N → N that provide recipes on
tween weak and strong context-freeness, whereby                      how to combine a number of non-terminals
the question shifts between generating all strings                   into a single non-terminal by rearranging and
or all constituent expressions of a language.",2022-03-02 12:30:21+00:00,Discontinuous Constituency and BERT: A Case Study of Dutch,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Konstantinos Kogkalidis'), arxiv.Result.Author('Gijs Wijnholds')]","In this paper, we set out to quantify the syntactic capacity of BERT in the
evaluation regime of non-context free patterns, as occurring in Dutch. We
devise a test suite based on a mildly context-sensitive formalism, from which
we derive grammars that capture the linguistic phenomena of control verb
nesting and verb raising. The grammars, paired with a small lexicon, provide us
with a large collection of naturalistic utterances, annotated with verb-subject
pairings, that serve as the evaluation test bed for an attention-based span
selection probe. Our results, backed by extensive analysis, suggest that the
models investigated fail in the implicit acquisition of the dependencies
examined."
2751,"To encourage further research                 1000         clean
in this domain, we release a benchmark for word-
level adversarial example detection on four attack              500
methods across four NLP models and four text clas-
siﬁcation datasets.","tain original semantics and remain unsuspicious to
human inspectors.","We also propose a simple but                0            logpθ(z)
effective detection method that utilizes density es-
timation in the feature space as shown in Fig.",2022-03-03 12:32:59+00:00,Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation,cs.CL,"['cs.CL', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('KiYoon Yoo'), arxiv.Result.Author('Jangho Kim'), arxiv.Result.Author('Jiho Jang'), arxiv.Result.Author('Nojun Kwak')]","Word-level adversarial attacks have shown success in NLP models, drastically
decreasing the performance of transformer-based models in recent years. As a
countermeasure, adversarial defense has been explored, but relatively few
efforts have been made to detect adversarial examples. However, detecting
adversarial examples may be crucial for automated tasks (e.g. review sentiment
analysis) that wish to amass information about a certain population and
additionally be a step towards a robust defense system. To this end, we release
a dataset for four popular attack methods on four datasets and four models to
encourage further research in this field. Along with it, we propose a
competitive baseline based on density estimation that has the highest AUC on 29
out of 30 dataset-attack-model combinations. Source code is available in
https://github.com/anoymous92874838/text-adv-detection."
2752,"Last, we dis-
for robust covariance estimation (Friedman et al.,             cuss more realistic scenarios for further study and
2008; Ledoit and Wolf, 2004).","A principled            sary with partial/full knowledge of the detection
way of removing outliers for parameter estimation              method (§4.6) and (ii) conduct experiments on a
has been an important research area in multivariate            character level attack (Appendix A.9) to demon-
statistics and various methods have been developed             strate the applicability of our method.","Among them, Mini-                conduct hyper-parameter and qualitative analysis
mum Covariance Determinant (Rousseeuw, 1984,                   (§4.7).",2022-03-03 12:32:59+00:00,Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation,cs.CL,"['cs.CL', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('KiYoon Yoo'), arxiv.Result.Author('Jangho Kim'), arxiv.Result.Author('Jiho Jang'), arxiv.Result.Author('Nojun Kwak')]","Word-level adversarial attacks have shown success in NLP models, drastically
decreasing the performance of transformer-based models in recent years. As a
countermeasure, adversarial defense has been explored, but relatively few
efforts have been made to detect adversarial examples. However, detecting
adversarial examples may be crucial for automated tasks (e.g. review sentiment
analysis) that wish to amass information about a certain population and
additionally be a step towards a robust defense system. To this end, we release
a dataset for four popular attack methods on four datasets and four models to
encourage further research in this field. Along with it, we propose a
competitive baseline based on density estimation that has the highest AUC on 29
out of 30 dataset-attack-model combinations. Source code is available in
https://github.com/anoymous92874838/text-adv-detection."
2785,"The application of these models has been so extensive that it may
even prompt questions regarding the need for further research on personality structure, especially
at the highest levels of parsimony (i.e., just a few dimensions).","The personality literature is replete with research
findings about the extent to which the broad-bandwidth variables that comprise these models
affect many domains of life, including features of self-concept and identity; one’s relationships
with peers, friends, and romantic partners; mental health, including well-being and the
experience of psychopathology; factors affecting physical health and longevity; choices relating
to occupational and educational trajectories (as well as performance in those domains); and the
extent and manner of engagement with broader social issues and institutions (Ozer & Benet-
Martinez, 2006; Soto, 2019).","Yet, several recent contributions have highlighted unresolved issues.",2022-03-04 02:06:10+00:00,Deep Lexical Hypothesis: Identifying personality structure in natural language,cs.CL,['cs.CL'],"[arxiv.Result.Author('Andrew Cutler'), arxiv.Result.Author('David M. Condon')]","Recent advances in natural language processing (NLP) have produced general
models that can perform complex tasks such as summarizing long passages and
translating across languages. Here, we introduce a method to extract adjective
similarities from language models as done with survey-based ratings in
traditional psycholexical studies but using millions of times more text in a
natural setting. The correlational structure produced through this method is
highly similar to that of self- and other-ratings of 435 terms reported by
Saucier and Goldberg (1996a). The first three unrotated factors produced using
NLP are congruent with those in survey data, with coefficients of 0.89, 0.79,
and 0.79. This structure is robust to many modeling decisions: adjective set,
including those with 1,710 terms (Goldberg, 1982) and 18,000 terms (Allport &
Odbert, 1936); the query used to extract correlations; and language model.
Notably, Neuroticism and Openness are only weakly and inconsistently recovered.
This is a new source of signal that is closer to the original (semantic) vision
of the Lexical Hypothesis. The method can be applied where surveys cannot: in
dozens of languages simultaneously, with tens of thousands of items, on
historical text, and at extremely large scale for little cost. The code is made
public to facilitate reproduction and fast iteration in new directions of
research."
2790,"A similar difference, although lower, appeared also in mimetic ex-
pressions, which could be included in further study on enlarging lex-      dislike               49 dislike             56
icon for the machine learning system.",extr.,"As for interjections and excla-      joy                   32 fondness            49
mations, although they appeared in a large number in both datasets,
more of them appeared in non-harmful entries.",2022-03-04 03:13:45+00:00,In the Service of Online Order: Tackling Cyber-Bullying with Machine Learning and Affect Analysis,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Michal Ptaszynski'), arxiv.Result.Author('Pawel Dybala'), arxiv.Result.Author('Tatsuaki Matsuba'), arxiv.Result.Author('Fumito Masui'), arxiv.Result.Author('Rafal Rzepka'), arxiv.Result.Author('Kenji Araki'), arxiv.Result.Author('Yoshio Momouchi')]","One of the burning problems lately in Japan has been cyber-bullying, or
slandering and bullying people online. The problem has been especially noticed
on unofficial Web sites of Japanese schools. Volunteers consisting of school
personnel and PTA (Parent-Teacher Association) members have started Online
Patrol to spot malicious contents within Web forums and blogs. In practise,
Online Patrol assumes reading through the whole Web contents, which is a task
difficult to perform manually. With this paper we introduce a research intended
to help PTA members perform Online Patrol more efficiently. We aim to develop a
set of tools that can automatically detect malicious entries and report them to
PTA members. First, we collected cyber-bullying data from unofficial school Web
sites. Then we performed analysis of this data in two ways. Firstly, we
analysed the entries with a multifaceted affect analysis system in order to
find distinctive features for cyber-bullying and apply them to a machine
learning classifier. Secondly, we applied a SVM based machine learning method
to train a classifier for detection of cyber-bullying. The system was able to
classify cyber-bullying entries with 88.2% of balanced F-score."
2793,"This is
an interesting area for further research.","Our proposal for mitigating the alignment tax, by incorporating pretraining data into RLHF ﬁne-
tuning, does not completely mitigate performance regressions, and may make certain undesirable
behaviors more likely for some tasks (if these behaviors are present in the pretraining data).","Another modiﬁcation that would likely improve our method
is to ﬁlter the pretraining mix data for toxic content (Ngo et al., 2021), or augment this data with
synthetic instructions.",2022-03-04 07:04:42+00:00,Training language models to follow instructions with human feedback,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Long Ouyang'), arxiv.Result.Author('Jeff Wu'), arxiv.Result.Author('Xu Jiang'), arxiv.Result.Author('Diogo Almeida'), arxiv.Result.Author('Carroll L. Wainwright'), arxiv.Result.Author('Pamela Mishkin'), arxiv.Result.Author('Chong Zhang'), arxiv.Result.Author('Sandhini Agarwal'), arxiv.Result.Author('Katarina Slama'), arxiv.Result.Author('Alex Ray'), arxiv.Result.Author('John Schulman'), arxiv.Result.Author('Jacob Hilton'), arxiv.Result.Author('Fraser Kelton'), arxiv.Result.Author('Luke Miller'), arxiv.Result.Author('Maddie Simens'), arxiv.Result.Author('Amanda Askell'), arxiv.Result.Author('Peter Welinder'), arxiv.Result.Author('Paul Christiano'), arxiv.Result.Author('Jan Leike'), arxiv.Result.Author('Ryan Lowe')]","Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent."
2870,"It also provides extra reference infor-
used to link key clues and supplement them into a complete               mation for further research.","In this process, key clues are used to lock               [14] phenomenon for the evaluation of cross-lingual
all the important points of articles and article graphs are              summarization.",and ﬂuent summary.,2022-03-05 18:01:11+00:00,CptGraphSum: Let key clues guide the cross-lingual abstractive summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shuyu Jiang'), arxiv.Result.Author('Dengbiao Tu'), arxiv.Result.Author('Xingshu Chen'), arxiv.Result.Author('Rui Tang'), arxiv.Result.Author('Wenxian Wang'), arxiv.Result.Author('Haizhou Wang')]","Cross-Lingual Summarization (CLS) is the task to generate a summary in one
language for an article in a different language. Previous studies on CLS mainly
take pipeline methods or train the end-to-end model using the translated
parallel data. However, the quality of generated cross-lingual summaries needs
more further efforts to improve, and the model performance has never been
evaluated on the hand-written CLS dataset. Therefore, we first propose a
clue-guided cross-lingual abstractive summarization method to improve the
quality of cross-lingual summaries, and then construct a novel hand-written CLS
dataset for evaluation. Specifically, we extract keywords, named entities, etc.
of the input article as key clues for summarization and then design a
clue-guided algorithm to transform an article into a graph with less noisy
sentences. One Graph encoder is built to learn sentence semantics and article
structures and one Clue encoder is built to encode and translate key clues,
ensuring the information of important parts are reserved in the generated
summary. These two encoders are connected by one decoder to directly learn
cross-lingual semantics. Experimental results show that our method has stronger
robustness for longer inputs and substantially improves the performance over
the strong baseline, achieving an improvement of 8.55 ROUGE-1
(English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English
summarization) scores over the existing SOTA."
2871,"It also provides extra reference infor-
used to link key clues and supplement them into a complete               mation for further research.","In this process, key clues are used to lock               [14] phenomenon for the evaluation of cross-lingual
all the important points of articles and article graphs are              summarization.",and ﬂuent summary.,2022-03-05 18:01:11+00:00,ClueGraphSum: Let Key Clues Guide the Cross-Lingual Abstractive Summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shuyu Jiang'), arxiv.Result.Author('Dengbiao Tu'), arxiv.Result.Author('Xingshu Chen'), arxiv.Result.Author('Rui Tang'), arxiv.Result.Author('Wenxian Wang'), arxiv.Result.Author('Haizhou Wang')]","Cross-Lingual Summarization (CLS) is the task to generate a summary in one
language for an article in a different language. Previous studies on CLS mainly
take pipeline methods or train the end-to-end model using the translated
parallel data. However, the quality of generated cross-lingual summaries needs
more further efforts to improve, and the model performance has never been
evaluated on the hand-written CLS dataset. Therefore, we first propose a
clue-guided cross-lingual abstractive summarization method to improve the
quality of cross-lingual summaries, and then construct a novel hand-written CLS
dataset for evaluation. Specifically, we extract keywords, named entities, etc.
of the input article as key clues for summarization and then design a
clue-guided algorithm to transform an article into a graph with less noisy
sentences. One Graph encoder is built to learn sentence semantics and article
structures and one Clue encoder is built to encode and translate key clues,
ensuring the information of important parts are reserved in the generated
summary. These two encoders are connected by one decoder to directly learn
cross-lingual semantics. Experimental results show that our method has stronger
robustness for longer inputs and substantially improves the performance over
the strong baseline, achieving an improvement of 8.55 ROUGE-1
(English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English
summarization) scores over the existing SOTA."
3086,"To further study the capability of the proposed
                                                                  OneRel in handling complex scenarios, we split the test set
tion function.","[; ] is     NYT∗ and WebNLG∗, and the second version as NYT and
the concatenation operation and φ(·) is the ReLU activa-          WebNLG.",The new deﬁnition brings the following ben-        by overlapping patterns and triple number.,2022-03-10 15:09:59+00:00,OneRel:Joint Entity and Relation Extraction with One Module in One Step,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yu-Ming Shang'), arxiv.Result.Author('Heyan Huang'), arxiv.Result.Author('Xian-Ling Mao')]","Joint entity and relation extraction is an essential task in natural language
processing and knowledge graph construction. Existing approaches usually
decompose the joint extraction task into several basic modules or processing
steps to make it easy to conduct. However, such a paradigm ignores the fact
that the three elements of a triple are interdependent and indivisible.
Therefore, previous joint methods suffer from the problems of cascading errors
and redundant information. To address these issues, in this paper, we propose a
novel joint entity and relation extraction model, named OneRel, which casts
joint extraction as a fine-grained triple classification problem. Specifically,
our model consists of a scoring-based classifier and a relation-specific horns
tagging strategy. The former evaluates whether a token pair and a relation
belong to a factual triple. The latter ensures a simple but effective decoding
process. Extensive experimental results on two widely used datasets demonstrate
that the proposed method performs better than the state-of-the-art baselines,
and delivers consistent performance gain on complex scenarios of various
overlapping patterns and multiple triples."
3087,"To further study the capability of the proposed
                                                                  OneRel in handling complex scenarios, we split the test set
tion function.","[; ] is     NYT∗ and WebNLG∗, and the second version as NYT and
the concatenation operation and φ(·) is the ReLU activa-          WebNLG.",The new deﬁnition brings the following ben-        by overlapping patterns and triple number.,2022-03-10 15:09:59+00:00,OneRel:Joint Entity and Relation Extraction with One Module in One Step,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yu-Ming Shang'), arxiv.Result.Author('Heyan Huang'), arxiv.Result.Author('Xian-Ling Mao')]","Joint entity and relation extraction is an essential task in natural language
processing and knowledge graph construction. Existing approaches usually
decompose the joint extraction task into several basic modules or processing
steps to make it easy to conduct. However, such a paradigm ignores the fact
that the three elements of a triple are interdependent and indivisible.
Therefore, previous joint methods suffer from the problems of cascading errors
and redundant information. To address these issues, in this paper, we propose a
novel joint entity and relation extraction model, named OneRel, which casts
joint extraction as a fine-grained triple classification problem. Specifically,
our model consists of a scoring-based classifier and a relation-specific horns
tagging strategy. The former evaluates whether a token pair and a relation
belong to a factual triple. The latter ensures a simple but effective decoding
process. Extensive experimental results on two widely used datasets demonstrate
that the proposed method performs better than the state-of-the-art baselines,
and delivers consistent performance gain on complex scenarios of various
overlapping patterns and multiple triples."
3129,"In the             der I. Rudnicky, Jason Williams, Joelle Pineau,
future, we will further study the possibility of us-         Mikhail S. Burtsev, and Jason Weston.","Experiment results show that our sys-          Emily Dinan, Varvara Logacheva, Valentin Malykh,
tem PLATO-LTM can make effective use of both                 Alexander H. Miller, Kurt Shuster, Jack Urbanek,
parties’ persona information from dialogue history           Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
to enhance dialogue consistency and engagingness             Lowe, Shrimai Prabhumoye, Alan W. Black, Alexan-
when conducting a long-term conversation.",2019.,2022-03-11 08:41:14+00:00,Long Time No See! Open-Domain Conversation with Long-Term Persona Memory,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xinchao Xu'), arxiv.Result.Author('Zhibin Gou'), arxiv.Result.Author('Wenquan Wu'), arxiv.Result.Author('Zheng-Yu Niu'), arxiv.Result.Author('Hua Wu'), arxiv.Result.Author('Haifeng Wang'), arxiv.Result.Author('Shihang Wang')]","Most of the open-domain dialogue models tend to perform poorly in the setting
of long-term human-bot conversations. The possible reason is that they lack the
capability of understanding and memorizing long-term dialogue history
information. To address this issue, we present a novel task of Long-term Memory
Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a
dialogue generation framework with Long-Term Memory (LTM) mechanism (called
PLATO-LTM). This LTM mechanism enables our system to accurately extract and
continuously update long-term persona memory without requiring multiple-session
dialogue datasets for model training. To our knowledge, this is the first
attempt to conduct real-time dynamic management of persona information of both
parties, including the user and the bot. Results on DuLeMon indicate that
PLATO-LTM can significantly outperform baselines in terms of long-term dialogue
consistency, leading to better dialogue engagingness."
3130,"In the             der I. Rudnicky, Jason Williams, Joelle Pineau,
future, we will further study the possibility of us-         Mikhail S. Burtsev, and Jason Weston.","Experiment results show that our sys-          Emily Dinan, Varvara Logacheva, Valentin Malykh,
tem PLATO-LTM can make effective use of both                 Alexander H. Miller, Kurt Shuster, Jack Urbanek,
parties’ persona information from dialogue history           Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
to enhance dialogue consistency and engagingness             Lowe, Shrimai Prabhumoye, Alan W. Black, Alexan-
when conducting a long-term conversation.",2019.,2022-03-11 08:41:14+00:00,Long Time No See! Open-Domain Conversation with Long-Term Persona Memory,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xinchao Xu'), arxiv.Result.Author('Zhibin Gou'), arxiv.Result.Author('Wenquan Wu'), arxiv.Result.Author('Zheng-Yu Niu'), arxiv.Result.Author('Hua Wu'), arxiv.Result.Author('Haifeng Wang'), arxiv.Result.Author('Shihang Wang')]","Most of the open-domain dialogue models tend to perform poorly in the setting
of long-term human-bot conversations. The possible reason is that they lack the
capability of understanding and memorizing long-term dialogue history
information. To address this issue, we present a novel task of Long-term Memory
Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a
dialogue generation framework with Long-Term Memory (LTM) mechanism (called
PLATO-LTM). This LTM mechanism enables our system to accurately extract and
continuously update long-term persona memory without requiring multiple-session
dialogue datasets for model training. To our knowledge, this is the first
attempt to conduct real-time dynamic management of persona information of both
parties, including the user and the bot. Results on DuLeMon indicate that
PLATO-LTM can significantly outperform baselines in terms of long-term dialogue
consistency, leading to better dialogue engagingness."
3209,"We can observe that:
                                                            First, when ﬁxing the prompt length, increasing
   Based on the observation above, we further study         the model size improves the Avg.",JGA and Table 5 shows FWT.,"JGA as well
that whether seeing more preceding tasks further            as the generalization ability measured by FWT in
helps CLInit.",2022-03-13 13:22:41+00:00,Continual Prompt Tuning for Dialog State Tracking,cs.CL,['cs.CL'],"[arxiv.Result.Author('Qi Zhu'), arxiv.Result.Author('Bing Li'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Xiaoyan Zhu'), arxiv.Result.Author('Minlie Huang')]","A desirable dialog system should be able to continually learn new skills
without forgetting old ones, and thereby adapt to new domains or tasks in its
life cycle. However, continually training a model often leads to a well-known
catastrophic forgetting issue. In this paper, we present Continual Prompt
Tuning, a parameter-efficient framework that not only avoids forgetting but
also enables knowledge transfer between tasks. To avoid forgetting, we only
learn and store a few prompt tokens' embeddings for each task while freezing
the backbone pre-trained model. To achieve bi-directional knowledge transfer
among tasks, we propose several techniques (continual prompt initialization,
query fusion, and memory replay) to transfer knowledge from preceding tasks and
a memory-guided technique to transfer knowledge from subsequent tasks.
Extensive experiments demonstrate the effectiveness and efficiency of our
proposed method on continual learning for dialog state tracking, compared with
state-of-the-art baselines."
3215,"We make our code and the SCINLI dataset
decrease when only the second sentence is used as     available to further research in scientiﬁc NLI.","improve the models’ ability to understand scientiﬁc
                                                      text.",input.,2022-03-13 18:23:37+00:00,SciNLI: A Corpus for Natural Language Inference on Scientific Text,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mobashir Sadat'), arxiv.Result.Author('Cornelia Caragea')]","Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23 showing
that there is substantial room for improvement."
3216,"We make our code and the SCINLI dataset
decrease when only the second sentence is used as     available to further research in scientiﬁc NLI.","improve the models’ ability to understand scientiﬁc
                                                      text.",input.,2022-03-13 18:23:37+00:00,SciNLI: A Corpus for Natural Language Inference on Scientific Text,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mobashir Sadat'), arxiv.Result.Author('Cornelia Caragea')]","Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%
showing that there is substantial room for improvement."
3223,"BLEUÒ        WERÓ         SI-SDRiÒ       We have open-sourced all the codes1 and re-
100%
                          2.32          63.58        9.23  leased a challenge3 to encourage further research
   FBANK                  5.66          58.49       10.19  of SSL in speech.","5.3.2 Training data size

To study the effect of data size, we create 3 pseudo
datasets per task by sub-sampling 10%, 5% and
Upstream               ST       OOD-ASR            SS      techniques in development.","We welcome the community
   TERA                   4.82          62.54       10.40  to participate and advance the research frontier to-
   Modiﬁed CPC           14.81          46.95              gether.",2022-03-14 04:26:40+00:00,SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Hsiang-Sheng Tsai'), arxiv.Result.Author('Heng-Jui Chang'), arxiv.Result.Author('Wen-Chin Huang'), arxiv.Result.Author('Zili Huang'), arxiv.Result.Author('Kushal Lakhotia'), arxiv.Result.Author('Shu-wen Yang'), arxiv.Result.Author('Shuyan Dong'), arxiv.Result.Author('Andy T. Liu'), arxiv.Result.Author('Cheng-I Jeff Lai'), arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Xuankai Chang'), arxiv.Result.Author('Phil Hall'), arxiv.Result.Author('Hsuan-Jui Chen'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Hung-yi Lee')]","Transfer learning has proven to be crucial in advancing the state of speech
and natural language processing research in recent years. In speech, a model
pre-trained by self-supervised learning transfers remarkably well on multiple
tasks. However, the lack of a consistent evaluation methodology is limiting
towards a holistic understanding of the efficacy of such models. SUPERB was a
step towards introducing a common benchmark to evaluate pre-trained models
across various speech tasks. In this paper, we introduce SUPERB-SG, a new
benchmark focused on evaluating the semantic and generative capabilities of
pre-trained models by increasing task diversity and difficulty over SUPERB. We
use a lightweight methodology to test the robustness of representations learned
by pre-trained models under shifts in data domain and quality across different
types of tasks. It entails freezing pre-trained model parameters, only using
simple task-specific trainable heads. The goal is to be inclusive of all
researchers, and encourage efficient use of computational resources. We also
show that the task diversity of SUPERB-SG coupled with limited task supervision
is an effective recipe for evaluating the generalizability of model
representation."
3229,"But as mentioned in §3.1: ADDITION, PT is the easiest method to implement and

                                                                                         20
5.1 Performance, Convergence and Efﬁciency

it is desirable to theoretically and empirically further study the convergence issue across different sizes of
PLMs.","Since PT lags far behind other tuning methods in both convergence rate and performance, we do not visualize
it in the above ﬁgures.","We also found empirically that, (1) for each delta tuning method, within a reasonably broad range,
both performance and convergence are not sensitive to the number of tunable parameters, but more sensitive to
the structures of the methods, and (2) with the scale of PLM growing larger, the convergence of delta tuning is
also accelerated (§5.3: SCALE).",2022-03-14 07:56:32+00:00,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Guang Yang'), arxiv.Result.Author('Fuchao Wei'), arxiv.Result.Author('Zonghan Yang'), arxiv.Result.Author('Yusheng Su'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yulin Chen'), arxiv.Result.Author('Chi-Min Chan'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Jing Yi'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Hai-Tao Zheng'), arxiv.Result.Author('Jianfei Chen'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Jie Tang'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Maosong Sun')]","Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning."
3230,"Language is a carrier of human views, so the prejudice and discrimination that exist in human society can
easily be mapped onto language, and how to alleviate such challenges of fairness is a question that is well
worth further study.","This is because PLMs
are pre-trained with large-scale realistic corpora, and these pre-trained data are likely to contain inherent bias.","Efforts could be made in two ways, ﬁrst, directly by normalizing the training corpus to
remove as many potential biases as possible, and second, by modifying model representations or outputs to
reduce the risks.",2022-03-14 07:56:32+00:00,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Guang Yang'), arxiv.Result.Author('Fuchao Wei'), arxiv.Result.Author('Zonghan Yang'), arxiv.Result.Author('Yusheng Su'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yulin Chen'), arxiv.Result.Author('Chi-Min Chan'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Jing Yi'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Hai-Tao Zheng'), arxiv.Result.Author('Jianfei Chen'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Jie Tang'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Maosong Sun')]","Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning."
3231,"But as mentioned in §3.1: ADDITION, PT is the easiest method to implement and
it is desirable to theoretically and empirically further study the convergence issue across different sizes of
PLMs.","Since PT lags far behind other tuning methods in both convergence rate and performance, we do not visualize
it in the above ﬁgures.","We also found empirically that, (1) for each delta tuning method, within a reasonably broad range,
both performance and convergence are not sensitive to the number of tunable parameters, but more sensitive to
the structures of the methods, and (2) with the scale of PLM growing larger, the convergence of delta tuning is
also accelerated (§5.3: SCALE).",2022-03-14 07:56:32+00:00,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Guang Yang'), arxiv.Result.Author('Fuchao Wei'), arxiv.Result.Author('Zonghan Yang'), arxiv.Result.Author('Yusheng Su'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yulin Chen'), arxiv.Result.Author('Chi-Min Chan'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Jing Yi'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Hai-Tao Zheng'), arxiv.Result.Author('Jianfei Chen'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Jie Tang'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Maosong Sun')]","Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning."
3232,"Language is a carrier of human views, so the prejudice and discrimination that exist in human society can
easily be mapped onto language, and how to alleviate such challenges of fairness is a question that is well
worth further study.","This is because PLMs
are pre-trained with large-scale realistic corpora, and these pre-trained data are likely to contain inherent bias.","Efforts could be made in two ways, ﬁrst, directly by normalizing the training corpus to
remove as many potential biases as possible, and second, by modifying model representations or outputs to
reduce the risks.",2022-03-14 07:56:32+00:00,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Guang Yang'), arxiv.Result.Author('Fuchao Wei'), arxiv.Result.Author('Zonghan Yang'), arxiv.Result.Author('Yusheng Su'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yulin Chen'), arxiv.Result.Author('Chi-Min Chan'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Jing Yi'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Hai-Tao Zheng'), arxiv.Result.Author('Jianfei Chen'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Jie Tang'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Maosong Sun')]","Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning."
3236,"We make it publicly
mation using the ensemble-based uncertainty decom-             available along with an interpreter that can exe-
position for each program token generated by the               cute programs so others can further research on
EHR-QA model.","Also, we propose a method for measuring the           • We generated a dataset to solve the problem
ambiguity of input questions with insuﬃcient infor-            without gold programs.","We empirically demonstrate the                   EHR-QA using this NLQ2Program model in the
eﬀectiveness of using uncertainty decomposition to             future.",2022-03-14 08:12:16+00:00,Uncertainty-Aware Text-to-Program for Question Answering on Structured Electronic Health Records,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Daeyoung Kim'), arxiv.Result.Author('Seongsu Bae'), arxiv.Result.Author('Seungho Kim'), arxiv.Result.Author('Edward Choi')]","Question Answering on Electronic Health Records (EHR-QA) has a significant
impact on the healthcare domain, and it is being actively studied. Previous
research on structured EHR-QA focuses on converting natural language queries
into query language such as SQL or SPARQL (NLQ2Query), so the problem scope is
limited to pre-defined data types by the specific query language. In order to
expand the EHR-QA task beyond this limitation to handle multi-modal medical
data and solve complex inference in the future, more primitive systemic
language is needed. In this paper, we design the program-based model
(NLQ2Program) for EHR-QA as the first step towards the future direction. We
tackle MIMICSPARQL*, the graph-based EHR-QA dataset, via a program-based
approach in a semi-supervised manner in order to overcome the absence of gold
programs. Without the gold program, our proposed model shows comparable
performance to the previous state-of-the-art model, which is an NLQ2Query model
(0.9\% gain). In addition, for a reliable EHR-QA model, we apply the
uncertainty decomposition method to measure the ambiguity in the input
question. We empirically confirmed data uncertainty is most indicative of the
ambiguity in the input question."
3237,"We make it publicly
mation using the ensemble-based uncertainty decom-             available along with an interpreter that can exe-
position for each program token generated by the               cute programs so others can further research on
EHR-QA model.","Also, we propose a method for measuring the           • We generated a dataset to solve the problem
ambiguity of input questions with insuﬃcient infor-            without gold programs.","We empirically demonstrate the                   EHR-QA using this NLQ2Program model in the
eﬀectiveness of using uncertainty decomposition to             future.",2022-03-14 08:12:16+00:00,Uncertainty-Aware Text-to-Program for Question Answering on Structured Electronic Health Records,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Daeyoung Kim'), arxiv.Result.Author('Seongsu Bae'), arxiv.Result.Author('Seungho Kim'), arxiv.Result.Author('Edward Choi')]","Question Answering on Electronic Health Records (EHR-QA) has a significant
impact on the healthcare domain, and it is being actively studied. Previous
research on structured EHR-QA focuses on converting natural language queries
into query language such as SQL or SPARQL (NLQ2Query), so the problem scope is
limited to pre-defined data types by the specific query language. In order to
expand the EHR-QA task beyond this limitation to handle multi-modal medical
data and solve complex inference in the future, more primitive systemic
language is needed. In this paper, we design the program-based model
(NLQ2Program) for EHR-QA as the first step towards the future direction. We
tackle MIMICSPARQL*, the graph-based EHR-QA dataset, via a program-based
approach in a semi-supervised manner in order to overcome the absence of gold
programs. Without the gold program, our proposed model shows comparable
performance to the previous state-of-the-art model, which is an NLQ2Query model
(0.9% gain). In addition, for a reliable EHR-QA model, we apply the uncertainty
decomposition method to measure the ambiguity in the input question. We
empirically confirmed data uncertainty is most indicative of the ambiguity in
the input question."
3238,this presents an interesting avenue for further research.,The dotted and dashed lines represent the naive and random baseline scores respectively.,"However, we ﬁnd a positive main eﬀect of neighbour-
We think it is possible for the VGS model to also learn    hood density, contrary to what we may expect given
to recognise actions, perhaps by ﬁne-tuning parts of       that denser neighbourhoods lead to more word com-
ResNet with the VGS model or training the visual side      petition.",2022-03-14 08:59:37+00:00,Modelling word learning and recognition using visually grounded speech,cs.CL,['cs.CL'],"[arxiv.Result.Author('Danny Merkx'), arxiv.Result.Author('Sebastiaan Scholten'), arxiv.Result.Author('Stefan L. Frank'), arxiv.Result.Author('Mirjam Ernestus'), arxiv.Result.Author('Odette Scharenborg')]","Background: Computational models of speech recognition often assume that the
set of target words is already given. This implies that these models do not
learn to recognise speech from scratch without prior knowledge and explicit
supervision. Visually grounded speech models learn to recognise speech without
prior knowledge by exploiting statistical dependencies between spoken and
visual input. While it has previously been shown that visually grounded speech
models learn to recognise the presence of words in the input, we explicitly
investigate such a model as a model of human speech recognition.
  Methods: We investigate the time-course of word recognition as simulated by
the model using a gating paradigm to test whether its recognition is affected
by well-known word-competition effects in human speech processing. We
furthermore investigate whether vector quantisation, a technique for discrete
representation learning, aids the model in the discovery and recognition of
words.
  Results/Conclusion: Our experiments show that the model is able to recognise
nouns in isolation and even learns to properly differentiate between plural and
singular nouns. We also find that recognition is influenced by word competition
from the word-initial cohort and neighbourhood density, mirroring word
competition effects in human speech comprehension. Lastly, we find no evidence
that vector quantisation is helpful in discovering and recognising words. Our
gating experiments even show that the vector quantised model requires more of
the input sequence for correct recognition."
3239,"This implies that,
an interesting avenue for further research.","The positive interaction between VQ and gate
from human performance in terms of word learning and        indicates that the eﬀect of gate is larger for the LSTM-
recognition, is not able to exploit such cues, but this is  VQ model than for the LSTM model.","16  Danny Merkx, Sebastiaan Scholten, Stefan L. Frank, Mirjam Ernestus and Odette Scharenborg

for early gates, the LSTM-VQ model performs worse          Acknowledgements
than the LSTM model.",2022-03-14 08:59:37+00:00,Modelling word learning and recognition using visually grounded speech,cs.CL,['cs.CL'],"[arxiv.Result.Author('Danny Merkx'), arxiv.Result.Author('Sebastiaan Scholten'), arxiv.Result.Author('Stefan L. Frank'), arxiv.Result.Author('Mirjam Ernestus'), arxiv.Result.Author('Odette Scharenborg')]","Background: Computational models of speech recognition often assume that the
set of target words is already given. This implies that these models do not
learn to recognise speech from scratch without prior knowledge and explicit
supervision. Visually grounded speech models learn to recognise speech without
prior knowledge by exploiting statistical dependencies between spoken and
visual input. While it has previously been shown that visually grounded speech
models learn to recognise the presence of words in the input, we explicitly
investigate such a model as a model of human speech recognition.
  Methods: We investigate the time-course of word recognition as simulated by
the model using a gating paradigm to test whether its recognition is affected
by well-known word-competition effects in human speech processing. We
furthermore investigate whether vector quantisation, a technique for discrete
representation learning, aids the model in the discovery and recognition of
words.
  Results/Conclusion: Our experiments show that the model is able to recognise
nouns in isolation and even learns to properly differentiate between plural and
singular nouns. We also find that recognition is influenced by word competition
from the word-initial cohort and neighbourhood density, mirroring word
competition effects in human speech comprehension. Lastly, we find no evidence
that vector quantisation is helpful in discovering and recognising words. Our
gating experiments even show that the vector quantised model requires more of
the input sequence for correct recognition."
3254,"To        observe that BERT & C performs worse than BERT
encourage further research we make our dataset               on all metrics.","We next
Table 1 contains our generated dataset statistics.","Comparing BERT | C to BERT we
publicly available.",2022-03-14 15:13:52+00:00,RED-ACE: Robust Error Detection for ASR using Confidence Embeddings,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Zorik Gekhman'), arxiv.Result.Author('Dina Zverinski'), arxiv.Result.Author('Jonathan Mallinson'), arxiv.Result.Author('Genady Beryozkin')]","ASR Error Detection (AED) models aim to post-process the output of Automatic
Speech Recognition (ASR) systems, in order to detect transcription errors.
Modern approaches usually use text-based input, comprised solely of the ASR
transcription hypothesis, disregarding additional signals from the ASR model.
Instead, we propose to utilize the ASR system's word-level confidence scores
for improving AED performance. Specifically, we add an ASR Confidence Embedding
(ACE) layer to the AED model's encoder, allowing us to jointly encode the
confidence scores and the transcribed text into a contextualized
representation. Our experiments show the benefits of ASR confidence scores for
AED, their complementary effect over the textual signal, as well as the
effectiveness and robustness of ACE for combining these signals. To foster
further research, we publish a novel AED dataset consisting of ASR outputs on
the LibriSpeech corpus with annotated transcription errors."
3255,"To foster further research,   2020; Liao et al., 2020; Leng et al., 2021a,b), an-
                                             we publish a novel AED dataset consisting of        other ASR post-processing task.","Recently, the Transformer has
                                             effectiveness and robustness of ACE for com-        been applied to ASR error correction (Mani et al.,
                                             bining these signals.","These models use
                                             ASR outputs on the LibriSpeech corpus with          only the transcription hypothesis text as input and
                                             annotated transcription errors.1                    discard other signals from the ASR model.",2022-03-14 15:13:52+00:00,RED-ACE: Robust Error Detection for ASR using Confidence Embeddings,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Zorik Gekhman'), arxiv.Result.Author('Dina Zverinski'), arxiv.Result.Author('Jonathan Mallinson'), arxiv.Result.Author('Genady Beryozkin')]","ASR Error Detection (AED) models aim to post-process the output of Automatic
Speech Recognition (ASR) systems, in order to detect transcription errors.
Modern approaches usually use text-based input, comprised solely of the ASR
transcription hypothesis, disregarding additional signals from the ASR model.
Instead, we propose to utilize the ASR system's word-level confidence scores
for improving AED performance. Specifically, we add an ASR Confidence Embedding
(ACE) layer to the AED model's encoder, allowing us to jointly encode the
confidence scores and the transcribed text into a contextualized
representation. Our experiments show the benefits of ASR confidence scores for
AED, their complementary effect over the textual signal, as well as the
effectiveness and robustness of ACE for combining these signals. To foster
further research, we publish a novel AED dataset consisting of ASR outputs on
the LibriSpeech corpus with annotated transcription errors."
3256,"To encourage further research we make it                mance of BERT-MLM indicates that supervised
publicly available.6                                             training on real transcription errors is crucial.","The low perfor-
dataset.","4 Experimental Setup                                                We next observe that BERT & C performs
                                                                 worse than BERT on all metrics.",2022-03-14 15:13:52+00:00,RED-ACE: Robust Error Detection for ASR using Confidence Embeddings,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Zorik Gekhman'), arxiv.Result.Author('Dina Zverinski'), arxiv.Result.Author('Jonathan Mallinson'), arxiv.Result.Author('Genady Beryozkin')]","ASR Error Detection (AED) models aim to post-process the output of Automatic
Speech Recognition (ASR) systems, in order to detect transcription errors.
Modern approaches usually use text-based input, comprised solely of the ASR
transcription hypothesis, disregarding additional signals from the ASR model.
Instead, we propose to utilize the ASR system's word-level confidence scores
for improving AED performance. Specifically, we add an ASR Confidence Embedding
(ACE) layer to the AED model's encoder, allowing us to jointly encode the
confidence scores and the transcribed text into a contextualized
representation. Our experiments show the benefits of ASR confidence scores for
AED, their complementary effect over the textual signal, as well as the
effectiveness and robustness of ACE for combining these signals. To foster
further research, we publish a novel AED dataset consisting of ASR outputs on
the LibriSpeech corpus with annotated transcription errors."
3257,"To foster further research,  only the transcription hypothesis text as input and
                                             we publish a novel AED dataset consisting of       discard other signals from the ASR model.","These models use
                                             bining these signals.","How-
                                             ASR outputs on the LibriSpeech corpus with         ever, earlier work on AED (not Transformer-based)
                                             annotated transcription errors.1                   has shown the beneﬁts of such signals (Allauzen,
                                                                                                2007; Pellegrini and Trancoso, 2009; Chen et al.,
                                        1 Introduction                                          2013) and speciﬁcally the beneﬁts of ASR word-
                                                                                                level conﬁdence scores (Zhou et al., 2005), which
                                        Automatic Speech Recognition (ASR) systems              are often provided in addition to the transcribed
                                        transcribe audio signals, consisting of speech, into
                                        text.",2022-03-14 15:13:52+00:00,RED-ACE: Robust Error Detection for ASR using Confidence Embeddings,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Zorik Gekhman'), arxiv.Result.Author('Dina Zverinski'), arxiv.Result.Author('Jonathan Mallinson'), arxiv.Result.Author('Genady Beryozkin')]","ASR Error Detection (AED) models aim to post-process the output of Automatic
Speech Recognition (ASR) systems, in order to detect transcription errors.
Modern approaches usually use text-based input, comprised solely of the ASR
transcription hypothesis, disregarding additional signals from the ASR model.
Instead, we propose to utilize the ASR system's word-level confidence scores
for improving AED performance. Specifically, we add an ASR Confidence Embedding
(ACE) layer to the AED model's encoder, allowing us to jointly encode the
confidence scores and the transcribed text into a contextualized
representation. Our experiments show the benefits of ASR confidence scores for
AED, their complementary effect over the textual signal, as well as the
effectiveness and robustness of ACE for combining these signals. To foster
further research, we publish a novel AED dataset consisting of ASR outputs on
the LibriSpeech corpus with annotated transcription errors."
3262,"tion framework along with extensive experiments
to further study fairness within the legal domain.","Otherwise only the original work
   The scope of this work is to provide an evalua-     should be cited.","Personal Information
Following the work of Angwin et al.",2022-03-14 16:10:28+00:00,FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ilias Chalkidis'), arxiv.Result.Author('Tommaso Pasini'), arxiv.Result.Author('Sheng Zhang'), arxiv.Result.Author('Letizia Tomada'), arxiv.Result.Author('Sebastian Felix Schwemer'), arxiv.Result.Author('Anders Søgaard')]","We present a benchmark suite of four datasets for evaluating the fairness of
pre-trained language models and the techniques used to fine-tune them for
downstream tasks. Our benchmarks cover four jurisdictions (European Council,
USA, Switzerland, and China), five languages (English, German, French, Italian
and Chinese) and fairness across five attributes (gender, age, region,
language, and legal area). In our experiments, we evaluate pre-trained language
models using several group-robust fine-tuning techniques and show that
performance group disparities are vibrant in many cases, while none of these
techniques guarantee fairness, nor consistently mitigate group disparities.
Furthermore, we provide a quantitative and qualitative analysis of our results,
highlighting open challenges in the development of robustness methods in legal
NLP."
3269,"One inter-
in frequency after the decomposition (e.g., push,                                                       esting extension is to further study and improve
hold, press).","On the                                                        and improve the performance of downstream tasks
other hand, verbs that describe the action itself gain                                                  such as retrieving instructional videos.","This observation follows our assump-                                                      the robustness of our two-stage method to tackle
tion that the decomposition would lead to more                                                          more complex linguistic structures of steps and
ﬁne-grained realizations of a complex procedure.",2022-03-14 16:42:35+00:00,Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Shuyan Zhou'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Yue Yang'), arxiv.Result.Author('Qing Lyu'), arxiv.Result.Author('Pengcheng Yin'), arxiv.Result.Author('Chris Callison-Burch'), arxiv.Result.Author('Graham Neubig')]","Procedures are inherently hierarchical. To ""make videos"", one may need to
""purchase a camera"", which in turn may require one to ""set a budget"". While
such hierarchical knowledge is critical for reasoning about complex procedures,
most existing work has treated procedures as shallow structures without
modeling the parent-child relation.In this work, we attempt to construct an
open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a
website containing more than 110k instructional articles, each documenting the
steps to carry out a complex procedure. To this end, we develop a simple and
efficient method that links steps (e.g., ""purchase a camera"") in an article to
other articles with similar goals (e.g., ""how to choose a camera""), recursively
constructing the KB. Our method significantly outperforms several strong
baselines according to automatic evaluation, human judgment, and application to
downstream tasks such as instructional video retrieval.
  A demo with partial data can be found at https://wikihow-hierarchy.github.io.
The code and the data are at https://github.com/shuyanzhou/wikihow_hierarchy."
3270,"One inter-
in frequency after the decomposition (e.g., push,                                                       esting extension is to further study and improve
hold, press).","On the                                                        and improve the performance of downstream tasks
other hand, verbs that describe the action itself gain                                                  such as retrieving instructional videos.","This observation follows our assump-                                                      the robustness of our two-stage method to tackle
tion that the decomposition would lead to more                                                          more complex linguistic structures of steps and
ﬁne-grained realizations of a complex procedure.",2022-03-14 16:42:35+00:00,Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Shuyan Zhou'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Yue Yang'), arxiv.Result.Author('Qing Lyu'), arxiv.Result.Author('Pengcheng Yin'), arxiv.Result.Author('Chris Callison-Burch'), arxiv.Result.Author('Graham Neubig')]","Procedures are inherently hierarchical. To ""make videos"", one may need to
""purchase a camera"", which in turn may require one to ""set a budget"". While
such hierarchical knowledge is critical for reasoning about complex procedures,
most existing work has treated procedures as shallow structures without
modeling the parent-child relation. In this work, we attempt to construct an
open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a
website containing more than 110k instructional articles, each documenting the
steps to carry out a complex procedure. To this end, we develop a simple and
efficient method that links steps (e.g., ""purchase a camera"") in an article to
other articles with similar goals (e.g., ""how to choose a camera""), recursively
constructing the KB. Our method significantly outperforms several strong
baselines according to automatic evaluation, human judgment, and application to
downstream tasks such as instructional video retrieval.
  A demo with partial data can be found at https://wikihow-hierarchy.github.io.
The code and the data are at https://github.com/shuyanzhou/wikihow_hierarchy."
3283,"This suggests the need for further research to improve uncertainty
estimates in this domain.","However, the correlation is weak even in the best case scenario, likely explaining
the failure of active learning.",We describe our experimental setup in the next section.,2022-03-14 20:13:21+00:00,Uncertainty Estimation for Language Reward Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', 'I.2.7']","[arxiv.Result.Author('Adam Gleave'), arxiv.Result.Author('Geoffrey Irving')]","Language models can learn a range of capabilities from unsupervised training
on text corpora. However, to solve a particular problem (such as text
summarization) it is typically necessary to fine-tune them on a task-specific
dataset. It is often easier for humans to choose between options than to
provide labeled data, and prior work has achieved state-of-the-art performance
by training a reward model from such preference comparisons. However,
collecting a large preference comparison dataset is still expensive -- and the
learned reward models are unreliable out-of-distribution. We seek to address
these problems via uncertainty estimation, which can improve sample efficiency
and robustness using active learning and risk-averse reinforcement learning
(RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble
of reward models differing in the initialization of their final layer.
Ensembles have proved successful in prior applications of active learning, but
we find that in our setting ensemble active learning does not outperform random
sampling. Further experiments show that while the aggregate predictions are
well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly
correlated with model error. We suspect this is because the ensemble members
are fine-tuned from a single model and so are similar to one another. This
suggests current pre-training methods will need to be modified to support
uncertainty estimation, e.g. by training multiple language models."
3284,"Generative
                                             nesses of the two readers is crucial not only         readers (Raffel et al., 2020; Lewis et al., 2020c;
                                             for making a more informed reader selection in        Izacard and Grave, 2021) have also shown remark-
                                             practice but also for developing a deeper under-      able performance, where the goal is to generate
                                             standing to foster further research on improv-        answers by autoregressively predicting tokens.",Characterizing the strengths and weak-          positions of the answer in the context.,ing readers in a principled manner.,2022-03-14 22:07:52+00:00,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,cs.CL,['cs.CL'],"[arxiv.Result.Author('Man Luo'), arxiv.Result.Author('Kazuma Hashimoto'), arxiv.Result.Author('Semih Yavuz'), arxiv.Result.Author('Zhiwei Liu'), arxiv.Result.Author('Chitta Baral'), arxiv.Result.Author('Yingbo Zhou')]","While both extractive and generative readers have been successfully applied
to the Question Answering (QA) task, little attention has been paid toward the
systematic comparison of them. Characterizing the strengths and weaknesses of
the two readers is crucial not only for making a more informed reader selection
in practice but also for developing a deeper understanding to foster further
research on improving readers in a principled manner. Motivated by this goal,
we make the first attempt to systematically study the comparison of extractive
and generative readers for question answering. To be aligned with the
state-of-the-art, we explore nine transformer-based large pre-trained language
models (PrLMs) as backbone architectures. Furthermore, we organize our findings
under two main categories: (1) keeping the architecture invariant, and (2)
varying the underlying PrLMs. Among several interesting findings, it is
important to highlight that (1) the generative readers perform better in long
context QA, (2) the extractive readers perform better in short context while
also showing better out-of-domain generalization, and (3) the encoder of
encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and
outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also
study the effect of multi-task learning on the two types of readers varying the
underlying PrLMs and perform qualitative and quantitative diagnosis to provide
further insights into future directions in modeling better readers."
3285,"noun              5.79/15.34       4.28/8.10
verb             -0.17/-2.95       6.25/-3.82            7 Gender Biases in SSSB
                 10.42/14.06       3.13/3.13
                 12.89/-3.74      0.22/-15.44            In this section, we further study the gender-related
                                                         biases in static and contextualised word and sense
Table 4: Bias in BERT and SenseBERT contextualised       embeddings using the noun vs. verb sense instances
word/sense embeddings.","Therefore, we recommend future work
colour                                                   studying social biases to consider not only word
nationality      10.19/14.81      -17.59/0.00            embedding models but also sense embedding mod-
language         -6.64/-2.96       -8.88/9.84            els.","In each row, between the AUL      (described in § 4.3) in the SSSB dataset.",2022-03-14 22:08:37+00:00,Sense Embeddings are also Biased--Evaluating Social Biases in Static and Contextualised Sense Embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yi Zhou'), arxiv.Result.Author('Masahiro Kaneko'), arxiv.Result.Author('Danushka Bollegala')]","Sense embedding learning methods learn different embeddings for the different
senses of an ambiguous word. One sense of an ambiguous word might be socially
biased while its other senses remain unbiased. In comparison to the numerous
prior work evaluating the social biases in pretrained word embeddings, the
biases in sense embeddings have been relatively understudied. We create a
benchmark dataset for evaluating the social biases in sense embeddings and
propose novel sense-specific bias evaluation measures. We conduct an extensive
evaluation of multiple static and contextualised sense embeddings for various
types of social biases using the proposed measures. Our experimental results
show that even in cases where no biases are found at word-level, there still
exist worrying levels of social biases at sense-level, which are often ignored
by the word-level bias evaluation measures."
3286,"-6.64/-2.96       -8.88/9.84
nationality                                              7 Gender Biases in SSSB
language          5.79/15.34       4.28/8.10
noun             -0.17/-2.95       6.25/-3.82
verb             10.42/14.06       3.13/3.13
                 12.89/-3.74      0.22/-15.44

Table 4: Bias in BERT and SenseBERT contextualised       In this section, we further study the gender-related
word/sense embeddings.","Therefore, we recommend future work
SSSB              -1.09/8.31       -1.47/6.51            studying social biases to consider not only word
race                                                     embedding models but also sense embedding mod-
colour           10.19/14.81      -17.59/0.00            els.","In each row, between the AUL      biases in static and contextualised word and sense
bias scores for the word vs. sense embeddings, the       embeddings using the noun vs. verb sense instances
larger deviation from 0 is shown in bold.",2022-03-14 22:08:37+00:00,Sense Embeddings are also Biased--Evaluating Social Biases in Static and Contextualised Sense Embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yi Zhou'), arxiv.Result.Author('Masahiro Kaneko'), arxiv.Result.Author('Danushka Bollegala')]","Sense embedding learning methods learn different embeddings for the different
senses of an ambiguous word. One sense of an ambiguous word might be socially
biased while its other senses remain unbiased. In comparison to the numerous
prior work evaluating the social biases in pretrained word embeddings, the
biases in sense embeddings have been relatively understudied. We create a
benchmark dataset for evaluating the social biases in sense embeddings and
propose novel sense-specific bias evaluation measures. We conduct an extensive
evaluation of multiple static and contextualised sense embeddings for various
types of social biases using the proposed measures. Our experimental results
show that even in cases where no biases are found at word-level, there still
exist worrying levels of social biases at sense-level, which are often ignored
by the word-level bias evaluation measures."
3296,"Taking into        We further study ﬁne-tuned models’ plagiarism
account a strong correlation of memorization and
data duplication, we speculate that the observed dis-  regarding ﬁne-tuning data.",through paraphrase or idea plagiarism.,"Our results highlight
crepancies may have been caused by different lev-      that Cord19GPT was strongly afﬁliated with plagia-
els of similarity between each ﬁne-tuning dataset      rism as opposed to ArxivAbstractGPT and Patent-
and OpenWebText.",2022-03-15 03:11:11+00:00,Do Language Models Plagiarize?,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jooyoung Lee'), arxiv.Result.Author('Thai Le'), arxiv.Result.Author('Jinghui Chen'), arxiv.Result.Author('Dongwon Lee')]","Past literature has illustrated that language models do not fully understand
the context and sensitivity of text and can sometimes memorize phrases or
sentences present in their training sets. In this paper, we investigate whether
they not only memorize but also plagiarize training samples when generating
artificial texts. Our findings support that they, especially GPT-2, reuse
particular pieces of texts from the training corpus with or without
obfuscation. We have four main results: 1) language models with more capacity
plagiarize more; 2) fine-tuned language models demonstrate differing patterns
of plagiarism based on characteristics of auxiliary data; 3) sampling from
truncated language modeling distributions tends to heighten the degree of
plagiarism as opposed to temperature sampling, and 4) plagiarism in language
models can have serious privacy consequences. Overall, our work implies that
future research on neural language models should take precautions to avoid
models plagiarizing their training datasets."
3317,"recommendations for further research in value pluralist
                                                                    alignment in LLMs.","Finally, we provide
addresses.","Human values vary enormously across nations,
communities, cultures[5], and time[6], and are often reflected      1.1 Values and language.",2022-03-15 11:06:54+00:00,The Ghost in the Machine has an American accent: value conflict in GPT-3,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Rebecca L Johnson'), arxiv.Result.Author('Giada Pistilli'), arxiv.Result.Author('Natalia Menédez-González'), arxiv.Result.Author('Leslye Denisse Dias Duran'), arxiv.Result.Author('Enrico Panai'), arxiv.Result.Author('Julija Kalpokiene'), arxiv.Result.Author('Donald Jay Bertulfo')]","The alignment problem in the context of large language models must consider
the plurality of human values in our world. Whilst there are many resonant and
overlapping values amongst the world's cultures, there are also many
conflicting, yet equally valid, values. It is important to observe which
cultural values a model exhibits, particularly when there is a value conflict
between input prompts and generated outputs. We discuss how the co-creation of
language and cultural value impacts large language models (LLMs). We explore
the constitution of the training data for GPT-3 and compare that to the world's
language and internet access demographics, as well as to reported statistical
profiles of dominant values in some Nation-states. We stress tested GPT-3 with
a range of value-rich texts representing several languages and nations;
including some with values orthogonal to dominant US public opinion as reported
by the World Values Survey. We observed when values embedded in the input text
were mutated in the generated outputs and noted when these conflicting values
were more aligned with reported dominant US values. Our discussion of these
results uses a moral value pluralism (MVP) lens to better understand these
value mutations. Finally, we provide recommendations for how our work may
contribute to other current work in the field."
3359,"Taking                hope these results will encourage further research
resources into consideration, the advantage of using          into the comparative strengths and weaknesses of
a model such as RREG-S and ML-S becomes more                  neural, non-neural and hybrid methods in NLP.","We
remarkably well on both WSJ and WEBNLG.",pronounced.,2022-03-15 21:47:25+00:00,Non-neural Models Matter: A Re-evaluation of Neural Referring Expression Generation Systems,cs.CL,['cs.CL'],"[arxiv.Result.Author('Fahime Same'), arxiv.Result.Author('Guanyi Chen'), arxiv.Result.Author('Kees van Deemter')]","In recent years, neural models have often outperformed rule-based and classic
Machine Learning approaches in NLG. These classic approaches are now often
disregarded, for example when new neural models are evaluated. We argue that
they should not be overlooked, since, for some tasks, well-designed non-neural
approaches achieve better performance than neural ones. In this paper, the task
of generating referring expressions in linguistic context is used as an
example. We examined two very different English datasets (WEBNLG and WSJ), and
evaluated each algorithm using both automatic and human evaluations. Overall,
the results of these evaluations suggest that rule-based systems with simple
rule sets achieve on-par or better performance on both datasets compared to
state-of-the-art neural REG systems. In the case of the more realistic dataset,
WSJ, a machine learning-based system with well-designed linguistic features
performed best. We hope that our work can encourage researchers to consider
non-neural models in future."
3360,"These           2015), which does not contain identifying infor-
ﬁndings motivate further study applying syntax.","Lastly is the
portance of syntax both in authorship attribution       publicly available WikiQA corpus (Yang et al.,
and social dynamics such as persuasion.",mation.,2022-03-15 22:33:26+00:00,FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric,cs.CL,['cs.CL'],"[arxiv.Result.Author('Maximillian Chen'), arxiv.Result.Author('Caitlyn Chen'), arxiv.Result.Author('Xiao Yu'), arxiv.Result.Author('Zhou Yu')]","Syntax is a fundamental component of language, yet few metrics have been
employed to capture syntactic similarity or coherence at the utterance- and
document-level. The existing standard document-level syntactic similarity
metric is computationally expensive and performs inconsistently when faced with
syntactically dissimilar documents. To address these challenges, we present
FastKASSIM, a metric for utterance- and document-level syntactic similarity
which pairs and averages the most similar constituency parse trees between a
pair of documents based on tree kernels. FastKASSIM is more robust to syntactic
dissimilarities and runs up to to 5.32 times faster than its predecessor over
documents in the r/ChangeMyView corpus. FastKASSIM's improvements allow us to
examine hypotheses in two settings with large documents. We find that
syntactically similar arguments on r/ChangeMyView tend to be more persuasive,
and that syntax is predictive of authorship attribution in the Australian High
Court Judgment corpus."
3370,"Apart from posing this question for further study,     Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
we hope our work will help biomedical practition-         Kristina Toutanova.",examples and vacuous relation examples for RE.,2019.,2022-03-16 05:56:08+00:00,Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Bernal Jiménez Gutiérrez'), arxiv.Result.Author('Nikolas McNeal'), arxiv.Result.Author('Clay Washington'), arxiv.Result.Author('You Chen'), arxiv.Result.Author('Lang Li'), arxiv.Result.Author('Huan Sun'), arxiv.Result.Author('Yu Su')]","The strong few-shot in-context learning capability of large pre-trained
language models (PLMs) such as GPT-3 is highly appealing for biomedical
applications where data annotation is particularly costly. In this paper, we
present the first systematic and comprehensive study to compare the few-shot
performance of GPT-3 in-context learning with fine-tuning smaller (i.e.,
BERT-sized) PLMs on two highly representative biomedical information extraction
tasks, named entity recognition and relation extraction. We follow the true
few-shot setting to avoid overestimating models' few-shot performance by model
selection over a large validation set. We also optimize GPT-3's performance
with known techniques such as contextual calibration and dynamic in-context
example retrieval. However, our results show that GPT-3 still significantly
underperforms compared with simply fine-tuning a smaller PLM using the same
small training set. Moreover, what is equally important for practical
applications is that adding more labeled data would reliably yield an
improvement in model performance. While that is the case when fine-tuning small
PLMs, GPT-3's performance barely improves when adding more data. In-depth
analyses further reveal issues of the in-context learning setting that may be
detrimental to information extraction tasks in general. Given the high cost of
experimenting with GPT-3, we hope our study provides guidance for biomedical
researchers and practitioners towards more promising directions such as
fine-tuning GPT-3 or small PLMs."
3371,"7 Limitations                                           Apart from posing this question for further study,
                                                        we hope our work will help biomedical practition-
While we have uncovered a large performance gap         ers and researchers think through the pros and cons
between current GPT-3 in-context learning tech-         of these methods and guide them towards more
niques and standard ﬁne-tuning in the true-few          promising and cost-effective tools for low-resource
shot setting, there are several important limitations   IE such as small PLM ﬁne-tuning or perhaps even
that are worth discussing.","handling the null class, such as entity-less NER
                                                        examples and vacuous relation examples for RE.",Our limited budget re-       directly ﬁne-tuning GPT-3.,2022-03-16 05:56:08+00:00,Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Bernal Jiménez Gutiérrez'), arxiv.Result.Author('Nikolas McNeal'), arxiv.Result.Author('Clay Washington'), arxiv.Result.Author('You Chen'), arxiv.Result.Author('Lang Li'), arxiv.Result.Author('Huan Sun'), arxiv.Result.Author('Yu Su')]","The strong few-shot in-context learning capability of large pre-trained
language models (PLMs) such as GPT-3 is highly appealing for application
domains such as biomedicine, which feature high and diverse demands of language
technologies but also high data annotation costs. In this paper, we present the
first systematic and comprehensive study to compare the few-shot performance of
GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on
two highly representative biomedical information extraction tasks, named entity
recognition and relation extraction. We follow the true few-shot setting to
avoid overestimating models' few-shot performance by model selection over a
large validation set. We also optimize GPT-3's performance with known
techniques such as contextual calibration and dynamic in-context example
retrieval. However, our results show that GPT-3 still significantly
underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3
in-context learning also yields smaller gains in accuracy when more training
data becomes available. Our in-depth analyses further reveal issues of the
in-context learning setting that may be detrimental to information extraction
tasks in general. Given the high cost of experimenting with GPT-3, we hope our
study provides guidance for biomedical researchers and practitioners towards
more promising directions such as fine-tuning small PLMs."
3379,"analogical question answering tasks, which in-
                                             vites further research in this area.",“Taxi” and “bus” are different ways of transportation.,"Project          B) magazine1:bookshelf2:reading room3
                                             page of E-KAR can be found at https://
                                             ekar-leaderboard.github.io.",2022-03-16 09:16:38+00:00,E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiangjie Chen'), arxiv.Result.Author('Rui Xu'), arxiv.Result.Author('Ziquan Fu'), arxiv.Result.Author('Wei Shi'), arxiv.Result.Author('Zhongqiao Li'), arxiv.Result.Author('Xinbo Zhang'), arxiv.Result.Author('Changzhi Sun'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Yanghua Xiao'), arxiv.Result.Author('Hao Zhou')]","The ability to recognize analogies is fundamental to human cognition.
Existing benchmarks to test word analogy do not reveal the underneath process
of analogical reasoning of neural models. Holding the belief that models
capable of reasoning should be right for the right reasons, we propose a
first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning
benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in
English) problems sourced from the Civil Service Exams, which require intensive
background knowledge to solve. More importantly, we design a free-text
explanation scheme to explain whether an analogy should be drawn, and manually
annotate them for each and every question and candidate answer. Empirical
results suggest that this benchmark is very challenging for some
state-of-the-art models for both explanation generation and analogical question
answering tasks, which invites further research in this area."
3383,"After further study of those negative examples
                                                                                             and their explanations, we decide to treat those
   NATURAL-INSTRUCTIONS was constructed in                                                   negative examples as positive and move the nega-
the following pipeline: Mishra et al.","INSTRUCTIONS, then describe our revised version
speciﬁc to our problem.","(2021) ﬁrst col-                                       tive learning phase as pretraining, i.e, pretrain on
lected some popular NLP benchmarks (e.g., Cos-                                               negative examples ﬁrst, then ﬁnetune on positive
mosQA (Huang et al., 2019), Quoref (Dasigi et al.,
2019), Winogrande (Sakaguchi et al., 2020), etc.)",2022-03-16 10:27:18+00:00,ConTinTin: Continual Learning from Task Instructions,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wenpeng Yin'), arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Caiming Xiong')]","The mainstream machine learning paradigms for NLP often work with two
underlying presumptions. First, the target task is predefined and static, a
system just needs to learn to solve it exclusively. Second, the supervision of
a task mainly comes from a set of labeled examples. A question arises: how to
build a system that can keep learning new tasks from their instructions? This
work defines a new learning paradigm ConTinTin (Continual Learning from Task
Instructions), in which a system should learn a sequence of new tasks one by
one, each task is explained by a piece of textual instruction. The system is
required to (i) generate the expected outputs of a new task by learning from
its instruction, (ii) transfer the knowledge acquired from upstream tasks to
help solve downstream tasks (i.e, forward-transfer), and (iii) retain or even
improve the performance on earlier tasks after learning new tasks (i.e.,
backward-transfer). This new problem is studied on a stream of more than 60
tasks, each equipped with an instruction. Technically, our method
InstructionSpeak contains two strategies that make full use of task
instructions to improve forward-transfer and backward-transfer: one is to learn
from the negative output, the other is to re-visit instructions of prior tasks.
To our knowledge, this is the first time to study ConTinTin in NLP. In addition
to the problem formulation and our promising approach, this work also
contributes to providing rich analyses for the community to better understand
this novel learning problem."
3384,"After further study of those negative examples                   Overall, our two strategies work jointly
and their explanations, we decide to treat those                 to enhance the forward-transfer and the
negative examples as positive and move the nega-                 backward-transfer performance.","Since
                                                         many “negative” outputs contain tokens that exist
                                                         in the gold answers, we suspect that minimizing
                                                         their probabilities will let the model have more
                                                         difﬁculty decoding the correct output.","Our system
tive learning phase as pretraining, i.e., pretrain on            InstructionSpeak is based on BART,
negative examples ﬁrst, then ﬁnetune on positive                 treating all tasks as a text-to-text problem.",2022-03-16 10:27:18+00:00,ConTinTin: Continual Learning from Task Instructions,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wenpeng Yin'), arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Caiming Xiong')]","The mainstream machine learning paradigms for NLP often work with two
underlying presumptions. First, the target task is predefined and static; a
system merely needs to learn to solve it exclusively. Second, the supervision
of a task mainly comes from a set of labeled examples. A question arises: how
to build a system that can keep learning new tasks from their instructions?
This work defines a new learning paradigm ConTinTin (Continual Learning from
Task Instructions), in which a system should learn a sequence of new tasks one
by one, each task is explained by a piece of textual instruction. The system is
required to (i) generate the expected outputs of a new task by learning from
its instruction, (ii) transfer the knowledge acquired from upstream tasks to
help solve downstream tasks (i.e., forward-transfer), and (iii) retain or even
improve the performance on earlier tasks after learning new tasks (i.e.,
backward-transfer). This new problem is studied on a stream of more than 60
tasks, each equipped with an instruction. Technically, our method
InstructionSpeak contains two strategies that make full use of task
instructions to improve forward-transfer and backward-transfer: one is to learn
from negative outputs, the other is to re-visit instructions of previous tasks.
To our knowledge, this is the first time to study ConTinTin in NLP. In addition
to the problem formulation and our promising approach, this work also
contributes to providing rich analyses for the community to better understand
this novel learning problem."
3408,"which leads to a higher F1 and KF1 scores.5

Sample Selection We study the effectiveness of            Multi-Stage Prompting vs. Single-Stage
our sample selection methods in both knowledge            Prompting To further study the effectiveness of
generation and response generation by using the
random selection as a comparison.","These results        generate responses based on the given knowledge,
conﬁrms the effectiveness of our proposed method.","From Table 5,               5More ablation studies and results of automatic metrics for
we can see that using randomly selected samples           the model scaling are in the Appendix A, B, and C.
consistently decreases the performance in all met-
rics.",2022-03-16 16:53:43+00:00,Multi-Stage Prompting for Knowledgeable Dialogue Generation,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Zihan Liu'), arxiv.Result.Author('Mostofa Patwary'), arxiv.Result.Author('Ryan Prenger'), arxiv.Result.Author('Shrimai Prabhumoye'), arxiv.Result.Author('Wei Ping'), arxiv.Result.Author('Mohammad Shoeybi'), arxiv.Result.Author('Bryan Catanzaro')]","Existing knowledge-grounded dialogue systems typically use finetuned versions
of a pretrained language model (LM) and large-scale knowledge bases. These
models typically fail to generalize on topics outside of the knowledge base,
and require maintaining separate potentially large checkpoints each time
finetuning is needed. In this paper, we aim to address these limitations by
leveraging the inherent knowledge stored in the pretrained LM as well as its
powerful generation ability. We propose a multi-stage prompting approach to
generate knowledgeable responses from a single pretrained LM. We first prompt
the LM to generate knowledge based on the dialogue context. Then, we further
prompt it to generate responses based on the dialogue context and the
previously generated knowledge. Results show that our knowledge generator
outperforms the state-of-the-art retrieval-based model by 5.8% when combining
knowledge relevance and correctness. In addition, our multi-stage prompting
outperforms the finetuning-based dialogue model in terms of response
knowledgeability and engagement by up to 10% and 5%, respectively. Furthermore,
we scale our model up to 530 billion parameters and show that larger LMs
improve the generation correctness score by up to 10%, and response relevance,
knowledgeability and engagement by up to 10%. Our code is available at:
https://github.com/NVIDIA/Megatron-LM."
3439,"We introduce the task setting
arXiv:2203.09101v1 [cs.CL] 17 Mar 2022       of Zero-Shot Relation Triplet Extraction (Ze-          Task Setting                               Input           Output        Supervision
                                             roRTE) to encourage further research in low-
                                             resource relation extraction methods.","in building and representing knowledge, less
                                             research is focused on generalizing to unseen                                                 etail                    ehead
                                             relations types.","Given            Relation Classiﬁcation                 S, ehead, etail         y             Full
                                             an input sentence, each extracted triplet con-         Zero-Shot Relation Classiﬁcation       S, ehead, etail         y          Zero-Shot
                                             sists of the head entity, relation label, and tail     Zero-Shot Relation Slot-Filling                              etail        Zero-Shot
                                             entity where the relation label is not seen at the     Relation Triplet Extraction             S, ehead, y     ehead, etail, y
                                             training stage.",2022-03-17 05:55:14+00:00,RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yew Ken Chia'), arxiv.Result.Author('Lidong Bing'), arxiv.Result.Author('Soujanya Poria'), arxiv.Result.Author('Luo Si')]","Despite the importance of relation extraction in building and representing
knowledge, less research is focused on generalizing to unseen relations types.
We introduce the task setting of Zero-Shot Relation Triplet Extraction
(ZeroRTE) to encourage further research in low-resource relation extraction
methods. Given an input sentence, each extracted triplet consists of the head
entity, relation label, and tail entity where the relation label is not seen at
the training stage. To solve ZeroRTE, we propose to synthesize relation
examples by prompting language models to generate structured texts. Concretely,
we unify language model prompts and structured text approaches to design a
structured prompt template for generating synthetic relation samples when
conditioning on relation label prompts (RelationPrompt). To overcome the
limitation for extracting multiple relation triplets in a sentence, we design a
novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL
datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot
relation classification. Our code and data are available at
github.com/declare-lab/RelationPrompt."
3440,"4.2 Effect of Generated Data Size

by design, it assumes that the training data may            We further study how the number of generated syn-
contain multi-triplet sentences, whereas our syn-           thetic samples effects the multi-triplet ZeroRTE
thetic data is limited to single triplet samples.",Table 5: Ablation results for multi-triplet ZeroRTE.,On        performance.,2022-03-17 05:55:14+00:00,RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yew Ken Chia'), arxiv.Result.Author('Lidong Bing'), arxiv.Result.Author('Soujanya Poria'), arxiv.Result.Author('Luo Si')]","Despite the importance of relation extraction in building and representing
knowledge, less research is focused on generalizing to unseen relations types.
We introduce the task setting of Zero-Shot Relation Triplet Extraction
(ZeroRTE) to encourage further research in low-resource relation extraction
methods. Given an input sentence, each extracted triplet consists of the head
entity, relation label, and tail entity where the relation label is not seen at
the training stage. To solve ZeroRTE, we propose to synthesize relation
examples by prompting language models to generate structured texts. Concretely,
we unify language model prompts and structured text approaches to design a
structured prompt template for generating synthetic relation samples when
conditioning on relation label prompts (RelationPrompt). To overcome the
limitation for extracting multiple relation triplets in a sentence, we design a
novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL
datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot
relation classification. Our code and data are available at
github.com/declare-lab/RelationPrompt."
3441,"Al-                                 to overcome fundamental limitations in previous
though there are data augmentation methods that                                 task settings and encourage further research in low-
can be applied to structured tasks such as named                                resource relation extraction.","Simple heuris-

tics such as token manipulation (Kobayashi, 2018) 6 Conclusions and Future Work

were initially developed, new methods in language                               In this work, we introduce the task setting of
modeling improved the quality of augmented sam-                                 Zero-Shot Relation Triplet Extraction (ZeroRTE)
ples (Xie et al., 2020; Wei and Zou, 2019).","To solve ZeroRTE, we
entity recognition (Ding et al., 2020) and relation                             propose RelationPrompt and show that language
extraction (Papanikolaou and Pierleoni, 2020; Lee                               models can effectively generate synthetic training
et al., 2021), they require existing training samples                           data through relation label prompts to output struc-
and cannot be easily adapted to zero-shot tasks.",2022-03-17 05:55:14+00:00,RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yew Ken Chia'), arxiv.Result.Author('Lidong Bing'), arxiv.Result.Author('Soujanya Poria'), arxiv.Result.Author('Luo Si')]","Despite the importance of relation extraction in building and representing
knowledge, less research is focused on generalizing to unseen relations types.
We introduce the task setting of Zero-Shot Relation Triplet Extraction
(ZeroRTE) to encourage further research in low-resource relation extraction
methods. Given an input sentence, each extracted triplet consists of the head
entity, relation label, and tail entity where the relation label is not seen at
the training stage. To solve ZeroRTE, we propose to synthesize relation
examples by prompting language models to generate structured texts. Concretely,
we unify language model prompts and structured text approaches to design a
structured prompt template for generating synthetic relation samples when
conditioning on relation label prompts (RelationPrompt). To overcome the
limitation for extracting multiple relation triplets in a sentence, we design a
novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL
datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot
relation classification. Our code and data are available at
github.com/declare-lab/RelationPrompt."
3457,"Our work opens possibilities to further research
                                                        on scene trees as a joint representation of object
   For the distance probe, there is a noticeable dif-   compositions in an image and the grammatical
ference between the single-stream (Figure 13a) and      structure of its caption.","an abstract type of information, where only basic
features are needed to predict the masked items.","Furthermore, we rec-
the dual-stream (Figure 13b) models, where single       ommend investigating the training of multimodal-
stream models beneﬁt from the multimodal interac-       BERTs with objectives that enforce the encoding
tions to retain structural knowledge.",2022-03-17 13:20:01+00:00,Finding Structural Knowledge in Multimodal-BERT,cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Author('Victor Milewski'), arxiv.Result.Author('Miryam de Lhoneux'), arxiv.Result.Author('Marie-Francine Moens')]","In this work, we investigate the knowledge learned in the embeddings of
multimodal-BERT models. More specifically, we probe their capabilities of
storing the grammatical structure of linguistic data and the structure learned
over objects in visual data. To reach that goal, we first make the inherent
structure of language and visuals explicit by a dependency parse of the
sentences that describe the image and by the dependencies between the object
regions in the image, respectively. We call this explicit visual structure the
\textit{scene tree}, that is based on the dependency tree of the language
description. Extensive probing experiments show that the multimodal-BERT models
do not encode these scene trees.Code available at
\url{https://github.com/VSJMilewski/multimodal-probes}."
3488,"Table 4 shows         further research in understating and modeling the
examples of these observations.","Future work
four automatic metrics did not reward Dynamichard       needs to build on the role of intensity modiﬁers to
for having better intensiﬁcation.",prosody in sign language.,2022-03-18 01:13:21+00:00,Modeling Intensification for Sign Language Generation: A Computational Approach,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Mert İnan'), arxiv.Result.Author('Yang Zhong'), arxiv.Result.Author('Sabit Hassan'), arxiv.Result.Author('Lorna Quandt'), arxiv.Result.Author('Malihe Alikhani')]","End-to-end sign language generation models do not accurately represent the
prosody in sign language. A lack of temporal and spatial variations leads to
poor-quality generated presentations that confuse human interpreters. In this
paper, we aim to improve the prosody in generated sign languages by modeling
intensification in a data-driven manner. We present different strategies
grounded in linguistics of sign language that inform how intensity modifiers
can be represented in gloss annotations. To employ our strategies, we first
annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,
with different levels of intensification. We then use a supervised intensity
tagger to extend the annotated dataset and obtain labels for the remaining
portion of it. This enhanced dataset is then used to train state-of-the-art
transformer models for sign language generation. We find that our efforts in
intensification modeling yield better results when evaluated with automatic
metrics. Human evaluation also indicates a higher preference of the videos
generated using our model."
3494,We further study this “ﬁxed        stances as prototype embeddings.,"ited by input length and can deal with arbitrary      Instance Mean: using the mean embeddings of in-
number of samples.","Bold: best results
model” scenario in § 6.1.",2022-03-18 07:07:56+00:00,Prototypical Verbalizer for Prompt-based Few-shot Tuning,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Longtao Huang'), arxiv.Result.Author('Zhiyuan Liu')]","Prompt-based tuning for pre-trained language models (PLMs) has shown its
effectiveness in few-shot learning. Typically, prompt-based tuning wraps the
input text into a cloze question. To make predictions, the model maps the
output words to labels via a verbalizer, which is either manually designed or
automatically built. However, manual verbalizers heavily depend on
domain-specific prior knowledge and human efforts, while finding appropriate
label words automatically still remains challenging.In this work, we propose
the prototypical verbalizer (ProtoVerb) which is built directly from training
data. Specifically, ProtoVerb learns prototype vectors as verbalizers by
contrastive learning. In this way, the prototypes summarize training instances
and are able to enclose rich class-level semantics. We conduct experiments on
both topic classification and entity typing tasks, and the results demonstrate
that ProtoVerb significantly outperforms current automatic verbalizers,
especially when training data is extremely scarce. More surprisingly, ProtoVerb
consistently boosts prompt-based tuning even on untuned PLMs, indicating an
elegant non-tuning way to utilize PLMs. Our codes are avaliable at
https://github.com/thunlp/OpenPrompt."
3512,"This will necessitate further research            arXiv:2011.00416.
into the capacity of different natural language models to
mimic authorship, alongside studies of authorship mimicry               Karadzhov, G.; Mihaylova, T.; Kiprov, Y.; Georgiev, G.; Koychev,
on other online platforms.",inconsistent outputs.,"Complementary research examin-               I.; and Nakov, P. 2017.",2022-03-18 09:19:14+00:00,Are You Robert or RoBERTa? Deceiving Online Authorship Attribution Models Using Neural Text Generators,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Keenan Jones'), arxiv.Result.Author('Jason R. C. Nurse'), arxiv.Result.Author('Shujun Li')]","Recently, there has been a rise in the development of powerful pre-trained
natural language models, including GPT-2, Grover, and XLM. These models have
shown state-of-the-art capabilities towards a variety of different NLP tasks,
including question answering, content summarisation, and text generation.
Alongside this, there have been many studies focused on online authorship
attribution (AA). That is, the use of models to identify the authors of online
texts. Given the power of natural language models in generating convincing
texts, this paper examines the degree to which these language models can
generate texts capable of deceiving online AA models. Experimenting with both
blog and Twitter data, we utilise GPT-2 language models to generate texts using
the existing posts of online users. We then examine whether these AI-based text
generators are capable of mimicking authorial style to such a degree that they
can deceive typical AA models. From this, we find that current AI-based text
generators are able to successfully mimic authorship, showing capabilities
towards this on both datasets. Our findings, in turn, highlight the current
capacity of powerful natural language models to generate original online posts
capable of mimicking authorial style sufficiently to deceive popular AA
methods; a key finding given the proposed role of AA in real world applications
such as spam-detection and forensic investigation."
3534,"Shortly after the launch of previous NLP benchmarks, the performance of submissions came
close to the level of non-expert humans, suggesting limited headroom for further research.","The organization and management of a permanent benchmark for dialog evaluation is challeng-
ing.","While
we have yet to observe a similar phenomenon over the past two years of development of metrics, a
benchmark for evaluation may need to be periodically expanded to encompass both new datasets,
new dimensions (e.g., ﬂuency, relevance, coherence, etc.",2022-03-18 15:21:11+00:00,Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Shikib Mehri'), arxiv.Result.Author('Jinho Choi'), arxiv.Result.Author(""Luis Fernando D'Haro""), arxiv.Result.Author('Jan Deriu'), arxiv.Result.Author('Maxine Eskenazi'), arxiv.Result.Author('Milica Gasic'), arxiv.Result.Author('Kallirroi Georgila'), arxiv.Result.Author('Dilek Hakkani-Tur'), arxiv.Result.Author('Zekang Li'), arxiv.Result.Author('Verena Rieser'), arxiv.Result.Author('Samira Shaikh'), arxiv.Result.Author('David Traum'), arxiv.Result.Author('Yi-Ting Yeh'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Yizhe Zhang'), arxiv.Result.Author('Chen Zhang')]","This is a report on the NSF Future Directions Workshop on Automatic
Evaluation of Dialog. The workshop explored the current state of the art along
with its limitations and suggested promising directions for future work in this
important and very rapidly changing area of research."
3563,"In general, these results encourage further research and analysis.","Although significant, the difference of the two groups was rather
small, indicating that our measure only explains a small fraction of what makes perfumes
pleasant.","Further exploration

After training different smell embedding models and evaluating them with Rank Biased
Overlap, we decided to conduct another experiment.",2022-03-19 10:44:20+00:00,From meaning to perception -- exploring the space between word and odor perception embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Janek Amann'), arxiv.Result.Author('Manex Agirrezabal')]","In this paper we propose the use of the Word2vec algorithm in order to obtain
odor perception embeddings (or smell embeddings), only using publicly available
perfume descriptions. Besides showing meaningful similarity relationships among
each other, these embeddings also demonstrate to possess some shared
information with their respective word embeddings. The meaningfulness of these
embeddings suggests that aesthetics might provide enough constraints for using
algorithms motivated by distributional semantics on non-randomly combined data.
Furthermore, they provide possibilities for new ways of classifying odors and
analyzing perfumes. We have also employed the embeddings in an attempt to
understand the aesthetic nature of perfumes, based on the difference between
real and randomly generated perfumes. In an additional tentative experiment we
explore the possibility of a mapping between the word embedding space and the
odor perception embedding space by fitting a regressor on the shared vocabulary
and then predict the odor perception embeddings of words without an a priori
associated smell, such as night or sky."
3565,"We hope that our work will trig-
                                                                 ger further research on this important but relatively
                                                                 understudied subﬁeld of fake news detection.","Our results show that manipulated text detection re-
                                                                 mains challenging.",Acknowledgements                                             networks.,2022-03-19 15:35:59+00:00,Automatic Detection of Entity-Manipulated Text using Factual Knowledge,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ganesh Jawahar'), arxiv.Result.Author('Muhammad Abdul-Mageed'), arxiv.Result.Author('Laks V. S. Lakshmanan')]","In this work, we focus on the problem of distinguishing a human written news
article from a news article that is created by manipulating entities in a human
written news article (e.g., replacing entities with factually incorrect
entities). Such manipulated articles can mislead the reader by posing as a
human written news article. We propose a neural network based detector that
detects manipulated news articles by reasoning about the facts mentioned in the
article. Our proposed detector exploits factual knowledge via graph
convolutional neural network along with the textual information in the news
article. We also create challenging datasets for this task by considering
various strategies to generate the new replacement entity (e.g., entity
generation from GPT-2). In all the settings, our proposed model either matches
or outperforms the state-of-the-art detector in terms of accuracy. Our code and
data are available at https://github.com/UBC-NLP/manipulated_entity_detection."
3600,"Thus, an important
avenue for further research is to examine whether
bias mitigation techniques like this one are effective
on more translation pairs and language varieties.","It consists of English-language tem-
plates in General American English that were then
translated into other languages.","Finally, we acknowledge that our approach as-
sumes a binary notion of gender and does not
account for other gender identities; we recom-
mend that future work explore avenues for gender-
inclusive translation as well.",2022-03-20 23:35:09+00:00,Mitigating Gender Bias in Machine Translation through Adversarial Learning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Eve Fleisig'), arxiv.Result.Author('Christiane Fellbaum')]","Machine translation and other NLP systems often contain significant biases
regarding sensitive attributes, such as gender or race, that worsen system
performance and perpetuate harmful stereotypes. Recent preliminary research
suggests that adversarial learning can be used as part of a model-agnostic bias
mitigation method that requires no data modifications. However, adapting this
strategy for machine translation and other modern NLP domains requires (1)
restructuring training objectives in the context of fine-tuning pretrained
large language models and (2) developing measures for gender or other protected
variables for tasks in which these attributes must be deduced from the data
itself.
  We present an adversarial learning framework that addresses these challenges
to mitigate gender bias in seq2seq machine translation. Our framework improves
the disparity in translation quality for sentences with male vs. female
entities by 86% for English-German translation and 91% for English-French
translation, with minimal effect on translation quality. The results suggest
that adversarial learning is a promising technique for mitigating gender bias
in machine translation."
3606,"While our experiments
                                                          focus on datasets consisting of formal long doc-
We further study if HIBRIDS can boost the sec-            uments, we recognize that long documents could
tion encoder of HIERENC.",and GOVREPORT-QS.,"Table 5 shows that               be written in informal languages where our model
HIERENC with HIBRIDS gains further improve-               might not perform reasonably and could generate
ments on generating QS hierarchies and full docu-         degraded or even incorrect outputs.",2022-03-21 05:27:35+00:00,HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shuyang Cao'), arxiv.Result.Author('Lu Wang')]","Document structure is critical for efficient information consumption.
However, it is challenging to encode it efficiently into the modern Transformer
architecture. In this work, we present HIBRIDS, which injects Hierarchical
Biases foR Incorporating Document Structure into the calculation of attention
scores. We further present a new task, hierarchical question-summary
generation, for summarizing salient content in the source document into a
hierarchy of questions and summaries, where each follow-up question inquires
about the content of its parent question-summary pair. We also annotate a new
dataset with 6,153 question-summary hierarchies labeled on long government
reports. Experiment results show that our model produces better
question-summary hierarchies than comparisons on both hierarchy quality and
content coverage, a finding also echoed by human judges. Additionally, our
model improves the generation of long-form summaries from lengthy government
reports and Wikipedia articles, as measured by ROUGE scores."
3620,"In
order to further study the role of two modules in UD, we remove RSM and ASM respectively.",The drop of performance indicates the importance of this strategy.,"The
decline in experimental results illustrates that they are both effective while RSM is more significant.",2022-03-21 12:49:44+00:00,General and Domain Adaptive Chinese Spelling Check with Error Consistent Pretraining,cs.CL,['cs.CL'],"[arxiv.Result.Author('Qi Lv'), arxiv.Result.Author('Ziqiang Cao'), arxiv.Result.Author('Lei Geng'), arxiv.Result.Author('Chunhui Ai'), arxiv.Result.Author('Xu Yan'), arxiv.Result.Author('Guohong Fu')]","The lack of label data is one of the significant bottlenecks for Chinese
Spelling Check (CSC). Existing researches use the method of automatic
generation by exploiting unlabeled data to expand the supervised corpus.
However, there is a big gap between the real input scenario and automatic
generated corpus. Thus, we develop a competitive general speller ECSpell which
adopts the Error Consistent masking strategy to create data for pretraining.
This error consistency masking strategy is used to specify the error types of
automatically generated sentences which is consistent with real scene. The
experimental result indicates our model outperforms previous state-of-the-art
models on the general benchmark. Moreover, spellers often work within a
particular domain in real life. Due to lots of uncommon domain terms,
experiments on our built domain specific datasets show that general models
perform terribly. Inspired by the common practice of input methods, we propose
to add an alterable user dictionary to handle the zero-shot domain adaption
problem. Specifically, we attach a User Dictionary guided inference module (UD)
to a general token classification based speller. Our experiments demonstrate
that ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other
baselines largely, even approaching the performance on the general benchmark."
3621,"In order to further study the role of two modules in UD, we remove RSM
and ASM, respectively.","The drop in performance indicates the
importance of this strategy.","The decline in experimental results illustrates that they are both effective
while ASM is more significant.",2022-03-21 12:49:44+00:00,General and Domain Adaptive Chinese Spelling Check with Error Consistent Pretraining,cs.CL,['cs.CL'],"[arxiv.Result.Author('Qi Lv'), arxiv.Result.Author('Ziqiang Cao'), arxiv.Result.Author('Lei Geng'), arxiv.Result.Author('Chunhui Ai'), arxiv.Result.Author('Xu Yan'), arxiv.Result.Author('Guohong Fu')]","The lack of label data is one of the significant bottlenecks for Chinese
Spelling Check (CSC). Existing researches use the method of automatic
generation by exploiting unlabeled data to expand the supervised corpus.
However, there is a big gap between the real input scenario and automatic
generated corpus. Thus, we develop a competitive general speller ECSpell which
adopts the Error Consistent masking strategy to create data for pretraining.
This error consistency masking strategy is used to specify the error types of
automatically generated sentences which is consistent with real scene. The
experimental result indicates our model outperforms previous state-of-the-art
models on the general benchmark. Moreover, spellers often work within a
particular domain in real life. Due to lots of uncommon domain terms,
experiments on our built domain specific datasets show that general models
perform terribly. Inspired by the common practice of input methods, we propose
to add an alterable user dictionary to handle the zero-shot domain adaption
problem. Specifically, we attach a User Dictionary guided inference module (UD)
to a general token classification based speller. Our experiments demonstrate
that ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other
baselines largely, even approaching the performance on the general benchmark."
3622,"In order to further study the role of two modules in UD, we remove RSM
and ASM, respectively.","The drop in performance indicates the
importance of this strategy.","The decline in experimental results illustrates that they are both effective
while ASM is more significant.",2022-03-21 12:49:44+00:00,General and Domain Adaptive Chinese Spelling Check with Error Consistent Pretraining,cs.CL,['cs.CL'],"[arxiv.Result.Author('Qi Lv'), arxiv.Result.Author('Ziqiang Cao'), arxiv.Result.Author('Lei Geng'), arxiv.Result.Author('Chunhui Ai'), arxiv.Result.Author('Xu Yan'), arxiv.Result.Author('Guohong Fu')]","The lack of label data is one of the significant bottlenecks for Chinese
Spelling Check (CSC). Existing researches use the method of automatic
generation by exploiting unlabeled data to expand the supervised corpus.
However, there is a big gap between the real input scenario and automatic
generated corpus. Thus, we develop a competitive general speller ECSpell which
adopts the Error Consistent masking strategy to create data for pretraining.
This error consistency masking strategy is used to specify the error types of
automatically generated sentences which is consistent with real scene. The
experimental result indicates our model outperforms previous state-of-the-art
models on the general benchmark. Moreover, spellers often work within a
particular domain in real life. Due to lots of uncommon domain terms,
experiments on our built domain specific datasets show that general models
perform terribly. Inspired by the common practice of input methods, we propose
to add an alterable user dictionary to handle the zero-shot domain adaption
problem. Specifically, we attach a User Dictionary guided inference module (UD)
to a general token classification based speller. Our experiments demonstrate
that ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other
baselines largely, even approaching the performance on the general benchmark."
3636,"Hence we further study if self-consistency can help improve
a language model’s robustness to imperfect prompts.","3.5 Self-Consistency Improves Robustness to Imperfect Prompts

For few-shot learning with manually constructed prompts, human annotators sometimes make minor
mistakes when creating the prompts.","We use the same prompts as before but
swapped all the numbers in the reasoning paths with random numbers, while keeping the last number
(corresponding to the ﬁnal answer) unchanged (e.g., we change the prompt: “There are 3 cars in
the parking lot already.",2022-03-21 17:48:52+00:00,Self-Consistency Improves Chain of Thought Reasoning in Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xuezhi Wang'), arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Dale Schuurmans'), arxiv.Result.Author('Quoc Le'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Denny Zhou')]","We explore a simple ensemble strategy, self-consistency, that significantly
improves the reasoning accuracy of large language models. The idea is to sample
a diverse set of outputs from a language model and return the most consistent
answer in the set. Such ensembling method improves reasoning accuracy when
combined with chain of thought prompting. For arithmetic and commonsense
reasoning benchmarks we find that self-consistency yields significant accuracy
improvements in a variety of datasets, such as GSM8K (+10%), SVAMP (+14%),
MultiArith (+24%), CommonsenseQA (+5%) and ARC (easy +4%, challenge +5%)."
3637,"Hence we further study if self-consistency can help improve a language model’s robustness
to imperfect prompts.","Self-Consistency Improves Robustness to Imperfect Prompts For few-shot learning with man-
ually constructed prompts, human annotators sometimes make minor mistakes when creating the
prompts.","We use the same prompts as before but swapped all the numbers in the
reasoning paths with random numbers, while keeping the last number (corresponding to the ﬁnal
answer) unchanged (e.g., we change the prompt: “There are 3 cars in the parking lot already.",2022-03-21 17:48:52+00:00,Self-Consistency Improves Chain of Thought Reasoning in Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xuezhi Wang'), arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Dale Schuurmans'), arxiv.Result.Author('Quoc Le'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Sharan Narang'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Denny Zhou')]","We explore a simple ensemble strategy, self-consistency, that significantly
improves the reasoning accuracy of large language models. The idea is to sample
a diverse set of reasoning paths from a language model via chain of thought
prompting then return the most consistent final answer in the set. We evaluate
self-consistency on a range of arithmetic and commonsense reasoning benchmarks,
and find that it robustly improves accuracy across a variety of language models
and model scales without the need for additional training or auxiliary models.
When combined with a recent large language model, PaLM-540B, self-consistency
increases performance to state-of-the-art levels across several benchmark
reasoning tasks, including GSM8K (56.5% -> 74.4%), SVAMP (79.0% -> 86.6%), AQuA
(35.8% -> 48.3%), StrategyQA (75.3% -> 81.6%) and ARC-challenge (85.2% ->
88.7%)."
3638,"We further study if self-consistency can help improve a language model’s robustness to
imperfect prompts.9 We show the results in Table 8: while imperfect prompts decrease accuracy with
greedy decoding (17.1 → 14.9), self-consistency can ﬁll in the gaps and robustly improve the results.","Self-Consistency Improves Robustness to Imperfect Prompts For few-shot learning with man-
ually constructed prompts, human annotators sometimes make minor mistakes when creating the
prompts.","Additionally, we found that the consistency (in terms of % of decodes agreeing with the ﬁnal
aggregated answer) is highly correlated with accuracy (Figure 5, over GSM8K).",2022-03-21 17:48:52+00:00,Self-Consistency Improves Chain of Thought Reasoning in Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xuezhi Wang'), arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Dale Schuurmans'), arxiv.Result.Author('Quoc Le'), arxiv.Result.Author('Ed Chi'), arxiv.Result.Author('Sharan Narang'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Denny Zhou')]","Chain-of-thought prompting combined with pre-trained large language models
has achieved encouraging results on complex reasoning tasks. In this paper, we
propose a new decoding strategy, self-consistency, to replace the naive greedy
decoding used in chain-of-thought prompting. It first samples a diverse set of
reasoning paths instead of only taking the greedy one, and then selects the
most consistent answer by marginalizing out the sampled reasoning paths.
Self-consistency leverages the intuition that a complex reasoning problem
typically admits multiple different ways of thinking leading to its unique
correct answer. Our extensive empirical evaluation shows that self-consistency
boosts the performance of chain-of-thought prompting with a striking margin on
a range of popular arithmetic and commonsense reasoning benchmarks, including
GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and
ARC-challenge (+3.9%)."
3654,"Perspective API (Perspective) by Google
inspire further research to improve their robustness        (Jigsaw) provides a toxicity model that clas-
against the presented and similar attacks.",We hope our work will         5.,"siﬁes whether a post is “rude, disrespectful,
                                                            or unreasonable.” The production model uses
2 Target Offensive Language Classiﬁers                      a CNN trained with ﬁne-tuned GloVe word
                                                            embeddings and provides “toxicity” probabil-
2.1 Threat model                                            ity.",2022-03-21 20:44:30+00:00,On The Robustness of Offensive Language Classifiers,cs.CL,"['cs.CL', 'cs.LG', 'I.2.7']","[arxiv.Result.Author('Jonathan Rusert'), arxiv.Result.Author('Zubair Shafiq'), arxiv.Result.Author('Padmini Srinivasan')]","Social media platforms are deploying machine learning based offensive
language classification systems to combat hateful, racist, and other forms of
offensive speech at scale. However, despite their real-world deployment, we do
not yet comprehensively understand the extent to which offensive language
classifiers are robust against adversarial attacks. Prior work in this space is
limited to studying robustness of offensive language classifiers against
primitive attacks such as misspellings and extraneous spaces. To address this
gap, we systematically analyze the robustness of state-of-the-art offensive
language classifiers against more crafty adversarial attacks that leverage
greedy- and attention-based word selection and context-aware embeddings for
word replacement. Our results on multiple datasets show that these crafty
adversarial attacks can degrade the accuracy of offensive language classifiers
by more than 50% while also being able to preserve the readability and meaning
of the modified text."
3660,"This effort minimizes the possibility that
enough for further research into the behavior of      our annotators repeat their errors too many times.","able questions, we asked our annotators to self-
This ensures that the unanswerable questions are      validate the questions that they have generated be-
similar to answerable ones, and the quality of plau-  fore and write short documents to reflect on their
sible answers for unanswerable questions is high      errors.",Question Answering models.,2022-03-22 00:44:41+00:00,VLSP 2021 Shared Task: Vietnamese Machine Reading Comprehension,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kiet Van Nguyen'), arxiv.Result.Author('Son Quoc Tran'), arxiv.Result.Author('Luan Thanh Nguyen'), arxiv.Result.Author('Tin Van Huynh'), arxiv.Result.Author('Son T. Luu'), arxiv.Result.Author('Ngan Luu-Thuy Nguyen')]","One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the shared task on Vietnamese MRC at the Eighth Workshop
on Vietnamese Language and Speech Processing (VLSP 2021). This task attracted
77 participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the shared task, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% EM and 67.43% F1-score on the private test set.
The Vietnamese MRC systems proposed by the top 3 teams use XLM-RoBERTa, a
powerful pre-trained language model using the transformer architecture. The
UIT-ViQuAD 2.0 dataset motivates more researchers to explore Vietnamese machine
reading comprehension, question answering, and question generation."
3661,"#Submitted Teams             24                        This ensures that the unanswerable questions are
#Paper Submissions            6                        similar to answerable ones, and the quality of plau-
                                                       sible answers for unanswerable questions is high
Table 3: Participation summary of the VLSP 2021 -      enough for further research into the behavior of
ViMRC Challenge.","The
#Joined Teams                42                        answers for answerable questions are then used as
#Signed Data Agreements      42                        the plausible answers for unanswerable questions.",Question Answering models.,2022-03-22 00:44:41+00:00,VLSP 2021 -- ViMRC Challenge: Vietnamese Machine Reading Comprehension,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kiet Van Nguyen'), arxiv.Result.Author('Son Quoc Tran'), arxiv.Result.Author('Luan Thanh Nguyen'), arxiv.Result.Author('Tin Van Huynh'), arxiv.Result.Author('Son T. Luu'), arxiv.Result.Author('Ngan Luu-Thuy Nguyen')]","One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference."
3662,"#Submitted Teams             24                        This ensures that the unanswerable questions are
#Paper Submissions            6                        similar to answerable ones, and the quality of plau-
                                                       sible answers for unanswerable questions is high
Table 3: Participation summary of the VLSP 2021 -      enough for further research into the behavior of
ViMRC Challenge.","The
#Joined Teams                42                        answers for answerable questions are then used as
#Signed Data Agreements      42                        the plausible answers for unanswerable questions.",Question Answering models.,2022-03-22 00:44:41+00:00,VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kiet Van Nguyen'), arxiv.Result.Author('Son Quoc Tran'), arxiv.Result.Author('Luan Thanh Nguyen'), arxiv.Result.Author('Tin Van Huynh'), arxiv.Result.Author('Son T. Luu'), arxiv.Result.Author('Ngan Luu-Thuy Nguyen')]","One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference."
3710,"We expect our work to foster       Adversarial training has seen success in other text
further research into new authorship obfuscation        domains including strengthening word embeddings
approaches that are resistant to deobfuscation.",sarial threat model.,"(Miyato et al., 2016), better classiﬁcation in cross-  Document Simpliﬁcation (DS-PAN).",2022-03-22 16:26:09+00:00,"A Girl Has A Name, And It's ... Adversarial Authorship Attribution for Deobfuscation",cs.CL,"['cs.CL', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Wanyue Zhai'), arxiv.Result.Author('Jonathan Rusert'), arxiv.Result.Author('Zubair Shafiq'), arxiv.Result.Author('Padmini Srinivasan')]","Recent advances in natural language processing have enabled powerful
privacy-invasive authorship attribution. To counter authorship attribution,
researchers have proposed a variety of rule-based and learning-based text
obfuscation approaches. However, existing authorship obfuscation approaches do
not consider the adversarial threat model. Specifically, they are not evaluated
against adversarially trained authorship attributors that are aware of
potential obfuscation. To fill this gap, we investigate the problem of
adversarial authorship attribution for deobfuscation. We show that
adversarially trained authorship attributors are able to degrade the
effectiveness of existing obfuscators from 20-30% to 5-10%. We also evaluate
the effectiveness of adversarial training when the attributor makes incorrect
assumptions about whether and which obfuscator was used. While there is a a
clear degradation in attribution accuracy, it is noteworthy that this
degradation is still at or above the attribution accuracy of the attributor
that is not adversarially trained at all. Our results underline the need for
stronger obfuscation approaches that are resistant to deobfuscation"
3761,"Moreover, we also provided the online
                                                     software demonstration and the codes of the software for further research.","Data augmenta-
                                                     tion and model ensemble are adopted for obtaining better results.","Code metadata

                                       Nr.",2022-03-23 15:22:34+00:00,Prompt-based Pre-trained Model for Personality and Interpersonal Reactivity Prediction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Bin Li'), arxiv.Result.Author('Yixuan Weng')]","This paper describes our proposed method for the Workshop on Computational
Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA) 2022
shared task on Personality Prediction (PER) and Reactivity Index Prediction
(IRI). In this paper, we adopt the prompt-based learning method with the
pre-trained language model to accomplish these tasks. Specifically, the prompt
is designed to provide knowledge of the extra personalized information for
enhancing the pre-trained model. Data augmentation and model ensemble are
adopted for obtaining better results. Moreover, we also provided the online
software demonstration and the codes of the software for further research."
3762,"Moreover, we also provided the online
                                                      software demonstration and the codes of the software for further research.","Data augmenta-
                                                      tion and model ensemble are adopted for obtaining better results.","Code metadata

                                        Nr.",2022-03-23 15:22:34+00:00,Prompt-based System for Personality and Interpersonal Reactivity Prediction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Bin Li'), arxiv.Result.Author('Yixuan Weng')]","This paper describes our proposed method for the Workshop on Computational
Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA) 2022
shared task on Personality Prediction (PER) and Reactivity Index Prediction
(IRI). In this paper, we adopt the prompt-based learning method with the
pre-trained language model to accomplish these tasks. Specifically, the prompt
is designed to provide knowledge of the extra personalized information for
enhancing the pre-trained model. Data augmentation and model ensemble are
adopted for obtaining better results. Moreover, we also provided the online
software demonstration and the codes of the software for further research."
3764,"Computational Linguis-
an interesting point for further study.","The
Besides the target summary, how to apply other in-      mathematics of statistical machine translation:
formation to assess the summary quality would be        Parameter estimation.","tics, 19(2):263–311.",2022-03-23 16:24:21+00:00,A Survey on Cross-Lingual Summarization,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiaan Wang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Duo Zheng'), arxiv.Result.Author('Yunlong Liang'), arxiv.Result.Author('Zhixu Li'), arxiv.Result.Author('Jianfeng Qu'), arxiv.Result.Author('Jie Zhou')]","Cross-lingual summarization is the task of generating a summary in one
language (e.g., English) for the given document(s) in a different language
(e.g., Chinese). Under the globalization background, this task has attracted
increasing attention of the computational linguistics community. Nevertheless,
there still remains a lack of comprehensive review for this task. Therefore, we
present the first systematic critical review on the datasets, approaches and
challenges in this field. Specifically, we carefully organize existing datasets
and approaches according to different construction methods and solution
paradigms, respectively. For each type of datasets or approaches, we thoroughly
introduce and summarize previous efforts and further compare them with each
other to provide deeper analyses. In the end, we also discuss promising
directions and offer our thoughts to facilitate future research. This survey is
for both beginners and experts in cross-lingual summarization, and we hope it
will serve as a starting point as well as a source of new ideas for researchers
and engineers interested in this area."
3765,"In addition, we give multiple perspective di-     of the 2021 Conference on Empirical Methods
rections to facilitate further research on XLS.","In Proceedings
yses.","We      in Natural Language Processing, pages 1671–
hope that this XLS survey could provide a clear         1683, Online and Punta Cana, Dominican Re-
picture of this topic and boost the development of      public.",2022-03-23 16:24:21+00:00,A Survey on Cross-Lingual Summarization,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiaan Wang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Duo Zheng'), arxiv.Result.Author('Yunlong Liang'), arxiv.Result.Author('Zhixu Li'), arxiv.Result.Author('Jianfeng Qu'), arxiv.Result.Author('Jie Zhou')]","Cross-lingual summarization is the task of generating a summary in one
language (e.g., English) for the given document(s) in a different language
(e.g., Chinese). Under the globalization background, this task has attracted
increasing attention of the computational linguistics community. Nevertheless,
there still remains a lack of comprehensive review for this task. Therefore, we
present the first systematic critical review on the datasets, approaches and
challenges in this field. Specifically, we carefully organize existing datasets
and approaches according to different construction methods and solution
paradigms, respectively. For each type of datasets or approaches, we thoroughly
introduce and summarize previous efforts and further compare them with each
other to provide deeper analyses. In the end, we also discuss promising
directions and offer our thoughts to facilitate future research. This survey is
for both beginners and experts in cross-lingual summarization, and we hope it
will serve as a starting point as well as a source of new ideas for researchers
and engineers interested in this area."
3766,"Nevertheless, this direction has not been      rections to facilitate further research on XLS.","In addition, we give multiple perspective di-
guages.","We
noticed by previous work.",2022-03-23 16:24:21+00:00,A Survey on Cross-Lingual Summarization,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiaan Wang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Duo Zheng'), arxiv.Result.Author('Yunlong Liang'), arxiv.Result.Author('Zhixu Li'), arxiv.Result.Author('Jianfeng Qu'), arxiv.Result.Author('Jie Zhou')]","Cross-lingual summarization is the task of generating a summary in one
language (e.g., English) for the given document(s) in a different language
(e.g., Chinese). Under the globalization background, this task has attracted
increasing attention of the computational linguistics community. Nevertheless,
there still remains a lack of comprehensive review for this task. Therefore, we
present the first systematic critical review on the datasets, approaches, and
challenges in this field. Specifically, we carefully organize existing datasets
and approaches according to different construction methods and solution
paradigms, respectively. For each type of datasets or approaches, we thoroughly
introduce and summarize previous efforts and further compare them with each
other to provide deeper analyses. In the end, we also discuss promising
directions and offer our thoughts to facilitate future research. This survey is
for both beginners and experts in cross-lingual summarization, and we hope it
will serve as a starting point as well as a source of new ideas for researchers
and engineers interested in this area."
3767,"Computational Linguistics and the 11th Inter-
Besides the target summary, how to apply other in-        national Joint Conference on Natural Language
formation to assess the summary quality would be          Processing (Volume 1: Long Papers), pages
an interesting point for further study.","In Proceedings of
XLS samples consist of source document, (tar-             the 59th Annual Meeting of the Association for
get document), source summary, target summary .","6910–6924, Online.",2022-03-23 16:24:21+00:00,A Survey on Cross-Lingual Summarization,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jiaan Wang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Duo Zheng'), arxiv.Result.Author('Yunlong Liang'), arxiv.Result.Author('Zhixu Li'), arxiv.Result.Author('Jianfeng Qu'), arxiv.Result.Author('Jie Zhou')]","Cross-lingual summarization is the task of generating a summary in one
language (e.g., English) for the given document(s) in a different language
(e.g., Chinese). Under the globalization background, this task has attracted
increasing attention of the computational linguistics community. Nevertheless,
there still remains a lack of comprehensive review for this task. Therefore, we
present the first systematic critical review on the datasets, approaches, and
challenges in this field. Specifically, we carefully organize existing datasets
and approaches according to different construction methods and solution
paradigms, respectively. For each type of datasets or approaches, we thoroughly
introduce and summarize previous efforts and further compare them with each
other to provide deeper analyses. In the end, we also discuss promising
directions and offer our thoughts to facilitate future research. This survey is
for both beginners and experts in cross-lingual summarization, and we hope it
will serve as a starting point as well as a source of new ideas for researchers
and engineers interested in this area."
3768,"shown that “old” data still has much to tell for the
We lay out some of the immediate potential path-      computational study of typology and comparative
ways for this further research in hopes of stimulat-  linguistics.","have
of research to be conducted into language history.",ing work in this area.,2022-03-23 16:36:24+00:00,Computational historical linguistics and language diversity in South Asia,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Aryaman Arora'), arxiv.Result.Author('Adam Farris'), arxiv.Result.Author('Samopriya Basu'), arxiv.Result.Author('Suresh Kolichala')]","South Asia is home to a plethora of languages, many of which severely lack
access to new language technologies. This linguistic diversity also results in
a research environment conducive to the study of comparative, contact, and
historical linguistics -- fields which necessitate the gathering of extensive
data from many languages. We claim that data scatteredness (rather than
scarcity) is the primary obstacle in the development of South Asian language
technology, and suggest that the study of language history is uniquely aligned
with surmounting this obstacle. We review recent developments in and at the
intersection of South Asian NLP and historical-comparative linguistics,
describing our and others' current efforts in this area. We also offer new
strategies towards breaking the data barrier."
3773,"One should
proach to beneﬁt from further research in coun-       interpret the results with this in perspective.",(2019).,"Some
terfactual data generation, especially for reducing   recent works, such as Blodgett et al.",2022-03-23 17:34:35+00:00,Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Umang Gupta'), arxiv.Result.Author('Jwala Dhamala'), arxiv.Result.Author('Varun Kumar'), arxiv.Result.Author('Apurv Verma'), arxiv.Result.Author('Yada Pruksachatkun'), arxiv.Result.Author('Satyapriya Krishna'), arxiv.Result.Author('Rahul Gupta'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Greg Ver Steeg'), arxiv.Result.Author('Aram Galstyan')]","Language models excel at generating coherent text, and model compression
techniques such as knowledge distillation have enabled their use in
resource-constrained settings. However, these models can be biased in multiple
ways, including the unfounded association of male and female genders with
gender-neutral professions. Therefore, knowledge distillation without any
fairness constraints may preserve or exaggerate the teacher model's biases onto
the distilled model. To this end, we present a novel approach to mitigate
gender disparity in text generation by learning a fair model during knowledge
distillation. We propose two modifications to the base knowledge distillation
based on counterfactual role reversal$\unicode{x2014}$modifying teacher
probabilities and augmenting the training set. We evaluate gender polarity
across professions in open-ended text generated from the resulting distilled
and finetuned GPT$\unicode{x2012}$2 models and demonstrate a substantial
reduction in gender disparity with only a minor compromise in utility. Finally,
we observe that language models that reduce gender polarity in language
generation do not improve embedding fairness or downstream classification
fairness."
3815,"We make our code and models publicly available
Our language models suffer the same issues as           for further research.","increase in topicality in this case (up from 15%
to 19%) indicating the search engine part of the
5 Limitations & Discussion                              model, as well as the advantage of interpretability.","other systems that exist today, speciﬁcally with
problems of occasional inconsistency, contradic-
tions, factual inaccuracies, potential repetition, and
lack of deeper reasoning, amongst other issues
(Roller et al., 2021; Ouyang et al., 2022).",2022-03-24 17:31:26+00:00,Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Leonard Adolphs'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Jason Weston')]","Language models (LMs) have recently been shown to generate more factual
responses by employing modularity (Zhou et al., 2021) in combination with
retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et
al. (2021) to include internet search as a module. Our SeeKeR (Search
engine->Knowledge->Response) method thus applies a single LM to three modular
tasks in succession: search, generating knowledge, and generating a final
response. We show that, when using SeeKeR as a dialogue model, it outperforms
the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain
knowledge-grounded conversations for the same number of parameters, in terms of
consistency, knowledge and per-turn engagingness. SeeKeR applied to topical
prompt completions as a standard language model outperforms GPT2 (Radford et
al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,
despite GPT3 being a vastly larger model. Our code and models are made publicly
available."
3816,"We make our code and models publicly available
Our language models suffer the same issues as           for further research.","increase in topicality in this case (up from 15%
to 19%) indicating the search engine part of the
5 Limitations & Discussion                              model, as well as the advantage of interpretability.","other systems that exist today, speciﬁcally with
problems of occasional inconsistency, contradic-
tions, factual inaccuracies, potential repetition, and
lack of deeper reasoning, amongst other issues
(Roller et al., 2021; Ouyang et al., 2022).",2022-03-24 17:31:26+00:00,Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Leonard Adolphs'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Jason Weston')]","Language models (LMs) have recently been shown to generate more factual
responses by employing modularity (Zhou et al., 2021) in combination with
retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et
al. (2021) to include internet search as a module. Our SeeKeR (Search
engine->Knowledge->Response) method thus applies a single LM to three modular
tasks in succession: search, generating knowledge, and generating a final
response. We show that, when using SeeKeR as a dialogue model, it outperforms
the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain
knowledge-grounded conversations for the same number of parameters, in terms of
consistency, knowledge and per-turn engagingness. SeeKeR applied to topical
prompt completions as a standard language model outperforms GPT2 (Radford et
al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,
despite GPT3 being a vastly larger model. Our code and models are made publicly
available."
3823,"Indonesia is one of the most populous coun-
fore, further research on the trade-off between the   tries and the second-most linguistically diverse
efﬁciency and quality of the models is also an in-    country of the world, with over 700 local languages,
teresting research direction.",There-  NLP.,"yet Indonesian NLP is underrepresented and under-
                                                      explored.",2022-03-24 22:07:22+00:00,"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia",cs.CL,['cs.CL'],"[arxiv.Result.Author('Alham Fikri Aji'), arxiv.Result.Author('Genta Indra Winata'), arxiv.Result.Author('Fajri Koto'), arxiv.Result.Author('Samuel Cahyawijaya'), arxiv.Result.Author('Ade Romadhony'), arxiv.Result.Author('Rahmad Mahendra'), arxiv.Result.Author('Kemal Kurniawan'), arxiv.Result.Author('David Moeljadi'), arxiv.Result.Author('Radityo Eko Prasojo'), arxiv.Result.Author('Timothy Baldwin'), arxiv.Result.Author('Jey Han Lau'), arxiv.Result.Author('Sebastian Ruder')]","NLP research is impeded by a lack of resources and awareness of the
challenges presented by underrepresented languages and dialects. Focusing on
the languages spoken in Indonesia, the second most linguistically diverse and
the fourth most populous nation of the world, we provide an overview of the
current state of NLP research for Indonesia's 700+ languages. We highlight
challenges in Indonesian NLP and how these affect the performance of current
NLP systems. Finally, we provide general recommendations to help develop NLP
technology not only for languages of Indonesia but also other underrepresented
languages."
3850,"(2021) further study tox-      4.1 Hofstede’s cultural dimensions theory
icity bias towards social groups.",Ousidhoum et al.,"Hofstede started his surveys of cross-cultural differ-
   To the best of our knowledge, there is no existing   ences in values in 1980.",2022-03-25 15:45:49+00:00,Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,cs.CL,['cs.CL'],"[arxiv.Result.Author('Arnav Arora'), arxiv.Result.Author('Lucie-Aimée Kaffee'), arxiv.Result.Author('Isabelle Augenstein')]","Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys."
3851,"7.3 RQ3: Alignment with Surveys
7.2 RQ2: Model Agreement
                                                       Finally, we investigate whether the models’ predic-
To further study whether scores across values and      tions for the value questionnaire are consistent with
categories are consistent across the three models,     existing value survey scores.","only, while it is on over 200 questions for WVS.","we check for correlation between the predicted
scores between the three models and outline them       Hofstede We outline the results of correlations
in Tables 2 and 3.",2022-03-25 15:45:49+00:00,Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,cs.CL,['cs.CL'],"[arxiv.Result.Author('Arnav Arora'), arxiv.Result.Author('Lucie-Aimée Kaffee'), arxiv.Result.Author('Isabelle Augenstein')]","Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys."
3932,"As such, further research on methods that can handle both implicit and explicit
aspects is required.","This poses a significant challenge for real-words
applications of ABSA methods.","Furthermore, when considering real-world applications of ABSA, users are typically interested
in the sentiment expressed towards aspects aggregated over a review or sets of reviews.",2022-03-27 10:15:00+00:00,A Survey on Aspect-Based Sentiment Classification,cs.CL,"['cs.CL', 'A.1; I.2.7']","[arxiv.Result.Author('Gianni Brauwers'), arxiv.Result.Author('Flavius Frasincar')]","With the constantly growing number of reviews and other sentiment-bearing
texts on the Web, the demand for automatic sentiment analysis algorithms
continues to expand. Aspect-based sentiment classification (ABSC) allows for
the automatic extraction of highly fine-grained sentiment information from text
documents or sentences. In this survey, the rapidly evolving state of the
research on ABSC is reviewed. A novel taxonomy is proposed that categorizes the
ABSC models into three major categories: knowledge-based, machine learning, and
hybrid models. This taxonomy is accompanied with summarizing overviews of the
reported model performances, and both technical and intuitive explanations of
the various ABSC models. State-of-the-art ABSC models are discussed, such as
models based on the transformer model, and hybrid deep learning models that
incorporate knowledge bases. Additionally, various techniques for representing
the model inputs and evaluating the model outputs are reviewed. Furthermore,
trends in the research on ABSC are identified and a discussion is provided on
the ways in which the field of ABSC can be advanced in the future."
3938,"The analysis was done man-            • PubMedBERT is followed by SciBERT (mix do-
ually for about 100 mispredictions that were sam-                  main pretraining) and BioBERT (continual pre-
pled.This could be used for further research to im-                training) in accuracy.","Examples of correct and incorrect predic-
The error analysis details on a sample set of mispre-              tions of the model is presented in Table A
dictions by the best baseline model (PubMedBERT)
is given in this section.","From this result, it can be
prove the models/methods on the dataset.",2022-03-27 18:59:16+00:00,MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ankit Pal'), arxiv.Result.Author('Logesh Kumar Umapathi'), arxiv.Result.Author('Malaikannan Sankarasubbu')]","This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question
Answering (MCQA) dataset designed to address real-world medical entrance exam
questions. More than 194k high-quality AIIMS \& NEET PG entrance exam MCQs
covering 2.4k healthcare topics and 21 medical subjects are collected with an
average token length of 12.77 and high topical diversity. Each sample contains
a question, correct answer(s), and other options which requires a deeper
language understanding as it tests the 10+ reasoning abilities of a model
across a wide range of medical subjects \& topics. A detailed explanation of
the solution, along with the above information, is provided in this study."
3960,"Largely, there are two classes of
2018c), further study points to this not being the               approaches to learn CLWEs: mapping and joint
best approach (Vulic´ et al., 2019).","While most recent work in low-                learning (Peng et al., 2021), NMT (Artetxe et al.,
resource CLWEs have focused on reducing the su-                  2018c), and Bilingual Lexicon Induction (BLI) (Pa-
pervision signal as much as possible (Artetxe et al.,            tra et al., 2019).",We claim that in            methods.,2022-03-28 10:39:07+00:00,Isomorphic Cross-lingual Embeddings for Low-Resource Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sonal Sannigrahi'), arxiv.Result.Author('Jesse Read')]","Cross-Lingual Word Embeddings (CLWEs) are a key component to transfer
linguistic information learnt from higher-resource settings into lower-resource
ones. Recent research in cross-lingual representation learning has focused on
offline mapping approaches due to their simplicity, computational efficacy, and
ability to work with minimal parallel resources. However, they crucially depend
on the assumption of embedding spaces being approximately isomorphic i.e.
sharing similar geometric structure, which does not hold in practice, leading
to poorer performance on low-resource and distant language pairs. In this
paper, we introduce a framework to learn CLWEs, without assuming isometry, for
low-resource pairs via joint exploitation of a related higher-resource
language. In our work, we first pre-align the low-resource and related language
embedding spaces using offline methods to mitigate the assumption of isometry.
Following this, we use joint training methods to develops CLWEs for the related
language and the target embed-ding space. Finally, we remap the pre-aligned
low-resource space and the target space to generate the final CLWEs. We show
consistent gains over current methods in both quality and degree of
isomorphism, as measured by bilingual lexicon induction (BLI) and eigenvalue
similarity respectively, across several language pairs: {Nepali, Finnish,
Romanian, Gujarati, Hungarian}-English. Lastly, our analysis also points to the
relatedness as well as the amount of related language data available as being
key factors in determining the quality of embeddings achieved."
3970,"We
plan to improve this problem in our further research in the future.","It
depends on the quality of the domain dictionary, if there is a lack of high-quality domain
dictionaries in the field to be processed, then we will need to invest more costs in domain data
annotation to achieve better processing results, thereby increasing manpower consumption.","Computer Science & Information Technology (CS & IT)             315

5.",2022-03-28 13:26:47+00:00,Using Domain Knowledge for Low Resource Named Entity Recognition,cs.CL,['cs.CL'],[arxiv.Result.Author('Yuan Shi')],"In recent years, named entity recognition has always been a popular research
in the field of natural language processing, while traditional deep learning
methods require a large amount of labeled data for model training, which makes
them not suitable for areas where labeling resources are scarce. In addition,
the existing cross-domain knowledge transfer methods need to adjust the entity
labels for different fields, so as to increase the training cost. To solve
these problems, enlightened by a processing method of Chinese named entity
recognition, we propose to use domain knowledge to improve the performance of
named entity recognition in areas with low resources. The domain knowledge
mainly applied by us is domain dictionary and domain labeled data. We use
dictionary information for each word to strengthen its word embedding and
domain labeled data to reinforce the recognition effect. The proposed model
avoids large-scale data adjustments in different domains while handling named
entities recognition with low resources. Experiments demonstrate the
effectiveness of our method, which has achieved impressive results on the data
set in the field of scientific and technological equipment, and the F1 score
has been significantly improved compared with many other baseline methods."
3978,"Finally, we also per-         handle imbalanced data (most news stories do not
                                             formed a comprehensive error analysis to bet-           contain PCL) and complex semantic understanding
                                             ter understand the limitations of the model and         to relate shallow solutions for helping vulnerable
                                             provide ideas for further research.","For instance, accurate models must
                                             on the ﬁnal test dataset.",populations.,2022-03-28 17:17:12+00:00,"UTSA NLP at SemEval-2022 Task 4: An Exploration of Simple Ensembles of Transformers, Convolutional, and Recurrent Neural Networks",cs.CL,['cs.CL'],"[arxiv.Result.Author('Xingmeng Zhao'), arxiv.Result.Author('Anthony Rios')]","The act of appearing kind or helpful via the use of but having a feeling of
superiority condescending and patronizing language can have have serious mental
health implications to those that experience it. Thus, detecting this
condescending and patronizing language online can be useful for online
moderation systems. Thus, in this manuscript, we describe the system developed
by Team UTSA SemEval-2022 Task 4, Detecting Patronizing and Condescending
Language. Our approach explores the use of several deep learning architectures
including RoBERTa, convolutions neural networks, and Bidirectional Long
Short-Term Memory Networks. Furthermore, we explore simple and effective
methods to create ensembles of neural network models. Overall, we experimented
with several ensemble models and found that the a simple combination of five
RoBERTa models achieved an F-score of .6441 on the development dataset and
.5745 on the final test dataset. Finally, we also performed a comprehensive
error analysis to better understand the limitations of the model and provide
ideas for further research."
4023,"In the present approach, we

accept the 411 classes as given, leaving it to further research to address the

question of how these classes might be grounded in unsupervised learning.","We have shown that clusters found by the unsupervised t-SNE

algorithm independently of the WordNet tags, are well-supported by the super-

vised LDA classiﬁcation using WordNet tags.","Regarding the second source of information, we gauged how straightforward

it is to classify singular nouns according to their semantics.",2022-03-29 10:42:47+00:00,Semantic properties of English nominal pluralization: Insights from word embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Elnaz Shafaei-Bajestan'), arxiv.Result.Author('Masoumeh Moradipour-Tari'), arxiv.Result.Author('Peter Uhrig'), arxiv.Result.Author('R. Harald Baayen')]","Semantic differentiation of nominal pluralization is grammaticalized in many
languages. For example, plural markers may only be relevant for human nouns.
English does not appear to make such distinctions. Using distributional
semantics, we show that English nominal pluralization exhibits semantic
clusters. For instance, pluralization of fruit words is more similar to one
another and less similar to pluralization of other semantic classes. Therefore,
reduction of the meaning shift in plural formation to the addition of an
abstract plural meaning is too simplistic. A semantically informed method,
called CosClassAvg, is introduced that outperforms pluralization methods in
distributional semantics which assume plural formation amounts to the addition
of a fixed plural vector. In comparison with our approach, a method from
compositional distributional semantics, called FRACSS, predicted plural vectors
that were more similar to the corpus-extracted plural vectors in terms of
direction but not vector length. A modeling study reveals that the observed
difference between the two predicted semantic spaces by CosClassAvg and FRACSS
carries over to how well a computational model of the listener can understand
previously unencountered plural forms. Mappings from word forms, represented
with triphone vectors, to predicted semantic vectors are more productive when
CosClassAvg-generated semantic vectors are employed as gold standard vectors
instead of FRACSS-generated vectors."
4024,We leave this issue to further research.,"If human

category decisions are also inﬂuenced by the lexical semantics of nouns, more

precise predictions can perhaps be obtained by further conditioning on the

semantic class of the noun.","18
    The clustering of plural shift vectors by semantic class likely reﬂects dif-
ferences in how plural objects conﬁgure in our (culture-speciﬁc) constructions
of the world.",2022-03-29 10:42:47+00:00,Semantic properties of English nominal pluralization: Insights from word embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Elnaz Shafaei-Bajestan'), arxiv.Result.Author('Masoumeh Moradipour-Tari'), arxiv.Result.Author('Peter Uhrig'), arxiv.Result.Author('R. Harald Baayen')]","Semantic differentiation of nominal pluralization is grammaticalized in many
languages. For example, plural markers may only be relevant for human nouns.
English does not appear to make such distinctions. Using distributional
semantics, we show that English nominal pluralization exhibits semantic
clusters. For instance, pluralization of fruit words is more similar to one
another and less similar to pluralization of other semantic classes. Therefore,
reduction of the meaning shift in plural formation to the addition of an
abstract plural meaning is too simplistic. A semantically informed method,
called CosClassAvg, is introduced that outperforms pluralization methods in
distributional semantics which assume plural formation amounts to the addition
of a fixed plural vector. In comparison with our approach, a method from
compositional distributional semantics, called FRACSS, predicted plural vectors
that were more similar to the corpus-extracted plural vectors in terms of
direction but not vector length. A modeling study reveals that the observed
difference between the two predicted semantic spaces by CosClassAvg and FRACSS
carries over to how well a computational model of the listener can understand
previously unencountered plural forms. Mappings from word forms, represented
with triphone vectors, to predicted semantic vectors are more productive when
CosClassAvg-generated semantic vectors are employed as gold standard vectors
instead of FRACSS-generated vectors."
4025,"It remains an issue for further research to clarify whether the present
conclusions for nominal pluralization generalize to contextual inﬂection.","31
    Noun pluralization has been characterized as being rather close to deriva-
tion: Booij (1996) characterizes it as inherent inﬂection, rather than contextual
inﬂection.","Agree-
ment marking on English simple present verbs makes for an interesting case
to pursue in parallel with the present results on nominal plurals.",2022-03-29 10:42:47+00:00,Semantic properties of English nominal pluralization: Insights from word embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Elnaz Shafaei-Bajestan'), arxiv.Result.Author('Masoumeh Moradipour-Tari'), arxiv.Result.Author('Peter Uhrig'), arxiv.Result.Author('R. Harald Baayen')]","Semantic differentiation of nominal pluralization is grammaticalized in many
languages. For example, plural markers may only be relevant for human nouns.
English does not appear to make such distinctions. Using distributional
semantics, we show that English nominal pluralization exhibits semantic
clusters. For instance, pluralization of fruit words is more similar to one
another and less similar to pluralization of other semantic classes. Therefore,
reduction of the meaning shift in plural formation to the addition of an
abstract plural meaning is too simplistic. A semantically informed method,
called CosClassAvg, is introduced that outperforms pluralization methods in
distributional semantics which assume plural formation amounts to the addition
of a fixed plural vector. In comparison with our approach, a method from
compositional distributional semantics, called FRACSS, predicted plural vectors
that were more similar to the corpus-extracted plural vectors in terms of
direction but not vector length. A modeling study reveals that the observed
difference between the two predicted semantic spaces by CosClassAvg and FRACSS
carries over to how well a computational model of the listener can understand
previously unencountered plural forms. Mappings from word forms, represented
with triphone vectors, to predicted semantic vectors are more productive when
CosClassAvg-generated semantic vectors are employed as gold standard vectors
instead of FRACSS-generated vectors."
4051,"5.4 Analysis

We further study when LinkBERT is especially
useful in downstream tasks.","world knowledge), while keeping performance
on sentence-level language understanding.",Festival held each spring.,2022-03-29 18:01:24+00:00,LinkBERT: Pretraining Language Models with Document Links,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Michihiro Yasunaga'), arxiv.Result.Author('Jure Leskovec'), arxiv.Result.Author('Percy Liang')]","Language model (LM) pretraining can learn various knowledge from text
corpora, helping downstream tasks. However, existing methods such as BERT model
a single document, and do not capture dependencies or knowledge that span
across documents. In this work, we propose LinkBERT, an LM pretraining method
that leverages links between documents, e.g., hyperlinks. Given a text corpus,
we view it as a graph of documents and create LM inputs by placing linked
documents in the same context. We then pretrain the LM with two joint
self-supervised objectives: masked language modeling and our new proposal,
document relation prediction. We show that LinkBERT outperforms BERT on various
downstream tasks across two domains: the general domain (pretrained on
Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with
citation links). LinkBERT is especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our
biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on
BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,
as well as code and data at https://github.com/michiyasunaga/LinkBERT."
4054,"To further study how reducing the encoded lin-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           G< l a t e x i t s h a 1 _ b a s e 6 4 = "" X b l X 3 N a T + m y M d j j U L M N F / d x 6 h z k = "" > A A A C A 3 i c b V D L S s N A F L 2 p r 1 p f U X e 6 C R b B V U l E 0 W X R h W 6 E C v Y B b S i T y a Q d O n k w c y O W U H D j r 7 h x o Y h b f 8 K d f + O 0 z U J b D w w c z r n 3 z r 3 H S w R X a N v f R m F h c W l 5 p b h a W l v f 2 N w y t 3 c a K k 4 l Z X U a i 1 i 2 P K K Y 4 B G r I 0 f B W o l k J P Q E a 3 q D y 7 H f v G d S 8 T i 6 w 2 H C 3 J D 0 I h 5 w S l B L X X O v g + w B F c 2 u J P E 5 i 9 C 6 S Q X y R L B R 1 y z b F X s C a 5 4 4 O S l D j l r X / O r 4 M U 1 D P Y U K o l T b s R N 0 M y K R U z 2 v 1 E k V S w g d k B 5 r a x q R k C k 3 m 9 w w s g 6 1 4 l t B L P X T W 0 z U 3 x 0 Z C Z U a h p 6 u D A n 2 1 a w 3 F v / z 2 i k G 5 2 7 G o y R F F t H p R 0 E q L I y t c S C W z y W j K I a a E C q 5 3 t W i f S I J R R 1 b S Y f g z J 4 8 T x r H F e e 0 Y t + e l K s X e R x F 2 I c D O A I H z q A K 1 1 C D O l B 4 h G d 4 h T f j y X g x 3 o 2 P a W n B y H t 2 4 Q + M z x / D 6 J g 8 < / l a t e x i t > radient                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Multiple

guistic information affects task performance, we                 address the multi-objective problems.","creasing the encoded linguistic information affects

the variance of task performance.","optimize a similar multi-objective problem:                      In  the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ﬁrst  step,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    we  adGoMpt < l a t e x i t s h a 1 _ b a s e 6 4 = "" o 3 1 d 3 v / s E n 0 1 P e U d E x V J x Q V p 5 P s = "" > A A A B + n i c b V D L S g N B E J y N r x h f G z 1 6 G Q y C p 7 A r i h 6 D H v S g E M E 8 I F n C 7 K S T D J l 9 M N O r h j W f 4 s W D I l 7 9 E m / + j Z N k D x o t a C i q u u n u 8 m M p N D r O l 5 V b W F x a X s m v F t b W N z a 3 7 O J 2 X U e J 4 l D j k Y x U 0 2 c a p A i h h g I l N G M F L P A l N P z h + c R v 3 I H S I g p v c R S D F 7 B + K H q C M z R S x y 6 2 E R 5 Q 8 / T i m l 6 x E a h x x y 4 5 Z W c K + p e 4 G S m R D N W O / d n u R j w J I E Q u m d Y t 1 4 n R S 5 l C w S W M C + 1 E Q 8 z 4 k P W h Z W j I A t B e O j 1 9 T P e N 0 q W 9 S J k K k U 7 V n x M p C 7 Q e B b 7 p D B g O 9 L w 3 E f / z W g n 2 T r 1 U h H G C E P L Z o l 4 i K U Z 0 k g P t C g U c 5 c g Q x p U w t 1 I + Y I p x N G k V T A j u / M t / S f 2 w 7 B 6 X n Z u j U u U s i y N P d s k e O S A u O S E V c k m q p E Y 4 u S d P 5 I W 8 W o / W s / V m v c 9 a c 1 Y 2 s 0 N + w f r 4 B j m H k / k = < / l a t e x i t >               Laanymerethod                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  to                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ﬁnd    the

arg min[Lθ(x, y); I(h, s)]                   (5)                 Pareto-optimal solutions to the problem.",2022-03-29 19:03:10+00:00,Visualizing the Relationship Between Encoded Linguistic Information and Task Performance,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jiannan Xiang'), arxiv.Result.Author('Huayang Li'), arxiv.Result.Author('Defu Lian'), arxiv.Result.Author('Guoping Huang'), arxiv.Result.Author('Taro Watanabe'), arxiv.Result.Author('Lemao Liu')]","Probing is popular to analyze whether linguistic information can be captured
by a well-trained deep neural model, but it is hard to answer how the change of
the encoded linguistic information will affect task performance. To this end,
we study the dynamic relationship between the encoded linguistic information
and task performance from the viewpoint of Pareto Optimality. Its key idea is
to obtain a set of models which are Pareto-optimal in terms of both objectives.
From this viewpoint, we propose a method to optimize the Pareto-optimal models
by formalizing it as a multi-objective optimization problem. We conduct
experiments on two popular NLP tasks, i.e., machine translation and language
modeling, and investigate the relationship between several kinds of linguistic
information and task performances. Experimental results demonstrate that the
proposed method is better than a baseline method. Our empirical findings
suggest that some syntactic information is helpful for NLP tasks whereas
encoding more syntactic information does not necessarily lead to better
performance, because the model architecture is also an important factor."
4075,"ing structured information for disﬂuency detection is a logical
step and might steer further research in this direction.","Here, we want to re-iterate the fact that includ-    which classiﬁes tokens in isolation.","Addition-       GCN vs w/o GCN-Highlighted words represent the pre-
ally, in the next section, we also make an effort to analyze the    diction made by our span classiﬁer with the GCN module and
beneﬁts of adding structured information to our span classiﬁer      ground truth annotations, while the underlined words represent
model.",2022-03-30 03:22:29+00:00,Span Classification with Structured Information for Disfluency Detection in Spoken Utterances,cs.CL,"['cs.CL', 'cs.MM', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Sreyan Ghosh'), arxiv.Result.Author('Sonal Kumar'), arxiv.Result.Author('Yaman Kumar Singla'), arxiv.Result.Author('Rajiv Ratn Shah'), arxiv.Result.Author('S. Umesh')]","Existing approaches in disfluency detection focus on solving a token-level
classification task for identifying and removing disfluencies in text.
Moreover, most works focus on leveraging only contextual information captured
by the linear sequences in text, thus ignoring the structured information in
text which is efficiently captured by dependency trees. In this paper, building
on the span classification paradigm of entity recognition, we propose a novel
architecture for detecting disfluencies in transcripts from spoken utterances,
incorporating both contextual information through transformers and
long-distance structured information captured by dependency trees, through
graph convolutional networks (GCNs). Experimental results show that our
proposed model achieves state-of-the-art results on the widely used English
Switchboard for disfluency detection and outperforms prior-art by a significant
margin. We make all our codes publicly available on GitHub
(https://github.com/Sreyan88/Disfluency-Detection-with-Span-Classification)"
4121,"analysis shows: (1) PLMs generate the miss-
                                             ing factual words more by the positionally             show a surprisingly strong ability to capture fac-
                                             close and highly co-occurred words than the            tual knowledge in such probings (Jiang et al., 2020;
                                             knowledge-dependent words; (2) the depen-              Shin et al., 2020; Zhong et al., 2021), which elicits
                                             dence on the knowledge-dependent words is              further research on a more in-depth question (Cao
                                             more effective than the positionally close and         et al., 2021; Elazar et al., 2021a): How do PLMs
                                             highly co-occurred words.",Our            missing words.,"Accordingly, we              capture the factual knowledge?",2022-03-31 02:01:26+00:00,How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shaobo Li'), arxiv.Result.Author('Xiaoguang Li'), arxiv.Result.Author('Lifeng Shang'), arxiv.Result.Author('Zhenhua Dong'), arxiv.Result.Author('Chengjie Sun'), arxiv.Result.Author('Bingquan Liu'), arxiv.Result.Author('Zhenzhou Ji'), arxiv.Result.Author('Xin Jiang'), arxiv.Result.Author('Qun Liu')]","Recently, there has been a trend to investigate the factual knowledge
captured by Pre-trained Language Models (PLMs). Many works show the PLMs'
ability to fill in the missing factual words in cloze-style prompts such as
""Dante was born in [MASK]."" However, it is still a mystery how PLMs generate
the results correctly: relying on effective clues or shortcut patterns? We try
to answer this question by a causal-inspired analysis that quantitatively
measures and evaluates the word-level patterns that PLMs depend on to generate
the missing words. We check the words that have three typical associations with
the missing words: knowledge-dependent, positionally close, and highly
co-occurred. Our analysis shows: (1) PLMs generate the missing factual words
more by the positionally close and highly co-occurred words than the
knowledge-dependent words; (2) the dependence on the knowledge-dependent words
is more effective than the positionally close and highly co-occurred words.
Accordingly, we conclude that the PLMs capture the factual knowledge
ineffectively because of depending on the inadequate associations."
4134,"Speaker information
Precise voice activity timestamps of each speaker, each sam-
ple’s topic label, and speakers’ demographic information are         There are a total of 663 speakers involved in the recording, of
also provided, allowing further research on different tasks.",2.2.,The     which 295 are female and 368 are male.,2022-03-31 07:01:06+00:00,Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset,cs.CL,"['cs.CL', 'eess.AS']","[arxiv.Result.Author('Zehui Yang'), arxiv.Result.Author('Yifan Chen'), arxiv.Result.Author('Lei Luo'), arxiv.Result.Author('Runyan Yang'), arxiv.Result.Author('Lingxuan Ye'), arxiv.Result.Author('Gaofeng Cheng'), arxiv.Result.Author('Ji Xu'), arxiv.Result.Author('Yaohui Jin'), arxiv.Result.Author('Qingqing Zhang'), arxiv.Result.Author('Pengyuan Zhang'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Yonghong Yan')]","This paper introduces a high-quality rich annotated Mandarin conversational
(RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains
180 hours of conversational speech data recorded from native speakers of
Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs
in MagicData-RAMC are classified into 15 diversified domains and tagged with
topic labels, ranging from science and technology to ordinary life. Accurate
transcription and precise speaker voice activity timestamps are manually
labeled for each sample. Speakers' detailed information is also provided. As a
Mandarin speech dataset designed for dialog scenarios with high quality and
rich annotations, MagicData-RAMC enriches the data diversity in the Mandarin
speech community and allows extensive research on a series of speech-related
tasks, including automatic speech recognition, speaker diarization, topic
detection, keyword search, text-to-speech, etc. We also conduct several
relevant tasks and provide experimental results to help evaluate the dataset."
4197,"We further study how CP-Tuning improves the model performance
     • P-tuning [22] 6: it employs continuous prompt embeddings        in various aspects.","comparison, where “Auto T”, “Auto L” and “Auto T+L” refer
        to the model with automatically generated templates, label     3.4 Detailed Model Analysis
        words and both, respectively.","Here, we treat SST-2, MR, MRPC and QQP as
        generated by light-weight neural nets and fixed verbalizers    pilot tasks to explore our method.",2022-04-01 02:24:24+00:00,Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ziyun Xu'), arxiv.Result.Author('Chengyu Wang'), arxiv.Result.Author('Minghui Qiu'), arxiv.Result.Author('Fuli Luo'), arxiv.Result.Author('Runxin Xu'), arxiv.Result.Author('Songfang Huang'), arxiv.Result.Author('Jun Huang')]","Pre-trained Language Models (PLMs) have achieved remarkable performance for
various language understanding tasks in IR systems, which require the
fine-tuning process based on labeled training data. For low-resource scenarios,
prompt-based learning for PLMs exploits prompts as task guidance and turns
downstream tasks into masked language problems for effective few-shot
fine-tuning. In most existing approaches, the high performance of prompt-based
learning heavily relies on handcrafted prompts and verbalizers, which may limit
the application of such approaches in real-world scenarios. To solve this
issue, we present CP-Tuning, the first end-to-end Contrastive Prompt Tuning
framework for fine-tuning PLMs without any manual engineering of task-specific
prompts and verbalizers. It is integrated with the task-invariant continuous
prompt encoding technique with fully trainable prompt parameters. We further
propose the pair-wise cost-sensitive contrastive learning procedure to optimize
the model in order to achieve verbalizer-free class mapping and enhance the
task-invariance of prompts. It explicitly learns to distinguish different
classes and makes the decision boundary smoother by assigning different costs
to easy and hard cases. Experiments over a variety of language understanding
tasks used in IR systems and different PLMs show that CP-Tuning outperforms
state-of-the-art methods."
4229,"To completely disambiguate                    probabilities
the effects of the different sizes of vocabularies
in the baseline and CipherDAug transformers, we       To further study the efﬁcacy of our method in
                                                      under-regularized scenarios, we compare the base-
   22mosesdecoder/scripts/generic/multi-bleu.perl     line transformer model with CipherDAug for the
   23tensorﬂow/tensor2tensor/utils/get_ende_bleu.sh
                                                         24https://github.com/takase/alone_
                                                      seq2seq
dropout values of 0 (no regularization), 0.1, 0,2         model on the resulting augmented data.","Using embeddings largely independent of the           A.3.2 Effect of different dropout
vocabulary size.","The results
and 0.3 in Table 11.",2022-04-01 19:02:14+00:00,CipherDAug: Ciphertext based Data Augmentation for Neural Machine Translation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Nishant Kambhatla'), arxiv.Result.Author('Logan Born'), arxiv.Result.Author('Anoop Sarkar')]","We propose a novel data-augmentation technique for neural machine translation
based on ROT-$k$ ciphertexts. ROT-$k$ is a simple letter substitution cipher
that replaces a letter in the plaintext with the $k$th letter after it in the
alphabet. We first generate multiple ROT-$k$ ciphertexts using different values
of $k$ for the plaintext which is the source side of the parallel data. We then
leverage this enciphered training data along with the original parallel data
via multi-source training to improve neural machine translation. Our method,
CipherDAug, uses a co-regularization-inspired training procedure, requires no
external data sources other than the original training data, and uses a
standard Transformer to outperform strong data augmentation techniques on
several datasets by a significant margin. This technique combines easily with
existing approaches to data augmentation, and yields particularly strong
results in low-resource settings."
4244,"Most
                                            for further research in this area.","Hindi is spoken by approximately 567 mil-
                                            different models are indicative of the need        lion people in the world (WorldData, 2021).","We release      of the lower (district) courts in northern India use
                                            the corpus and model implementation code           Hindi as the ofﬁcial language.",2022-04-02 08:22:52+00:00,HLDC: Hindi Legal Documents Corpus,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Arnav Kapoor'), arxiv.Result.Author('Mudit Dhawan'), arxiv.Result.Author('Anmol Goel'), arxiv.Result.Author('T. H. Arjun'), arxiv.Result.Author('Akshala Bhatnagar'), arxiv.Result.Author('Vibhu Agrawal'), arxiv.Result.Author('Amul Agrawal'), arxiv.Result.Author('Arnab Bhattacharya'), arxiv.Result.Author('Ponnurangam Kumaraguru'), arxiv.Result.Author('Ashutosh Modi')]","Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: https://github.com/Exploration-Lab/HLDC"
4247,"As further research, we would like to investigate the performance of Elasticsearch with a cluster of nodes where it can
show its beneﬁts against the other two in-memory approaches.","The results have shown
that FAISS outperforms the other two approaches and that Elasticsearch fails to keep up, especially when bigger datasets
are used in a centralized scenario with one node.","Moreover, we would like to examine other efﬁcient
vector comparison approaches, like spatial and vector databases.",2022-04-02 09:08:34+00:00,Efficient comparison of sentence embeddings,cs.CL,"['cs.CL', 'cs.LG', 'I.2.7; H.4.0']","[arxiv.Result.Author('Spyros Zoupanos'), arxiv.Result.Author('Stratis Kolovos'), arxiv.Result.Author('Athanasios Kanavos'), arxiv.Result.Author('Orestis Papadimitriou'), arxiv.Result.Author('Manolis Maragoudakis')]","The domain of natural language processing (NLP), which has greatly evolved
over the last years, has highly benefited from the recent developments in word
and sentence embeddings. Such embeddings enable the transformation of complex
NLP tasks, like semantic similarity or Question and Answering (Q\&A), into much
simpler to perform vector comparisons. However, such a problem transformation
raises new challenges like the efficient comparison of embeddings and their
manipulation. In this work, we will discuss about various word and sentence
embeddings algorithms, we will select a sentence embedding algorithm, BERT, as
our algorithm of choice and we will evaluate the performance of two vector
comparison approaches, FAISS and Elasticsearch, in the specific problem of
sentence embeddings. According to the results, FAISS outperforms Elasticsearch
when used in a centralized environment with only one node, especially when big
datasets are included."
4248,"As further research, we would like to investigate the performance of Elasticsearch with a cluster of nodes where it can
show its beneﬁts against the other two in-memory approaches.","The results have shown
that FAISS outperforms the other two approaches and that Elasticsearch fails to keep up, especially when bigger datasets
are used in a centralized scenario with one node.","Moreover, we would like to examine other efﬁcient
vector comparison approaches, like spatial and vector databases.",2022-04-02 09:08:34+00:00,Efficient comparison of sentence embeddings,cs.CL,"['cs.CL', 'cs.LG', 'I.2.7; H.4.0']","[arxiv.Result.Author('Spyros Zoupanos'), arxiv.Result.Author('Stratis Kolovos'), arxiv.Result.Author('Athanasios Kanavos'), arxiv.Result.Author('Orestis Papadimitriou'), arxiv.Result.Author('Manolis Maragoudakis')]","The domain of natural language processing (NLP), which has greatly evolved
over the last years, has highly benefited from the recent developments in word
and sentence embeddings. Such embeddings enable the transformation of complex
NLP tasks, like semantic similarity or Question and Answering (Q&A), into much
simpler to perform vector comparisons. However, such a problem transformation
raises new challenges like the efficient comparison of embeddings and their
manipulation. In this work, we will discuss about various word and sentence
embeddings algorithms, we will select a sentence embedding algorithm, BERT, as
our algorithm of choice and we will evaluate the performance of two vector
comparison approaches, FAISS and Elasticsearch, in the specific problem of
sentence embeddings. According to the results, FAISS outperforms Elasticsearch
when used in a centralized environment with only one node, especially when big
datasets are included."
4376,"However, the two are not necessarily di-
Hence, we further study the performance of mod-         rectly comparable.",evidence.,"First, in Table 4, the two test
els on incorrect evidence.",2022-04-05 06:12:42+00:00,Fact Checking with Insufficient Evidence,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Pepa Atanasova'), arxiv.Result.Author('Jakob Grue Simonsen'), arxiv.Result.Author('Christina Lioma'), arxiv.Result.Author('Isabelle Augenstein')]","Automating the fact checking (FC) process relies on information obtained from
external sources. In this work, we posit that it is crucial for FC models to
make veracity predictions only when there is sufficient evidence and otherwise
indicate when it is not enough. To this end, we are the first to study what
information FC models consider sufficient by introducing a novel task and
advancing it with three main contributions. First, we conduct an in-depth
empirical analysis of the task with a new fluency-preserving method for
omitting information from the evidence at the constituent and sentence level.
We identify when models consider the remaining evidence (in)sufficient for FC,
based on three trained models with different Transformer architectures and
three FC datasets. Second, we ask annotators whether the omitted evidence was
important for FC, resulting in a novel diagnostic dataset, SufficientFacts, for
FC with omitted evidence. We find that models are least successful in detecting
missing evidence when adverbial modifiers are omitted (21% accuracy), whereas
it is easiest for omitted date modifiers (63% accuracy). Finally, we propose a
novel data augmentation strategy for contrastive self-learning of missing
evidence by employing the proposed omission method combined with tri-training.
It improves performance for Evidence Sufficiency Prediction by up to 17.8 F1
score, which in turn improves FC performance by up to 2.6 F1 score."
4432,"These results
suggest that end-to-end unsupervised ASR could be possible but further research and development
are necessary.","Unfortunately, we also discover that the word error
rate of the letter-based model is signiﬁcantly higher than the phone-based model.","5 Conclusion

An end-to-end approach for unsupervised ASR is key to increasing applicability to low-resource
languages.",2022-04-05 21:22:38+00:00,Towards End-to-end Unsupervised Speech Recognition,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Alexander H. Liu'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Michael Auli'), arxiv.Result.Author('Alexei Baevski')]","Unsupervised speech recognition has shown great potential to make Automatic
Speech Recognition (ASR) systems accessible to every language. However,
existing methods still heavily rely on hand-crafted pre-processing. Similar to
the trend of making supervised speech recognition end-to-end, we introduce
\wvu~which does away with all audio-side pre-processing and improves accuracy
through better architecture. In addition, we introduce an auxiliary
self-supervised objective that ties model predictions back to the input.
Experiments show that \wvu~improves unsupervised recognition results across
different languages while being conceptually simpler."
4433,"These results
suggest that end-to-end unsupervised ASR could be possible but further research and development
are necessary.","Unfortunately, we also discover that the word error
rate of the letter-based model is signiﬁcantly higher than the phone-based model.","5 Conclusion

An end-to-end approach for unsupervised ASR is key to increasing applicability to low-resource
languages.",2022-04-05 21:22:38+00:00,Towards End-to-end Unsupervised Speech Recognition,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Alexander H. Liu'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Michael Auli'), arxiv.Result.Author('Alexei Baevski')]","Unsupervised speech recognition has shown great potential to make Automatic
Speech Recognition (ASR) systems accessible to every language. However,
existing methods still heavily rely on hand-crafted pre-processing. Similar to
the trend of making supervised speech recognition end-to-end, we introduce
wav2vec-U 2.0 which does away with all audio-side pre-processing and improves
accuracy through better architecture. In addition, we introduce an auxiliary
self-supervised objective that ties model predictions back to the input.
Experiments show that wav2vec-U 2.0 improves unsupervised recognition results
across different languages while being conceptually simpler."
4470,"Over the years, many eye-tracking studies have been successfully done for translation
                                       process research, leading to key insights.10–13 In recent years, many studies involving EEG signals, too, have been conducted to
                                       understand the neural underpinnings of language.14 Our dataset will hopefully foster further research in that direction.","The tasks that the participants are subjected to have been
                                       designed so that similar experiments can be performed on the state-of-the-art algorithms for natural language processing.3

                                           Whether the observed eye-tracking and EEG data are sufﬁcient to relate human and machine processing of text accompanied
                                       by an image is an empirical question.","2 Methods

2.1 Equipment Used
An EyeLink 1000 Plus by SR-Research was used for recording the gaze data.",2022-04-06 15:47:55+00:00,"EMMT: A simultaneous eye-tracking, 4-electrode EEG and audio corpus for multi-modal reading and translation scenarios",cs.CL,"['cs.CL', 'cs.HC']","[arxiv.Result.Author('Sunit Bhattacharya'), arxiv.Result.Author('Věra Kloudová'), arxiv.Result.Author('Vilém Zouhar'), arxiv.Result.Author('Ondřej Bojar')]","We present the Eyetracked Multi-Modal Translation (EMMT) corpus, a dataset
containing monocular eye movement recordings, audio and 4-electrode
electroencephalogram (EEG) data of 43 participants. The objective was to
collect cognitive signals as responses of participants engaged in a number of
language intensive tasks involving different text-image stimuli settings when
translating from English to Czech.
  Each participant was exposed to 32 text-image stimuli pairs and asked to (1)
read the English sentence, (2) translate it into Czech, (3) consult the image,
(4) translate again, either updating or repeating the previous translation. The
text stimuli consisted of 200 unique sentences with 616 unique words coupled
with 200 unique images as the visual stimuli.
  The recordings were collected over a two week period and all the participants
included in the study were Czech natives with strong English skills. Due to the
nature of the tasks involved in the study and the relatively large number of
participants involved, the corpus is well suited for research in Translation
Process Studies, Cognitive Sciences among other disciplines."
4484,"Another direction is to further study the
integration of KID with full size foundation models, like GPT-3 175B to understand KID’s potential.","One could also investigate combining KID with prompt based generations to further
boost the performance, especially in the few-shot settings.",8We adopt the public implementation of GPT3—GPT-Neo (github.com/EleutherAI/gpt-neo).,2022-04-06 20:58:32+00:00,Knowledge Infused Decoding,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ruibo Liu'), arxiv.Result.Author('Guoqing Zheng'), arxiv.Result.Author('Shashank Gupta'), arxiv.Result.Author('Radhika Gaonkar'), arxiv.Result.Author('Chongyang Gao'), arxiv.Result.Author('Soroush Vosoughi'), arxiv.Result.Author('Milad Shokouhi'), arxiv.Result.Author('Ahmed Hassan Awadallah')]","Pre-trained language models (LMs) have been shown to memorize a substantial
amount of knowledge from the pre-training corpora; however, they are still
limited in recalling factually correct knowledge given a certain context.
Hence, they tend to suffer from counterfactual or hallucinatory generation when
used in knowledge-intensive natural language generation (NLG) tasks. Recent
remedies to this problem focus on modifying either the pre-training or task
fine-tuning objectives to incorporate knowledge, which normally require
additional costly training or architecture modification of LMs for practical
applications. We present Knowledge Infused Decoding (KID) -- a novel decoding
algorithm for generative LMs, which dynamically infuses external knowledge into
each step of the LM decoding. Specifically, we maintain a local knowledge
memory based on the current context, interacting with a dynamically created
external knowledge trie, and continuously update the local memory as a
knowledge-aware constraint to guide decoding via reinforcement learning. On six
diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)
armed with KID outperform many task-optimized state-of-the-art models, and show
particularly strong performance in few-shot scenarios over seven related
knowledge-infusion techniques. Human evaluation confirms KID's ability to
generate more relevant and factual language for the input context when compared
with multiple baselines. Finally, KID also alleviates exposure bias and
provides stable generation quality when generating longer sequences. Code for
KID is available at https://github.com/microsoft/KID."
4501,"In
els across domains indicates that the model is potentially learning  further research we will look at the use of more complex sentence
from common phrase structures seen across scientiﬁc disciplines.",The robustness of the mod-      are varying degrees of reliance on the structure of the article.,"embeddings, which can take into account tokens such as protein
This would then explain why the inclusion of additional article      and genes names which are currently treated as random embed-
and sentence-level features resulted in marginal increases in model  dings.",2022-04-07 09:09:33+00:00,Sequence-Based Extractive Summarisation for Scientific Articles,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Daniel Kershaw'), arxiv.Result.Author('Rob Koeling')]","This paper presents the results of research on supervised extractive text
summarisation for scientific articles. We show that a simple sequential tagging
model based only on the text within a document achieves high results against a
simple classification model. Improvements can be achieved through additional
sentence-level features, though these were minimal. Through further analysis,
we show the potential of the sequential model relying on the structure of the
document depending on the academic discipline which the document is from."
4524,"The explainability [22] and robustness [77] of deep learning-based models are also important
aspects that require further research.","This
makes tasks like the extension of existing data sets, data augmentation [78], and the creation of
synthetic data via generative models [70] very relevant to promote future progress in the field.","These can be indeed instrumental characteristics to assess
whether a model is capable of robust decision making and can be applied successfully to grade

, Vol.",2022-03-11 13:47:08+00:00,Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Stefan Haller'), arxiv.Result.Author('Adina Aldea'), arxiv.Result.Author('Christin Seifert'), arxiv.Result.Author('Nicola Strisciuglio')]","Automated short answer grading (ASAG) has gained attention in education as a
means to scale educational tasks to the growing number of students. Recent
progress in Natural Language Processing and Machine Learning has largely
influenced the field of ASAG, of which we survey the recent research
advancements. We complement previous surveys by providing a comprehensive
analysis of recently published methods that deploy deep learning approaches. In
particular, we focus our analysis on the transition from hand engineered
features to representation learning approaches, which learn representative
features for the task at hand automatically from large corpora of data. We
structure our analysis of deep learning methods along three categories: word
embeddings, sequential models, and attention-based methods. Deep learning
impacted ASAG differently than other fields of NLP, as we noticed that the
learned representations alone do not contribute to achieve the best results,
but they rather show to work in a complementary way with hand-engineered
features. The best performance are indeed achieved by methods that combine the
carefully hand-engineered features with the power of the semantic descriptions
provided by the latest models, like transformers architectures. We identify
challenges and provide an outlook on research direction that can be addressed
in the future"
4530,"Indeed recent papers on this topic [27,23,2,8] highlight that after more than
                                        ten years of research from the seminal work in [17], process extraction from text is a task
                                        far from being resolved and further research in this direction is needed with the purpose
                                        of improving the quality of the process model generation.","The ambiguous nature of natural language, the multiple possible writing styles,
                                        and the great variability of possible domains of application make this task extremely
                                        challenging.","By looking at the state of
                                        the art in this ﬁeld, most of the existing approaches rely on template and rule based
                                        approaches, which often lack the ﬂexibility needed to fully cover the great variability of
                                        writing styles and process domains [5,3,1,29,17,14,19,15,25].",2022-03-31 09:00:46+00:00,Leveraging pre-trained language models for conversational information seeking from text,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Patrizio Bellan'), arxiv.Result.Author('Mauro Dragoni'), arxiv.Result.Author('Chiara Ghidini')]","Recent advances in Natural Language Processing, and in particular on the
construction of very large pre-trained language representation models, is
opening up new perspectives on the construction of conversational information
seeking (CIS) systems. In this paper we investigate the usage of in-context
learning and pre-trained language representation models to address the problem
of information extraction from process description documents, in an incremental
question and answering oriented fashion. In particular we investigate the usage
of the native GPT-3 (Generative Pre-trained Transformer 3) model, together with
two in-context learning customizations that inject conceptual definitions and a
limited number of samples in a few shot-learning fashion. The results highlight
the potential of the approach and the usefulness of the in-context learning
customizations, which can substantially contribute to address the ""training
data challenge"" of deep learning based NLP techniques the BPM field. It also
highlight the challenge posed by control flow relations for which further
training needs to be devised."
4531,"We reported a suite of lessons learned from this experience that will drive
the development of further research questions.","The results obtained by the in-context learning
strategy opened the possibility to use this technique to address the construction of
business process model by starting from natural language text in scenarios where it is
necessary to manage the low-resources issues and by exploiting the human-in-the-loop
paradigm given the role of the domain expert in processing the information provided by
the model.","References

 1. de A. R. Gonçalves, J.C., Santoro, F.M., Baião, F.A.",2022-03-31 09:00:46+00:00,Leveraging pre-trained language models for conversational information seeking from text,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Patrizio Bellan'), arxiv.Result.Author('Mauro Dragoni'), arxiv.Result.Author('Chiara Ghidini')]","Recent advances in Natural Language Processing, and in particular on the
construction of very large pre-trained language representation models, is
opening up new perspectives on the construction of conversational information
seeking (CIS) systems. In this paper we investigate the usage of in-context
learning and pre-trained language representation models to address the problem
of information extraction from process description documents, in an incremental
question and answering oriented fashion. In particular we investigate the usage
of the native GPT-3 (Generative Pre-trained Transformer 3) model, together with
two in-context learning customizations that inject conceptual definitions and a
limited number of samples in a few shot-learning fashion. The results highlight
the potential of the approach and the usefulness of the in-context learning
customizations, which can substantially contribute to address the ""training
data challenge"" of deep learning based NLP techniques the BPM field. It also
highlight the challenge posed by control flow relations for which further
training needs to be devised."
4563,"focused shifted towards detecting and attenuating                                                                                We hope that with this work, especially the novel
biases in their successors contextualized word em-                                                                               AB A resource, we will foster further research on
beddings (Dev and Phillips, 2019; Dev et al., 2020;                                                                              fair computational argumentation.","An additional downstream evaluation on ar-
Goldberg, 2019; Dev and Phillips, 2019; Manzini                                                                                  gument quality prediction indicated that debiasing
et al., 2019; Lauscher et al., 2020a), and later, the                                                                            can even lead in some cases to improved results.","Tan and Celis, 2019).",2022-04-08 12:23:46+00:00,Fair and Argumentative Language Modeling for Computational Argumentation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Carolin Holtermann'), arxiv.Result.Author('Anne Lauscher'), arxiv.Result.Author('Simone Paolo Ponzetto')]","Although much work in NLP has focused on measuring and mitigating
stereotypical bias in semantic spaces, research addressing bias in
computational argumentation is still in its infancy. In this paper, we address
this research gap and conduct a thorough investigation of bias in argumentative
language models. To this end, we introduce ABBA, a novel resource for bias
measurement specifically tailored to argumentation. We employ our resource to
assess the effect of argumentative fine-tuning and debiasing on the intrinsic
bias found in transformer-based language models using a lightweight
adapter-based approach that is more sustainable and parameter-efficient than
full fine-tuning. Finally, we analyze the potential impact of language model
debiasing on the performance in argument quality prediction, a downstream task
of computational argumentation. Our results show that we are able to
successfully and sustainably remove bias in general and argumentative language
models while preserving (and sometimes improving) model performance in
downstream tasks. We make all experimental code and data available at
https://github.com/umanlp/FairArgumentativeLM."
4590,"European Language Re-
encourages further research in the area.","In Proceedings of the 12th Lan-
hope our work leads to building more accurate              guage Resources and Evaluation Conference, pages
TOD systems with similar or less overhead, and             422–428, Marseille, France.",sources Association.,2022-04-08 23:27:18+00:00,"Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",cs.CL,['cs.CL'],"[arxiv.Result.Author('Raghav Gupta'), arxiv.Result.Author('Harrison Lee'), arxiv.Result.Author('Jeffrey Zhao'), arxiv.Result.Author('Abhinav Rastogi'), arxiv.Result.Author('Yuan Cao'), arxiv.Result.Author('Yonghui Wu')]","Building universal dialogue systems that can seamlessly operate across
multiple domains/APIs and generalize to new ones with minimal supervision and
maintenance is a critical challenge. Recent works have leveraged natural
language descriptions for schema elements to enable such systems; however,
descriptions can only indirectly convey schema semantics. In this work, we
propose Show, Don't Tell, a prompt format for seq2seq modeling which uses a
short labeled example dialogue to show the semantics of schema elements rather
than tell the model via descriptions. While requiring similar effort from
service developers, we show that using short examples as schema representations
with large language models results in stronger performance and better
generalization on two popular dialogue state tracking benchmarks: the
Schema-Guided Dialogue dataset and the MultiWoZ leave-one-out benchmark."
4591,encourages further research in the area.,"Many MultiWOZ cross-                hope our work leads to building more accurate
domain models leverage slot names/descriptions           TOD systems with similar or less overhead and
(Wu et al., 2019; Lee et al., 2019; Lin et al., 2021a).","Pretrained generative LLMs (Raffel et al., 2020;      References
Brown et al., 2020) have enabled framing NLP
tasks as seq2seq problems.",2022-04-08 23:27:18+00:00,"Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",cs.CL,['cs.CL'],"[arxiv.Result.Author('Raghav Gupta'), arxiv.Result.Author('Harrison Lee'), arxiv.Result.Author('Jeffrey Zhao'), arxiv.Result.Author('Abhinav Rastogi'), arxiv.Result.Author('Yuan Cao'), arxiv.Result.Author('Yonghui Wu')]","Building universal dialogue systems that operate across multiple domains/APIs
and generalize to new ones with minimal overhead is a critical challenge.
Recent works have leveraged natural language descriptions of schema elements to
enable such systems; however, descriptions only indirectly convey schema
semantics. In this work, we propose Show, Don't Tell, which prompts seq2seq
models with a labeled example dialogue to show the semantics of schema elements
rather than tell the model through descriptions. While requiring similar effort
from service developers as generating descriptions, we show that using short
examples as schema representations with large language models results in
state-of-the-art performance on two popular dialogue state tracking benchmarks
designed to measure zero-shot generalization - the Schema-Guided Dialogue
dataset and the MultiWOZ leave-one-out benchmark."
4601,useful for further research on the topic.,"We further take advantage of the in-       the focus is on detection of social media posts from
                                            terpretability of the Logistic Regression model   users with different degrees of depression and in
                                            and we make an attempt to interpret the model     some other cases, the goal was to analyze the lan-
                                            coefﬁcients with the hope that these will be      guage style of people with depression.","There is a large number of works that have at-
                                       1 Introduction                                         tempted to detect depression from Social Media
                                                                                              text.",2022-04-09 14:27:13+00:00,KUCST@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text,cs.CL,['cs.CL'],"[arxiv.Result.Author('Manex Agirrezabal'), arxiv.Result.Author('Janek Amann')]","In this paper we present our approach for detecting signs of depression from
social media text. Our model relies on word unigrams, part-of-speech tags,
readabilitiy measures and the use of first, second or third person and the
number of words. Our best model obtained a macro F1-score of 0.439 and ranked
25th, out of 31 teams. We further take advantage of the interpretability of the
Logistic Regression model and we make an attempt to interpret the model
coefficients with the hope that these will be useful for further research on
the topic."
4602,for further research.,"A better
                                                        approach would be to use a bootstrapping approach,
   As the logistic regression model features are in-    training several models from subsets of the training
terpretable, we decided to analyze them more thor-      corpus and analyzing the weight importance among
oughly, with the hope that this analysis is helpful     several of those models.","For this analysis, we used the
second model that makes use of the all the features     6 Conclusion and Future Work
and they were obtained after training the model
                                                        In this paper we presented our attempt to classify
     7All parameters are set to the default values.",2022-04-09 14:27:13+00:00,KUCST@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text,cs.CL,['cs.CL'],"[arxiv.Result.Author('Manex Agirrezabal'), arxiv.Result.Author('Janek Amann')]","In this paper we present our approach for detecting signs of depression from
social media text. Our model relies on word unigrams, part-of-speech tags,
readabilitiy measures and the use of first, second or third person and the
number of words. Our best model obtained a macro F1-score of 0.439 and ranked
25th, out of 31 teams. We further take advantage of the interpretability of the
Logistic Regression model and we make an attempt to interpret the model
coefficients with the hope that these will be useful for further research on
the topic."
4603,"Below we outline
                                                        some possibilities for further research.","As mentioned
                                                        above, the model has several aspects that could be
                                                        improved given its performance.","Following recent advances in Natural Language
                                                        Processing, we think that including a pretrained
                                                        word embedding model, such as BERT (Devlin
                                                        et al., 2019) would have positively contributed to
                                                        the performance.",2022-04-09 14:27:13+00:00,KUCST@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text,cs.CL,['cs.CL'],"[arxiv.Result.Author('Manex Agirrezabal'), arxiv.Result.Author('Janek Amann')]","In this paper we present our approach for detecting signs of depression from
social media text. Our model relies on word unigrams, part-of-speech tags,
readabilitiy measures and the use of first, second or third person and the
number of words. Our best model obtained a macro F1-score of 0.439 and ranked
25th, out of 31 teams. We further take advantage of the interpretability of the
Logistic Regression model and we make an attempt to interpret the model
coefficients with the hope that these will be useful for further research on
the topic."
4633,"Arguably those sentences that have             Relational Category To further study the im-
been correctly labeled by distant supervision (e.g.","et al., 2021).","pact of bag-level and sentence-level training on
Fig.",2022-04-10 22:07:25+00:00,MedDistant19: A Challenging Benchmark for Distantly Supervised Biomedical Relation Extraction,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Saadullah Amin'), arxiv.Result.Author('Pasquale Minervini'), arxiv.Result.Author('David Chang'), arxiv.Result.Author('Günter Neumann'), arxiv.Result.Author('Pontus Stenetorp')]","Relation Extraction in the biomedical domain is challenging due to the lack
of labeled data and high annotation costs, needing domain experts. Distant
supervision is commonly used as a way to tackle the scarcity of annotated data
by automatically pairing knowledge graph relationships with raw texts.
Distantly Supervised Biomedical Relation Extraction (Bio-DSRE) models can
seemingly produce very accurate results in several benchmarks. However, given
the challenging nature of the task, we set out to investigate the validity of
such impressive results. We probed the datasets used by Amin et al. (2020) and
Hogan et al. (2021) and found a significant overlap between training and
evaluation relationships that, once resolved, reduced the accuracy of the
models by up to 71%. Furthermore, we noticed several inconsistencies with the
data construction process, such as creating negative samples and improper
handling of redundant relationships. We mitigate these issues and present
MedDistant19, a new benchmark dataset obtained by aligning the MEDLINE
abstracts with the widely used SNOMED Clinical Terms (SNOMED CT) knowledge
base. We experimented with several state-of-the-art models achieving an AUC of
55.4% and 49.8% at sentence- and bag-level, showing that there is still plenty
of room for improvement."
4634,"To further study the im-
                                                                 pact of bag-level pooling strategies, we analyze the
Table 8: Averaged F1-micro score on relation-speciﬁc             relation category-speciﬁc results.","(2021) found ATT to produce less ac-
BERT+bag+ONE 52.6 33.2 47.1                                      curate results with LMs, which we also ﬁnd to hold
BERT+bag+ATT 56.4 30.7 26.4                                      true for MEDDISTANT19.","Following Chang
category for bag pooling methods.",2022-04-10 22:07:25+00:00,MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Saadullah Amin'), arxiv.Result.Author('Pasquale Minervini'), arxiv.Result.Author('David Chang'), arxiv.Result.Author('Pontus Stenetorp'), arxiv.Result.Author('Günter Neumann')]","Relation extraction in the biomedical domain is challenging due to the lack
of labeled data and high annotation costs, needing domain experts. Distant
supervision is commonly used to tackle the scarcity of annotated data by
automatically pairing knowledge graph relationships with raw texts. Such a
pipeline is prone to noise and has added challenges to scale for covering a
large number of biomedical concepts. We investigated existing broad-coverage
distantly supervised biomedical relation extraction benchmarks and found a
significant overlap between training and test relationships ranging from 26% to
86%. Furthermore, we noticed several inconsistencies in the data construction
process of these benchmarks, and where there is no train-test leakage, the
focus is on interactions between narrower entity types. This work presents a
more accurate benchmark MedDistant19 for broad-coverage distantly supervised
biomedical relation extraction that addresses these shortcomings and is
obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical
Terms knowledge base. Lacking thorough evaluation with domain-specific language
models, we also conduct experiments validating general domain relation
extraction findings to biomedical relation extraction."
4635,"We thoroughly evaluated the benchmark
ical model, highlighting the presence of shallow                   with baselines and state-of-the-art, showing there
heuristics in the data common to the general and                   is room to conduct further research.","With-                          bridged this gap by utilizing SNOMED CT for con-
out domain-speciﬁc knowledge, BERT performs                        structing the benchmark and laying out the best
slightly worse than the lowest-performing biomed-                  practices.",biomedical domains.,2022-04-10 22:07:25+00:00,MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Saadullah Amin'), arxiv.Result.Author('Pasquale Minervini'), arxiv.Result.Author('David Chang'), arxiv.Result.Author('Pontus Stenetorp'), arxiv.Result.Author('Günter Neumann')]","Relation extraction in the biomedical domain is challenging due to the lack
of labeled data and high annotation costs, needing domain experts. Distant
supervision is commonly used to tackle the scarcity of annotated data by
automatically pairing knowledge graph relationships with raw texts. Such a
pipeline is prone to noise and has added challenges to scale for covering a
large number of biomedical concepts. We investigated existing broad-coverage
distantly supervised biomedical relation extraction benchmarks and found a
significant overlap between training and test relationships ranging from 26% to
86%. Furthermore, we noticed several inconsistencies in the data construction
process of these benchmarks, and where there is no train-test leakage, the
focus is on interactions between narrower entity types. This work presents a
more accurate benchmark MedDistant19 for broad-coverage distantly supervised
biomedical relation extraction that addresses these shortcomings and is
obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical
Terms knowledge base. Lacking thorough evaluation with domain-specific language
models, we also conduct experiments validating general domain relation
extraction findings to biomedical relation extraction."
4643,"F Intended Use

We hope our work will inform further research into
style and its representations.","Weg-
mann and Nguyen (2021) state the intended use of
developing improved style(-sensitive) measures.","We invite researchers
to reuse any of our provided results, code and data
for this purpose.",2022-04-11 07:15:06+00:00,Same Author or Just Same Topic? Towards Content-Independent Style Representations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Anna Wegmann'), arxiv.Result.Author('Marijn Schraagen'), arxiv.Result.Author('Dong Nguyen')]","Linguistic style is an integral component of language. Recent advances in the
development of style representations have increasingly used training objectives
from authorship verification (AV): Do two texts have the same author? The
assumption underlying the AV training task (same author approximates same
writing style) enables self-supervised and, thus, extensive training. However,
a good performance on the AV task does not ensure good ""general-purpose"" style
representations. For example, as the same author might typically write about
certain topics, representations trained on AV might also encode content
information instead of style alone. We introduce a variation of the AV training
task that controls for content using conversation or domain labels. We evaluate
whether known style dimensions are represented and preferred over content
information through an original variation to the recently proposed STEL
framework. We find that representations trained by controlling for conversation
are better than representations trained with domain or no content control at
representing style independent from content."
4644,"We would like to distinguish our work from
release our code, the new annotated English CSRL        the work (Wu et al., 2021b) which purely focuses
test sets and checkpoints of our best models to facil-  on improving the monolingual CSRL performance
itate the further research at https://github.",(4) We    CSRL.,"where they try to model predicate-aware represen-
com/hahahawu/Zero-Shot-XCSRL.",2022-04-11 07:29:39+00:00,Zero-shot Cross-lingual Conversational Semantic Role Labeling,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Han Wu'), arxiv.Result.Author('Haochen Tan'), arxiv.Result.Author('Kun Xu'), arxiv.Result.Author('Shuqi Liu'), arxiv.Result.Author('Lianwei Wu'), arxiv.Result.Author('Linqi Song')]","While conversational semantic role labeling (CSRL) has shown its usefulness
on Chinese conversational tasks, it is still under-explored in non-Chinese
languages due to the lack of multilingual CSRL annotations for the parser
training. To avoid expensive data collection and error-propagation of
translation-based methods, we present a simple but effective approach to
perform zero-shot cross-lingual CSRL. Our model implicitly learns
language-agnostic, conversational structure-aware and semantically rich
representations with the hierarchical encoders and elaborately designed
pre-training objectives. Experimental results show that our model outperforms
all baselines by large margins on two newly collected English CSRL test sets.
More importantly, we confirm the usefulness of CSRL to non-Chinese
conversational tasks such as the question-in-context rewriting task in English
and the multi-turn dialogue response generation tasks in English, German and
Japanese by incorporating the CSRL information into the downstream
conversation-based models. We believe this finding is significant and will
facilitate the research of non-Chinese dialogue tasks which suffer the problems
of ellipsis and anaphora."
4646,"To analyze this phe-
putational resources, we sub-sampled texts for each     nomenon, we intend to launch further research and
of the ﬁve runs.","As a reminder, since       a dataset for experiments, seeds play a signiﬁcant
we collected a massive dataset and had limited com-     role (see results in Figure 4).","Sub-samples between different          use noise ratio (Northcutt et al., 2021) and data car-
models stay the same.",2022-04-11 08:22:05+00:00,Assessment of Massively Multilingual Sentiment Classifiers,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Krzysztof Rajda'), arxiv.Result.Author('Łukasz Augustyniak'), arxiv.Result.Author('Piotr Gramacki'), arxiv.Result.Author('Marcin Gruza'), arxiv.Result.Author('Szymon Woźniak'), arxiv.Result.Author('Tomasz Kajdanowicz')]","Models are increasing in size and complexity in the hunt for SOTA. But what
if those 2\% increase in performance does not make a difference in a production
use case? Maybe benefits from a smaller, faster model outweigh those slight
performance gains. Also, equally good performance across languages in
multilingual tasks is more important than SOTA results on a single one. We
present the biggest, unified, multilingual collection of sentiment analysis
datasets. We use these to assess 11 models and 80 high-quality sentiment
datasets (out of 342 raw datasets collected) in 27 languages and included
results on the internally annotated datasets. We deeply evaluate multiple
setups, including fine-tuning transformer-based models for measuring
performance. We compare results in numerous dimensions addressing the imbalance
in both languages coverage and dataset sizes. Finally, we present some best
practices for working with such a massive collection of datasets and models
from a multilingual perspective."
4647,"It is hard to tell why with-  be, in our opinion, a good start to a comprehensive
out in-depth analysis, hence we intend to conduct       analysis of datasets quality for the multi-lingual
further research on the topic of data quality in sen-   sentiment classiﬁcation task which we intend to
timent analysis tasks using techniques like noise       perform.","This will
tinctive characteristics.","ratio (Northcutt et al., 2021) or data cartography
(Swayamdipta et al., 2020).",2022-04-11 08:22:05+00:00,Assessment of Massively Multilingual Sentiment Classifiers,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Krzysztof Rajda'), arxiv.Result.Author('Łukasz Augustyniak'), arxiv.Result.Author('Piotr Gramacki'), arxiv.Result.Author('Marcin Gruza'), arxiv.Result.Author('Szymon Woźniak'), arxiv.Result.Author('Tomasz Kajdanowicz')]","Models are increasing in size and complexity in the hunt for SOTA. But what
if those 2\% increase in performance does not make a difference in a production
use case? Maybe benefits from a smaller, faster model outweigh those slight
performance gains. Also, equally good performance across languages in
multilingual tasks is more important than SOTA results on a single one. We
present the biggest, unified, multilingual collection of sentiment analysis
datasets. We use these to assess 11 models and 80 high-quality sentiment
datasets (out of 342 raw datasets collected) in 27 languages and included
results on the internally annotated datasets. We deeply evaluate multiple
setups, including fine-tuning transformer-based models for measuring
performance. We compare results in numerous dimensions addressing the imbalance
in both languages coverage and dataset sizes. Finally, we present some best
practices for working with such a massive collection of datasets and models
from a multilingual perspective."
4648,"It is very meaningful in realistic sce-
2019), N-way K-shot accuracy is the standard           nario and can contribute to the further research.","Methods      5-way   OOS    GFSL   5-way                 Liu   GFSL   5-way   FaqIr    GFSL
             92.20  10-way  61.94  82.46               10-way  47.66  89.83  10-way    60.78
Proto        89.78   87.91  58.34  78.25                73.23  41.95  86.74   81.56    53.85
Matching     80.44   84.41  34.00  65.58                67.45  24.73  71.62   78.77    20.10
Induction    92.84   70.92  65.52  82.38                51.56  51.27  85.01   56.99    62.62
Proto-HATT   95.99   89.11  74.39  87.39                75.29  57.24  94.77   76.17    74.42
MLMAN        96.36   93.41  76.23  87.84                79.82  57.66  95.14   89.49    75.81
MGIMN(ours)          94.00                              80.60                 90.69

Table 2: Experiment results of standard FSL(5-way 5-shot and 10-way 5-shot) and generalized FSL settings on
intent classiﬁcation datasets(OOS,Liu and FaqIr datasets)

Methods      5-way  Amzn    GFSL                       Methods        5-way  Huffpost  GFSL
             78.40  10-way  41.03                                     51.57  10-way    16.47
Proto        75.73   69.02  38.34                      Proto          49.77    36.74   14.18
Matching     64.02   64.17  20.09                      Matching       44.69    34.28   10.40
Induction    78.05   50.12  41.81                      Induction      51.23    29.35   16.06
Proto-HATT   85.64   69.00  46.71                      Proto-HATT     52.76    36.65   16.78
MLMAN        85.96   79.39  49.46                      MLMAN          54.98    38.22   19.61
MGIMN(ours)          80.07                             MGIMN(ours)             40.12

Table 3: Experiment results of standard FSL (5-way 5-  Table 4: Experiment results of standard FSL (5-way 5-
shot and 10-way 5-shot) and generalized FSL settings   shot and 10-way 5-shot) and generalized FSL settings
on Amzn datasets                                       on Huffpost datasets

Generalized FSL In most studies of few-shot            FSL evaluation is worse and more challenging than
text classiﬁcation (Bao et al., 2019; Gao et al.,      standard FSL.","It
evaluation metric.",2022-04-11 08:58:55+00:00,MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jianhai Zhang'), arxiv.Result.Author('Mieradilijiang Maimaiti'), arxiv.Result.Author('Xing Gao'), arxiv.Result.Author('Yuanhang Zheng'), arxiv.Result.Author('Ji Zhang')]","Text classification struggles to generalize to unseen classes with very few
labeled text instances per class. In such a few-shot learning (FSL) setting,
metric-based meta-learning approaches have shown promising results. Previous
studies mainly aim to derive a prototype representation for each class.
However, they neglect that it is challenging-yet-unnecessary to construct a
compact representation which expresses the entire meaning for each class. They
also ignore the importance to capture the inter-dependency between query and
the support set for few-shot text classification. To deal with these issues, we
propose a meta-learning based method MGIMN which performs instance-wise
comparison followed by aggregation to generate class-wise matching vectors
instead of prototype learning. The key of instance-wise comparison is the
interactive matching within the class-specific context and episode-specific
context. Extensive experiments demonstrate that the proposed method
significantly outperforms the existing state-of-the-art approaches, under both
the standard FSL and generalized FSL settings."
4649,"It is very meaningful in realistic sce-
2019), N-way K-shot accuracy is the standard           nario and can contribute to the further research.","Methods      5-way   OOS    GFSL   5-way                 Liu   GFSL   5-way   FaqIr    GFSL
             92.20  10-way  61.94  82.46               10-way  47.66  89.83  10-way    60.78
Proto        89.78   87.91  58.34  78.25                73.23  41.95  86.74   81.56    53.85
Matching     80.44   84.41  34.00  65.58                67.45  24.73  71.62   78.77    20.10
Induction    92.84   70.92  65.52  82.38                51.56  51.27  85.01   56.99    62.62
Proto-HATT   95.99   89.11  74.39  87.39                75.29  57.24  94.77   76.17    74.42
MLMAN        96.36   93.41  76.23  87.84                79.82  57.66  95.14   89.49    75.81
MGIMN(ours)          94.00                              80.60                 90.69

Table 2: Experiment results of standard FSL(5-way 5-shot and 10-way 5-shot) and generalized FSL settings on
intent classiﬁcation datasets(OOS,Liu and FaqIr datasets)

Methods      5-way  Amzn    GFSL                       Methods        5-way  Huffpost  GFSL
             78.40  10-way  41.03                                     51.57  10-way    16.47
Proto        75.73   69.02  38.34                      Proto          49.77    36.74   14.18
Matching     64.02   64.17  20.09                      Matching       44.69    34.28   10.40
Induction    78.05   50.12  41.81                      Induction      51.23    29.35   16.06
Proto-HATT   85.64   69.00  46.71                      Proto-HATT     52.76    36.65   16.78
MLMAN        85.96   79.39  49.46                      MLMAN          54.98    38.22   19.61
MGIMN(ours)          80.07                             MGIMN(ours)             40.12

Table 3: Experiment results of standard FSL (5-way 5-  Table 4: Experiment results of standard FSL (5-way 5-
shot and 10-way 5-shot) and generalized FSL settings   shot and 10-way 5-shot) and generalized FSL settings
on Amzn datasets                                       on Huffpost datasets

Generalized FSL In most studies of few-shot            FSL evaluation is worse and more challenging than
text classiﬁcation (Bao et al., 2019; Gao et al.,      standard FSL.","It
evaluation metric.",2022-04-11 08:58:55+00:00,MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jianhai Zhang'), arxiv.Result.Author('Mieradilijiang Maimaiti'), arxiv.Result.Author('Xing Gao'), arxiv.Result.Author('Yuanhang Zheng'), arxiv.Result.Author('Ji Zhang')]","Text classification struggles to generalize to unseen classes with very few
labeled text instances per class. In such a few-shot learning (FSL) setting,
metric-based meta-learning approaches have shown promising results. Previous
studies mainly aim to derive a prototype representation for each class.
However, they neglect that it is challenging-yet-unnecessary to construct a
compact representation which expresses the entire meaning for each class. They
also ignore the importance to capture the inter-dependency between query and
the support set for few-shot text classification. To deal with these issues, we
propose a meta-learning based method MGIMN which performs instance-wise
comparison followed by aggregation to generate class-wise matching vectors
instead of prototype learning. The key of instance-wise comparison is the
interactive matching within the class-specific context and episode-specific
context. Extensive experiments demonstrate that the proposed method
significantly outperforms the existing state-of-the-art approaches, under both
the standard FSL and generalized FSL settings."
4650,"It is very meaningful in realistic sce-
eralized FSL settings on Amzn datasets, while the FSL  nario and can contribute to the further research.","As shown
MGIMN(ours)          80.07                             in Table 2, 3 and 4, the performance of generalized
                                                       FSL evaluation is worse and more challenging than
Table 3: Experiment results of standard FSL and gen-   standard FSL.","It
setting is same with Table 2.                          is noteworthy that, comparing with Proto, our pro-
                                                       posed approach makes bigger improvement in the
the averaged scores over 15 runs (different seen-      challenging generalized FSL metric (GFSL) than
unseen class splits and random seeds as introduced     the improvements in standard FSL metric(FSL),
in section 4.1.1) for each dataset and model.",2022-04-11 08:58:55+00:00,MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jianhai Zhang'), arxiv.Result.Author('Mieradilijiang Maimaiti'), arxiv.Result.Author('Xing Gao'), arxiv.Result.Author('Yuanhang Zheng'), arxiv.Result.Author('Ji Zhang')]","Text classification struggles to generalize to unseen classes with very few
labeled text instances per class. In such a few-shot learning (FSL) setting,
metric-based meta-learning approaches have shown promising results. Previous
studies mainly aim to derive a prototype representation for each class.
However, they neglect that it is challenging-yet-unnecessary to construct a
compact representation which expresses the entire meaning for each class. They
also ignore the importance to capture the inter-dependency between query and
the support set for few-shot text classification. To deal with these issues, we
propose a meta-learning based method MGIMN which performs instance-wise
comparison followed by aggregation to generate class-wise matching vectors
instead of prototype learning. The key of instance-wise comparison is the
interactive matching within the class-specific context and episode-specific
context. Extensive experiments demonstrate that the proposed method
significantly outperforms the existing state-of-the-art approaches, under both
the standard FSL and generalized FSL settings."
4668,"Some of these issues require
long-term collaborative eﬀorts within the community as well as substantial
support from academic funding agencies for further research.","We cautiously
note that not all the problematic issues could easily be resolved by individual
researchers and research groups immediately.","In short, we hope that our survey and its companion webpage will serve as
a useful reference for locating resources for existing fundamental and applied
research and for creating future resources and projects for Turkish and/or other
languages.",2022-04-11 12:23:07+00:00,Resources for Turkish Natural Language Processing: A critical survey,cs.CL,['cs.CL'],"[arxiv.Result.Author('Çağrı Çöltekin'), arxiv.Result.Author('A. Seza Doğruöz'), arxiv.Result.Author('Özlem Çetinoğlu')]","This paper presents a comprehensive survey of corpora and lexical resources
available for Turkish. We review a broad range of resources, focusing on the
ones that are publicly available. In addition to providing information about
the available linguistic resources, we present a set of recommendations, and
identify gaps in the data available for conducting research and building
applications in Turkish Linguistics and Natural Language Processing."
4669,"Some of these is-
sues require long-term collaborative eﬀorts within the community as well as

substantial support from academic funding agencies for further research.","We

cautiously note that not all the problematic issues could easily be resolved

by individual researchers and research groups immediately.","The

issues we raise in this paper are based on our impression from published pa-
pers and cursory inspection of the available corpora.",2022-04-11 12:23:07+00:00,Resources for Turkish Natural Language Processing: A critical survey,cs.CL,['cs.CL'],"[arxiv.Result.Author('Çağrı Çöltekin'), arxiv.Result.Author('A. Seza Doğruöz'), arxiv.Result.Author('Özlem Çetinoğlu')]","This paper presents a comprehensive survey of corpora and lexical resources
available for Turkish. We review a broad range of resources, focusing on the
ones that are publicly available. In addition to providing information about
the available linguistic resources, we present a set of recommendations, and
identify gaps in the data available for conducting research and building
applications in Turkish Linguistics and Natural Language Processing."
4749,"We hope that the proposed techniques will
motivate further research in this ﬁeld, including ex-    Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-
ploration of the same phenomenon of cross-lingual           Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-
transfer in other language families and multilingual        ishnan, Marc’Aurelio Ranzato, Francisco Guzman,
tasks.",2020).,and Angela Fan.,2022-04-12 13:52:54+00:00,MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Gokul Karthik Kumar'), arxiv.Result.Author('Abhishek Singh Gehlot'), arxiv.Result.Author('Sahal Shaji Mullappilly'), arxiv.Result.Author('Karthik Nandakumar')]","Accuracy of English-language Question Answering (QA) systems has improved
significantly in recent years with the advent of Transformer-based models
(e.g., BERT). These models are pre-trained in a self-supervised fashion with a
large English text corpus and further fine-tuned with a massive English QA
dataset (e.g., SQuAD). However, QA datasets on such a scale are not available
for most of the other languages. Multi-lingual BERT-based models (mBERT) are
often used to transfer knowledge from high-resource languages to low-resource
languages. Since these models are pre-trained with huge text corpora containing
multiple languages, they typically learn language-agnostic embeddings for
tokens from different languages. However, directly training an mBERT-based QA
system for low-resource languages is challenging due to the paucity of training
data. In this work, we augment the QA samples of the target language using
translation and transliteration into other languages and use the augmented data
to fine-tune an mBERT-based QA model, which is already pre-trained in English.
Experiments on the Google ChAII dataset show that fine-tuning the mBERT model
with translations from the same language family boosts the question-answering
performance, whereas the performance degrades in the case of cross-language
families. We further show that introducing a contrastive loss between the
translated question-context feature pairs during the fine-tuning process,
prevents such degradation with cross-lingual family translations and leads to
marginal improvement. The code for this work is available at
https://github.com/gokulkarthik/mucot."
4750,"Any opin-
                                                      ions, ﬁndings, conclusions, or recommendations ex-
   For further research, we will investigate the se-  pressed herein are those of the authors and should
lection of label surface names and how to bet-        not be interpreted as necessarily representing the
ter leverage the semantics from the pre-trained       views, either expressed or implied, of the U.S. Gov-
sequence-to-sequence models.","from Google, Adobe, and Teradata.",We also notice that      ernment.,2022-03-30 18:30:42+00:00,Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Zilong Wang'), arxiv.Result.Author('Jingbo Shang')]","Entity recognition is a fundamental task in understanding document images.
Traditional sequence labeling frameworks treat the entity types as class IDs
and rely on extensive data and high-quality annotations to learn semantics
which are typically expensive in practice. In this paper, we aim to build an
entity recognition model requiring only a few shots of annotated document
images. To overcome the data limitation, we propose to leverage the label
surface names to better inform the model of the target entity type semantics
and also embed the labels into the spatial embedding space to capture the
spatial correspondence between regions and labels. Specifically, we go beyond
sequence labeling and develop a novel label-aware seq2seq framework, LASER. The
proposed model follows a new labeling scheme that generates the label surface
names word-by-word explicitly after generating the entities. During training,
LASER refines the label semantics by updating the label surface name
representations and also strengthens the label-region correlation. In this way,
LASER recognizes the entities from document images through both semantic and
layout correspondence. Extensive experiments on two benchmark datasets
demonstrate the superiority of LASER under the few-shot setting."
4752,"We ﬁnd this simple relation quite striking, and believe it merits further study.","This analysis suggests that these RL learning curves might be associated with changes in the RL policy that
behave very similarly to simply rejection sampling from the initial distribution.","At a conjectural level, it might
have a variety of implications and uses when RL-ﬁnetuning large generative models:

        • These relations provide a rough prediction for ‘how much does the policy need to change to achieve a
          speciﬁc reward’.",2022-04-12 15:02:38+00:00,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Yuntao Bai'), arxiv.Result.Author('Andy Jones'), arxiv.Result.Author('Kamal Ndousse'), arxiv.Result.Author('Amanda Askell'), arxiv.Result.Author('Anna Chen'), arxiv.Result.Author('Nova DasSarma'), arxiv.Result.Author('Dawn Drain'), arxiv.Result.Author('Stanislav Fort'), arxiv.Result.Author('Deep Ganguli'), arxiv.Result.Author('Tom Henighan'), arxiv.Result.Author('Nicholas Joseph'), arxiv.Result.Author('Saurav Kadavath'), arxiv.Result.Author('Jackson Kernion'), arxiv.Result.Author('Tom Conerly'), arxiv.Result.Author('Sheer El-Showk'), arxiv.Result.Author('Nelson Elhage'), arxiv.Result.Author('Zac Hatfield-Dodds'), arxiv.Result.Author('Danny Hernandez'), arxiv.Result.Author('Tristan Hume'), arxiv.Result.Author('Scott Johnston'), arxiv.Result.Author('Shauna Kravec'), arxiv.Result.Author('Liane Lovitt'), arxiv.Result.Author('Neel Nanda'), arxiv.Result.Author('Catherine Olsson'), arxiv.Result.Author('Dario Amodei'), arxiv.Result.Author('Tom Brown'), arxiv.Result.Author('Jack Clark'), arxiv.Result.Author('Sam McCandlish'), arxiv.Result.Author('Chris Olah'), arxiv.Result.Author('Ben Mann'), arxiv.Result.Author('Jared Kaplan')]","We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work."
4798,"One possible explanation
   To enable more detailed research of MEL, we                  is that it may simply be near the ceiling of what
propose a manually-annotated MEL dataset named                  can be achieved for these datasets, and it is difﬁcult
WIKIDiverse with multiple topics and multiple                   to conduct further research based on them.","methods have achieved high and similar results
                                                                within recent three years.",entity types.,2022-04-13 12:52:40+00:00,WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xuwu Wang'), arxiv.Result.Author('Junfeng Tian'), arxiv.Result.Author('Min Gui'), arxiv.Result.Author('Zhixu Li'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Ming Yan'), arxiv.Result.Author('Lihan Chen'), arxiv.Result.Author('Yanghua Xiao')]","Multimodal Entity Linking (MEL) which aims at linking mentions with
multimodal contexts to the referent entities from a knowledge base (e.g.,
Wikipedia), is an essential task for many multimodal applications. Although
much attention has been paid to MEL, the shortcomings of existing MEL datasets
including limited contextual topics and entity types, simplified mention
ambiguity, and restricted availability, have caused great obstacles to the
research and application of MEL. In this paper, we present WikiDiverse, a
high-quality human-annotated MEL dataset with diversified contextual topics and
entity types from Wikinews, which uses Wikipedia as the corresponding knowledge
base. A well-tailored annotation procedure is adopted to ensure the quality of
the dataset. Based on WikiDiverse, a sequence of well-designed MEL models with
intra-modality and inter-modality attentions are implemented, which utilize the
visual information of images more adequately than existing MEL models do.
Extensive experimental analyses are conducted to investigate the contributions
of different modalities in terms of MEL, facilitating the future research on
this task. The dataset and baseline models are available at
https://github.com/wangxw5/wikiDiverse."
4811,"To further research on AfricaNLP and reproducibility, we are releasing language SFTs,
AfroXLMR-base, AfroXLMR-small, and AfroXLMR-mini to the HuggingFace Model Hub11.","We hope that future work improves vocabu-
lary reduction to provide even smaller models with strong performance on distant and low-resource
languages.","ACKNOWLEDGMENTS

David Adelani acknowledges the EU-funded Horizon 2020 projects: ROXANNE under grant num-
ber 833635 and COMPRISE (http://www.compriseh2020.eu/) under grant agreement
No.",2022-04-13 16:13:49+00:00,Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jesujoba O. Alabi'), arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Marius Mosbach'), arxiv.Result.Author('Dietrich Klakow')]","Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks on both high resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is language adaptive
fine-tuning (LAFT) -- fine-tuning a multilingual PLM on monolingual texts of a
language using the same pre-training objective. However, African languages with
large monolingual texts are few, and adapting to each of them individually
takes large disk space and limits the cross-lingual transfer abilities of the
resulting models because they have been specialized for a single language. In
this paper, we perform multilingual adaptive fine-tuning (MAFT) on 17
most-resourced African languages and three other high-resource languages widely
spoken on the African continent -- English, French, and Arabic to encourage
cross-lingual transfer learning. Additionally, to further specialize the
multilingual PLM, we removed vocabulary tokens from the embedding layer that
corresponds to non-African writing scripts before MAFT, thus reducing the model
size by around 50\%. Our evaluation on two multilingual PLMs (AfriBERTa and
XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space. Finally, we
show that our adapted PLM also improves the zero-shot cross-lingual transfer
abilities of parameter efficient fine-tuning methods."
4812,"To further research on NLP for African
                                                           languages and reproducibility, we have uploaded our
Hyper-parameters for adapters We train the task            language adapters, language SFTs, AfroXLMR-base,
adapter using the following hyper-parameters: batch        AfroXLMR-small, and AfroXLMR-mini models to the
size of 8, 10 epochs, “pfeiffer” adapter conﬁg, adapter    HuggingFace Model Hub15.",languages.,"reduction factor of 8, and learning rate of 5e-5.",2022-04-13 16:13:49+00:00,Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jesujoba O. Alabi'), arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Marius Mosbach'), arxiv.Result.Author('Dietrich Klakow')]","Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods."
4813,"To further research on NLP for African
                                                           languages and reproducibility, we have uploaded our
Hyper-parameters for adapters We train the task            language adapters, language SFTs, AfroXLMR-base,
adapter using the following hyper-parameters: batch        AfroXLMR-small, and AfroXLMR-mini models to the
size of 8, 10 epochs, “pfeiffer” adapter conﬁg, adapter    HuggingFace Model Hub15.",languages.,"reduction factor of 8, and learning rate of 5e-5.",2022-04-13 16:13:49+00:00,Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jesujoba O. Alabi'), arxiv.Result.Author('David Ifeoluwa Adelani'), arxiv.Result.Author('Marius Mosbach'), arxiv.Result.Author('Dietrich Klakow')]","Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods."
4840,"Be-
sides, constructing adversarial training instances         In the future work, we would like to further study
with diverse literal forms via word replacing or em-    the vicinal risk minimization with the combination
bedding interpolating (Wang et al., 2018; Cheng         of multi-lingual aligned scenarios and large-scale
et al., 2020) is beneﬁcial to improve the generaliza-   monolingual data, and development it as a pure data
tion performance of NMT models.",target sentences back into the source language.,augmentator merged into the vanilla Transformer.,2022-04-14 08:16:28+00:00,Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xiangpeng Wei'), arxiv.Result.Author('Heng Yu'), arxiv.Result.Author('Yue Hu'), arxiv.Result.Author('Rongxiang Weng'), arxiv.Result.Author('Weihua Luo'), arxiv.Result.Author('Jun Xie'), arxiv.Result.Author('Rong Jin')]","The principal task in supervised neural machine translation (NMT) is to learn
to generate target sentences conditioned on the source inputs from a set of
parallel sentence pairs, and thus produce a model capable of generalizing to
unseen instances. However, it is commonly observed that the generalization
performance of the model is highly influenced by the amount of parallel data
used in training. Although data augmentation is widely used to enrich the
training data, conventional methods with discrete manipulations fail to
generate diverse and faithful training samples. In this paper, we present a
novel data augmentation paradigm termed Continuous Semantic Augmentation
(CsaNMT), which augments each training instance with an adjacency semantic
region that could cover adequate variants of literal expression under the same
meaning. We conduct extensive experiments on both rich-resource and
low-resource settings involving various language pairs, including WMT14
English-{German,French}, NIST Chinese-English and multiple low-resource IWSLT
translation tasks. The provided empirical evidences show that CsaNMT sets a new
level of performance among existing augmentation techniques, improving on the
state-of-the-art by a large margin. The core codes are contained in Appendix E."
4845,"classiﬁer for each Dark Web corpus on the remain-
ing documents with the same conﬁguration used in                    We hope that our dataset and our work motivates
                                                                 further research in the ﬁeld of language-based Dark
   17Refer to Appendix F for the full list of website names and  Web analysis.",We then train a BERT-based                   mance of such applications.,URLs.,2022-04-14 11:17:22+00:00,Shedding New Light on the Language of the Dark Web,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Youngjin Jin'), arxiv.Result.Author('Eugene Jang'), arxiv.Result.Author('Yongjae Lee'), arxiv.Result.Author('Seungwon Shin'), arxiv.Result.Author('Jin-Woo Chung')]","The hidden nature and the limited accessibility of the Dark Web, combined
with the lack of public datasets in this domain, make it difficult to study its
inherent characteristics such as linguistic properties. Previous works on text
classification of Dark Web domain have suggested that the use of deep neural
models may be ineffective, potentially due to the linguistic differences
between the Dark and Surface Webs. However, not much work has been done to
uncover the linguistic characteristics of the Dark Web. This paper introduces
CoDA, a publicly available Dark Web dataset consisting of 10000 web documents
tailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a
thorough linguistic analysis of the Dark Web and examine the textual
differences between the Dark Web and the Surface Web. We also assess the
performance of various methods of Dark Web page classification. Finally, we
compare CoDA with an existing public Dark Web dataset and evaluate their
suitability for various use cases."
4846,"further research in the ﬁeld of language-based Dark
Web analysis.","Consequently,
                                                       the authors and the annotators do not have access
   We hope that our dataset and our work motivates     to media that are illegal by law.","It is worth noting that CoDA still contains texts
                                                       of various activities that occur in the Dark Web,
Ethical Considerations                                 some of which are illegal in nature (drug trade,
                                                       counterfeit products, etc.).",2022-04-14 11:17:22+00:00,Shedding New Light on the Language of the Dark Web,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Youngjin Jin'), arxiv.Result.Author('Eugene Jang'), arxiv.Result.Author('Yongjae Lee'), arxiv.Result.Author('Seungwon Shin'), arxiv.Result.Author('Jin-Woo Chung')]","The hidden nature and the limited accessibility of the Dark Web, combined
with the lack of public datasets in this domain, make it difficult to study its
inherent characteristics such as linguistic properties. Previous works on text
classification of Dark Web domain have suggested that the use of deep neural
models may be ineffective, potentially due to the linguistic differences
between the Dark and Surface Webs. However, not much work has been done to
uncover the linguistic characteristics of the Dark Web. This paper introduces
CoDA, a publicly available Dark Web dataset consisting of 10000 web documents
tailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a
thorough linguistic analysis of the Dark Web and examine the textual
differences between the Dark Web and the Surface Web. We also assess the
performance of various methods of Dark Web page classification. Finally, we
compare CoDA with an existing public Dark Web dataset and evaluate their
suitability for various use cases."
4847,"More effective use                    F1 score in the 3-nest target setting is 14.41 lower
of open-domain data requires further research.","The
boost the model performance.",than that in 1-nest targets experiment.,2022-04-14 11:44:02+00:00,Challenges for Open-domain Targeted Sentiment Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yun Luo'), arxiv.Result.Author('Hongjie Cai'), arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Yanxia Qin'), arxiv.Result.Author('Rui Xia'), arxiv.Result.Author('Yue Zhang')]","Since previous studies on open-domain targeted sentiment analysis are limited
in dataset domain variety and sentence level, we propose a novel dataset
consisting of 6,013 human-labeled data to extend the data domains in topics of
interest and document level. Furthermore, we offer a nested target annotation
schema to extract the complete sentiment information in documents, boosting the
practicality and effectiveness of open-domain targeted sentiment analysis.
Moreover, we leverage the pre-trained model BART in a sequence-to-sequence
generation method for the task. Benchmark results show that there exists large
room for improvement of open-domain targeted sentiment analysis. Meanwhile,
experiments have shown that challenges remain in the effective use of
open-domain data, long documents, the complexity of target structure, and
domain variances."
4848,"More effective use                    F1 score in the 3-nest target setting is 14.41 lower
of open-domain data requires further research.","The
boost the model performance.",than that in 1-nest targets experiment.,2022-04-14 11:44:02+00:00,Challenges for Open-domain Targeted Sentiment Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yun Luo'), arxiv.Result.Author('Hongjie Cai'), arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Yanxia Qin'), arxiv.Result.Author('Rui Xia'), arxiv.Result.Author('Yue Zhang')]","Since previous studies on open-domain targeted sentiment analysis are limited
in dataset domain variety and sentence level, we propose a novel dataset
consisting of 6,013 human-labeled data to extend the data domains in topics of
interest and document level. Furthermore, we offer a nested target annotation
schema to extract the complete sentiment information in documents, boosting the
practicality and effectiveness of open-domain targeted sentiment analysis.
Moreover, we leverage the pre-trained model BART in a sequence-to-sequence
generation method for the task. Benchmark results show that there exists large
room for improvement of open-domain targeted sentiment analysis. Meanwhile,
experiments have shown that challenges remain in the effective use of
open-domain data, long documents, the complexity of target structure, and
domain variances."
4868,"evidence, validating our hypothesis that signals       We propose further study on the effects of STOP
in style and tone have become a crutch for fact-       and POS, as well as experimenting with different
checking models.","america great again.” (Trump Inauguration Ad-
                                                              dress, 2017)
7 Conclusion
                                                          By removing stopwords “we”, “will” and
Many pre-processing steps increase both the            “again”, the model relies less on the text’s rhetoric
model’s F1 scores and its need for claims and          style and more on the entailment we are seeking.","Rather than doing entailment,         emotional vectors and EmoAttention to make fact-
they are leveraging other signals – perhaps similar    checking models more robust.",2022-04-14 21:05:37+00:00,"Automatic Fake News Detection: Are current models ""fact-checking"" or ""gut-checking""?",cs.CL,['cs.CL'],"[arxiv.Result.Author('Ian Kelk'), arxiv.Result.Author('Benjamin Basseri'), arxiv.Result.Author('Wee Yi Lee'), arxiv.Result.Author('Richard Qiu'), arxiv.Result.Author('Chris Tanner')]","Automatic fake news detection models are ostensibly based on logic, where the
truth of a claim made in a headline can be determined by supporting or refuting
evidence found in a resulting web query. These models are believed to be
reasoning in some way; however, it has been shown that these same results, or
better, can be achieved without considering the claim at all -- only the
evidence. This implies that other signals are contained within the examined
evidence, and could be based on manipulable factors such as emotion, sentiment,
or part-of-speech (POS) frequencies, which are vulnerable to adversarial
inputs. We neutralize some of these signals through multiple forms of both
neural and non-neural pre-processing and style transfer, and find that this
flattening of extraneous indicators can induce the models to actually require
both claims and evidence to perform well. We conclude with the construction of
a model using emotion vectors built off a lexicon and passed through an
""emotional attention"" mechanism to appropriately weight certain emotions. We
provide quantifiable results that prove our hypothesis that manipulable
features are being used for fact-checking."
4872,"Given the lack of publicly available data, we        Exercise Maker Generates gaps using rules and a
make our test set available with this paper so as             pre-compiled list of commonly gapped words
to provide a common benchmark for the task and                from a variety of Cambridge English main
to encourage further research in this area.","Details of         for each task based on the average probability
our dataset are shown in Table 1.                             distribution of gapped PoS in the training data.","All the           suite exams (Malafeev, 2014).",2022-04-14 21:16:05+00:00,Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mariano Felice'), arxiv.Result.Author('Shiva Taslimipoor'), arxiv.Result.Author('Paula Buttery')]","This paper presents the first multi-objective transformer model for
constructing open cloze tests that exploits generation and discrimination
capabilities to improve performance. Our model is further enhanced by tweaking
its loss function and applying a post-processing re-ranking algorithm that
improves overall test structure. Experiments using automatic and human
evaluation show that our approach can achieve up to 82% accuracy according to
experts, outperforming previous work and baselines. We also release a
collection of high-quality open cloze tests along with sample system output and
human annotations that can serve as a future benchmark."
4890,"This may hint that
                                                         the current QA data sources are still helpful for
                    63.42 60.91    62.61 58.70           improving the performance of the causal reasoning
                                                         QA task, although further research is required, as
                    47.66 41.06    41.49 35.68           to what extent models can actually beneﬁt from the
                                                         additional data for the generalization is hard to be
                    79.79 71.43    76.34 67.87           evaluated.","On the other hand, we
Cause       8,428                                        note a distinct improvement of using SQuAD2.0
Cause_By    7,437   84.17 86.49    84.40 85.55           data for initially training for both RoBERT-base
Enable      5,506                                        (from 83.11 to 84.34) and RoBERTa-large (from
Enable_By   2,367   73.60 73.93    74.00 76.61           84.35 to 84.65), which indicates that the training
Prevent     1,086                                        with additional well-labelled data could bring sig-
Prevent_By  369     80.21 84.94    79.62 83.69           niﬁcant beneﬁts for Causal QA.","55.88 52.78    64.46 65.00           5.4.2 Challenges by Causal QA

Table 5: Error analysis for ﬁne-grained classiﬁcations.",2022-04-15 10:12:46+00:00,Towards Fine-grained Causal Reasoning and QA,cs.CL,"['cs.CL', 'cs.AI', 'cs.LO']","[arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Yuxiang Wu'), arxiv.Result.Author('Jie Yang'), arxiv.Result.Author('Yue Zhang')]","Understanding causality is key to the success of NLP applications, especially
in high-stakes domains. Causality comes in various perspectives such as enable
and prevent that, despite their importance, have been largely ignored in the
literature. This paper introduces a novel fine-grained causal reasoning dataset
and presents a series of novel predictive tasks in NLP, such as causality
detection, event causality extraction, and Causal QA. Our dataset contains
human annotations of 25K cause-effect event pairs and 24K question-answering
pairs within multi-sentence samples, where each can have multiple causal
relationships. Through extensive experiments and analysis, we show that the
complex relations in our dataset bring unique challenges to state-of-the-art
methods across all three tasks and highlight potential research opportunities,
especially in developing ""causal-thinking"" methods."
4891,"This may suggest that the
RoBERTa-large       84.28 61.69    84.35 61.76           model tends to output the partially right answer
                                                         but fails to output the utterly correct answer, al-
SQuAD2.0-only       63.99 26.02    63.82 25.26           though further research is required, as the model
                                                         still could be easily perturbed by the length of an
SQuAD2.0-enhanced 84.65 61.63      84.65 61.58           event.","Moreover, we ﬁnd that the state-of-the-art
SQuAD2.0-only       64.87 26.71    65.20 27.36           result on our dataset (RoBERTa-SQuAD) is dra-
                                                         matically worse than the best performance on other
SQuAD2.0-enhanced 84.39 61.22      84.34 61.17           datasets (EM = 90.9 on SQuAD2.0 while EM =
                                                         61.6 on Causal QA).","Meanwhile, the human performance is still
                                                         ahead of the best-performing model’s result in the
               Generative Methods                        causal reasoning QA task.",2022-04-15 10:12:46+00:00,Towards Fine-grained Causal Reasoning and QA,cs.CL,"['cs.CL', 'cs.AI', 'cs.LO']","[arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Yuxiang Wu'), arxiv.Result.Author('Jie Yang'), arxiv.Result.Author('Yue Zhang')]","Understanding causality is key to the success of NLP applications, especially
in high-stakes domains. Causality comes in various perspectives such as enable
and prevent that, despite their importance, have been largely ignored in the
literature. This paper introduces a novel fine-grained causal reasoning dataset
and presents a series of novel predictive tasks in NLP, such as causality
detection, event causality extraction, and Causal QA. Our dataset contains
human annotations of 25K cause-effect event pairs and 24K question-answering
pairs within multi-sentence samples, where each can have multiple causal
relationships. Through extensive experiments and analysis, we show that the
complex relations in our dataset bring unique challenges to state-of-the-art
methods across all three tasks and highlight potential research opportunities,
especially in developing ""causal-thinking"" methods."
4914,"ACM Transactions on Com-
cant room for further research in this domain.","Biomedical
performance of the proposed model remains far            named entity recognition via knowledge guidance
from the SOTA for some tasks, indicating signiﬁ-         and question answering.","puting for Healthcare, 2(4):1–24.",2022-04-15 18:06:22+00:00,In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mihir Parmar'), arxiv.Result.Author('Swaroop Mishra'), arxiv.Result.Author('Mirali Purohit'), arxiv.Result.Author('Man Luo'), arxiv.Result.Author('M. Hassan Murad'), arxiv.Result.Author('Chitta Baral')]","Single-task models have proven pivotal in solving specific tasks; however,
they have limitations in real-world applications where multi-tasking is
necessary and domain shifts are exhibited. Recently, instructional prompts have
shown significant improvement towards multi-task generalization; however, the
effect of instructional prompts and Multi-Task Learning (MTL) has not been
systematically studied in the biomedical domain. Motivated by this, this paper
explores the impact of instructional prompts for biomedical MTL. We introduce
the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)
various categories. Using this meta-dataset, we propose a unified model termed
In-BoXBART, that can jointly learn all tasks of the BoX without any
task-specific modules. To the best of our knowledge, this is the first attempt
to propose a unified model in the biomedical domain and use instructions to
achieve generalization across several biomedical tasks. Experimental results
indicate that the proposed model: 1) outperforms the single-task baseline by
~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)
shows ~23% improvement compared to the single-task baseline in few-shot
learning (i.e., 32 instances per task) on an average. Our analysis indicates
that there is significant room for improvement across tasks in the BoX,
implying the scope for future research direction."
4921,"We hope our work encourages              ACM International Conference on Information &
further research in such data-centric methods to           Knowledge Management.","Proceedings of the 29th
during inference.","improve robustness of NLP models for practical
applications of conversational modeling.",2022-04-15 23:39:41+00:00,DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling,cs.CL,['cs.CL'],"[arxiv.Result.Author('Lahari Poddar'), arxiv.Result.Author('Peiyao Wang'), arxiv.Result.Author('Julia Reinspach')]","Retrieval-based conversational systems learn to rank response candidates for
a given dialogue context by computing the similarity between their vector
representations. However, training on a single textual form of the multi-turn
context limits the ability of a model to learn representations that generalize
to natural perturbations seen during inference. In this paper we propose a
framework that incorporates augmented versions of a dialogue context into the
learning objective. We utilize contrastive learning as an auxiliary objective
to learn robust dialogue context representations that are invariant to
perturbations injected through the augmentation method. We experiment with four
benchmark dialogue datasets and demonstrate that our framework combines well
with existing augmentation methods and can significantly improve over baseline
BERT-based ranking architectures. Furthermore, we propose a novel data
augmentation method, ConMix, that adds token level perturbations through
stochastic mixing of tokens from other contexts in the batch. We show that our
proposed augmentation method outperforms previous data augmentation approaches,
and provides dialogue representations that are more robust to common
perturbations seen during inference."
4932,We contribute it to the community for further research 1.,numerical commonsense reasoning capabilities.,"(2) We deﬁne
a domain-speciﬁc language (DSL) that is more concise than full featured programming languages like Python, but
expressive enough with the ability to imitate the state of the process by deﬁning variables, simulate iterative process
using loops and examine multiple possibilities using if-conditions.",2022-04-16 16:10:41+00:00,What If: Generating Code to Answer Simulation Questions,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Gal Peretz'), arxiv.Result.Author('Kira Radinsky')]","Many texts, especially in chemistry and biology, describe complex processes.
We focus on texts that describe a chemical reaction process and questions that
ask about the process's outcome under different environmental conditions. To
answer questions about such processes, one needs to understand the interactions
between the different entities involved in the process and to simulate their
state transitions during the process execution under different conditions. A
state transition is defined as the memory modification the program does to the
variables during the execution. We hypothesize that generating code and
executing it to simulate the process will allow answering such questions. We,
therefore, define a domain-specific language (DSL) to represent processes. We
contribute to the community a unique dataset curated by chemists and annotated
by computer scientists. The dataset is composed of process texts, simulation
questions, and their corresponding computer codes represented by the DSL.We
propose a neural program synthesis approach based on reinforcement learning
with a novel state-transition semantic reward. The novel reward is based on the
run-time semantic similarity between the predicted code and the reference code.
This allows simulating complex process transitions and thus answering
simulation questions. Our approach yields a significant boost in accuracy for
simulation questions: 88\% accuracy as opposed to 83\% accuracy of the
state-of-the-art neural program synthesis approaches and 54\% accuracy of
state-of-the-art end-to-end text-based approaches."
4940,"We believe that our paper will spur further research
on retrieval-augmentation methods for cross-task generalization.","We also ﬁnd the distribution of retrieved data for analyzing the
behavior differences between ReCross and others.","Interesting future directions include:
1) improve the re-learning stage by including more information from query examples, 2) extend the
distant supervision mining process as a self-training procedure, 3) rigorously analyze the correlation
between upstream data and target tasks, etc.",2022-04-17 06:05:13+00:00,Unsupervised Cross-Task Generalization via Retrieval Augmentation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Bill Yuchen Lin'), arxiv.Result.Author('Kangmin Tan'), arxiv.Result.Author('Chris Miller'), arxiv.Result.Author('Beiwen Tian'), arxiv.Result.Author('Xiang Ren')]","Humans can perform unseen tasks by recalling relevant skills that are
acquired previously and then generalizing them to the target tasks, even if
there is no supervision at all. In this paper, we aim to improve such
cross-task generalization ability of massive multi-task language models such as
T0 (Sanh et al., 2021) in an unsupervised setting. We propose a
retrieval-augmentation method named ReCross that takes a few unlabelled
examples as queries to retrieve a small subset of upstream data and uses them
to update the multi-task model for better generalization. Our empirical results
show that the proposed ReCross consistently outperforms non-retrieval baselines
by a significant margin."
4941,"We believe that our paper will spur further research
on retrieval-augmentation methods for cross-task generalization.","We also ﬁnd the distribution of retrieved data for analyzing the
behavior differences between ReCross and others.","Interesting future directions include:
1) improve the re-learning stage by including more information from query examples, 2) extend the
distant supervision mining process as a self-training procedure, 3) rigorously analyze the correlation
between upstream data and target tasks, etc.",2022-04-17 06:05:13+00:00,Unsupervised Cross-Task Generalization via Retrieval Augmentation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Bill Yuchen Lin'), arxiv.Result.Author('Kangmin Tan'), arxiv.Result.Author('Chris Miller'), arxiv.Result.Author('Beiwen Tian'), arxiv.Result.Author('Xiang Ren')]","Humans can perform unseen tasks by recalling relevant skills acquired
previously and then generalizing them to the target tasks, even if there is no
supervision at all. In this paper, we aim to improve this kind of cross-task
generalization ability of massive multi-task language models, such as T0 and
FLAN, in an unsupervised setting. We propose a retrieval-augmentation method
named ReCross that takes a few unlabelled examples as queries to retrieve a
small subset of upstream data and uses them to update the multi-task model for
better generalization. ReCross is a straightforward yet effective retrieval
method that combines both efficient dense retrieval and effective pair-wise
reranking. Our results and analysis show that it significantly outperforms both
non-retrieval methods and other baseline methods."
4945,further research.,"ruT5-large also yielded good performance       evaluation on them remains a separate point for
on reviews: 1,542 pairs were left after ﬁltration.","The explanation might be that review texts as a
                                                       8 Conclusions
   15https://github.com/IlyaGusev/gazeta
   16https://tatianashavrina.github.io/                We propose WikiOmnia, the new largest question-
taiga_site/                                            answering dataset for Russian: it contains QA pairs
   17https://github.com/sismetanin/                    and corresponding Russian Wikipedia article sum-
rureviews                                              maries.",2022-04-17 12:59:36+00:00,WikiOmnia: generative QA corpus on the whole Russian Wikipedia,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Dina Pisarevskaya'), arxiv.Result.Author('Tatiana Shavrina')]","The General QA field has been developing the methodology referencing the
Stanford Question answering dataset (SQuAD) as the significant benchmark.
However, compiling factual questions is accompanied by time- and
labour-consuming annotation, limiting the training data's potential size. We
present the WikiOmnia dataset, a new publicly available set of QA-pairs and
corresponding Russian Wikipedia article summary sections, composed with a fully
automated generative pipeline. The dataset includes every available article
from Wikipedia for the Russian language. The WikiOmnia pipeline is available
open-source and is also tested for creating SQuAD-formatted QA on other
domains, like news texts, fiction, and social media. The resulting dataset
includes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs
with paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for
ruT5-large) and cleaned data with strict automatic verification (over 160,000
QA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with
paragraphs for ruT5-large)."
4947,"Moreover, such oﬄine evalua-
                                        model to be used for further research on the topic.1                                        tion remains limited to single-turn interaction, as the pre-deﬁned
                                                                                                                                    questions are associated with corresponding answers and are not
                                        KEYWORDS                                                                                    aware of any previous interactions.","We provide the code, data, and the pre-trained                          fer well to the real-world scenario.","User simulation has been pro-
                                                                                                                                    posed to tackle the shortcomings of corpus-based and user-based
                                        Conversational Search, Mixed-initiative Search, User Simulation                             evaluation methodologies.",2022-04-17 16:27:33+00:00,Evaluating Mixed-initiative Conversational Search Systems via User Simulation,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Ivan Sekulić'), arxiv.Result.Author('Mohammad Aliannejadi'), arxiv.Result.Author('Fabio Crestani')]","Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic."
4948,"they do not correlate well with the coherence of the text, we per-
                                                                       form a crowdsourcing study to evaluate the naturalness of gener-
   In order to further study the eﬀects certain clarifying questions   ated answers.","As several NLG met-
struct in 500 conversations up to depth of three, i.e., we have three  rics received criticism from the NLP community, especially since
sequential question-answers pairs for a topic and its facet.","In order to evaluate whether the generated answers
have on the whole search experience, we construct several edge         are in line with the actual information need, we carry out addi-
cases.",2022-04-17 16:27:33+00:00,Evaluating Mixed-initiative Conversational Search Systems via User Simulation,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Ivan Sekulić'), arxiv.Result.Author('Mohammad Aliannejadi'), arxiv.Result.Author('Fabio Crestani')]","Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic."
4949,"Moreover, such oﬄine evalua-
                                        model to be used for further research on the topic.1                                        tion remains limited to single-turn interaction, as the pre-deﬁned
                                                                                                                                    questions are associated with corresponding answers and are not
                                        KEYWORDS                                                                                    aware of any previous interactions.","We provide the code, data, and the pre-trained                          fer well to the real-world scenario.","User simulation has been pro-
                                                                                                                                    posed to tackle the shortcomings of corpus-based and user-based
                                        Conversational Search, Mixed-initiative Search, User Simulation                             evaluation methodologies.",2022-04-17 16:27:33+00:00,Evaluating Mixed-initiative Conversational Search Systems via User Simulation,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Ivan Sekulić'), arxiv.Result.Author('Mohammad Aliannejadi'), arxiv.Result.Author('Fabio Crestani')]","Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic."
4950,"they do not correlate well with the coherence of the text, we per-
                                                                       form a crowdsourcing study to evaluate the naturalness of gener-
   In order to further study the eﬀects certain clarifying questions   ated answers.","As several NLG met-
struct in 500 conversations up to depth of three, i.e., we have three  rics received criticism from the NLP community, especially since
sequential question-answers pairs for a topic and its facet.","In order to evaluate whether the generated answers
have on the whole search experience, we construct several edge         are in line with the actual information need, we carry out addi-
cases.",2022-04-17 16:27:33+00:00,Evaluating Mixed-initiative Conversational Search Systems via User Simulation,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Ivan Sekulić'), arxiv.Result.Author('Mohammad Aliannejadi'), arxiv.Result.Author('Fabio Crestani')]","Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic."
4961,"Another direction is to further study the integration
of LaMer with larger size foundation models, like GPT-3 175B to understand LaMers potential in
few-shot settings.","One could also
investigate combining the alignment module of LaMer with the text2text LM in one model, and
jointly train to further boost the performance.","9
Published as a conference paper at ICLR 2022

6 ETHICS AND REPRODUCIBILITY STATEMENT

The goal of LaMer is to provide a simple but efﬁcient general-purpose text style transfer framework
by leveraging large-scale pre-trained LMs.",2022-04-18 01:38:35+00:00,Non-Parallel Text Style Transfer with Self-Parallel Supervision,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ruibo Liu'), arxiv.Result.Author('Chongyang Gao'), arxiv.Result.Author('Chenyan Jia'), arxiv.Result.Author('Guangxuan Xu'), arxiv.Result.Author('Soroush Vosoughi')]","The performance of existing text style transfer models is severely limited by
the non-parallel datasets on which the models are trained. In non-parallel
datasets, no direct mapping exists between sentences of the source and target
style; the style transfer models thus only receive weak supervision of the
target sentences during training, which often leads the model to discard too
much style-independent information, or utterly fail to transfer the style. In
this work, we propose LaMer, a novel text style transfer framework based on
large-scale language models. LaMer first mines the roughly parallel expressions
in the non-parallel datasets with scene graphs, and then employs MLE training,
followed by imitation learning refinement, to leverage the intrinsic
parallelism within the data. On two benchmark tasks (sentiment & formality
transfer) and a newly proposed challenging task (political stance transfer),
our model achieves qualitative advances in transfer accuracy, content
preservation, and fluency. Further empirical and human evaluations demonstrate
that our model not only makes training more efficient, but also generates more
readable and diverse expressions than previous models."
4966,problem and needs further research.,"Association for Computational
application to more complex tasks is still an open           Linguistics.","Andrea Madotto, Zhaojiang Lin, Genta Indra Winata,
References                                                   and Pascale Fung.",2022-04-18 05:29:54+00:00,A Study on Prompt-based Few-Shot Learning Methods for Belief State Tracking in Task-oriented Dialog Systems,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Debjoy Saha'), arxiv.Result.Author('Bishal Santra'), arxiv.Result.Author('Pawan Goyal')]","We tackle the Dialogue Belief State Tracking(DST) problem of task-oriented
conversational systems. Recent approaches to this problem leveraging
Transformer-based models have yielded great results. However, training these
models is expensive, both in terms of computational resources and time.
Additionally, collecting high quality annotated dialogue datasets remains a
challenge for researchers because of the extensive annotation required for
training these models. Driven by the recent success of pre-trained language
models and prompt-based learning, we explore prompt-based few-shot learning for
Dialogue Belief State Tracking. We formulate the DST problem as a 2-stage
prompt-based language modelling task and train language models for both tasks
and present a comprehensive empirical analysis of their separate and joint
performance. We demonstrate the potential of prompt-based methods in few-shot
learning for DST and provide directions for future improvement."
4984,"sentence from a set of negative examples, and thus
encourages representations of similar sentences be-        To facilitate further research, codes are pub-
tween source language and target language closer.","With         experiments demonstrate that GL-CLEF has suc-
the use of CL, our model is able to learn to dis-       cessfully reduced the representation gap between
tinguish the code-switched utterance of an input        different languages.","licly available at https://github.com/
                                                        LightChen233/GL-CLeF.",2022-04-18 13:56:58+00:00,GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding,cs.CL,['cs.CL'],"[arxiv.Result.Author('Libo Qin'), arxiv.Result.Author('Qiguang Chen'), arxiv.Result.Author('Tianbao Xie'), arxiv.Result.Author('Qixin Li'), arxiv.Result.Author('Jian-Guang Lou'), arxiv.Result.Author('Wanxiang Che'), arxiv.Result.Author('Min-Yen Kan')]","Due to high data demands of current methods, attention to zero-shot
cross-lingual spoken language understanding (SLU) has grown, as such approaches
greatly reduce human annotation effort. However, existing models solely rely on
shared parameters, which can only perform implicit alignment across languages.
We present Global--Local Contrastive Learning Framework (GL-CLeF) to address
this shortcoming. Specifically, we employ contrastive learning, leveraging
bilingual dictionaries to construct multilingual views of the same utterance,
then encourage their representations to be more similar than negative example
pairs, which achieves to explicitly aligned representations of similar
sentences across languages. In addition, a key step in GL-CLeF is a proposed
Local and Global component, which achieves a fine-grained cross-lingual
transfer (i.e., sentence-level Local intent transfer, token-level Local slot
transfer, and semantic-level Global transfer across intent and slot).
Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and
successfully pulls representations of similar sentences across languages
closer."
4989,"We observe that there is a mis-
match between the scale at which this code-mixed language is used and the data
that is available for further research.","In this internet era, we see the usage of code-mixed data preva-
lently in social media and chat platforms [13].","As Hindi is the third most spoken language in the world after English and
Mandarin4.",2022-04-18 16:49:59+00:00,L3Cube-HingCorpus and HingBERT: A Code Mixed Hindi-English Dataset and BERT Language Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Ravindra Nayak'), arxiv.Result.Author('Raviraj Joshi')]","Code-switching occurs when more than one language is mixed in a given
sentence or a conversation. This phenomenon is more prominent on social media
platforms and its adoption is increasing over time. Therefore code-mixed NLP
has been extensively studied in the literature. As pre-trained
transformer-based architectures are gaining popularity, we observe that real
code-mixing data are scarce to pre-train large language models. We present
L3Cube-HingCorpus, the first large-scale real Hindi-English code mixed data in
a Roman script. It consists of 52.93M sentences and 1.04B tokens, scraped from
Twitter. We further present HingBERT, HingMBERT, HingRoBERTa, and HingGPT. The
BERT models have been pre-trained on codemixed HingCorpus using masked language
modelling objectives. We show the effectiveness of these BERT models on the
subsequent downstream tasks like code-mixed sentiment analysis, POS tagging,
NER, and LID from the GLUECoS benchmark. The HingGPT is a GPT2 based generative
transformer model capable of generating full tweets. We also release
L3Cube-HingLID Corpus, the largest code-mixed Hindi-English language
identification(LID) dataset and HingBERT-LID, a production-quality LID model to
facilitate capturing of more code-mixed data using the process outlined in this
work. The dataset and models are available at
https://github.com/l3cube-pune/code-mixed-nlp ."
4990,"4
    https://en.wikipedia.org/wiki/Hindi

 5
    https://huggingface.co/l3cube-pune/hing-bert

 6
    https://huggingface.co/l3cube-pune/hing-mbert

 7
    https://huggingface.co/l3cube-pune/hing-mbert-mixed

 8
    https://huggingface.co/l3cube-pune/hing-roberta

 9
    https://huggingface.co/l3cube-pune/hing-robera-mixed

10
    https://huggingface.co/l3cube-pune/hing-gpt

11
    https://huggingface.co/l3cube-pune/hing-gpt-devanagari

12
    https://huggingface.co/l3cube-pune/hing-bert-lid
L3Cube-HingCorpus and HingBERT  3

    The data and models is publicly13 released to enable further research in
Hinglish NLP.","This is the largest LID dataset
for the Hi-En pair.","2 Related Work

In this section, we will try to mainly discuss previous attempts in the creation
of code-mixed datasets.",2022-04-18 16:49:59+00:00,L3Cube-HingCorpus and HingBERT: A Code Mixed Hindi-English Dataset and BERT Language Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Ravindra Nayak'), arxiv.Result.Author('Raviraj Joshi')]","Code-switching occurs when more than one language is mixed in a given
sentence or a conversation. This phenomenon is more prominent on social media
platforms and its adoption is increasing over time. Therefore code-mixed NLP
has been extensively studied in the literature. As pre-trained
transformer-based architectures are gaining popularity, we observe that real
code-mixing data are scarce to pre-train large language models. We present
L3Cube-HingCorpus, the first large-scale real Hindi-English code mixed data in
a Roman script. It consists of 52.93M sentences and 1.04B tokens, scraped from
Twitter. We further present HingBERT, HingMBERT, HingRoBERTa, and HingGPT. The
BERT models have been pre-trained on codemixed HingCorpus using masked language
modelling objectives. We show the effectiveness of these BERT models on the
subsequent downstream tasks like code-mixed sentiment analysis, POS tagging,
NER, and LID from the GLUECoS benchmark. The HingGPT is a GPT2 based generative
transformer model capable of generating full tweets. We also release
L3Cube-HingLID Corpus, the largest code-mixed Hindi-English language
identification(LID) dataset and HingBERT-LID, a production-quality LID model to
facilitate capturing of more code-mixed data using the process outlined in this
work. The dataset and models are available at
https://github.com/l3cube-pune/code-mixed-nlp ."
4999,"To further study the importance of            pare the performance with two language mod-
full imagination, we ablate the data side by con-          els (BERT and RoBERTa) of two architectures
structing a textual-only model denoted as Textual          (""6L/512H"" and ""12L/768H""), and a strong visu-
Only, a visual-only imagination denoted as Visual          ally supervised pre-trained baseline VOKEN (Tan
Only and a single directional imagination input            and Bansal, 2020).","To validate such
Imagination Composition Ablation The com-                  model-agnostic effectiveness of our proposed novel
position of the imagination is essential for the           paradigm in processing natural language, we com-
performance.","Table 4 shows the metric comparison on GLUE
                                                           and SWAG.",2022-04-18 19:39:36+00:00,Imagination-Augmented Natural Language Understanding,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yujie Lu'), arxiv.Result.Author('Wanrong Zhu'), arxiv.Result.Author('Xin Eric Wang'), arxiv.Result.Author('Miguel Eckstein'), arxiv.Result.Author('William Yang Wang')]","Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances."
5000,"To further study the importance of
                                                           full imagination, we ablate the data side by con-
Method Design Ablation Two method variants                 structing a textual-only model denoted as Textual
of our imagination-augmented encoder are built             Only, a visual-only imagination denoted as Visual
as baselines to validate the importance of our             Only and a single directional imagination input
bi-directional cross-modal imagination design in           denoted as Visual+Textual.",performance.,"Visual Only and Vi-
iACE.",2022-04-18 19:39:36+00:00,Imagination-Augmented Natural Language Understanding,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yujie Lu'), arxiv.Result.Author('Wanrong Zhu'), arxiv.Result.Author('Xin Eric Wang'), arxiv.Result.Author('Miguel Eckstein'), arxiv.Result.Author('William Yang Wang')]","Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances."
5001,"To further study the importance of
                                                           full imagination, we ablate the data side by con-
Method Design Ablation Two method variants                 structing a textual-only model denoted as Textual
of our imagination-augmented encoder are built             Only, a visual-only imagination denoted as Visual
as baselines to validate the importance of our             Only and a single directional imagination input
bi-directional cross-modal imagination design in           denoted as Visual+Textual.",performance.,"Visual Only and Vi-
iACE.",2022-04-18 19:39:36+00:00,Imagination-Augmented Natural Language Understanding,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yujie Lu'), arxiv.Result.Author('Wanrong Zhu'), arxiv.Result.Author('Xin Eric Wang'), arxiv.Result.Author('Miguel Eckstein'), arxiv.Result.Author('William Yang Wang')]","Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances."
5052,"Further,
Table 3: Sample translations taken from the test dataset  there were self-attention models proposed such as

    5https://fairseq.readthedocs.io/en/latest/
    6https://fairseq.readthedocs.io/en/latest/index.html
    7https://www.nltk.org/api/nltk.translate.html
transformers which aided to further research in               2014.","The encoder and decoder can be deep
  sn                                                      neural networks such as RNN (Bahdanau et al.,
  en                                                      2014), CNN (Gehring et al., 2017), or feed-forward
                                                          neural networks (Vaswani et al., 2017).","Neural machine translation by jointly learning
NMT.",2022-04-19 19:04:05+00:00,PICT@DravidianLangTech-ACL2022: Neural Machine Translation On Dravidian Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Aditya Vyawahare'), arxiv.Result.Author('Rahul Tangsali'), arxiv.Result.Author('Aditya Mandke'), arxiv.Result.Author('Onkar Litake'), arxiv.Result.Author('Dipali Kadam')]","This paper presents a summary of the findings that we obtained based on the
shared task on machine translation of Dravidian languages. We stood first in
three of the five sub-tasks which were assigned to us for the main shared task.
We carried out neural machine translation for the following five language
pairs: Kannada to Tamil, Kannada to Telugu, Kannada to Malayalam, Kannada to
Sanskrit, and Kannada to Tulu. The datasets for each of the five language pairs
were used to train various translation models, including Seq2Seq models such as
LSTM, bidirectional LSTM, Conv2Seq, and training state-of-the-art as
transformers from scratch, and fine-tuning already pre-trained models. For some
models involving monolingual corpora, we implemented backtranslation as well.
These models' accuracy was later tested with a part of the same dataset using
BLEU score as an evaluation metric."
5054,"Results show that the hybrid technique outperforms the existing
techniques which calls for further study in this direction.","Hence, a hybrid technique is proposed
where the re-rank model is used for the first hop and a single-hop dense passage retrieval model
[131] is used for the second hop.",4.1.2 Final retrieval.,2022-04-19 21:55:18+00:00,A Survey on Multi-hop Question Answering and Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', '68T07', 'I.2.7']","[arxiv.Result.Author('Vaibhav Mavi'), arxiv.Result.Author('Anubhav Jangra'), arxiv.Result.Author('Adam Jatowt')]","The problem of Question Answering (QA) has attracted significant research
interest for long. Its relevance to language understanding and knowledge
retrieval tasks, along with the simple setting makes the task of QA crucial for
strong AI systems. Recent success on simple QA tasks has shifted the focus to
more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most
researched tasks over the recent years. The ability to answer multi-hop
questions and perform multi step reasoning can significantly improve the
utility of NLP systems. Consequently, the field has seen a sudden surge with
high quality datasets, models and evaluation strategies. The notion of
`multiple hops' is somewhat abstract which results in a large variety of tasks
that require multi-hop reasoning. This implies that different datasets and
models differ significantly which makes the field challenging to generalize and
survey. This work aims to provide a general and formal definition of MHQA task,
and organize and summarize existing MHQA frameworks. We also outline the best
methods to create MHQA datasets. The paper provides a systematic and thorough
introduction as well as the structuring of the existing attempts to this highly
interesting, yet quite challenging task."
5055,"These results point out to several limitations of existing
systems and call for further research.","Some
challenging benchmarks and evaluation methods have been recently proposed that bring out some
surprising and interesting observations.","Below, we list and discuss some of these directions for future research (including ones originating
from the currently recognized shortcomings) which we believe could be promising to be explored.",2022-04-19 21:55:18+00:00,A Survey on Multi-hop Question Answering and Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', '68T07', 'I.2.7']","[arxiv.Result.Author('Vaibhav Mavi'), arxiv.Result.Author('Anubhav Jangra'), arxiv.Result.Author('Adam Jatowt')]","The problem of Question Answering (QA) has attracted significant research
interest for long. Its relevance to language understanding and knowledge
retrieval tasks, along with the simple setting makes the task of QA crucial for
strong AI systems. Recent success on simple QA tasks has shifted the focus to
more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most
researched tasks over the recent years. The ability to answer multi-hop
questions and perform multi step reasoning can significantly improve the
utility of NLP systems. Consequently, the field has seen a sudden surge with
high quality datasets, models and evaluation strategies. The notion of
`multiple hops' is somewhat abstract which results in a large variety of tasks
that require multi-hop reasoning. This implies that different datasets and
models differ significantly which makes the field challenging to generalize and
survey. This work aims to provide a general and formal definition of MHQA task,
and organize and summarize existing MHQA frameworks. We also outline the best
methods to create MHQA datasets. The paper provides a systematic and thorough
introduction as well as the structuring of the existing attempts to this highly
interesting, yet quite challenging task."
5056,"(2020) further study in smaller models, in the
based on interpreting regular expressions (RegExs).","To remedy this, we adopt a synthetic             tention mechanism (Hahn, 2020) that Bhattamishra
approach by building an instructional environment           et al.",example learning setting.,2022-04-19 22:11:47+00:00,What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('Matthew Finlayson'), arxiv.Result.Author('Kyle Richardson'), arxiv.Result.Author('Ashish Sabharwal'), arxiv.Result.Author('Peter Clark')]","The instruction learning paradigm -- where a model learns to perform new
tasks from task descriptions alone -- has become popular in general-purpose
model research. The capabilities of large transformer models as instruction
learners, however, remain poorly understood. We use a controlled synthetic
environment to characterize such capabilities. Specifically, we use the task of
deciding whether a given string matches a regular expression (viewed as an
instruction) to identify properties of tasks, instructions, and instances that
make instruction learning challenging. For instance, we find that our model, a
fine-tuned T5-based text2text transformer, struggles with large regular
languages, suggesting that less precise instructions are challenging for
models. Additionally, instruction executions that require tracking longer
contexts of prior steps are also more difficult. We use our findings to
systematically construct a challenging instruction learning dataset, which we
call Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to
correctly interpret only 65.6% of test instructions (with at least 90%
accuracy), and 11%-24% of the instructions in out-of-distribution
generalization settings. We propose Hard RegSet as a challenging instruction
learning task, and a controlled environment for studying instruction learning."
5057,(2020) further study in smaller         natural language instruction following.,"(2020) who build ZEST, a benchmark for
Bhattamishra et al.","In their
models, in the example learning setting.",2022-04-19 22:11:47+00:00,What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('Matthew Finlayson'), arxiv.Result.Author('Kyle Richardson'), arxiv.Result.Author('Ashish Sabharwal'), arxiv.Result.Author('Peter Clark')]","The instruction learning paradigm -- where a model learns to perform new
tasks from task descriptions alone -- has become popular in general-purpose
model research. The capabilities of large transformer models as instruction
learners, however, remain poorly understood. We use a controlled synthetic
environment to characterize such capabilities. Specifically, we use the task of
deciding whether a given string matches a regular expression (viewed as an
instruction) to identify properties of tasks, instructions, and instances that
make instruction learning challenging. For instance, we find that our model, a
fine-tuned T5-based text2text transformer, struggles with large regular
languages, suggesting that less precise instructions are challenging for
models. Additionally, instruction executions that require tracking longer
contexts of prior steps are also more difficult. We use our findings to
systematically construct a challenging instruction learning dataset, which we
call Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to
correctly interpret only 65.6% of test instructions (with at least 90%
accuracy), and 11%-24% of the instructions in out-of-distribution
generalization settings. We propose Hard RegSet as a challenging instruction
learning task, and a controlled environment for studying instruction learning."
5076,"Based on our experimental evidence, we established the following set of axioms
that may provide a useful resource for further research in the ﬁeld:

   • All pre-trained language models leak demographic data.","Without high-quality
resources with useful indicative variable annotations in the language in question, large
scale cross-comparison of privacy mechanisms could prove difﬁcult in the extreme.","We were able to
          extract useful leaked information from all of the models during our
          testing, and convert that into inferences about user demographics that
          they could reasonably expect to remain private.",2022-04-20 11:12:53+00:00,You Are What You Write: Preserving Privacy in the Era of Large Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Richard Plant'), arxiv.Result.Author('Valerio Giuffrida'), arxiv.Result.Author('Dimitra Gkatzia')]","Large scale adoption of large language models has introduced a new era of
convenient knowledge transfer for a slew of natural language processing tasks.
However, these models also run the risk of undermining user trust by exposing
unwanted information about the data subjects, which may be extracted by a
malicious party, e.g. through adversarial attacks. We present an empirical
investigation into the extent of the personal information encoded into
pre-trained representations by a range of popular models, and we show a
positive correlation between the complexity of a model, the amount of data used
in pre-training, and data leakage. In this paper, we present the first wide
coverage evaluation and comparison of some of the most popular
privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment
analysis annotated with demographic information (location, age and gender). The
results show since larger and more complex models are more prone to leaking
private information, use of privacy-preserving methods is highly desirable. We
also find that highly privacy-preserving technologies like differential privacy
(DP) can have serious model utility effects, which can be ameliorated using
hybrid or metric-DP techniques."
5094,"are used in Software Engineering tasks such as bug detection and         We note here that further study is still required for a more compre-
program repair [7, 19, 59, 64].","These tasks may benefit from more
constructs are reported to be useful in code comprehension and           information on a programming language’s semantics and syntax.","We note that AST, which provides         hensive evaluation.",2022-04-05 20:53:06+00:00,LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition,cs.CL,"['cs.CL', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Rishab Sharma'), arxiv.Result.Author('Fuxiang Chen'), arxiv.Result.Author('Fatemeh Fard')]","Code comment generation is the task of generating a high-level natural
language description for a given code method or function. Although researchers
have been studying multiple ways to generate code comments automatically,
previous work mainly considers representing a code token in its entirety
semantics form only (e.g., a language model is used to learn the semantics of a
code token), and additional code properties such as the tree structure of a
code are included as an auxiliary input to the model. There are two
limitations: 1) Learning the code token in its entirety form may not be able to
capture information succinctly in source code, and 2) The code token does not
contain additional syntactic information, inherently important in programming
languages.
  In this paper, we present LAnguage Model and Named Entity Recognition
(LAMNER), a code comment generator capable of encoding code constructs
effectively and capturing the structural property of a code token. A
character-level language model is used to learn the semantic representation to
encode a code token. For the structural property of a token, a Named Entity
Recognition model is trained to learn the different types of code tokens. These
representations are then fed into an encoder-decoder architecture to generate
code comments. We evaluate the generated comments from LAMNER and other
baselines on a popular Java dataset with four commonly used metrics. Our
results show that LAMNER is effective and improves over the best baseline model
in BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,
18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,
we fused LAMNER's code representation with the baseline models, and the fused
models consistently showed improvement over the non-fused models. The human
evaluation further shows that LAMNER produces high-quality code comments."
5102,"dings, but this topic warrants further study.","We hypothesize that this deterioration cor-
models had to classify these inputs among three         responded to generating out-of-distribution embed-
classes of entailment, contradiction, or neutrality.","Ultimately, we failed to ﬁnd any evidence that          Lastly, we performed a similar experiment us-
either the NLI or the NLI-HANS model used syn-          ing the NLI and NLI-HANS models using 486
tactic information causally.",2022-04-20 18:09:36+00:00,When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Mycal Tucker'), arxiv.Result.Author('Tiwalayo Eisape'), arxiv.Result.Author('Peng Qian'), arxiv.Result.Author('Roger Levy'), arxiv.Result.Author('Julie Shah')]","Recent causal probing literature reveals when language models and syntactic
probes use similar representations. Such techniques may yield ""false negative""
causality results: models may use representations of syntax, but probes may
have learned to use redundant encodings of the same syntactic information. We
demonstrate that models do encode syntactic information redundantly and
introduce a new probe design that guides probes to consider all syntactic
information present in embeddings. Using these probes, we find evidence for the
use of syntax in models where prior methods did not, allowing us to boost model
performance by injecting syntactic information into representations."
5103,"Natural
                                                        extensions include studying pretrained models be-
                                                        yond those considered in this work, further research
                                                        into redundancy in embeddings, more investigation
                                                        into inserting symbolic knowledge into neural rep-
                                                        resentations, and new methods for training models
to respond appropriately to interventions.","What was green?” An-          pretrained models, future work remains.","7 Acknowledgements

6 Ethical and Broader Impacts                           TE acknowledges support from the GEM consor-
                                                        tium and the National Science Foundation Gradu-
While the majority of this paper details the tech-      ate Research Fellowship under Grant No.",2022-04-20 18:09:36+00:00,When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Mycal Tucker'), arxiv.Result.Author('Tiwalayo Eisape'), arxiv.Result.Author('Peng Qian'), arxiv.Result.Author('Roger Levy'), arxiv.Result.Author('Julie Shah')]","Recent causal probing literature reveals when language models and syntactic
probes use similar representations. Such techniques may yield ""false negative""
causality results: models may use representations of syntax, but probes may
have learned to use redundant encodings of the same syntactic information. We
demonstrate that models do encode syntactic information redundantly and
introduce a new probe design that guides probes to consider all syntactic
information present in embeddings. Using these probes, we find evidence for the
use of syntax in models where prior methods did not, allowing us to boost model
performance by injecting syntactic information into representations."
5118,We also discuss here a few limitations of this study that may inspire further research.,"Also, our new single-sentence dataset could be a valuable resource for evaluating few-shot learning capabilities, such as
training sophisticated language modeling systems [7], relevant language representations [64], or generative language
patterns [39].","A more complete model of
contextual informativeness would include an individual component capturing a specific user’s background knowledge
of the target concept, although such models can be challenging to train and evaluate.",2022-04-21 05:17:49+00:00,An Attention-Based Model for Predicting Contextual Informativeness and Curriculum Learning Applications,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sungjin Nam'), arxiv.Result.Author('David Jurgens'), arxiv.Result.Author('Kevyn Collins-Thompson')]","Both humans and machines learn the meaning of unknown words through
contextual information in a sentence, but not all contexts are equally helpful
for learning. We introduce an effective method for capturing the level of
contextual informativeness with respect to a given target word. Our study makes
three main contributions. First, we develop models for estimating contextual
informativeness, focusing on the instructional aspect of sentences. Our
attention-based approach using pre-trained embeddings demonstrates
state-of-the-art performance on our single-context dataset and an existing
multi-sentence context dataset. Second, we show how our model identifies key
contextual elements in a sentence that are likely to contribute most to a
reader's understanding of the target word. Third, we examine how our contextual
informativeness model, originally developed for vocabulary learning
applications for students, can be used for developing better training curricula
for word embedding models in batch learning and few-shot machine learning
settings. We believe our results open new possibilities for applications that
support language learning for both human and machine learners"
5149,"In
indeed further research on reward speciﬁcation, en-       Advances in Neural Information Processing Systems,
suring truthful outputs, and other constraint strate-     pages 5055–5065.",Hindsight experience replay.,"gies for dialogue systems that combine language
models and reward maximization is a promising          Layla El Asri, Jing He, and Kaheer Suleman.",2022-04-18 17:23:11+00:00,Context-Aware Language Modeling for Goal-Oriented Dialogue Systems,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Charlie Snell'), arxiv.Result.Author('Sherry Yang'), arxiv.Result.Author('Justin Fu'), arxiv.Result.Author('Yi Su'), arxiv.Result.Author('Sergey Levine')]","Goal-oriented dialogue systems face a trade-off between fluent language
generation and task-specific control. While supervised learning with large
language models is capable of producing realistic text, how to steer such
responses towards completing a specific task without sacrificing language
quality remains an open question. In this work, we formulate goal-oriented
dialogue as a partially observed Markov decision process, interpreting the
language model as a representation of both the dynamics and the policy. This
view allows us to extend techniques from learning-based control, such as task
relabeling, to derive a simple and effective method to finetune language models
in a goal-aware way, leading to significantly improved task performance. We
additionally introduce a number of training strategies that serve to better
focus the model on the task at hand. We evaluate our method, Context-Aware
Language Models (CALM), on a practical flight-booking task using AirDialogue.
Empirically, CALM outperforms the state-of-the-art method by 7% in terms of
task success, matching human-level task performance."
5150,"In
indeed further research on reward speciﬁcation, en-       Advances in Neural Information Processing Systems,
suring truthful outputs, and other constraint strate-     pages 5055–5065.",Hindsight experience replay.,"gies for dialogue systems that combine language
models and reward maximization is a promising          Layla El Asri, Jing He, and Kaheer Suleman.",2022-04-18 17:23:11+00:00,Context-Aware Language Modeling for Goal-Oriented Dialogue Systems,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Charlie Snell'), arxiv.Result.Author('Mengjiao Yang'), arxiv.Result.Author('Justin Fu'), arxiv.Result.Author('Yi Su'), arxiv.Result.Author('Sergey Levine')]","Goal-oriented dialogue systems face a trade-off between fluent language
generation and task-specific control. While supervised learning with large
language models is capable of producing realistic text, how to steer such
responses towards completing a specific task without sacrificing language
quality remains an open question. In this work, we formulate goal-oriented
dialogue as a partially observed Markov decision process, interpreting the
language model as a representation of both the dynamics and the policy. This
view allows us to extend techniques from learning-based control, such as task
relabeling, to derive a simple and effective method to finetune language models
in a goal-aware way, leading to significantly improved task performance. We
additionally introduce a number of training strategies that serve to better
focus the model on the task at hand. We evaluate our method, Context-Aware
Language Models (CALM), on a practical flight-booking task using AirDialogue.
Empirically, CALM outperforms the state-of-the-art method by 7% in terms of
task success, matching human-level task performance."
5286,"[2021] showed  We believe that further research is needed to detect and miti-
that none of the ME methods outperformed fasttext source        gate such biases in MEs.","trinsic evaluation tasks, Garc´ıa-Ferrero et al.","embedding on GLUE benchmarks [Wang et al., 2019].",2022-04-25 13:51:48+00:00,A Survey on Word Meta-Embedding Learning,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Danushka Bollegala'), arxiv.Result.Author(""James O'Neill"")]","Meta-embedding (ME) learning is an emerging approach that attempts to learn
more accurate word embeddings given existing (source) word embeddings as the
sole input.
  Due to their ability to incorporate semantics from multiple source embeddings
in a compact manner with superior performance, ME learning has gained
popularity among practitioners in NLP.
  To the best of our knowledge, there exist no prior systematic survey on ME
learning and this paper attempts to fill this need.
  We classify ME learning methods according to multiple factors such as whether
they (a) operate on static or contextualised embeddings, (b) trained in an
unsupervised manner or (c) fine-tuned for a particular task/domain.
  Moreover, we discuss the limitations of existing ME learning methods and
highlight potential future research directions."
5290,"3)
Our model, MolT5, whose weights we will release,
will allow further research in the NLP community
on the applications of multimodal text-molecule
models.","Speciﬁcally-designed
molecular solutions have the potential to revolution-
ize ﬁelds such as medicine and material science.","Acknowledgements

This work was supported by the Molecule Maker
Lab Institute: An AI Research Institutes program
supported by NSF under Award No.",2022-04-25 17:48:09+00:00,Translation between Molecules and Natural Language,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Carl Edwards'), arxiv.Result.Author('Tuan Lai'), arxiv.Result.Author('Kevin Ros'), arxiv.Result.Author('Garrett Honke'), arxiv.Result.Author('Heng Ji')]","Joint representations between images and text have been deeply investigated
in the literature. In computer vision, the benefits of incorporating natural
language have become clear for enabling semantic-level control of images. In
this work, we present $\textbf{MolT5}-$a self-supervised learning framework for
pretraining models on a vast amount of unlabeled natural language text and
molecule strings. $\textbf{MolT5}$ allows for new, useful, and challenging
analogs of traditional vision-language tasks, such as molecule captioning and
text-based de novo molecule generation (altogether: translation between
molecules and language), which we explore for the first time. Furthermore,
since $\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome
the chemistry domain shortcoming of data scarcity. Additionally, we consider
several metrics, including a new cross-modal embedding-based metric, to
evaluate the tasks of molecule captioning and text-based molecule generation.
By interfacing molecules with natural language, we enable a higher semantic
level of control over molecule discovery and understanding--a critical task for
scientific domains such as drug discovery and material design. Our results show
that $\textbf{MolT5}$-based models are able to generate outputs, both molecule
and text, which in many cases are high quality and match the input modality. On
molecule generation, our best model achieves 30% exact matching test accuracy
(i.e., it generates the correct structure for about one-third of the captions
in our held-out test set)."
5291,"3)
Our model, MolT5, whose weights we will release,
will allow further research in the NLP community
on the applications of multimodal text-molecule
models.","Speciﬁcally-designed
molecular solutions have the potential to revolution-
ize ﬁelds such as medicine and material science.","Acknowledgements

This work was supported by the Molecule Maker
Lab Institute: An AI Research Institutes program
supported by NSF under Award No.",2022-04-25 17:48:09+00:00,Translation between Molecules and Natural Language,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Carl Edwards'), arxiv.Result.Author('Tuan Lai'), arxiv.Result.Author('Kevin Ros'), arxiv.Result.Author('Garrett Honke'), arxiv.Result.Author('Heng Ji')]","Joint representations between images and text have been deeply investigated
in the literature. In computer vision, the benefits of incorporating natural
language have become clear for enabling semantic-level control of images. In
this work, we present $\textbf{MolT5}-$a self-supervised learning framework for
pretraining models on a vast amount of unlabeled natural language text and
molecule strings. $\textbf{MolT5}$ allows for new, useful, and challenging
analogs of traditional vision-language tasks, such as molecule captioning and
text-based de novo molecule generation (altogether: translation between
molecules and language), which we explore for the first time. Furthermore,
since $\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome
the chemistry domain shortcoming of data scarcity. Additionally, we consider
several metrics, including a new cross-modal embedding-based metric, to
evaluate the tasks of molecule captioning and text-based molecule generation.
By interfacing molecules with natural language, we enable a higher semantic
level of control over molecule discovery and understanding--a critical task for
scientific domains such as drug discovery and material design. Our results show
that $\textbf{MolT5}$-based models are able to generate outputs, both molecule
and text, which in many cases are high quality and match the input modality. On
molecule generation, our best model achieves 30% exact matching test accuracy
(i.e., it generates the correct structure for about one-third of the captions
in our held-out test set)."
5315,"Our experiments on Symlink
                                                demonstrate the challenges of the symbol-description linking task for ex-
                                                isting models and call for further research eﬀort in this area.","Symlink anno-
                                                tates scientiﬁc papers of 5 diﬀerent domains (i.e., computer science, biol-
                                                ogy, physics, mathematics, and economics).","We will
                                                publicly release Symlink to facilitate future research.",2022-04-26 04:36:14+00:00,Symlink: A New Dataset for Scientific Symbol-Description Linking,cs.CL,['cs.CL'],"[arxiv.Result.Author('Viet Dac Lai'), arxiv.Result.Author('Amir Pouran Ben Veyseh'), arxiv.Result.Author('Franck Dernoncourt'), arxiv.Result.Author('Thien Huu Nguyen')]","Mathematical symbols and descriptions appear in various forms across document
section boundaries without explicit markup. In this paper, we present a new
large-scale dataset that emphasizes extracting symbols and descriptions in
scientific documents. Symlink annotates scientific papers of 5 different
domains (i.e., computer science, biology, physics, mathematics, and economics).
Our experiments on Symlink demonstrate the challenges of the symbol-description
linking task for existing models and call for further research effort in this
area. We will publicly release Symlink to facilitate future research."
5326,"To further study, we ﬁnetune        context emotion and lack ﬁne-grained empathetic
a BERT classiﬁer (Devlin et al., 2019) on the re-        intent modeling.","However, there are no empirical experiments              We believe this phenomenon is caused by that ex-
have shown how empathetic dialogue models ex-            isting models only generate responses according to
press their empathy?","Therefore, we propose EmpHi,
leased EmpatheticIntents1 dataset (Welivita and          which generates empathetic responses with human-
Pu, 2020).",2022-04-26 09:49:49+00:00,EmpHi: Generating Empathetic Responses with Human-like Intents,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Mao Yan Chen'), arxiv.Result.Author('Siheng Li'), arxiv.Result.Author('Yujiu Yang')]","In empathetic conversations, humans express their empathy to others with
empathetic intents. However, most existing empathetic conversational methods
suffer from a lack of empathetic intents, which leads to monotonous empathy. To
address the bias of the empathetic intents distribution between empathetic
dialogue models and humans, we propose a novel model to generate empathetic
responses with human-consistent empathetic intents, EmpHi for short. Precisely,
EmpHi learns the distribution of potential empathetic intents with a discrete
latent variable, then combines both implicit and explicit intent representation
to generate responses with various empathetic intents. Experiments show that
EmpHi outperforms state-of-the-art models in terms of empathy, relevance, and
diversity on both automatic and human evaluation. Moreover, the case studies
demonstrate the high interpretability and outstanding performance of our model."
5329,"For further research where
Although Artetxe et al.",ative back-translation.,"(2020) recommended to use         there are far more languages accommodated, ran-
unsupervised validation criteria for systematic tun-      dom online back-translation (ROBT) proposed by
ing, we follow the setting of (Conneau and Lample,        Zhang et al.",2022-04-26 11:00:32+00:00,Flow-Adapter Architecture for Unsupervised Machine Translation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yihong Liu'), arxiv.Result.Author('Haris Jabbar'), arxiv.Result.Author('Hinrich Schütze')]","In this work, we propose a flow-adapter architecture for unsupervised NMT. It
leverages normalizing flows to explicitly model the distributions of
sentence-level latent representations, which are subsequently used in
conjunction with the attention mechanism for the translation task. The primary
novelties of our model are: (a) capturing language-specific sentence
representations separately for each language using normalizing flows and (b)
using a simple transformation of these latent representations for translating
from one language to another. This architecture allows for unsupervised
training of each language independently. While there is prior work on latent
variables for supervised MT, to the best of our knowledge, this is the first
work that uses latent variables and normalizing flows for unsupervised MT. We
obtain competitive results on several unsupervised MT benchmarks."
5340,present a major problem hampering further research.,"Section 2), datasets for AI research on
situations, including tackling false information in medical domain                fact-checking, particularly datasets providing a mapping between
that (from its inherent characteristics) requires accurate, easily                documents and claims (claim presence and document stance), still
explainable and robust approaches for misinformation detection.","Fact-checking can be done either manually by professional fact-                   In this paper, we are introducing a novel medical misinformation
checkers or (semi-)automatically with the help of AI.",2022-04-26 13:18:27+00:00,Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked Claims,cs.CL,"['cs.CL', 'cs.CY', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Ivan Srba'), arxiv.Result.Author('Branislav Pecher'), arxiv.Result.Author('Matus Tomlein'), arxiv.Result.Author('Robert Moro'), arxiv.Result.Author('Elena Stefancova'), arxiv.Result.Author('Jakub Simko'), arxiv.Result.Author('Maria Bielikova')]","False information has a significant negative influence on individuals as well
as on the whole society. Especially in the current COVID-19 era, we witness an
unprecedented growth of medical misinformation. To help tackle this problem
with machine learning approaches, we are publishing a feature-rich dataset of
approx. 317k medical news articles/blogs and 3.5k fact-checked claims. It also
contains 573 manually and more than 51k automatically labelled mappings between
claims and articles. Mappings consist of claim presence, i.e., whether a claim
is contained in a given article, and article stance towards the claim. We
provide several baselines for these two tasks and evaluate them on the manually
labelled part of the dataset. The dataset enables a number of additional tasks
related to medical misinformation, such as misinformation characterisation
studies or studies of misinformation diffusion between sources."
5351,There        to further study static embeddings.,"Therefore, we consider it necessary
                                        modern natural language processing (NLP).","are currently two large classes of word embedding
                                        models: they build (1) static and (2) contextualized      Several recent works (Nickel and Kiela, 2017;
                                        word vectors correspondingly.",2022-04-26 17:52:59+00:00,From Hyperbolic Geometry Back to Word Embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sultan Nurmukhamedov'), arxiv.Result.Author('Thomas Mach'), arxiv.Result.Author('Arsen Sheverdin'), arxiv.Result.Author('Zhenisbek Assylbekov')]","We choose random points in the hyperbolic disc and claim that these points
are already word representations. However, it is yet to be uncovered which
point corresponds to which word of the human language of interest. This
correspondence can be approximately established using a pointwise mutual
information between words and recent alignment techniques."
5409,"Finally, we benchmark a se-       speciﬁc set of intent labels and values (Rastogi
                                             ries of current state-of-the-art NLU models on    et al., 2019; Heck et al., 2020; Dai et al., 2021),
                                             NLU++; the results demonstrate the challeng-      followed by (ii) the policy module, which makes
                                             ing nature of the dataset, especially in low-     decisions based on the information extracted by the
                                             data regimes, the validity of ‘intent modular-    NLU (Gašic´ et al., 2012; Casanueva et al., 2017;
                                             isation’, and call for further research on ToD    Lubis et al., 2020; Wang et al., 2020a)
                                             NLU.","collected, ﬁltered and carefully annotated by
                                             dialogue NLU experts, yielding high-quality       (NLU) module maps user utterances into a domain-
                                             annotated data.","The NLU module is a critical part of any ToD
                                        1 Introduction                                         system, as it must extract the relevant information
                                                                                               from the user’s utterances.",2022-04-27 16:00:23+00:00,"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Iñigo Casanueva'), arxiv.Result.Author('Ivan Vulić'), arxiv.Result.Author('Georgios Spithourakis'), arxiv.Result.Author('Paweł Budzianowski')]","We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. \textbf{1)} NLU++ provides fine-grained
domain ontologies with a large set of challenging \textit{multi-intent}
sentences, introducing and validating the idea of \textit{intent modules} that
can be combined into complex intents that convey complex user goals, combined
with finer-grained and thus more challenging slot sets. \textbf{2)} The
ontology is divided into \textit{domain-specific} and \textit{generic} (i.e.,
domain-universal) intent modules that overlap across domains, promoting
cross-domain reusability of annotated examples. \textbf{3)} The dataset design
has been inspired by the problems observed in industrial ToD systems, and
\textbf{4)} it has been collected, filtered and carefully annotated by dialogue
NLU experts, yielding high-quality annotated data. Finally, we benchmark a
series of current state-of-the-art NLU models on NLU++; the results demonstrate
the challenging nature of the dataset, especially in low-data regimes, the
validity of `intent modularisation', and call for further research on ToD NLU."
5410,"3) Their ontolo-             2022), and warrant further research on ToD NLU.","Our benchmark comparisons also
enable only much simpler single-label ID experi-                 demonstrate strong performance and shed new light
ments; such setups are not realistic in more com-                on the (ability of) recently emerging QA-based
plex industry settings (see Figure 1 again) and lead             NLU models (Namazifar et al., 2021; Fuisz et al.,
to unnecessarily large intent sets.","gies are tied to speciﬁc domains, making it difﬁcult             The NLU++ dataset is available at: github.com/
to reuse already available annotated data in other               PolyAI-LDN/task-specific-datasets/nlupp.",2022-04-27 16:00:23+00:00,"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Iñigo Casanueva'), arxiv.Result.Author('Ivan Vulić'), arxiv.Result.Author('Georgios Spithourakis'), arxiv.Result.Author('Paweł Budzianowski')]","We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. \textbf{1)} NLU++ provides fine-grained
domain ontologies with a large set of challenging \textit{multi-intent}
sentences, introducing and validating the idea of \textit{intent modules} that
can be combined into complex intents that convey complex user goals, combined
with finer-grained and thus more challenging slot sets. \textbf{2)} The
ontology is divided into \textit{domain-specific} and \textit{generic} (i.e.,
domain-universal) intent modules that overlap across domains, promoting
cross-domain reusability of annotated examples. \textbf{3)} The dataset design
has been inspired by the problems observed in industrial ToD systems, and
\textbf{4)} it has been collected, filtered and carefully annotated by dialogue
NLU experts, yielding high-quality annotated data. Finally, we benchmark a
series of current state-of-the-art NLU models on NLU++; the results demonstrate
the challenging nature of the dataset, especially in low-data regimes, the
validity of `intent modularisation', and call for further research on ToD NLU."
5411,"Finally, we benchmark a se-       speciﬁc set of intent labels and values (Rastogi
                                             ries of current state-of-the-art NLU models on    et al., 2019; Heck et al., 2020; Dai et al., 2021),
                                             NLU++; the results demonstrate the challeng-      followed by (ii) the policy module, which makes
                                             ing nature of the dataset, especially in low-     decisions based on the information extracted by the
                                             data regimes, the validity of ‘intent modular-    NLU (Gašic´ et al., 2012; Casanueva et al., 2017;
                                             isation’, and call for further research on ToD    Lubis et al., 2020; Wang et al., 2020a)
                                             NLU.","collected, ﬁltered and carefully annotated by
                                             dialogue NLU experts, yielding high-quality       (NLU) module maps user utterances into a domain-
                                             annotated data.","The NLU module is a critical part of any ToD
                                        1 Introduction                                         system, as it must extract the relevant information
                                                                                               from the user’s utterances.",2022-04-27 16:00:23+00:00,"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Iñigo Casanueva'), arxiv.Result.Author('Ivan Vulić'), arxiv.Result.Author('Georgios Spithourakis'), arxiv.Result.Author('Paweł Budzianowski')]","We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. 1) NLU++ provides fine-grained domain
ontologies with a large set of challenging multi-intent sentences, introducing
and validating the idea of intent modules that can be combined into complex
intents that convey complex user goals, combined with finer-grained and thus
more challenging slot sets. 2) The ontology is divided into domain-specific and
generic (i.e., domain-universal) intent modules that overlap across domains,
promoting cross-domain reusability of annotated examples. 3) The dataset design
has been inspired by the problems observed in industrial ToD systems, and 4) it
has been collected, filtered and carefully annotated by dialogue NLU experts,
yielding high-quality annotated data. Finally, we benchmark a series of current
state-of-the-art NLU models on NLU++; the results demonstrate the challenging
nature of the dataset, especially in low-data regimes, the validity of `intent
modularisation', and call for further research on ToD NLU."
5412,"3) Their ontolo-             et al., 2021; Fuisz et al., 2022), and warrant
gies are tied to speciﬁc domains, making it difﬁcult             further research on ToD NLU.","Our benchmark
enable only much simpler single-label ID experi-                 comparisons also demonstrate strong performance
ments; such setups are not realistic in more com-                and shed new light on the (ability of) recently
plex industry settings (see Figure 1 again) and lead             emerging QA-based NLU models (Namazifar
to unnecessarily large intent sets.","The NLU++
to reuse already available annotated data in other               dataset is available at: https://github.com/
domains.",2022-04-27 16:00:23+00:00,"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Iñigo Casanueva'), arxiv.Result.Author('Ivan Vulić'), arxiv.Result.Author('Georgios Spithourakis'), arxiv.Result.Author('Paweł Budzianowski')]","We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. 1) NLU++ provides fine-grained domain
ontologies with a large set of challenging multi-intent sentences, introducing
and validating the idea of intent modules that can be combined into complex
intents that convey complex user goals, combined with finer-grained and thus
more challenging slot sets. 2) The ontology is divided into domain-specific and
generic (i.e., domain-universal) intent modules that overlap across domains,
promoting cross-domain reusability of annotated examples. 3) The dataset design
has been inspired by the problems observed in industrial ToD systems, and 4) it
has been collected, filtered and carefully annotated by dialogue NLU experts,
yielding high-quality annotated data. Finally, we benchmark a series of current
state-of-the-art NLU models on NLU++; the results demonstrate the challenging
nature of the dataset, especially in low-data regimes, the validity of `intent
modularisation', and call for further research on ToD NLU."
5413,"Finally, we benchmark a se-       speciﬁc set of intent labels and values (Rastogi
                                            ries of current state-of-the-art NLU models on    et al., 2019; Heck et al., 2020; Dai et al., 2021),
                                            NLU++; the results demonstrate the challeng-      followed by (ii) the policy module, which makes
                                            ing nature of the dataset, especially in low-     decisions based on the information extracted by the
                                            data regimes, the validity of ‘intent modular-    NLU (Gašic´ et al., 2012; Casanueva et al., 2017;
                                            isation’, and call for further research on ToD    Lubis et al., 2020; Wang et al., 2020a)
                                            NLU.","collected, ﬁltered and carefully annotated by
                                            dialogue NLU experts, yielding high-quality       (NLU) module maps user utterances into a domain-
                                            annotated data.","The NLU module is a critical part of any ToD
                                       1 Introduction                                         system, as it must extract the relevant information
                                                                                              from the user’s utterances.",2022-04-27 16:00:23+00:00,"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Iñigo Casanueva'), arxiv.Result.Author('Ivan Vulić'), arxiv.Result.Author('Georgios P. Spithourakis'), arxiv.Result.Author('Paweł Budzianowski')]","We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. 1) NLU++ provides fine-grained domain
ontologies with a large set of challenging multi-intent sentences, introducing
and validating the idea of intent modules that can be combined into complex
intents that convey complex user goals, combined with finer-grained and thus
more challenging slot sets. 2) The ontology is divided into domain-specific and
generic (i.e., domain-universal) intent modules that overlap across domains,
promoting cross-domain reusability of annotated examples. 3) The dataset design
has been inspired by the problems observed in industrial ToD systems, and 4) it
has been collected, filtered and carefully annotated by dialogue NLU experts,
yielding high-quality annotated data. Finally, we benchmark a series of current
state-of-the-art NLU models on NLU++; the results demonstrate the challenging
nature of the dataset, especially in low-data regimes, the validity of `intent
modularisation', and call for further research on ToD NLU."
5436,"The ability to understand multi-      dialogue systems and the necessity for a dataset
                                        ple knowledge forms is critical in developing more     such as HYBRIDIALOGUE to further research in
                                        general-purpose and realistic conversational mod-      this space.","However, knowledge comes in many forms          they demonstrate the challenges with respect to
                                        other than text.",els.,2022-04-28 00:52:16+00:00,HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kai Nakamura'), arxiv.Result.Author('Sharon Levy'), arxiv.Result.Author('Yi-Lin Tuan'), arxiv.Result.Author('Wenhu Chen'), arxiv.Result.Author('William Yang Wang')]","A pressing challenge in current dialogue systems is to successfully converse
with users on topics with information distributed across different modalities.
Previous work in multiturn dialogue systems has primarily focused on either
text or table information. In more realistic scenarios, having a joint
understanding of both is critical as knowledge is typically distributed over
both unstructured and structured forms. We present a new dialogue dataset,
HybriDialogue, which consists of crowdsourced natural conversations grounded on
both Wikipedia text and tables. The conversations are created through the
decomposition of complex multihop questions into simple, realistic multiturn
dialogue interactions. We propose retrieval, system state tracking, and
dialogue response generation tasks for our dataset and conduct baseline
experiments for each. Our results show that there is still ample opportunity
for improvement, demonstrating the importance of building stronger dialogue
systems that can reason over the complex setting of information-seeking
dialogue grounded on tables and text."
5443,"Overall, the impact of step size
                                                                    on robustness needs further study.",attacks decrease.,"5.2 Results with Other Attacks
                                                                    5.5 Impact of Training Epochs
We have shown that GAT brings signiﬁcant im-
provement in robustness against three greedy-based                  Ishida et al.",2022-04-28 07:07:47+00:00,Improving robustness of language models from a geometry-aware perspective,cs.CL,['cs.CL'],"[arxiv.Result.Author('Bin Zhu'), arxiv.Result.Author('Zhaoquan Gu'), arxiv.Result.Author('Le Wang'), arxiv.Result.Author('Jinyin Chen'), arxiv.Result.Author('Qi Xuan')]","Recent studies have found that removing the norm-bounded projection and
increasing search steps in adversarial training can significantly improve
robustness. However, we observe that a too large number of search steps can
hurt accuracy. We aim to obtain strong robustness efficiently using fewer
steps. Through a toy experiment, we find that perturbing the clean data to the
decision boundary but not crossing it does not degrade the test accuracy.
Inspired by this, we propose friendly adversarial data augmentation (FADA) to
generate friendly adversarial data. On top of FADA, we propose geometry-aware
adversarial training (GAT) to perform adversarial training on friendly
adversarial data so that we can save a large number of search steps.
Comprehensive experiments across two widely used datasets and three pre-trained
language models demonstrate that GAT can obtain stronger robustness via fewer
steps. In addition, we provide extensive empirical results and in-depth
analyses on robustness to facilitate future studies."
5486,We release this dataset with our code and models for further research.,"We ﬁll this gap through this work, which we
                                        hope will signiﬁcantly help NLP for Hindi.","Keywords: named entity recognition, dataset, Hindi, human-annotated, low-resource language

                                                        1.",2022-04-28 19:14:21+00:00,HiNER: A Large Hindi Named Entity Recognition Dataset,cs.CL,['cs.CL'],"[arxiv.Result.Author('Rudra Murthy'), arxiv.Result.Author('Pallab Bhattacharjee'), arxiv.Result.Author('Rahul Sharnagat'), arxiv.Result.Author('Jyotsana Khatri'), arxiv.Result.Author('Diptesh Kanojia'), arxiv.Result.Author('Pushpak Bhattacharyya')]","Named Entity Recognition (NER) is a foundational NLP task that aims to
provide class labels like Person, Location, Organisation, Time, and Number to
words in free text. Named Entities can also be multi-word expressions where the
additional I-O-B annotation information helps label them during the NER
annotation process. While English and European languages have considerable
annotated data for the NER task, Indian languages lack on that front -- both in
terms of quantity and following annotation standards. This paper releases a
significantly sized standard-abiding Hindi NER dataset containing 109,146
sentences and 2,220,856 tokens, annotated with 11 tags. We discuss the dataset
statistics in all their essential detail and provide an in-depth analysis of
the NER tag-set used with our data. The statistics of tag-set in our dataset
show a healthy per-tag distribution, especially for prominent classes like
Person, Location and Organisation. Since the proof of resource-effectiveness is
in building models with the resource and testing the model on benchmark data
and against the leader-board entries in shared tasks, we do the same with the
aforesaid data. We use different language models to perform the sequence
labelling task for NER and show the efficacy of our data by performing a
comparative evaluation with models trained on another dataset available for the
Hindi NER task. Our dataset helps achieve a weighted F1 score of 88.78 with all
the tags and 92.22 when we collapse the tag-set, as discussed in the paper. To
the best of our knowledge, no available dataset meets the standards of volume
(amount) and variability (diversity), as far as Hindi NER is concerned. We fill
this gap through this work, which we hope will significantly help NLP for
Hindi. We release this dataset with our code and models at
https://github.com/cfiltnlp/HiNER"
5501,"[2020]
3https://github.com/alipay/Parameter_Inference_Efﬁcient_PIE

                                                                           6
                                                                                        A PREPRINT - MAY 2, 2022

We further study the inﬂuence of the parameter size to the performance (Figure 4).","2The ofﬁcial code for wikiKG90M is utilized, which is based on DGL-KEZheng et al.","On YAGO3-10, the performances
of PairRE and PairRE+LRE increase with the increase of parameter size.",2022-04-29 09:06:56+00:00,PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Linlin Chao'), arxiv.Result.Author('Taifeng Wang'), arxiv.Result.Author('Wei Chu')]","Knowledge graph (KG) embedding methods which map entities and relations to
unique embeddings in the KG have shown promising results on many reasoning
tasks. However, the same embedding dimension for both dense entities and sparse
entities will cause either over parameterization (sparse entities) or under
fitting (dense entities). Normally, a large dimension is set to get better
performance. Meanwhile, the inference time grows log-linearly with the number
of entities for all entities are traversed and compared. Both the parameter and
inference become challenges when working with huge amounts of entities. Thus,
we propose PIE, a \textbf{p}arameter and \textbf{i}nference \textbf{e}fficient
solution. Inspired from tensor decomposition methods, we find that decompose
entity embedding matrix into low rank matrices can reduce more than half of the
parameters while maintaining comparable performance. To accelerate model
inference, we propose a self-supervised auxiliary task, which can be seen as
fine-grained entity typing. By randomly masking and recovering entities'
connected relations, the task learns the co-occurrence of entity and relations.
Utilizing the fine grained typing, we can filter unrelated entities during
inference and get targets with possibly sub-linear time requirement.
Experiments on link prediction benchmarks demonstrate the proposed key
capabilities. Moreover, we prove effectiveness of the proposed solution on the
Open Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the
state of the art performance."
5502,"[2020]
3https://github.com/alipay/Parameter_Inference_Efﬁcient_PIE

                                                                           6
                                                                                        A PREPRINT - MAY 6, 2022

We further study the inﬂuence of the parameter size to the performance (Figure 4).","2The ofﬁcial code for wikiKG90M is utilized, which is based on DGL-KEZheng et al.","On YAGO3-10, the performances
of PairRE and PairRE+LRE increase with the increase of parameter size.",2022-04-29 09:06:56+00:00,PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Linlin Chao'), arxiv.Result.Author('Xiexiong Lin'), arxiv.Result.Author('Taifeng Wang'), arxiv.Result.Author('Wei Chu')]","Knowledge graph (KG) embedding methods which map entities and relations to
unique embeddings in the KG have shown promising results on many reasoning
tasks. However, the same embedding dimension for both dense entities and sparse
entities will cause either over parameterization (sparse entities) or under
fitting (dense entities). Normally, a large dimension is set to get better
performance. Meanwhile, the inference time grows log-linearly with the number
of entities for all entities are traversed and compared. Both the parameter and
inference become challenges when working with huge amounts of entities. Thus,
we propose PIE, a \textbf{p}arameter and \textbf{i}nference \textbf{e}fficient
solution. Inspired from tensor decomposition methods, we find that decompose
entity embedding matrix into low rank matrices can reduce more than half of the
parameters while maintaining comparable performance. To accelerate model
inference, we propose a self-supervised auxiliary task, which can be seen as
fine-grained entity typing. By randomly masking and recovering entities'
connected relations, the task learns the co-occurrence of entity and relations.
Utilizing the fine grained typing, we can filter unrelated entities during
inference and get targets with possibly sub-linear time requirement.
Experiments on link prediction benchmarks demonstrate the proposed key
capabilities. Moreover, we prove effectiveness of the proposed solution on the
Open Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the
state of the art performance."
5510,"Typical
performances of FSL based medical NLP systems are not yet good enough to be suitable for real-world
application, and further research on improving performance is required.","Meta-learning and transfer learning were commonly used
strategies, and a number of studies reported on the benefits of incorporating attention mechanisms.","Lack of public datasets specialized for
FSL presents an obstacle to progressing research on the topic, and future research should consider creating such
datasets and benchmarks for comparative analyses.",2022-04-21 18:15:51+00:00,Few-shot learning for medical text: A systematic review,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Yao Ge'), arxiv.Result.Author('Yuting Guo'), arxiv.Result.Author('Yuan-Chi Yang'), arxiv.Result.Author('Mohammed Ali Al-Garadi'), arxiv.Result.Author('Abeed Sarker')]","Objective: Few-shot learning (FSL) methods require small numbers of labeled
instances for training. As many medical topics have limited annotated textual
data in practical settings, FSL-based natural language processing (NLP) methods
hold substantial promise. We aimed to conduct a systematic review to explore
the state of FSL methods for medical NLP. Materials and Methods: We searched
for articles published between January 2016 and August 2021 using
PubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. To
identify the latest relevant methods, we also searched other sources such as
preprint servers (eg., medRxiv) via Google Scholar. We included all articles
that involved FSL and any type of medical text. We abstracted articles based on
data source(s), aim(s), training set size(s), primary method(s)/approach(es),
and evaluation method(s). Results: 31 studies met our inclusion criteria-all
published after 2018; 22 (71%) since 2020. Concept extraction/named entity
recognition was the most frequently addressed task (13/31; 42%), followed by
text classification (10/31; 32%). Twenty-one (68%) studies reconstructed
existing datasets to create few-shot scenarios synthetically, and MIMIC-III was
the most frequently used dataset (7/31; 23%). Common methods included FSL with
attention mechanisms (12/31; 39%), prototypical networks (8/31; 26%), and
meta-learning (6/31; 19%). Discussion: Despite the potential for FSL in
biomedical NLP, progress has been limited compared to domain-independent FSL.
This may be due to the paucity of standardized, public datasets, and the
relative underperformance of FSL methods on biomedical topics. Creation and
release of specialized datasets for biomedical FSL may aid method development
by enabling comparative analyses."
5536,"Re-training and evaluating the three existing NER    would lead to further research and development of a
      models on 10 randomly generated splits instead      comprehensive evaluation suite for SOTA NER.","We hope that
                                                          the results and resources generated through this paper
  4.","of the standard splits showed that there was some-
      times over 4% variation in performance across the   6.2.",2022-04-29 18:35:53+00:00,What do we Really Know about State of the Art NER?,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sowmya Vajjala'), arxiv.Result.Author('Ramya Balasubramaniam')]","Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future."
5537,"Re-training and evaluating the three existing NER    would lead to further research and development of a
      models on 10 randomly generated splits instead      comprehensive evaluation suite for SOTA NER.","We hope that
                                                          the results and resources generated through this paper
  4.","of the standard splits showed that there was some-
      times over 4% variation in performance across the   6.2.",2022-04-29 18:35:53+00:00,What do we Really Know about State of the Art NER?,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sowmya Vajjala'), arxiv.Result.Author('Ramya Balasubramaniam')]","Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future."
5538,"We further study the speciﬁc causes of errors on
the 42 unambiguous pairs.","answer consolidation that would largely beneﬁt
                                                      real-world open-domain QA systems.","Examples of distinct        Acknowledgement
error causes are described in Table 4 We ﬁnd that
16.7% of the falsely classiﬁed sentence pairs con-    We appreciate the reviewers for their insightful
tain one answer that entails the other instead of     comments and suggestions.",2022-04-29 18:57:23+00:00,Answer Consolidation: Formulation and Benchmarking,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wenxuan Zhou'), arxiv.Result.Author('Qiang Ning'), arxiv.Result.Author('Heba Elfardy'), arxiv.Result.Author('Kevin Small'), arxiv.Result.Author('Muhao Chen')]","Current question answering (QA) systems primarily consider the single-answer
scenario, where each question is assumed to be paired with one correct answer.
However, in many real-world QA applications, multiple answer scenarios arise
where consolidating answers into a comprehensive and non-redundant set of
answers is a more efficient user interface. In this paper, we formulate the
problem of answer consolidation, where answers are partitioned into multiple
groups, each representing different aspects of the answer set. Then, given this
partitioning, a comprehensive and non-redundant set of answers can be
constructed by picking one answer from each group. To initiate research on
answer consolidation, we construct a dataset consisting of 4,699 questions and
24,006 sentences and evaluate multiple models. Despite a promising performance
achieved by the best-performing supervised models, we still believe this task
has room for further improvements."
5559,"We release a Korean dialogue        To address these issues, we study methods for
                                             dataset we built for further research1.","terances, keeping competitive performance on
                                             general metrics.","Role Speciﬁed Open-Domain Dialogue (RSODD)
                                                                                               systems.",2022-04-30 06:23:06+00:00,Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sanghwan Bae'), arxiv.Result.Author('Donghyun Kwak'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Donghoon Ham'), arxiv.Result.Author('Soyoung Kang'), arxiv.Result.Author('Sang-Woo Lee'), arxiv.Result.Author('Woomyoung Park')]","Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research."
5605,"To use
                                                                                                                       reader impressions proﬁtably in further research, they must be
                                                      I.",level understanding of how we proceed in the task.,GENRES ARE NOT RULE-BOUND                                     interpreted or analysed further in some way.,2022-05-01 16:44:55+00:00,Conventions and Mutual Expectations -- understanding sources for web genres,cs.CL,['cs.CL'],[arxiv.Result.Author('Jussi Karlgren')],"Genres can be understood in many different ways. They are often perceived as
a primarily sociological construction, or, alternatively, as a
stylostatistically observable objective characteristic of texts. The latter
view is more common in the research field of information and language
technology. These two views can be quite compatible and can inform each other;
this present investigation discusses knowledge sources for studying genre
variation and change by observing reader and author behaviour rather than
performing analyses on the information objects themselves."
5617,"Based on
                                                        Table 2, we further study the model’s performance
                                                        on data of different properties: language formality,
                                                        training size, document length, and aggregation
                                                Ideology Prediction                                   Stance Detection                         All
                                                                                                                                               avg
                         YT     CongS           HP AllS YT                       TW    Ideo.",On Texts of Different Characteristics.,SEval    SEval Basil      VAST   Basil   Stan.,2022-05-02 02:10:05+00:00,POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yujian Liu'), arxiv.Result.Author('Xinliang Frederick Zhang'), arxiv.Result.Author('David Wegsman'), arxiv.Result.Author('Nick Beauchamp'), arxiv.Result.Author('Lu Wang')]","Ideology is at the core of political science research. Yet, there still does
not exist general-purpose tools to characterize and predict ideology across
different genres of text. To this end, we study Pretrained Language Models
using novel ideology-driven pretraining objectives that rely on the comparison
of articles on the same story written by media of different ideologies. We
further collect a large-scale dataset, consisting of more than 3.6M political
news articles, for pretraining. Our model POLITICS outperforms strong baselines
and the previous state-of-the-art models on ideology prediction and stance
detection tasks. Further analyses show that POLITICS is especially good at
understanding long or formally written texts, and is also robust in few-shot
learning scenarios."
5636,"Furthermore, Lee           ing these challenges will spur further research in these
et al.","It is hoped that identify-
Muir, 1987; Nass and Brave, 2005).",(2019) observed that conversational AI that con-       areas.,2022-05-02 15:08:18+00:00,State-of-the-art in Open-domain Conversational AI: A Survey,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tosin Adewumi'), arxiv.Result.Author('Foteini Liwicki'), arxiv.Result.Author('Marcus Liwicki')]","We survey SoTA open-domain conversational AI models with the purpose of
presenting the prevailing challenges that still exist to spur future research.
In addition, we provide statistics on the gender of conversational AI in order
to guide the ethics discussion surrounding the issue. Open-domain
conversational AI are known to have several challenges, including bland
responses and performance degradation when prompted with figurative language,
among others. First, we provide some background by discussing some topics of
interest in conversational AI. We then discuss the method applied to the two
investigations carried out that make up this study. The first investigation
involves a search for recent SoTA open-domain conversational AI models while
the second involves the search for 100 conversational AI to assess their
gender. Results of the survey show that progress has been made with recent SoTA
conversational AI, but there are still persistent challenges that need to be
solved, and the female gender is more common than the male for conversational
AI. One main take-away is that hybrid models of conversational AI offer more
advantages than any single architecture. The key contributions of this survey
are 1) the identification of prevailing challenges in SoTA open-domain
conversational AI, 2) the unusual discussion about open-domain conversational
AI for low-resource languages, and 3) the discussion about the ethics
surrounding the gender of conversational AI."
5671,"We hope this dataset initiates further research in            arXiv:2010.12821.
the area of skill classiﬁcation.","arXiv preprint
ing.","Devlin, J., Chang, M.-W., Lee, K., and Toutanova,
            7.",2022-05-03 09:13:55+00:00,Kompetencer: Fine-grained Skill Classification in Danish Job Postings via Distant Supervision and Transfer Learning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mike Zhang'), arxiv.Result.Author('Kristian Nørgaard Jensen'), arxiv.Result.Author('Barbara Plank')]","Skill Classification (SC) is the task of classifying job competences from job
postings. This work is the first in SC applied to Danish job vacancy data. We
release the first Danish job posting dataset: Kompetencer (en: competences),
annotated for nested spans of competences. To improve upon coarse-grained
annotations, we make use of The European Skills, Competences, Qualifications
and Occupations (ESCO; le Vrang et al., 2014) taxonomy API to obtain
fine-grained labels via distant supervision. We study two setups: The zero-shot
and few-shot classification setting. We fine-tune English-based models and
RemBERT (Chung et al., 2020) and compare them to in-language Danish models. Our
results show RemBERT significantly outperforms all other models in both the
zero-shot and the few-shot setting."
5683,"Multi-linguality, multi-task, and multi-label see       (2021) further study knowledge transferring across
many impacts on NLP problems due to the diversity       tasks and languages.",Tarunesh et al.,"The authors combine Reptile
of human languages.",2022-05-03 13:58:38+00:00,Meta Learning for Natural Language Processing: A Survey,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Ngoc Thang Vu')]","Deep learning has been the mainstream technique in natural language
processing (NLP) area. However, the techniques require many labeled data and
are less generalizable across domains. Meta-learning is an arising field in
machine learning studying approaches to learn better learning algorithms.
Approaches aim at improving algorithms in various aspects, including data
efficiency and generalizability. Efficacy of approaches has been shown in many
NLP tasks, but there is no systematic survey of these approaches in NLP, which
hinders more researchers from joining the field. Our goal with this survey
paper is to offer researchers pointers to relevant meta-learning works in NLP
and attract more attention from the NLP community to drive future innovation.
This paper first introduces the general concepts of meta-learning and the
common approaches. Then we summarize task construction settings and application
of meta-learning for various NLP problems and review the development of
meta-learning in NLP community."
5684,"(2021) further study
anced performance over attributes (e.g., languages,     knowledge transferring across tasks and languages.",Tarunesh et al.,"tasks, labels), a common approach is to weight the      The authors combine Reptile and DDS to meta-
training examples for data selection to learn models    learn samplers with six different languages (en, hi,
with balanced performance over the attributes, and      es, de, fr, and zh) and ﬁve different tasks (QA,
it is a natural assumption that meta-learning tech-     NLI, paraphrase identiﬁcation, POS tagging, and
niques derive more generalizable weighting than         NER) and demonstrate competitive performance
manually tuned hyperparameters.",2022-05-03 13:58:38+00:00,Meta Learning for Natural Language Processing: A Survey,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Ngoc Thang Vu')]","Deep learning has been the mainstream technique in natural language
processing (NLP) area. However, the techniques require many labeled data and
are less generalizable across domains. Meta-learning is an arising field in
machine learning studying approaches to learn better learning algorithms.
Approaches aim at improving algorithms in various aspects, including data
efficiency and generalizability. Efficacy of approaches has been shown in many
NLP tasks, but there is no systematic survey of these approaches in NLP, which
hinders more researchers from joining the field. Our goal with this survey
paper is to offer researchers pointers to relevant meta-learning works in NLP
and attract more attention from the NLP community to drive future innovation.
This paper first introduces the general concepts of meta-learning and the
common approaches. Then we summarize task construction settings and application
of meta-learning for various NLP problems and review the development of
meta-learning in NLP community."
5757,"However, there still
exists a limitation on our paper which should be resolved in further research; since we were unable to collect appropriate
data from non-native Korean speakers, it was impossible to effectively implement our suggested application.","Our
paper is signiﬁcant in that it proposed a new Korean pronunciation correction system for foreigners.","0.5 Reference

[1] Martinroll.",2022-05-04 11:19:29+00:00,Design of a novel Korean learning application for efficient pronunciation correction,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Minjong Cheon'), arxiv.Result.Author('Minseon Kim'), arxiv.Result.Author('Hanseon Joo')]","The Korean wave, which denotes the global popularity of South Korea's
cultural economy, contributes to the increasing demand for the Korean language.
However, as there does not exist any application for foreigners to learn
Korean, this paper suggested a design of a novel Korean learning application.
Speech recognition, speech-to-text, and speech-to-waveform are the three key
systems in the proposed system. The Google API and the librosa library will
transform the user's voice into a sentence and MFCC. The software will then
display the user's phrase and answer, with mispronounced elements highlighted
in red, allowing users to more easily recognize the incorrect parts of their
pronunciation. Furthermore, the Siamese network might utilize those translated
spectrograms to provide a similarity score, which could subsequently be used to
offer feedback to the user. Despite the fact that we were unable to collect
sufficient foreigner data for this research, it is notable that we presented a
novel Korean pronunciation correction method for foreigners."
5795,"However, argument types do not        which warrant further study.","(2021), in many relation extraction      tence indicates the drugs are used in combination,
benchmarks (Han et al., 2018; Sabo et al., 2021;         and the passage suggests that the combination has
Zhang et al., 2017), the argument types serve as         additive, synergistic, or otherwise beneﬁcial effects
an effective clue.","apply naturally to the drug combination task, in
which all possible relation arguments are entities       Non-positive combination (OTHER_COMB):
of the same type (drugs) and we need to identify         the sentence indicates the drugs are used in com-
speciﬁc subsets of them.",2022-05-04 19:01:16+00:00,A Dataset for N-ary Relation Extraction of Drug Combinations,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Aryeh Tiktinsky'), arxiv.Result.Author('Vijay Viswanathan'), arxiv.Result.Author('Danna Niezni'), arxiv.Result.Author('Dana Meron Azagury'), arxiv.Result.Author('Yosi Shamay'), arxiv.Result.Author('Hillel Taub-Tabib'), arxiv.Result.Author('Tom Hope'), arxiv.Result.Author('Yoav Goldberg')]","Combination therapies have become the standard of care for diseases such as
cancer, tuberculosis, malaria and HIV. However, the combinatorial set of
available multi-drug treatments creates a challenge in identifying effective
combination therapies available in a situation. To assist medical professionals
in identifying beneficial drug-combinations, we construct an expert-annotated
dataset for extracting information about the efficacy of drug combinations from
the scientific literature. Beyond its practical utility, the dataset also
presents a unique NLP challenge, as the first relation extraction dataset
consisting of variable-length relations. Furthermore, the relations in this
dataset predominantly require language understanding beyond the sentence level,
adding to the challenge of this task. We provide a promising baseline model and
identify clear areas for further improvement. We release our dataset, code, and
baseline models publicly to encourage the NLP community to participate in this
task."
5799,"Updating this set and providing similar sets for other low resource
languages is an important research area that is worthy of further research.","The research also
contributed to the resourcing of the Swahili language which is important for communication
around the globe.","Keywords
Swahili, Question Answer, low resource languages

1.",2022-05-04 23:53:23+00:00,KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Barack Wanjawa'), arxiv.Result.Author('Lilian Wanzare'), arxiv.Result.Author('Florence Indede'), arxiv.Result.Author('Owen McOnyango'), arxiv.Result.Author('Lawrence Muchemi'), arxiv.Result.Author('Edward Ombui')]","This research developed a Kencorpus Swahili Question Answering Dataset
KenSwQuAD from raw data of Swahili language, which is a low resource language
predominantly spoken in Eastern African and also has speakers in other parts of
the world. Question Answering datasets are important for machine comprehension
of natural language processing tasks such as internet search and dialog
systems. However, before such machine learning systems can perform these tasks,
they need training data such as the gold standard Question Answering (QA) set
that is developed in this research. The research engaged annotators to
formulate question answer pairs from Swahili texts that had been collected by
the Kencorpus project, a Kenyan languages corpus that collected data from three
Kenyan languages. The total Swahili data collection had 2,585 texts, out of
which we annotated 1,445 story texts with at least 5 QA pairs each, resulting
into a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the
annotated texts was subjected to re-evaluation by different annotators who
confirmed that the QA pairs were all correctly annotated. A proof of concept on
applying the set to machine learning on the question answering task confirmed
that the dataset can be used for such practical tasks. The research therefore
developed KenSwQuAD, a question-answer dataset for Swahili that is useful to
the natural language processing community who need training and gold standard
sets for their machine learning applications. The research also contributed to
the resourcing of the Swahili language which is important for communication
around the globe. Updating this set and providing similar sets for other low
resource languages is an important research area that is worthy of further
research."
5800,"Finally, section 7 provides the
conclusion and points out to areas of further research.","Section 5 provides the
results of the work, with section 6 discussing these results.","3
3.",2022-05-04 23:53:23+00:00,KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Barack Wanjawa'), arxiv.Result.Author('Lilian Wanzare'), arxiv.Result.Author('Florence Indede'), arxiv.Result.Author('Owen McOnyango'), arxiv.Result.Author('Lawrence Muchemi'), arxiv.Result.Author('Edward Ombui')]","This research developed a Kencorpus Swahili Question Answering Dataset
KenSwQuAD from raw data of Swahili language, which is a low resource language
predominantly spoken in Eastern African and also has speakers in other parts of
the world. Question Answering datasets are important for machine comprehension
of natural language processing tasks such as internet search and dialog
systems. However, before such machine learning systems can perform these tasks,
they need training data such as the gold standard Question Answering (QA) set
that is developed in this research. The research engaged annotators to
formulate question answer pairs from Swahili texts that had been collected by
the Kencorpus project, a Kenyan languages corpus that collected data from three
Kenyan languages. The total Swahili data collection had 2,585 texts, out of
which we annotated 1,445 story texts with at least 5 QA pairs each, resulting
into a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the
annotated texts was subjected to re-evaluation by different annotators who
confirmed that the QA pairs were all correctly annotated. A proof of concept on
applying the set to machine learning on the question answering task confirmed
that the dataset can be used for such practical tasks. The research therefore
developed KenSwQuAD, a question-answer dataset for Swahili that is useful to
the natural language processing community who need training and gold standard
sets for their machine learning applications. The research also contributed to
the resourcing of the Swahili language which is important for communication
around the globe. Updating this set and providing similar sets for other low
resource languages is an important research area that is worthy of further
research."
5801,These are opportunities for further research.,"short prose texts,
hence longer stories that may probably have been challenging in machine learning systems
were not considered.","This QA dataset of Swahili, KenSwQuAD, is now available as both a collection of 1,445 story
texts and a separate comma separated values (CSV) file with the 7,526 QA pairs with at least
5 QA pairs per story.",2022-05-04 23:53:23+00:00,KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Barack Wanjawa'), arxiv.Result.Author('Lilian Wanzare'), arxiv.Result.Author('Florence Indede'), arxiv.Result.Author('Owen McOnyango'), arxiv.Result.Author('Lawrence Muchemi'), arxiv.Result.Author('Edward Ombui')]","This research developed a Kencorpus Swahili Question Answering Dataset
KenSwQuAD from raw data of Swahili language, which is a low resource language
predominantly spoken in Eastern African and also has speakers in other parts of
the world. Question Answering datasets are important for machine comprehension
of natural language processing tasks such as internet search and dialog
systems. However, before such machine learning systems can perform these tasks,
they need training data such as the gold standard Question Answering (QA) set
that is developed in this research. The research engaged annotators to
formulate question answer pairs from Swahili texts that had been collected by
the Kencorpus project, a Kenyan languages corpus that collected data from three
Kenyan languages. The total Swahili data collection had 2,585 texts, out of
which we annotated 1,445 story texts with at least 5 QA pairs each, resulting
into a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the
annotated texts was subjected to re-evaluation by different annotators who
confirmed that the QA pairs were all correctly annotated. A proof of concept on
applying the set to machine learning on the question answering task confirmed
that the dataset can be used for such practical tasks. The research therefore
developed KenSwQuAD, a question-answer dataset for Swahili that is useful to
the natural language processing community who need training and gold standard
sets for their machine learning applications. The research also contributed to
the resourcing of the Swahili language which is important for communication
around the globe. Updating this set and providing similar sets for other low
resource languages is an important research area that is worthy of further
research."
5802,"Finally, section 7 provides the conclusion and points out to areas of further research.","Section 5 provides the results of the research, with section 6 discussing these
results.","3 RELATED WORK
There are many different question answering datasets available for exploration and research.",2022-05-04 23:53:23+00:00,KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language,cs.CL,"['cs.CL', 'cs.LG', 'I.2.7']","[arxiv.Result.Author('Barack W. Wanjawa'), arxiv.Result.Author('Lilian D. A. Wanzare'), arxiv.Result.Author('Florence Indede'), arxiv.Result.Author('Owen McOnyango'), arxiv.Result.Author('Lawrence Muchemi'), arxiv.Result.Author('Edward Ombui')]","The need for Question Answering datasets in low resource languages is the
motivation of this research, leading to the development of Kencorpus Swahili
Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story
texts of Swahili low resource language, which is a predominantly spoken in
Eastern African and in other parts of the world. Question Answering (QA)
datasets are important for machine comprehension of natural language for tasks
such as internet search and dialog systems. Machine learning systems need
training data such as the gold standard Question Answering set developed in
this research. The research engaged annotators to formulate QA pairs from
Swahili texts collected by the Kencorpus project, a Kenyan languages corpus.
The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA
pairs each, resulting into a final dataset of 7,526 QA pairs. A quality
assurance set of 12.5% of the annotated texts confirmed that the QA pairs were
all correctly annotated. A proof of concept on applying the set to the QA task
confirmed that the dataset can be usable for such tasks. KenSwQuAD has also
contributed to resourcing of the Swahili language."
5803,"These are opportunities
for further research.","short prose texts, hence longer texts
that may probably have been useful in challenging machine learning systems were not considered.","This QA dataset of Swahili, KenSwQuAD, is now available as both a collection of 1,445 story texts and a separate
comma separated values (CSV) file with the 7,526 QA pairs with at least 5 QA pairs per text.",2022-05-04 23:53:23+00:00,KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language,cs.CL,"['cs.CL', 'cs.LG', 'I.2.7']","[arxiv.Result.Author('Barack W. Wanjawa'), arxiv.Result.Author('Lilian D. A. Wanzare'), arxiv.Result.Author('Florence Indede'), arxiv.Result.Author('Owen McOnyango'), arxiv.Result.Author('Lawrence Muchemi'), arxiv.Result.Author('Edward Ombui')]","The need for Question Answering datasets in low resource languages is the
motivation of this research, leading to the development of Kencorpus Swahili
Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story
texts of Swahili low resource language, which is a predominantly spoken in
Eastern African and in other parts of the world. Question Answering (QA)
datasets are important for machine comprehension of natural language for tasks
such as internet search and dialog systems. Machine learning systems need
training data such as the gold standard Question Answering set developed in
this research. The research engaged annotators to formulate QA pairs from
Swahili texts collected by the Kencorpus project, a Kenyan languages corpus.
The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA
pairs each, resulting into a final dataset of 7,526 QA pairs. A quality
assurance set of 12.5% of the annotated texts confirmed that the QA pairs were
all correctly annotated. A proof of concept on applying the set to the QA task
confirmed that the dataset can be usable for such tasks. KenSwQuAD has also
contributed to resourcing of the Swahili language."
5806,"is considered low-resourced with regards to NLP,            Corcoran, P., Palmer, G., Arman, L., Knight, D., and
this dataset will enable further research works in             Spasic´, I.",Springer.,(2021).,2022-05-05 10:12:45+00:00,Introducing the Welsh Text Summarisation Dataset and Baseline Systems,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Ignatius Ezeani'), arxiv.Result.Author('Mahmoud El-Haj'), arxiv.Result.Author('Jonathan Morris'), arxiv.Result.Author('Dawn Knight')]","Welsh is an official language in Wales and is spoken by an estimated 884,300
people (29.2% of the population of Wales). Despite this status and estimated
increase in speaker numbers since the last (2011) census, Welsh remains a
minority language undergoing revitalization and promotion by Welsh Government
and relevant stakeholders. As part of the effort to increase the availability
of Welsh digital technology, this paper introduces the first Welsh
summarisation dataset, which we provide freely for research purposes to help
advance the work on Welsh text summarization. The dataset was created by Welsh
speakers by manually summarising Welsh Wikipedia articles. In addition, the
paper discusses the implementation and evaluation of different summarisation
systems for Welsh. The summarization systems and results will serve as
benchmarks for the development of summarises in other minority language
contexts."
5811,"However, overall judgements of difﬁculty
      models, as a benchmark for further research.","The framing
   • We release a novel dataset containing 55,125       of word complexity in a continuous fashion allows
      English word complexity annotations from          for more subjective approaches of CWI to be intro-
      1,225 ESL participants, as well as all trained    duced.","in this dataset are still produced by averaging the
                                                        labels across multiple annotators.",2022-05-05 10:53:31+00:00,One Size Does Not Fit All: The Case for Personalised Word Complexity Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'cs.LG']","[arxiv.Result.Author('Sian Gooding'), arxiv.Result.Author('Manuel Tragut')]","Complex Word Identification (CWI) aims to detect words within a text that a
reader may find difficult to understand. It has been shown that CWI systems can
improve text simplification, readability prediction and vocabulary acquisition
modelling. However, the difficulty of a word is a highly idiosyncratic notion
that depends on a reader's first language, proficiency and reading experience.
In this paper, we show that personal models are best when predicting word
complexity for individual readers. We use a novel active learning framework
that allows models to be tailored to individuals and release a dataset of
complexity annotations and models as a benchmark for further research."
5812,"In B. Schölkopf, J. C. Platt,
so further research is needed.","Analysis of Representations
generation, while dialogue is naturally multi-turn               for Domain Adaptation.","and T. Hoffman, editors, Advances in Neural Infor-
                                                                 mation Processing Systems 19, pages 137–144.",2022-05-05 11:10:54+00:00,Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yujie Xing'), arxiv.Result.Author('Jinglun Cai'), arxiv.Result.Author('Nils Barlaug'), arxiv.Result.Author('Peng Liu'), arxiv.Result.Author('Jon Atle Gulla')]","Open-domain conversational systems are assumed to generate equally good
responses on multiple domains. Previous work achieved good performance on the
single corpus, but training and evaluating on multiple corpora from different
domains are less studied. This paper explores methods of generating relevant
responses for each of multiple multi-domain corpora. We first examine
interleaved learning which intermingles multiple corpora as the baseline. We
then investigate two multi-domain learning methods, labeled learning and
multi-task labeled learning, which encode each corpus through a unique corpus
embedding. Furthermore, we propose Domain-specific Frequency (DF), a novel
word-level importance weight that measures the relative importance of a word
for a specific corpus compared to other corpora. Based on DF, we propose
weighted learning, a method that integrates DF to the loss function. We also
adopt DF as a new evaluation metric. Extensive experiments show that our
methods gain significant improvements on both automatic and human evaluation.
We share our code and data for reproducibility"
5842,"Thirdly, we highlight areas for further research,
while also pointing towards the dramatic potential of such      channel           # ﬁles  channel              # ﬁles
models for cultural heritage institutions with large collec-    P4 Blekinge      247993   P4 Jo¨nko¨ping      258820
tions of previously unlabelled audiovisual data.","Secondly, we present the       2014  456860     2015     455665   2016       456759
results of our evaluation process, where we compare the         2017  455460     2018     455541   2019       458422
performance of our model, VoxRex, with existing models          2020  474282     2021     390536
for Swedish ASR, including the monolingual VoxPopuli-sv
and the multilingual XLSR (Wang et al., 2021; Conneau et              Table 1: Broadcast year distribution
al., 2020).","P4 Norrbotten    262690   P4 Va¨sterbotten    246940
                                                                P4 Plus*                  P4 Dalarna          262158
       2.",2022-05-06 06:06:00+00:00,Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language,cs.CL,['cs.CL'],"[arxiv.Result.Author('Martin Malmsten'), arxiv.Result.Author('Chris Haffenden'), arxiv.Result.Author('Love Börjeson')]","This paper explains our work in developing new acoustic models for automated
speech recognition (ASR) at KBLab, the infrastructure for data-driven research
at the National Library of Sweden (KB). We evaluate different approaches for a
viable speech-to-text pipeline for audiovisual resources in Swedish, using the
wav2vec 2.0 architecture in combination with speech corpuses created from KB's
collections. These approaches include pretraining an acoustic model for Swedish
from the ground up, and fine-tuning existing monolingual and multilingual
models. The collections-based corpuses we use have been sampled from millions
of hours of speech, with a conscious attempt to balance regional dialects to
produce a more representative, and thus more democratic, model. The acoustic
model this enabled, ""VoxRex"", outperforms existing models for Swedish ASR. We
also evaluate combining this model with various pretrained language models,
which further enhanced performance. We conclude by highlighting the potential
of such technology for cultural heritage institutions with vast collections of
previously unlabelled audiovisual data. Our models are released for further
exploration and research here: https://huggingface.co/KBLab."
5843,"Thirdly, we highlight areas for further research,
while also pointing towards the dramatic potential of such      channel           # ﬁles  channel              # ﬁles
models for cultural heritage institutions with large collec-    P4 Blekinge      247993   P4 Jo¨nko¨ping      258820
tions of previously unlabelled audiovisual data.","Secondly, we present the       2014  456860     2015     455665   2016       456759
results of our evaluation process, where we compare the         2017  455460     2018     455541   2019       458422
performance of our model, VoxRex, with existing models          2020  474282     2021     390536
for Swedish ASR, including the monolingual VoxPopuli-sv
and the multilingual XLSR (Wang et al., 2021a; Conneau et             Table 1: Broadcast year distribution
al., 2020).","P4 Norrbotten    262690   P4 Va¨sterbotten    246940
                                                                P4 Plus*                  P4 Dalarna          262158
       2.",2022-05-06 06:06:00+00:00,Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language,cs.CL,['cs.CL'],"[arxiv.Result.Author('Martin Malmsten'), arxiv.Result.Author('Chris Haffenden'), arxiv.Result.Author('Love Börjeson')]","This paper explains our work in developing new acoustic models for automated
speech recognition (ASR) at KBLab, the infrastructure for data-driven research
at the National Library of Sweden (KB). We evaluate different approaches for a
viable speech-to-text pipeline for audiovisual resources in Swedish, using the
wav2vec 2.0 architecture in combination with speech corpuses created from KB's
collections. These approaches include pretraining an acoustic model for Swedish
from the ground up, and fine-tuning existing monolingual and multilingual
models. The collections-based corpuses we use have been sampled from millions
of hours of speech, with a conscious attempt to balance regional dialects to
produce a more representative, and thus more democratic, model. The acoustic
model this enabled, ""VoxRex"", outperforms existing models for Swedish ASR. We
also evaluate combining this model with various pretrained language models,
which further enhanced performance. We conclude by highlighting the potential
of such technology for cultural heritage institutions with vast collections of
previously unlabelled audiovisual data. Our models are released for further
exploration and research here: https://huggingface.co/KBLab."
5879,"Through our datasets and methods, we
2019) with an Adam optimizer and a learning rate       hope to incite further research at the intersection of
of 2 × 10−4 is used.","In the classiﬁcation objective, we
5.3.1 Training settings                                ﬁnd that the addition of entities provides a consid-
                                                       erable boost in FITB evident through the quality of
For the FITB experiment, DistilBERT (Sanh et al.,      predictions.",The model is trained for 3        numeracy and NER.,2022-05-07 05:22:43+00:00,Number Entity Recognition,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Dhanasekar Sundararaman'), arxiv.Result.Author('Vivek Subramanian'), arxiv.Result.Author('Guoyin Wang'), arxiv.Result.Author('Liyan Xu'), arxiv.Result.Author('Lawrence Carin')]","Numbers are essential components of text, like any other word tokens, from
which natural language processing (NLP) models are built and deployed. Though
numbers are typically not accounted for distinctly in most NLP tasks, there is
still an underlying amount of numeracy already exhibited by NLP models. In this
work, we attempt to tap this potential of state-of-the-art NLP models and
transfer their ability to boost performance in related tasks. Our proposed
classification of numbers into entities helps NLP models perform well on
several tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on
question answering using joint embeddings, outperforming the BERT and RoBERTa
baseline classification."
5880,"Intro-
hope to incite further research at the intersection of      duction to the conll-2003 shared task: Language-
numeracy and NER.",2003.,independent named entity recognition.,2022-05-07 05:22:43+00:00,Improving Downstream Task Performance by Treating Numbers as Entities,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Dhanasekar Sundararaman'), arxiv.Result.Author('Vivek Subramanian'), arxiv.Result.Author('Guoyin Wang'), arxiv.Result.Author('Liyan Xu'), arxiv.Result.Author('Lawrence Carin')]","Numbers are essential components of text, like any other word tokens, from
which natural language processing (NLP) models are built and deployed. Though
numbers are typically not accounted for distinctly in most NLP tasks, there is
still an underlying amount of numeracy already exhibited by NLP models. In this
work, we attempt to tap this potential of state-of-the-art NLP models and
transfer their ability to boost performance in related tasks. Our proposed
classification of numbers into entities helps NLP models perform well on
several tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on
question answering using joint embeddings, outperforming the BERT and RoBERTa
baseline classification."
5959,"With this work, the gap between academic                  [16] K. Ramesh, S. Ravishankaran, A. Joshi, and K. Chandrasekaran,
research and production systems has been narrowed,                            “A survey of design techniques for conversational agents,”
remaining at the same time open for further research                          in Information, Communication and Computing Technology
and implementations.","373–383, Springer International Publishing, 2020.
ventional chatbot engines has been presented, developed
and reviewed.","(S. Kaushik, D. Gupta, L. Kharb, and D. Chahal, eds.",2022-05-09 14:26:52+00:00,ISA-bEL: Intelligent Search Algorithm based on Entity Linking,cs.CL,['cs.CL'],"[arxiv.Result.Author('Rubén González Sendino'), arxiv.Result.Author('Mónica Ortega'), arxiv.Result.Author('Carlos Carrasco')]","Nowadays, the way in which the people interact with computers has changed.
Text- or voice-based interfaces are being widely applied in different
industries. Among the most used ways of processing the user input are those
based on intents or retrieval algorithms. In these solutions, important
information of the user could be lost in the process. For the proposed natural
language processing pipeline the entities are going to take a principal role,
under the assumption that entities are where the purpose of the user resides.
Entities fed with context will be projected to a specific domain supported by a
knowledge graph, resulting in what has been named as linked entities. These
linked entities serve then as a key for searching a top level aggregation
concept within our knowledge graph."
5983,"To extract mechanism summaries we ﬁrst         • To encourage reproducibility and further research,
collected a small set of mechanism sentences with the            we release the dataset and the code used during its
help of domain experts.","information extraction system (Valenzuela-Esca´rcega
et al., 2018).",We use this to bootstrap a larger        creation.,2022-05-10 03:42:30+00:00,SuMe: A Dataset Towards Summarizing Biomedical Mechanisms,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Mohaddeseh Bastan'), arxiv.Result.Author('Nishant Shankar'), arxiv.Result.Author('Mihai Surdeanu'), arxiv.Result.Author('Niranjan Balasubramanian')]","Can language models read biomedical texts and explain the biomedical
mechanisms discussed? In this work we introduce a biomedical mechanism
summarization task. Biomedical studies often investigate the mechanisms behind
how one entity (e.g., a protein or a chemical) affects another in a biological
context. The abstracts of these publications often include a focused set of
sentences that present relevant supporting statements regarding such
relationships, associated experimental evidence, and a concluding sentence that
summarizes the mechanism underlying the relationship. We leverage this
structure and create a summarization task, where the input is a collection of
sentences and the main entities in an abstract, and the output includes the
relationship and a sentence that summarizes the mechanism. Using a small amount
of manually labeled mechanism sentences, we train a mechanism sentence
classifier to filter a large biomedical abstract collection and create a
summarization dataset with 22k instances. We also introduce conclusion sentence
generation as a pretraining task with 611k instances. We benchmark the
performance of large bio-domain language models. We find that while the
pretraining task help improves performance, the best model produces acceptable
mechanism outputs in only 32% of the instances, which shows the task presents
significant challenges in biomedical language understanding and summarization."
6041,"Our study presents a
comprehensive study covering this gap, which we argue is a key step towards better understanding the problem to
further research in temporal persistent classifier design.","While these present
clever approaches to mitigate the impact of time, there is still little understanding about how, when and why models
deteriorate over time, in a way that can inform design of text classifiers in the first instance.","Our work extends substantially the limited scope of previous
research into temporal classification, which has largely focused on specific datasets and tasks solely on future data, by
providing a comprehensive study looking at 6 classification algorithms, 6 pre-trained language models, 3 longitudinal
datasets covering up to 18 years, as well as looking at temporal persistence on both past and future datasets, among
others.",2022-05-11 12:21:14+00:00,Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Rabab Alkhalifa'), arxiv.Result.Author('Elena Kochkina'), arxiv.Result.Author('Arkaitz Zubiaga')]","Where performance of text classification models drops over time due to
changes in data, development of models whose performance persists over time is
important. An ability to predict a model's ability to persist over time can
help design models that can be effectively used over a longer period of time.
In this paper, we look at this problem from a practical perspective by
assessing the ability of a wide range of language models and classification
algorithms to persist over time, as well as how dataset characteristics can
help predict the temporal stability of different models. We perform
longitudinal classification experiments on three datasets spanning between 6
and 19 years, and involving diverse tasks and types of data. We find that one
can estimate how a model will retain its performance over time based on (i) how
well the model performs over a restricted time period and its extrapolation to
a longer time period, and (ii) the linguistic characteristics of the dataset,
such as the familiarity score between subsets from different years. Findings
from these experiments have important implications for the design of text
classification models with the aim of preserving performance over time."
6042,"Our study provides important insights into
model deterioration that help inform further research in the development of temporally-stable classification models.","Despite these
differences, we observe that different language models and algorithms consistently underperform on temporally distant
test data, which is particularly impacted by the degree of variability of the dataset in question; while more closed-
domain datasets such as book reviews show more stable performance over time, models struggle particularly with
open-domain datasets exhibiting more prominent changes in language use.","While our study has focused on understanding the capacity and limitations of widely-used classification
approaches, and drawing a set of best practices from this analysis, future research could further look into tackling
the problem through domain adaptation and transfer learning.",2022-05-11 12:21:14+00:00,Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Rabab Alkhalifa'), arxiv.Result.Author('Elena Kochkina'), arxiv.Result.Author('Arkaitz Zubiaga')]","Performance of text classification models tends to drop over time due to
changes in data, which limits the lifetime of a pretrained model. Therefore an
ability to predict a model's ability to persist over time can help design
models that can be effectively used over a longer period of time. In this
paper, we provide a thorough discussion into the problem, establish an
evaluation setup for the task. We look at this problem from a practical
perspective by assessing the ability of a wide range of language models and
classification algorithms to persist over time, as well as how dataset
characteristics can help predict the temporal stability of different models. We
perform longitudinal classification experiments on three datasets spanning
between 6 and 19 years, and involving diverse tasks and types of data. By
splitting the longitudinal datasets into years, we perform a comprehensive set
of experiments by training and testing across data that are different numbers
of years apart from each other, both in the past and in the future. This
enables a gradual investigation into the impact of the temporal gap between
training and test sets on the classification performance, as well as measuring
the extent of the persistence over time."
6106,"Integrated Gradients (Sundararajan et al., 2017)
   • We further release our code, models, and data       focuses on explaining predictions by integrating
      to facilitate further research in the ﬁeld1.","However, often            process on offensive span identiﬁcation by

                                             ∗Corresponding Author
      proposing masked data augmentation and mul-        2.2 Integrated Gradients (IG)
      tilabel training.","the gradient along some trajectory in input space
                                                         connecting two points.",2022-05-12 14:32:12+00:00,Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Manikandan Ravikiran'), arxiv.Result.Author('Bharathi Raja Chakravarthi')]","This paper investigates the effectiveness of sentence-level transformers for
zero-shot offensive span identification on a code-mixed Tamil dataset. More
specifically, we evaluate rationale extraction methods of Local Interpretable
Model Agnostic Explanations (LIME) \cite{DBLP:conf/kdd/Ribeiro0G16} and
Integrated Gradients (IG) \cite{DBLP:conf/icml/SundararajanTY17} for adapting
transformer based offensive language classification models for zero-shot
offensive span identification. To this end, we find that LIME and IG show
baseline $F_{1}$ of 26.35\% and 44.83\%, respectively. Besides, we study the
effect of data set size and training process on the overall accuracy of span
identification. As a result, we find both LIME and IG to show significant
improvement with Masked Data Augmentation and Multilabel Training, with $F_{1}$
of 50.23\% and 47.38\% respectively. \textit{Disclaimer : This paper contains
examples that may be considered profane, vulgar, or offensive. The examples do
not represent the views of the authors or their employers/graduate schools
towards any person(s), group(s), practice(s), or entity/entities. Instead they
are used to emphasize only the linguistic research challenges.}"
6108,"To further study the
                     56.4 42.8    60.8 45.1       50.7 43.1   inﬂuence of IC, we compare the performances of
GTEE-BASE            71.0 53.7    73.0 53.2       65.5 53.5   using no IC, trained IC, and gold IC.","The accuracy of IC is
   w/ IC (trained)   75.2 57.5    76.6 55.8       71.6 56.9   95.4%, 93.5% and 94.2% for ACE05E, ACE05E+
   w/ IC (gold)                                               and ERE-EN, respectively.","The compared
   w/o IC            74.6 55.9    75.1 54.8       70.7 56.5   F1 scores are listed in Table 7.
   w/ IC (trained)
   w/ IC (gold)                                                  First, we ﬁnd that with the help of our trained

Table 7: The F1 scores under different irrelevance clas-
siﬁer settings on ACE05-E, ACE05-E+ and ERE-EN.",2022-05-12 15:38:34+00:00,Dynamic Prefix-Tuning for Generative Template-based Event Extraction,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xiao Liu'), arxiv.Result.Author('Heyan Huang'), arxiv.Result.Author('Ge Shi'), arxiv.Result.Author('Bo Wang')]","We consider event extraction in a generative manner with template-based
conditional generation. Although there is a rising trend of casting the task of
event extraction as a sequence generation problem with prompts, these
generation-based methods have two significant challenges, including using
suboptimal prompts and static event type information. In this paper, we propose
a generative template-based event extraction method with dynamic prefix
(GTEE-DynPref) by integrating context information with type-specific prefixes
to learn a context-specific prefix for each context. Experimental results show
that our model achieves competitive results with the state-of-the-art
classification-based model OneIE on ACE 2005 and achieves the best performances
on ERE. Additionally, our model is proven to be portable to new types of events
effectively."
6119,"It is our
pared with the average score of the top-3 source           hope that FETA enables further research not only
                                                           in task transfer, but also in other learning settings,
                                                           and in the generalizability and efﬁciency of model
                                                           architectures and pre-training datasets.",Table 3 shows the multi-source results com-         broad conclusions on transfer learning.,"Broader Impact                                           References

A concern regarding any work that includes large-        Alon Albalak.",2022-05-12 17:59:00+00:00,FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue,cs.CL,['cs.CL'],"[arxiv.Result.Author('Alon Albalak'), arxiv.Result.Author('Yi-Lin Tuan'), arxiv.Result.Author('Pegah Jandaghi'), arxiv.Result.Author('Connor Pryor'), arxiv.Result.Author('Luke Yoffe'), arxiv.Result.Author('Deepak Ramachandran'), arxiv.Result.Author('Lise Getoor'), arxiv.Result.Author('Jay Pujara'), arxiv.Result.Author('William Yang Wang')]","Task transfer, transferring knowledge contained in related tasks, holds the
promise of reducing the quantity of labeled data required to fine-tune language
models. Dialogue understanding encompasses many diverse tasks, yet task
transfer has not been thoroughly studied in conversational AI. This work
explores conversational task transfer by introducing FETA: a benchmark for
few-sample task transfer in open-domain dialogue. FETA contains two underlying
sets of conversations upon which there are 10 and 7 tasks annotated, enabling
the study of intra-dataset task transfer; task transfer without domain
adaptation. We utilize three popular language models and three learning
algorithms to analyze the transferability between 132 source-target task pairs
and create a baseline for future work. We run experiments in the single- and
multi-source settings and report valuable findings, e.g., most performance
trends are model-specific, and span extraction and multiple-choice tasks
benefit the most from task transfer. In addition to task transfer, FETA can be
a valuable resource for future research into the efficiency and
generalizability of pre-training datasets and model architectures, as well as
for learning settings such as continual and multitask learning."
6120,"It is our       (2020) propose a transfer learning method that min-
hope that FETA enables further research not only        imizes negative task interference via meta-learning
in task transfer, but also in other learning settings,  for multilingual models, Albalak et al.",broad conclusions on transfer learning.,"(2022) pro-
and in the generalizability and efﬁciency of model      pose a policy-guided algorithm for task transfer in
architectures and pre-training datasets.",2022-05-12 17:59:00+00:00,FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue,cs.CL,['cs.CL'],"[arxiv.Result.Author('Alon Albalak'), arxiv.Result.Author('Yi-Lin Tuan'), arxiv.Result.Author('Pegah Jandaghi'), arxiv.Result.Author('Connor Pryor'), arxiv.Result.Author('Luke Yoffe'), arxiv.Result.Author('Deepak Ramachandran'), arxiv.Result.Author('Lise Getoor'), arxiv.Result.Author('Jay Pujara'), arxiv.Result.Author('William Yang Wang')]","Task transfer, transferring knowledge contained in related tasks, holds the
promise of reducing the quantity of labeled data required to fine-tune language
models. Dialogue understanding encompasses many diverse tasks, yet task
transfer has not been thoroughly studied in conversational AI. This work
explores conversational task transfer by introducing FETA: a benchmark for
few-sample task transfer in open-domain dialogue. FETA contains two underlying
sets of conversations upon which there are 10 and 7 tasks annotated, enabling
the study of intra-dataset task transfer; task transfer without domain
adaptation. We utilize three popular language models and three learning
algorithms to analyze the transferability between 132 source-target task pairs
and create a baseline for future work. We run experiments in the single- and
multi-source settings and report valuable findings, e.g., most performance
trends are model-specific, and span extraction and multiple-choice tasks
benefit the most from task transfer. In addition to task transfer, FETA can be
a valuable resource for future research into the efficiency and
generalizability of pre-training datasets and model architectures, as well as
for learning settings such as continual and multitask learning."
6186,"We suggest measuring the quality of      community to further study the different aspects of
                                                        real-world complex claims, especially the implied
arguments behind the surface information.","By introducing this claim decom-
 not know what minor distinctions in questions may      position task and the dataset, we will enable the
 be important.","in Natural Language Processing and the 9th Inter-
                                                           national Joint Conference on Natural Language Pro-
Acknowledgments                                            cessing (EMNLP-IJCNLP), pages 4685–4697, Hong
                                                           Kong, China.",2022-05-14 00:40:57+00:00,Generating Literal and Implied Subquestions to Fact-check Complex Claims,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jifan Chen'), arxiv.Result.Author('Aniruddh Sriram'), arxiv.Result.Author('Eunsol Choi'), arxiv.Result.Author('Greg Durrett')]","Verifying complex political claims is a challenging task, especially when
politicians use various tactics to subtly misrepresent the facts. Automatic
fact-checking systems fall short here, and their predictions like ""half-true""
are not very useful in isolation, since we have no idea which parts of the
claim are true and which are not. In this work, we focus on decomposing a
complex claim into a comprehensive set of yes-no subquestions whose answers
influence the veracity of the claim. We present ClaimDecomp, a dataset of
decompositions for over 1000 claims. Given a claim and its verification
paragraph written by fact-checkers, our trained annotators write subquestions
covering both explicit propositions of the original claim and its implicit
facets, such as asking about additional political context that changes our view
of the claim's veracity. We study whether state-of-the-art models can generate
such subquestions, showing that these models generate reasonable questions to
ask, but predicting the comprehensive set of subquestions from the original
claim without evidence remains challenging. We further show that these
subquestions can help identify relevant evidence to fact-check the full claim
and derive the veracity through their answers, suggesting that they can be
useful pieces of a fact-checking pipeline."
6187,"We suggest measuring the quality of      community to further study the different aspects of
 the generated questions on some downstream tasks,      real-world complex claims, especially the implied
 e.g., evidence retrieval.","By introducing this claim decom-
 not know what minor distinctions in questions may      position task and the dataset, we will enable the
 be important.",arguments behind the surface information.,2022-05-14 00:40:57+00:00,Generating Literal and Implied Subquestions to Fact-check Complex Claims,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jifan Chen'), arxiv.Result.Author('Aniruddh Sriram'), arxiv.Result.Author('Eunsol Choi'), arxiv.Result.Author('Greg Durrett')]","Verifying complex political claims is a challenging task, especially when
politicians use various tactics to subtly misrepresent the facts. Automatic
fact-checking systems fall short here, and their predictions like ""half-true""
are not very useful in isolation, since we have no idea which parts of the
claim are true and which are not. In this work, we focus on decomposing a
complex claim into a comprehensive set of yes-no subquestions whose answers
influence the veracity of the claim. We present ClaimDecomp, a dataset of
decompositions for over 1000 claims. Given a claim and its verification
paragraph written by fact-checkers, our trained annotators write subquestions
covering both explicit propositions of the original claim and its implicit
facets, such as asking about additional political context that changes our view
of the claim's veracity. We study whether state-of-the-art models can generate
such subquestions, showing that these models generate reasonable questions to
ask, but predicting the comprehensive set of subquestions from the original
claim without evidence remains challenging. We further show that these
subquestions can help identify relevant evidence to fact-check the full claim
and derive the veracity through their answers, suggesting that they can be
useful pieces of a fact-checking pipeline."
6240,"We observe
that COSQL task desires less CQR knowledge               To further study the effects of CQR integration
(best choice of λ2 is 1.0) compare with SPARC            for contextual text-to-SQL task, we train mod-
(best choice of λ2 is 3.0), because COSQL dataset        els in End-to-End, Two-Stage and CQR-SQL ap-
contains much more user focus change questions           proaches based on different pre-trained language
than SPARC, which do not need to be reformu-             models (PLMs), as shown in Table 8.","We vary the weight λ2 of consistency loss in {1.0,       B.4 Effects of CQR Integration with
2.0, 3.0, 4.0} and train CQR-SQL on SPARC and                   Different PLMs
COSQL datasets, as shown in Table 6.","We can see
lated (Yu et al., 2019b).",2022-05-16 13:52:42+00:00,CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Dongling Xiao'), arxiv.Result.Author('Linzheng Chai'), arxiv.Result.Author('Qian-Wen Zhang'), arxiv.Result.Author('Zhao Yan'), arxiv.Result.Author('Zhoujun Li'), arxiv.Result.Author('Yunbo Cao')]","Context-dependent text-to-SQL is the task of translating multi-turn questions
into database-related SQL queries. Existing methods typically focus on making
full use of history context or previously predicted SQL for currently SQL
parsing, while neglecting to explicitly comprehend the schema and
conversational dependency, such as co-reference, ellipsis and user focus
change. In this paper, we propose CQR-SQL, which uses auxiliary Conversational
Question Reformulation (CQR) learning to explicitly exploit schema and decouple
contextual dependency for SQL parsing. Specifically, we first present a schema
enhanced recursive CQR method to produce domain-relevant self-contained
questions. Secondly, we train CQR-SQL models to map the semantics of multi-turn
questions and auxiliary self-contained questions into the same latent space
through schema grounding consistency task and tree-structured SQL parsing
consistency task, which enhances the abilities of SQL parsing by adequately
contextual understanding. At the time of writing, our CQR-SQL achieves new
state-of-the-art results on two context-dependent text-to-SQL benchmarks SParC
and CoSQL."
6350,"We hope these experiments help motivate
                                                                                                   further research interest in parser improvement for
                                            1*: Work done during internship at Semantic Machines.",A            2020).,realistic scenarios.,2022-05-18 01:14:47+00:00,Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kevin Yang'), arxiv.Result.Author('Olivia Deng'), arxiv.Result.Author('Charles Chen'), arxiv.Result.Author('Richard Shin'), arxiv.Result.Author('Subhro Roy'), arxiv.Result.Author('Benjamin Van Durme')]","We introduce a novel setup for low-resource task-oriented semantic parsing
which incorporates several constraints that may arise in real-world scenarios:
(1) lack of similar datasets/models from a related domain, (2) inability to
sample useful logical forms directly from a grammar, and (3) privacy
requirements for unlabeled natural utterances. Our goal is to improve a
low-resource semantic parser using utterances collected through user
interactions. In this highly challenging but realistic setting, we investigate
data augmentation approaches involving generating a set of structured canonical
utterances corresponding to logical forms, before simulating corresponding
natural language and filtering the resulting pairs. We find that such
approaches are effective despite our restrictive setup: in a low-resource
setting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), we
observe 33% relative improvement over a non-data-augmented baseline in top-1
match."
6443,"To further study how biases are learnt during the
ﬁne-tuning process, we track the extrinsic bias                             Furthermore, according to the intrinsic evalua-
score of an MLM (and its debiased variants) over                         tions in Figure 1, the biases were reduced in both
the number of ﬁne-tuning training iterations on a                        roberta-l and bert-bu by all debiasing methods.","This shows that
                                                                         careful evaluation using various evaluation datasets
5.1 Re-learning Biases via Fine-Tuning                                   and MLMs is necessary when verifying the effec-
                                                                         tiveness of debiasing methods.",particular downstream task.,2022-05-19 21:20:47+00:00,Debiasing isn't enough! -- On the Effectiveness of Debiasing MLMs and their Social Biases in Downstream Tasks,cs.CL,['cs.CL'],"[arxiv.Result.Author('Masahiro Kaneko'), arxiv.Result.Author('Danushka Bollegala'), arxiv.Result.Author('Naoaki Okazaki')]","We study the relationship between task-agnostic intrinsic and task-specific
extrinsic social bias evaluation measures for Masked Language Models (MLMs),
and find that there exists only a weak correlation between these two types of
evaluation measures. Moreover, we find that MLMs debiased using different
methods still re-learn social biases during fine-tuning on downstream tasks. We
identify the social biases in both training instances as well as their assigned
labels as reasons for the discrepancy between intrinsic and extrinsic bias
evaluation measurements. Overall, our findings highlight the limitations of
existing MLM bias evaluation measures and raise concerns on the deployment of
MLMs in downstream applications using those measures."
6473,"In the end, the authors point
out the need for further research on privacy metrics aggregation concerning cases in which personal privacy is affected
by other parties.","A
procedure for choosing the most suitable privacy measures for different backdrops is also provided, followed by the
advice on choosing more than a single metric to cover a larger number of privacy aspects.",Humbert et al.,2022-05-20 11:29:44+00:00,How to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Samuel Sousa'), arxiv.Result.Author('Roman Kern')]","Deep learning (DL) models for natural language processing (NLP) tasks often
handle private data, demanding protection against breaches and disclosures.
Data protection laws, such as the European Union's General Data Protection
Regulation (GDPR), thereby enforce the need for privacy. Although many
privacy-preserving NLP methods have been proposed in recent years, no
categories to organize them have been introduced yet, making it hard to follow
the progress of the literature. To close this gap, this article systematically
reviews over sixty DL methods for privacy-preserving NLP published between 2016
and 2020, covering theoretical foundations, privacy-enhancing technologies, and
analysis of their suitability for real-world scenarios. First, we introduce a
novel taxonomy for classifying the existing methods into three categories: data
safeguarding methods, trusted methods, and verification methods. Second, we
present an extensive summary of privacy threats, datasets for applications, and
metrics for privacy evaluation. Third, throughout the review, we describe
privacy issues in the NLP pipeline in a holistic view. Further, we discuss open
challenges in privacy-preserving NLP regarding data traceability, computation
overhead, dataset size, the prevalence of human biases in embeddings, and the
privacy-utility tradeoff. Finally, this review presents future research
directions to guide successive research and development of privacy-preserving
NLP models."
6528,"Y. Chung, Y. Zhang, W. Han, C. Chiu, J. Qin, R. Pang, and Y. Wu,
[86] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,                        “W2v-BERT: Combining contrastive learning and masked language
      K. Han, and X. Li, “A further study of unsupervised pretraining                  modeling for self-supervised speech pre-training,” 2021.
      for Transformer based speech recognition,” in Proceedings of IEEE                D. Jiang, W. Li, M. Cao, W. Zou, and X. Li, “Speech SIMCLR:
      International Conference on Acoustics, Speech and Signal Processing,             Combining contrastive and reconstruction objective for self-supervised
      2021.                                                                            speech representation learning,” in Proceedings of the Annual Confer-
                                                                                       ence of the International Speech Communication Association, 2021.","S. Sadhu, D. He, C.-W. Huang, S. H. Mallidi, M. Wu, A. Rastrow,
[85] W. Wang, Q. Tang, and K. Livescu, “Unsupervised pre-training of bidi-             A. Stolcke, J. Droppo, and R. Maas, “wav2vec-C: A Self-Supervised
      rectional speech encoders via masked reconstruction,” in Proceedings             Model for Speech Representation Learning,” in Proceedings of the
      of IEEE International Conference on Acoustics, Speech and Signal                 Annual Conference of the International Speech Communication As-
      Processing, 2020.                                                                sociation, 2021.","[87] X. Yue and H. Li, “Phonetically motivated self-supervised speech                  A. Baevski, M. Auli, and A. Mohamed, “Effectiveness of self-
      representation learning,” Proceedings of the Annual Conference of the            supervised pre-training for speech recognition,” arXiv preprint
      International Speech Communication Association, 2021.                            arXiv:1911.03912, 2019.",2022-05-21 16:52:57+00:00,Self-Supervised Speech Representation Learning: A Review,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Lasse Borgholt'), arxiv.Result.Author('Jakob D. Havtorn'), arxiv.Result.Author('Joakim Edin'), arxiv.Result.Author('Christian Igel'), arxiv.Result.Author('Katrin Kirchhoff'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Karen Livescu'), arxiv.Result.Author('Lars Maaløe'), arxiv.Result.Author('Tara N. Sainath'), arxiv.Result.Author('Shinji Watanabe')]","Although supervised deep learning has revolutionized speech and audio
processing, it has necessitated the building of specialist models for
individual tasks and application scenarios. It is likewise difficult to apply
this to dialects and languages for which only limited labeled data is
available. Self-supervised representation learning methods promise a single
universal model that would benefit a wide variety of tasks and domains. Such
methods have shown success in natural language processing and computer vision
domains, achieving new levels of performance while reducing the number of
labels required for many downstream scenarios. Speech representation learning
is experiencing similar progress in three main categories: generative,
contrastive, and predictive methods. Other approaches rely on multi-modal data
for pre-training, mixing text or visual data streams with speech. Although
self-supervised speech representation is still a nascent research area, it is
closely related to acoustic word embedding and learning with zero lexical
resources, both of which have seen active research for many years. This review
presents approaches for self-supervised speech representation learning and
their connection to other research areas. Since many current methods focus
solely on automatic speech recognition as a downstream task, we review recent
efforts on benchmarking learned representations to extend the application
beyond speech recognition."
6529,"Y. Chung, Y. Zhang, W. Han, C. Chiu, J. Qin, R. Pang, and Y. Wu,
[86] D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,                        “W2v-BERT: Combining contrastive learning and masked language
      K. Han, and X. Li, “A further study of unsupervised pretraining                  modeling for self-supervised speech pre-training,” 2021.
      for Transformer based speech recognition,” in Proceedings of IEEE                D. Jiang, W. Li, M. Cao, W. Zou, and X. Li, “Speech SIMCLR:
      International Conference on Acoustics, Speech and Signal Processing,             Combining contrastive and reconstruction objective for self-supervised
      2021.                                                                            speech representation learning,” in Proceedings of the Annual Confer-
                                                                                       ence of the International Speech Communication Association, 2021.","S. Sadhu, D. He, C.-W. Huang, S. H. Mallidi, M. Wu, A. Rastrow,
[85] W. Wang, Q. Tang, and K. Livescu, “Unsupervised pre-training of bidi-             A. Stolcke, J. Droppo, and R. Maas, “wav2vec-C: A Self-Supervised
      rectional speech encoders via masked reconstruction,” in Proceedings             Model for Speech Representation Learning,” in Proceedings of the
      of IEEE International Conference on Acoustics, Speech and Signal                 Annual Conference of the International Speech Communication As-
      Processing, 2020.                                                                sociation, 2021.","[87] X. Yue and H. Li, “Phonetically motivated self-supervised speech                  A. Baevski, M. Auli, and A. Mohamed, “Effectiveness of self-
      representation learning,” Proceedings of the Annual Conference of the            supervised pre-training for speech recognition,” arXiv preprint
      International Speech Communication Association, 2021.                            arXiv:1911.03912, 2019.",2022-05-21 16:52:57+00:00,Self-Supervised Speech Representation Learning: A Review,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Lasse Borgholt'), arxiv.Result.Author('Jakob D. Havtorn'), arxiv.Result.Author('Joakim Edin'), arxiv.Result.Author('Christian Igel'), arxiv.Result.Author('Katrin Kirchhoff'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Karen Livescu'), arxiv.Result.Author('Lars Maaløe'), arxiv.Result.Author('Tara N. Sainath'), arxiv.Result.Author('Shinji Watanabe')]","Although supervised deep learning has revolutionized speech and audio
processing, it has necessitated the building of specialist models for
individual tasks and application scenarios. It is likewise difficult to apply
this to dialects and languages for which only limited labeled data is
available. Self-supervised representation learning methods promise a single
universal model that would benefit a wide variety of tasks and domains. Such
methods have shown success in natural language processing and computer vision
domains, achieving new levels of performance while reducing the number of
labels required for many downstream scenarios. Speech representation learning
is experiencing similar progress in three main categories: generative,
contrastive, and predictive methods. Other approaches rely on multi-modal data
for pre-training, mixing text or visual data streams with speech. Although
self-supervised speech representation is still a nascent research area, it is
closely related to acoustic word embedding and learning with zero lexical
resources, both of which have seen active research for many years. This review
presents approaches for self-supervised speech representation learning and
their connection to other research areas. Since many current methods focus
solely on automatic speech recognition as a downstream task, we review recent
efforts on benchmarking learned representations to extend the application
beyond speech recognition."
6530,"A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Self-supervised
       D. Jiang, W. Li, R. Zhang, M. Cao, N. Luo, Y. Han, W. Zou,                       learning of discrete speech representations,” in Proceedings of Inter-
       K. Han, and X. Li, “A further study of unsupervised pretraining                  national Conference on Learning Representations, 2020.
       for Transformer based speech recognition,” in Proceedings of IEEE                A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:
       International Conference on Acoustics, Speech and Signal Processing,             A framework for self-supervised learning of speech representations,”
       2021.","Processing, 2020.","Proceedings of Advances in Neural Information Processing Systems,
       X. Yue and H. Li, “Phonetically motivated self-supervised speech                 vol.",2022-05-21 16:52:57+00:00,Self-Supervised Speech Representation Learning: A Review,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Hung-yi Lee'), arxiv.Result.Author('Lasse Borgholt'), arxiv.Result.Author('Jakob D. Havtorn'), arxiv.Result.Author('Joakim Edin'), arxiv.Result.Author('Christian Igel'), arxiv.Result.Author('Katrin Kirchhoff'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Karen Livescu'), arxiv.Result.Author('Lars Maaløe'), arxiv.Result.Author('Tara N. Sainath'), arxiv.Result.Author('Shinji Watanabe')]","Although supervised deep learning has revolutionized speech and audio
processing, it has necessitated the building of specialist models for
individual tasks and application scenarios. It is likewise difficult to apply
this to dialects and languages for which only limited labeled data is
available. Self-supervised representation learning methods promise a single
universal model that would benefit a wide variety of tasks and domains. Such
methods have shown success in natural language processing and computer vision
domains, achieving new levels of performance while reducing the number of
labels required for many downstream scenarios. Speech representation learning
is experiencing similar progress in three main categories: generative,
contrastive, and predictive methods. Other approaches rely on multi-modal data
for pre-training, mixing text or visual data streams with speech. Although
self-supervised speech representation is still a nascent research area, it is
closely related to acoustic word embedding and learning with zero lexical
resources, both of which have seen active research for many years. This review
presents approaches for self-supervised speech representation learning and
their connection to other research areas. Since many current methods focus
solely on automatic speech recognition as a downstream task, we review recent
efforts on benchmarking learned representations to extend the application
beyond speech recognition."
6542,"We hope our ﬁndings
                                                      would encourage further research in this domain.",on multilingual models.,"Since metrics are based on the error rate, the
lower values show that the model is less biased.",2022-05-22 13:54:44+00:00,What Do Compressed Multilingual Machine Translation Models Forget?,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Alireza Mohammadshahi'), arxiv.Result.Author('Vassilina Nikoulina'), arxiv.Result.Author('Alexandre Berard'), arxiv.Result.Author('Caroline Brun'), arxiv.Result.Author('James Henderson'), arxiv.Result.Author('Laurent Besacier')]","Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the model and therefore its
inference time with negligible impact on top-tier metrics. However, the general
performance hides a drastic performance drop on under-represented features,
which could result in the amplification of biases encoded by the model. In this
work, we analyze the impacts of compression methods on Multilingual Neural
Machine Translation models (MNMT) for various language groups and semantic
features by extensive analysis of compressed models on different NMT
benchmarks, e.g. FLORES-101, MT-Gender, and DiBiMT. Our experiments show that
the performance of under-represented languages drops significantly, while the
average BLEU metric slightly decreases. Interestingly, the removal of noisy
memorization with the compression leads to a significant improvement for some
medium-resource languages. Finally, we demonstrate that the compression
amplifies intrinsic gender and semantic biases, even in high-resource
languages."
6543,Clwi (σ) is the index of     encourage further research on this topic.,"We hope our ﬁndings would
for noun shot in English.",synset (σ) in ΠL(lwi).,2022-05-22 13:54:44+00:00,What Do Compressed Multilingual Machine Translation Models Forget?,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Alireza Mohammadshahi'), arxiv.Result.Author('Vassilina Nikoulina'), arxiv.Result.Author('Alexandre Berard'), arxiv.Result.Author('Caroline Brun'), arxiv.Result.Author('James Henderson'), arxiv.Result.Author('Laurent Besacier')]","Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the models and therefore
their inference time with negligible impact on top-tier metrics. However, the
general performance averaged across multiple tasks and/or languages may hide a
drastic performance drop on under-represented features, which could result in
the amplification of biases encoded by the models. In this work, we assess the
impact of compression methods on Multilingual Neural Machine Translation models
(MNMT) for various language groups, gender, and semantic biases by extensive
analysis of compressed models on different machine translation benchmarks, i.e.
FLORES-101, MT-Gender, and DiBiMT. We show that the performance of
under-represented languages drops significantly, while the average BLEU metric
only slightly decreases. Interestingly, the removal of noisy memorization with
compression leads to a significant improvement for some medium-resource
languages. Finally, we demonstrate that compression amplifies intrinsic gender
and semantic biases, even in high-resource languages. Code:
https://github.com/alirezamshi/bias-compressedMT"
6544,encourage further research on this topic.,"We hope our ﬁndings would
synset (σ) in ΠL(lwi).","SFII is calculated as the error rate averaged          6 Conclusion
over Clwi (σ) for different positions and words
wi.",2022-05-22 13:54:44+00:00,What Do Compressed Multilingual Machine Translation Models Forget?,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Alireza Mohammadshahi'), arxiv.Result.Author('Vassilina Nikoulina'), arxiv.Result.Author('Alexandre Berard'), arxiv.Result.Author('Caroline Brun'), arxiv.Result.Author('James Henderson'), arxiv.Result.Author('Laurent Besacier')]","Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the models and therefore
their inference time with negligible impact on top-tier metrics. However, the
general performance averaged across multiple tasks and/or languages may hide a
drastic performance drop on under-represented features, which could result in
the amplification of biases encoded by the models. In this work, we assess the
impact of compression methods on Multilingual Neural Machine Translation models
(MNMT) for various language groups, gender, and semantic biases by extensive
analysis of compressed models on different machine translation benchmarks, i.e.
FLORES-101, MT-Gender, and DiBiMT. We show that the performance of
under-represented languages drops significantly, while the average BLEU metric
only slightly decreases. Interestingly, the removal of noisy memorization with
compression leads to a significant improvement for some medium-resource
languages. Finally, we demonstrate that compression amplifies intrinsic gender
and semantic biases, even in high-resource languages. Code:
https://github.com/alirezamshi/bias-compressedMT"
6555,"Moreover, human evaluation is often taken as a gold standard in evaluating natural language generation
models, but further study is needed to ﬁnd the more appropriate way to instruct the annotators.","When annotating text for
emotions, if there are not enough options, the same phenomenon of learning the “correct emotion” for face images is
likely to occur.","8 Other Aspects of Stories and Suggestions for Future Directions in this Area

In this paper, we focused on emotions, though we understand that emotions are not the only important aspects of
stories.",2022-05-23 00:21:59+00:00,Computational Storytelling and Emotions: A Survey,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yusuke Mori'), arxiv.Result.Author('Hiroaki Yamane'), arxiv.Result.Author('Yusuke Mukuta'), arxiv.Result.Author('Tatsuya Harada')]","Storytelling has always been vital for human nature. From ancient times,
humans have used stories for several objectives including entertainment,
advertisement, and education. Various analyses have been conducted by
researchers and creators to determine the way of producing good stories. The
deep relationship between stories and emotions is a prime example. With the
advancement in deep learning technology, computers are expected to understand
and generate stories. This survey paper is intended to summarize and further
contribute to the development of research being conducted on the relationship
between stories and emotions. We believe creativity research is not to replace
humans with computers, but to find a way of collaboration between humans and
computers to enhance the creativity. With the intention of creating a new
intersection between computational storytelling research and human creative
writing, we introduced creative techniques used by professional storytellers."
6579,"Identification                   Detection
                                             of multiple psychiatric disorders, to facilitate
                                             further research progress.","This paper introduces PsySym, the        problem with    Symptom                         Disease        OCD
                                             ﬁrst annotated symptom identiﬁcation corpus        hoarding ….","PsySym is anno-                       Diagnostic Criteria for Obsessive-Compulsive Disorder (OCD):
                                             tated according to a knowledge graph of the
                                             38 symptom classes related to 7 mental dis-                      ✔ Obsession                      ✔ Compulsions
                                             eases complied from established clinical man-                         • Hoarding                       • ……
                                             uals and scales, and a novel annotation frame-                        • contamination obsessions
                                             work for diversity and quality.",2022-05-23 13:51:48+00:00,Symptom Identification for Interpretable Detection of Multiple Mental Disorders,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zhiling Zhang'), arxiv.Result.Author('Siyuan Chen'), arxiv.Result.Author('Mengyue Wu'), arxiv.Result.Author('Kenny Q. Zhu')]","Mental disease detection (MDD) from social media has suffered from poor
generalizability and interpretability, due to lack of symptom modeling. This
paper introduces PsySym, the first annotated symptom identification corpus of
multiple psychiatric disorders, to facilitate further research progress. PsySym
is annotated according to a knowledge graph of the 38 symptom classes related
to 7 mental diseases complied from established clinical manuals and scales, and
a novel annotation framework for diversity and quality. Experiments show that
symptom-assisted MDD enabled by PsySym can outperform strong pure-text
baselines. We also exhibit the convincing MDD explanations provided by symptom
predictions with case studies, and point to their further potential
applications."
6589,"3.2).6
tion of gender bias but encourage further research      For our realism metric, we then use this model’s
to empirically measure allocational harm, so long       predicted probability that an ad is real.","In this work, we      validate and test using an 80:10:10 split taken from
focus on gendered word lists as one overt presenta-     the real and generated ads (described in Sec.","To assess
as any experiments consider the ethical issues of       the robustness of this metric, we randomly sam-
posting “fake” ads online.",2022-05-23 15:05:27+00:00,Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Conrad Borchers'), arxiv.Result.Author('Dalia Sara Gala'), arxiv.Result.Author('Benjamin Gilburt'), arxiv.Result.Author('Eduard Oravkin'), arxiv.Result.Author('Wilfried Bounsi'), arxiv.Result.Author('Yuki M. Asano'), arxiv.Result.Author('Hannah Rose Kirk')]","The growing capability and availability of generative language models has
enabled a wide range of new downstream tasks. Academic research has identified,
quantified and mitigated biases present in language models but is rarely
tailored to downstream tasks where wider impact on individuals and society can
be felt. In this work, we leverage one popular generative language model,
GPT-3, with the goal of writing unbiased and realistic job advertisements. We
first assess the bias and realism of zero-shot generated advertisements and
compare them to real-world advertisements. We then evaluate prompt-engineering
and fine-tuning as debiasing methods. We find that prompt-engineering with
diversity-encouraging prompts gives no significant improvement to bias, nor
realism. Conversely, fine-tuning, especially on unbiased real advertisements,
can improve realism and reduce bias."
6590,"Our assump-
                                                                      tion for this project is that human-written job ads
5.1 Limitations and Future Work                                       follow styles, conventions and a level of detail that
                                                                      effectively encourage prospective employees to ap-
Measurements Our measures of bias and real-                           ply, but further research is required to understand
ism are relatively simplistic.",appear human-written to achieve this.,"On bias, using lists                   whether ads clearly identiﬁed as machine-written
of gender words is a blunt tool and may in fact                       can be equally or more effective in this regard.",2022-05-23 15:05:27+00:00,Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Conrad Borchers'), arxiv.Result.Author('Dalia Sara Gala'), arxiv.Result.Author('Benjamin Gilburt'), arxiv.Result.Author('Eduard Oravkin'), arxiv.Result.Author('Wilfried Bounsi'), arxiv.Result.Author('Yuki M. Asano'), arxiv.Result.Author('Hannah Rose Kirk')]","The growing capability and availability of generative language models has
enabled a wide range of new downstream tasks. Academic research has identified,
quantified and mitigated biases present in language models but is rarely
tailored to downstream tasks where wider impact on individuals and society can
be felt. In this work, we leverage one popular generative language model,
GPT-3, with the goal of writing unbiased and realistic job advertisements. We
first assess the bias and realism of zero-shot generated advertisements and
compare them to real-world advertisements. We then evaluate prompt-engineering
and fine-tuning as debiasing methods. We find that prompt-engineering with
diversity-encouraging prompts gives no significant improvement to bias, nor
realism. Conversely, fine-tuning, especially on unbiased real advertisements,
can improve realism and reduce bias."
6592,"Through manual annotation of the questions with
                                                                  the highest toxicity scores, we have determined thresholds
Future work StreamingQA highlights challenges of tem-             for removing questions as follows: for each 0.05 band of the
poral reasoning and invites further research into this area:      scores from 1 to 0 (e.g., [1.0, 0.95], [0.95, 0.90], .",norities).,.,2022-05-23 15:33:41+00:00,StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Adam Liška'), arxiv.Result.Author('Tomáš Kočiský'), arxiv.Result.Author('Elena Gribovskaya'), arxiv.Result.Author('Tayfun Terzi'), arxiv.Result.Author('Eren Sezener'), arxiv.Result.Author('Devang Agrawal'), arxiv.Result.Author(""Cyprien de Masson d'Autume""), arxiv.Result.Author('Tim Scholtes'), arxiv.Result.Author('Manzil Zaheer'), arxiv.Result.Author('Susannah Young'), arxiv.Result.Author('Ellen Gilsenan-McMahon'), arxiv.Result.Author('Sophia Austin'), arxiv.Result.Author('Phil Blunsom'), arxiv.Result.Author('Angeliki Lazaridou')]","Knowledge and language understanding of models evaluated through question
answering (QA) has been usually studied on static snapshots of knowledge, like
Wikipedia. However, our world is dynamic, evolves over time, and our models'
knowledge becomes outdated. To study how semi-parametric QA models and their
underlying parametric language models (LMs) adapt to evolving knowledge, we
construct a new large-scale dataset, StreamingQA, with human written and
generated questions asked on a given date, to be answered from 14 years of
time-stamped news articles. We evaluate our models quarterly as they read new
articles not seen in pre-training. We show that parametric models can be
updated without full retraining, while avoiding catastrophic forgetting. For
semi-parametric models, adding new articles into the search space allows for
rapid adaptation, however, models with an outdated underlying LM under-perform
those with a retrained LM. For questions about higher-frequency named entities,
parametric updates are particularly beneficial. In our dynamic world, the
StreamingQA dataset enables a more realistic evaluation of QA models, and our
experiments highlight several promising directions for future research."
6595,"Napoles et al., 2019; Flachs et al., 2020) and eval-
                                                        uation metrics (Dahlmeier and Ng, 2012; Felice
   We hope that TETRA and IRC encourage the com-
munity to further study automated document revi-            2In this paper, we focus on GEC research literature after
sion modeling and metrics beyond GEC and ﬂu-            2000’s when statistical methods began to be applied widely.","Along with this expan-
vised method is able to choose better snippets with     sion, the community has proposed new benchmark
an accuracy of 0.85 - 0.96, indicating the feasibility  datasets (Napoles et al., 2017; Bryant et al., 2019;
of evaluation for automated document revision.",ency edits.,2022-05-23 17:37:20+00:00,"Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",cs.CL,['cs.CL'],"[arxiv.Result.Author('Masato Mita'), arxiv.Result.Author('Keisuke Sakaguchi'), arxiv.Result.Author('Masato Hagiwara'), arxiv.Result.Author('Tomoya Mizumoto'), arxiv.Result.Author('Jun Suzuki'), arxiv.Result.Author('Kentaro Inui')]","Natural language processing technology has rapidly improved automated
grammatical error correction tasks, and the community begins to explore
document-level revision as one of the next challenges. To go beyond
sentence-level automated grammatical error correction to NLP-based
document-level revision assistant, there are two major obstacles: (1) there are
few public corpora with document-level revisions being annotated by
professional editors, and (2) it is not feasible to elicit all possible
references and evaluate the quality of revision with such references because
there are infinite possibilities of revision. This paper tackles these
challenges. First, we introduce a new document-revision corpus, TETRA, where
professional editors revised academic papers sampled from the ACL anthology
which contain few trivial grammatical errors that enable us to focus more on
document- and paragraph-level edits such as coherence and consistency. Second,
we explore reference-less and interpretable methods for meta-evaluation that
can detect quality improvements by document revision. We show the uniqueness of
TETRA compared with existing document revision corpora and demonstrate that a
fine-tuned pre-trained language model can discriminate the quality of documents
after revision even when the difference is subtle. This promising result will
encourage the community to further explore automated document revision models
and metrics in future."
6596,"Pear-
community to further study automated document                son Education Canada.",4.,"revision models and metrics beyond sentence-level
error corrections.",2022-05-23 17:37:20+00:00,"Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",cs.CL,['cs.CL'],"[arxiv.Result.Author('Masato Mita'), arxiv.Result.Author('Keisuke Sakaguchi'), arxiv.Result.Author('Masato Hagiwara'), arxiv.Result.Author('Tomoya Mizumoto'), arxiv.Result.Author('Jun Suzuki'), arxiv.Result.Author('Kentaro Inui')]","Natural language processing technology has rapidly improved automated
grammatical error correction tasks, and the community begins to explore
document-level revision as one of the next challenges. To go beyond
sentence-level automated grammatical error correction to NLP-based
document-level revision assistant, there are two major obstacles: (1) there are
few public corpora with document-level revisions being annotated by
professional editors, and (2) it is not feasible to elicit all possible
references and evaluate the quality of revision with such references because
there are infinite possibilities of revision. This paper tackles these
challenges. First, we introduce a new document-revision corpus, TETRA, where
professional editors revised academic papers sampled from the ACL anthology
which contain few trivial grammatical errors that enable us to focus more on
document- and paragraph-level edits such as coherence and consistency. Second,
we explore reference-less and interpretable methods for meta-evaluation that
can detect quality improvements by document revision. We show the uniqueness of
TETRA compared with existing document revision corpora and demonstrate that a
fine-tuned pre-trained language model can discriminate the quality of documents
after revision even when the difference is subtle. This promising result will
encourage the community to further explore automated document revision models
and metrics in future."
6598,"(v) In order to encourage and           Differently from these two families, one recent
facilitate further research in the area, we establish      work (Reif et al., 2022) uses enormous pre-trained
a set of benchmarks for arbitrary TST (including           language models to tackle TST, an idea motivated
cleaned versions of the popular sentiment transfer         by the remarkable performance of pre-trained LMs
datasets AMAZON and YELP) along with accom-                in other areas of NLP (Radford et al., 2019; Devlin
panying automatic evaluation metrics.","and delimiter-pair choice, on the quality of style
transfer generations.","et al., 2019; Yang et al., 2019; Liu et al., 2019).",2022-05-23 17:57:15+00:00,Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mirac Suzgun'), arxiv.Result.Author('Luke Melas-Kyriazi'), arxiv.Result.Author('Dan Jurafsky')]","We propose a method for arbitrary textual style transfer (TST)--the task of
transforming a text into any given style--utilizing general-purpose pre-trained
language models. Our method, Prompt-and-Rerank, is based on a mathematical
formulation of the TST task, decomposing it into three constituent components:
textual similarity, target style strength, and fluency. Specifically, our
method first uses zero-shot or few-shot prompting to obtain a set of candidate
generations in the target style, and then re-ranks these candidates according
to a combination of the three components above. Empirically, our method enables
small pre-trained language models to perform on par with state-of-the-art
large-scale models while consuming two orders of magnitude less compute and
memory. Finally, we conduct a systematic investigation of the effect of model
size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on
style transfer quality across seven diverse textual style transfer datasets."
6607,"Low-
                                                      resource language sentences in the target side may
                                                      be seen for the model as target preﬁx perturbations
                                                      (§5.4), although further research is required.",less prone to enter in hallucination mode.,"6 Conclusions                                              Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and
                                                              Qun Liu.",2022-05-23 20:59:14+00:00,Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer,cs.CL,['cs.CL'],"[arxiv.Result.Author('Javier Ferrando'), arxiv.Result.Author('Gerard I. Gállego'), arxiv.Result.Author('Belen Alastruey'), arxiv.Result.Author('Carlos Escolano'), arxiv.Result.Author('Marta R. Costa-jussà')]","In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
focused solely on source sentence tokens attributions. Therefore, we lack a
full understanding of the influences of every input token (source sentence and
target prefix) in the model predictions. In this work, we propose an
interpretability method that tracks complete input token attributions. Our
method, which can be extended to any encoder-decoder Transformer-based model,
allows us to better comprehend the inner workings of current NMT models. We
apply the proposed method to both bilingual and multilingual Transformers and
present insights into their behaviour."
6608,"The only drop in its contribution seems to   (§5.4), although further research is required.","We observe an almost uniform         resource language sentences in the target side may
contribution of the language tags across different    be seen by the model as target preﬁx perturbations
outputs.","happen when translating proper nouns (e.g., ""Mr.
Williams"") or anglicisms (e.g., ""hobby""), which is    6 Conclusions
observed for other language pairs too (Appendix C),
and repeated across the dataset.",2022-05-23 20:59:14+00:00,Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer,cs.CL,['cs.CL'],"[arxiv.Result.Author('Javier Ferrando'), arxiv.Result.Author('Gerard I. Gállego'), arxiv.Result.Author('Belen Alastruey'), arxiv.Result.Author('Carlos Escolano'), arxiv.Result.Author('Marta R. Costa-jussà')]","In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
mainly focused solely on source sentence tokens' attributions. Therefore, we
lack a full understanding of the influences of every input token (source
sentence and target prefix) in the model predictions. In this work, we propose
an interpretability method that tracks input tokens' attributions for both
contexts. Our method, which can be extended to any encoder-decoder
Transformer-based model, allows us to better comprehend the inner workings of
current NMT models. We apply the proposed method to both bilingual and
multilingual Transformers and present insights into their behaviour."
6623,"aging its openness and generality to help a model understands
To facilitate further research on this topic, the authors con-    what entities, relations, and facts are.","In our vision, OpenIE will
2021] explores using document-level context to solve syn-         become a basic pre-training objective for universal IE, lever-
tactic ambiguities when extracting facts at sentence-level.",tribute an OpenIE dataset with document-level context.,2022-05-24 02:24:55+00:00,A Survey on Neural Open Information Extraction: Current Status and Future Directions,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shaowen Zhou'), arxiv.Result.Author('Bowen Yu'), arxiv.Result.Author('Aixin Sun'), arxiv.Result.Author('Cheng Long'), arxiv.Result.Author('Jingyang Li'), arxiv.Result.Author('Jian Sun')]","Open Information Extraction (OpenIE) facilitates domain-independent discovery
of relational facts from large corpora. The technique well suits many
open-world natural language understanding scenarios, such as automatic
knowledge base construction, open-domain question answering, and explicit
reasoning. Thanks to the rapid development in deep learning technologies,
numerous neural OpenIE architectures have been proposed and achieve
considerable performance improvement. In this survey, we provide an extensive
overview of the-state-of-the-art neural OpenIE models, their key design
decisions, strengths and weakness. Then, we discuss limitations of current
solutions and the open issues in OpenIE problem itself. Finally we list recent
trends that could help expand its scope and applicability, setting up promising
directions for future research in OpenIE. To our best knowledge, this paper is
the first review on this specific topic."
6624,"aging its openness and generality to help a model understands
To facilitate further research on this topic, the authors con-    what entities, relations, and facts are.","In our vision, OpenIE will
2021] explores using document-level context to solve syn-         become a basic pre-training objective for universal IE, lever-
tactic ambiguities when extracting facts at sentence-level.",tribute an OpenIE dataset with document-level context.,2022-05-24 02:24:55+00:00,A Survey on Neural Open Information Extraction: Current Status and Future Directions,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shaowen Zhou'), arxiv.Result.Author('Bowen Yu'), arxiv.Result.Author('Aixin Sun'), arxiv.Result.Author('Cheng Long'), arxiv.Result.Author('Jingyang Li'), arxiv.Result.Author('Haiyang Yu'), arxiv.Result.Author('Jian Sun'), arxiv.Result.Author('Yongbin Li')]","Open Information Extraction (OpenIE) facilitates domain-independent discovery
of relational facts from large corpora. The technique well suits many
open-world natural language understanding scenarios, such as automatic
knowledge base construction, open-domain question answering, and explicit
reasoning. Thanks to the rapid development in deep learning technologies,
numerous neural OpenIE architectures have been proposed and achieve
considerable performance improvement. In this survey, we provide an extensive
overview of the-state-of-the-art neural OpenIE models, their key design
decisions, strengths and weakness. Then, we discuss limitations of current
solutions and the open issues in OpenIE problem itself. Finally we list recent
trends that could help expand its scope and applicability, setting up promising
directions for future research in OpenIE. To our best knowledge, this paper is
the first review on this specific topic."
6638,"In this paper, we present a new task of CQAEL and create               [Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
a data set QuoraEL to foster further study.","In ACL,
6 Conclusion                                                              pages 1595–1604, 2018.","We propose a                  jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin
novel transformer-based framework which can leverage dif-                 Stoyanov, and Luke Zettlemoyer.",2022-05-24 09:25:18+00:00,Community Question Answering Entity Linking via Leveraging Auxiliary Data,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yuhan Li'), arxiv.Result.Author('Wei Shen'), arxiv.Result.Author('Jianbo Gao'), arxiv.Result.Author('Yadong Wang')]","Community Question Answering (CQA) platforms contain plenty of CQA texts
(i.e., questions and answers corresponding to the question) where named
entities appear ubiquitously. In this paper, we define a new task of CQA entity
linking (CQAEL) as linking the textual entity mentions detected from CQA texts
with their corresponding entities in a knowledge base. This task can facilitate
many downstream applications including expert finding and knowledge base
enrichment. Traditional entity linking methods mainly focus on linking entities
in news documents, and are suboptimal over this new task of CQAEL since they
cannot effectively leverage various informative auxiliary data involved in the
CQA platform to aid entity linking, such as parallel answers and two types of
meta-data (i.e., topic tags and users). To remedy this crucial issue, we
propose a novel transformer-based framework to effectively harness the
knowledge delivered by different kinds of auxiliary data to promote the linking
performance. We validate the superiority of our framework through extensive
experiments over a newly released CQAEL data set against state-of-the-art
entity linking methods."
6659,"raises an interesting question for further research
                                                                on how to improve both aspects simultaneously.","This
                   tion was herald?","s2 As of that day, the new constitution heralding    Second, overﬁtting harms expressiveness.",2022-05-24 16:41:30+00:00,Learning for Expressive Task-Related Sentence Representations,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Xueying Bai'), arxiv.Result.Author('Jinghuan Shang'), arxiv.Result.Author('Yifan Sun'), arxiv.Result.Author('Niranjan Balasubramanian')]","NLP models learn sentence representations for downstream tasks by tuning a
model which is pre-trained by masked language modeling. However, after tuning,
the learned sentence representations may be skewed heavily toward label space
and thus are not expressive enough to represent whole samples, which should
contain task-related information of both sentence inputs and labels. In this
work, we learn expressive sentence representations for supervised tasks which
(1). contain task-related information in the sentence inputs, and (2). enable
correct label predictions. To achieve this goal, we first propose a new
objective which explicitly points out the label token space in the input, and
predicts categories of labels via an added [MASK] token. This objective
encourages fusing the semantic information of both the label and sentence. Then
we develop a neighbor attention module, added on a frozen pre-trained model, to
build connections between label/sentence tokens via their neighbors. The
propagation can be further guided by the regularization on neighborhood
representations to encourage expressiveness. Experimental results show that,
despite tuning only 5% additional parameters over a frozen pre-trained model,
our model can achieve classification results comparable to the SOTA while
maintaining strong expressiveness as well."
6661,"We intend to further research automatic age
 1000                                                     estimation which so far has exclusively been performed
  500                                                     on multi-speaker databases.","This indicates that the
00   5  10 15 20 25                                       learnt embeddings – although they are supposed to be
        Audio Duration (in sec)                           speaker-speciﬁc – change with age of the speaker and/or
                                                          recording.",00.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14              4.2.,2022-05-24 16:48:07+00:00,Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel's Weekly Video Podcasts,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Debjoy Saha'), arxiv.Result.Author('Shravan Nayak'), arxiv.Result.Author('Timo Baumann')]","We introduce the Merkel Podcast Corpus, an audio-visual-text corpus in German
collected from 16 years of (almost) weekly Internet podcasts of former German
chancellor Angela Merkel. To the best of our knowledge, this is the first
single speaker corpus in the German language consisting of audio, visual and
text modalities of comparable size and temporal extent. We describe the methods
used with which we have collected and edited the data which involves
downloading the videos, transcripts and other metadata, forced alignment,
performing active speaker recognition and face detection to finally curate the
single speaker dataset consisting of utterances spoken by Angela Merkel. The
proposed pipeline is general and can be used to curate other datasets of
similar nature, such as talk show contents. Through various statistical
analyses and applications of the dataset in talking face generation and TTS, we
show the utility of the dataset. We argue that it is a valuable contribution to
the research community, in particular, due to its realistic and challenging
material at the boundary between prepared and spontaneous speech."
6665,"Pre-trained on a     further study the best language to probe the knowl-
collection of multilingual corpora, mPLMs may         edge about a particular country.","We
resented in its native language.","Surprisingly, we
accumulate the geo-speciﬁc knowledge and seem-        ﬁnd that the best language is not the native lan-
ingly become geo-diverse.",2022-05-24 17:54:50+00:00,GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Da Yin'), arxiv.Result.Author('Hritik Bansal'), arxiv.Result.Author('Masoud Monajatipoor'), arxiv.Result.Author('Liunian Harold Li'), arxiv.Result.Author('Kai-Wei Chang')]","Recent work has shown that Pre-trained Language Models (PLMs) have the
ability to store the relational knowledge from pre-training data in their model
parameters. However, it is not clear up to what extent do PLMs store
geo-diverse commonsense knowledge, the knowledge associated with a culture and
only shared locally. For instance, the color of bridal dress is white in
American weddings whereas it is red in Chinese weddings. Here, we wish to probe
if PLMs can predict red and white as the color of the bridal dress when queried
for American and Chinese weddings, respectively. To this end, we introduce a
framework for geo-diverse commonsense probing on multilingual PLMs (mPLMs) and
introduce a corresponding benchmark Geo-diverse Commonsense Multilingual
Language Model Analysis (GeoMLAMA) dataset. GeoMLAMA contains 3125 prompts in
English, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts
shared by people from American, Chinese, Indian, Iranian and Kenyan cultures.
We benchmark 11 standard mPLMs which include variants of mBERT, XLM, mT5, and
XGLM on GeoMLAMA. Interestingly, we find that 1) larger mPLM variants do not
necessarily store geo-diverse concepts better than its smaller variant; 2)
mPLMs are not intrinsically biased towards knowledge from the Western countries
(the United States); 3) the native language of a country may not be the best
language to probe its knowledge and 4) a language may better probe knowledge
about a non-native country than its native country."
6666,"We
in different regions may speak different languages,      further study the best language to probe the knowl-
and it is natural to assume that geo-speciﬁc knowl-      edge about a particular country.",People     have the best performance on our benchmark.,"Surprisingly, we
edge is better represented in its native language.",2022-05-24 17:54:50+00:00,GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Da Yin'), arxiv.Result.Author('Hritik Bansal'), arxiv.Result.Author('Masoud Monajatipoor'), arxiv.Result.Author('Liunian Harold Li'), arxiv.Result.Author('Kai-Wei Chang')]","Recent work has shown that Pre-trained Language Models (PLMs) store the
relational knowledge learned from data and utilize it for performing downstream
tasks. However, commonsense knowledge across different regions may vary. For
instance, the color of bridal dress is white in American weddings whereas it is
red in Chinese weddings. In this paper, we introduce a benchmark dataset,
Geo-Diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for
probing the diversity of the relational knowledge in multilingual PLMs.
GeoMLAMA contains 3,125 prompts in English, Chinese, Hindi, Persian, and
Swahili, with a wide coverage of concepts shared by people from American,
Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard
multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger
multilingual PLMs variants do not necessarily store geo-diverse concepts better
than its smaller variant; 2) multilingual PLMs are not intrinsically biased
towards knowledge from the Western countries (the United States); 3) the native
language of a country may not be the best language to probe its knowledge and
4) a language may better probe knowledge about a non-native country than its
native country. Code and data are released at
https://github.com/WadeYin9712/GeoMLAMA."
6667,"30                                 We hope that this work enables further research
                                                       into tool augmented language models, a promising
                       25                              direction to enhance model capabilities with less
                                                       dependency on scale than many contemporary ap-
                       20       Pa7r7a0mMeters  3000M  proaches.","We con-
                       40                       TALM   clude that the combination of tool augmentation
                                                       and iterative self-play enables smaller models to
                       35                       LM     outperform larger non-augmented LMs.","220M
                                                       References
Figure 7: Performance of TALM compared with LM of
different model sizes on MathQA.",2022-05-24 17:58:13+00:00,TALM: Tool Augmented Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Aaron Parisi'), arxiv.Result.Author('Yao Zhao'), arxiv.Result.Author('Noah Fiedel')]","Transformer based language models (LMs) demonstrate increasing performance
with scale across a wide variety of tasks. Scale alone however cannot enable
models to solve tasks that require access to ephemeral, changing, or private
data that was unavailable at training time. Many useful tasks may also benefit
from LMs being able to access APIs that read or modify state. In this work, we
present Tool Augmented Language Models (TALM), combining a text-only approach
to augment language models with non-differentiable tools, and an iterative
""self-play"" technique to bootstrap performance starting from few tool
demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA
task and a reasoning oriented math task with simple tools. At a given model
scale, TALM significantly outperforms non-augmented LMs. We further demonstrate
that TALM successfully performs out-of-distribution inferences on both QA and
math tasks, where non-augmented LMs fail. Our results suggest that Tool
Augmented Language Models are a promising direction to enrich LMs'
capabilities, with less dependence on scale."
6668,"“Q0” and “not Q0” or trees                                   for further research, one would be to construct more
which only include one type of operator, e.g.","There are a number of potential avenues
question trees, i.e.","“Q0                                 expressive trees for policy summarisation, e.g.",2022-05-24 17:59:31+00:00,Policy Compliance Detection via Expression Tree Inference,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Neema Kotonya'), arxiv.Result.Author('Andreas Vlachos'), arxiv.Result.Author('Majid Yazdani'), arxiv.Result.Author('Lambert Mathias'), arxiv.Result.Author('Marzieh Saeidi')]","Policy Compliance Detection (PCD) is a task we encounter when reasoning over
texts, e.g. legal frameworks. Previous work to address PCD relies heavily on
modeling the task as a special case of Recognizing Textual Entailment.
Entailment is applicable to the problem of PCD, however viewing the policy as a
single proposition, as opposed to multiple interlinked propositions, yields
poor performance and lacks explainability. To address this challenge, more
recent proposals for PCD have argued for decomposing policies into expression
trees consisting of questions connected with logic operators. Question
answering is used to obtain answers to these questions with respect to a
scenario. Finally, the expression tree is evaluated in order to arrive at an
overall solution. However, this work assumes expression trees are provided by
experts, thus limiting its applicability to new policies. In this work, we
learn how to infer expression trees automatically from policy texts. We ensure
the validity of the inferred trees by introducing constrained decoding using a
finite state automaton to ensure the generation of valid trees. We determine
through automatic evaluation that 63% of the expression trees generated by our
constrained generation model are logically equivalent to gold trees. Human
evaluation shows that 88% of trees generated by our model are correct."
6671,"Studies exploring GPT, how-          properties such as garden path effects by ex-
                                        ever, tend to focus on its external behavior alone,
      amining geometric relationships between vec-      biguating token, from which they count the per-
      tors in GPT’s hidden states such as Manhattan     centage of completions that follow either possible
      distance and cosine similarity                    parse to show that both an LSTM model and GPT-2
                                                        implicitly consider both possible parses, but ulti-
   • to motivate further study of the hidden states     mately prefer the same parse that human readers
      GPT and other decoder models maintain as          do.","Within           • to introduce a robust and diverse dataset of
                                        2 years of BERT’s release, over 150 studies on              garden path sentences, along with construc-
                                        had investigated BERT’s structure, exploring how            tion functions to negate or extend the effect
                                        its internal representations enable its powerful and        within each sentence
                                        ﬂexible language comprehension (Coenen et al.,
                                        2019) (Kovaleva et al., 2019) (Tenney et al., 2019)      • to provide methods of analyzing syntactic
                                        (Rogers et al., 2020).","Their innovative method allows for analysis
      a more thorough alternative to the surprisal-     without requiring a mirror sentence with the garden
      based methods that is typically used to analyze   path effect negated, but still relies on the output
      language models.",2022-05-24 18:21:58+00:00,Garden-Path Traversal within GPT-2,cs.CL,['cs.CL'],"[arxiv.Result.Author('William Jurayj'), arxiv.Result.Author('William Rudman'), arxiv.Result.Author('Carsten Eickhoff')]","In recent years, massive language models consisting exclusively of
transformer decoders, led by the GPT-x family, have become increasingly
popular. While studies have examined the behavior of these models, they tend to
only focus on the output of the language model, avoiding analyzing their
internal states despite such analyses being popular tools used within BERTology
to study transformer encoders. We present a collection of methods for analyzing
GPT-2's hidden states, and use the model's navigation of garden path sentences
as a case study to demonstrate the utility of studying this model's behavior
beyond its output alone. To support this analysis, we introduce a novel dataset
consisting of 3 different types of garden path sentences, along with scripts to
manipulate them. We find that measuring Manhattan distances and cosine
similarities between hidden states shows that GPT-2 navigates these sentences
more intuitively than conventional methods that predict from the model's output
alone."
6672,"Moreover, we expect that Manhattan             • to motivate further study of the hidden states
distances will exhibit less variance in the effect of        of transformer decoders as a more thorough
negating a given sentence type than either surprisals        alternative to the surprisal-based methods that
or cosine similarities after a zero-mean transforma-         are typically used to analyze language mod-
tion, because Manhattan distances are resilient to           els.1
extreme values in a single dimension (Aggarwal
et al., 2001).","tence,

   We expect that by looking at the hidden states         • to demonstrate the advantage of analyzing syn-
from which next word likelihoods are computed,               tactic properties such as garden path effects by
we can observe the same patterns that surprisal              examining geometric relationships between
analysis reveals, while uncovering more nuanced              vectors in GPT-2’s hidden states using Man-
trends that surprisal misses because it depends on           hattan distance and cosine similarity,
the joint distribution of the hidden state and the
next word.","On the other hand, the next word like-  1.1 Related Work
lihoods used to compute surprisal tend to depend
heavily on these dimensions, while the zero-mean       Many studies into GPT or BERT involve ﬁne-
translation required to create meaningful angular      grained analyses of how the model handles spe-
                                                       ciﬁc syntactic phenomena, such as the garden path

                                                           1Code available at https://github.com/wjurayj/garden-path-
                                                       gpt2
Sentence Type  Sentence Form                                    Sentence
   NP/Z
   NP/S          Garden Path          When the dog scratched the vet took off the muzzle.",2022-05-24 18:21:58+00:00,Garden-Path Traversal in GPT-2,cs.CL,['cs.CL'],"[arxiv.Result.Author('William Jurayj'), arxiv.Result.Author('William Rudman'), arxiv.Result.Author('Carsten Eickhoff')]","In recent years, large-scale transformer decoders such as the GPT-x family of
models have become increasingly popular. Studies examining the behavior of
these models tend to focus only on the output of the language modeling head and
avoid analysis of the internal states of the transformer decoder. In this
study, we present a collection of methods to analyze the hidden states of GPT-2
and use the model's navigation of garden path sentences as a case study. To
enable this, we compile the largest currently available dataset of garden path
sentences. We show that Manhattan distances and cosine similarities provide
more reliable insights compared to established surprisal methods that analyze
next-token probabilities computed by a language modeling head. Using these
methods, we find that negating tokens have minimal impacts on the model's
representations for unambiguous forms of sentences with ambiguity solely over
what the object of a verb is, but have a more substantial impact of
representations for unambiguous sentences whose ambiguity would stem from the
voice of a verb. Further, we find that analyzing the decoder model's hidden
states reveals periods of ambiguity that might conclude in a garden path effect
but happen not to, whereas surprisal analyses routinely miss this detail."
6681,"Initially, we focused on shopping data, but we
                                                                intend to extend our work to other verticals to fa-
                                                                cilitate further research in model generalization
                                                                and domain adaptation.","4Some sites did not have 5 errors to sample, in which case
we examined all errors.","We would also like to in-
                                                                vestigate how PLAtE performs with other neural
                                                                and non-neural models.",2022-05-24 22:26:58+00:00,PLAtE: A Large-scale Dataset for List Page Web Extraction,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Aidan San'), arxiv.Result.Author('Jan Bakus'), arxiv.Result.Author('Colin Lockard'), arxiv.Result.Author('David Ciemiewicz'), arxiv.Result.Author('Yangfeng Ji'), arxiv.Result.Author('Sandeep Atluri'), arxiv.Result.Author('Kevin Small'), arxiv.Result.Author('Heba Elfardy')]","Recently, neural models have been leveraged to significantly improve the
performance of information extraction from semi-structured websites. However, a
barrier for continued progress is the small number of datasets large enough to
train these models. In this work, we introduce the PLAtE (Pages of Lists
Attribute Extraction) dataset as a challenging new web extraction task. PLAtE
focuses on shopping data, specifically extractions from product review pages
with multiple items. PLAtE encompasses both the tasks of: (1) finding
product-list segmentation boundaries and (2) extracting attributes for each
product. PLAtE is composed of 53, 905 items from 6, 810 pages, making it the
first large-scale list page web extraction dataset. We construct PLAtE by
collecting list pages from Common Crawl, then annotating them on Mechanical
Turk. Quantitative and qualitative analyses are performed to demonstrate PLAtE
has high-quality annotations. We establish strong baseline performance on PLAtE
with a SOTA model achieving an F1-score of 0.750 for attribute classification
and 0.915 for segmentation, indicating opportunities for future research
innovations in web extraction."
6684,"To further study the effect of bottleneck dimension,
we conduct experiments by varying the dimension size of adapters from 8 to 64 in AdaMix with
BERT-base encoder, and to 32 with RoBERTa-large encoder.",Impact of adapter bottleneck dimension size.,"Table 7 shows that the model perfor-
mance improves as we increase the number of trainable parameters by increasing the bottleneck
dimension with diminishing returns after a certain point.",2022-05-24 23:41:22+00:00,AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yaqing Wang'), arxiv.Result.Author('Subhabrata Mukherjee'), arxiv.Result.Author('Xiaodong Liu'), arxiv.Result.Author('Jing Gao'), arxiv.Result.Author('Ahmed Hassan Awadallah'), arxiv.Result.Author('Jianfeng Gao')]","Fine-tuning large-scale pre-trained language models to downstream tasks
require updating hundreds of millions of parameters. This not only increases
the serving cost to store a large copy of the model weights for every task, but
also exhibits instability during few-shot task adaptation. Parameter-efficient
techniques have been developed that tune small trainable components (e.g.,
adapters) injected in the large model while keeping most of the model weights
frozen. The prevalent mechanism to increase adapter capacity is to increase the
bottleneck dimension which increases the adapter parameters. In this work, we
introduce a new mechanism to improve adapter capacity without increasing
parameters or computational cost by two key techniques. (i) We introduce
multiple shared adapter components in each layer of the Transformer
architecture. We leverage sparse learning via random routing to update the
adapter parameters (encoder is kept frozen) resulting in the same amount of
computational cost (FLOPs) as that of training a single adapter. (ii) We
propose a simple merging mechanism to average the weights of multiple adapter
components to collapse to a single adapter in each Transformer layer, thereby,
keeping the overall parameters also the same but with significant performance
improvement. We demonstrate these techniques to work well across multiple task
settings including fully supervised and few-shot Natural Language Understanding
tasks. By only tuning 0.23% of a pre-trained language model's parameters, our
model outperforms the full model fine-tuning performance and several competing
methods."
6687,"Considering both effectiveness and efﬁ-           hope that our work stimulates further research in
ciency, MetaPT is able to achieve promising re-           how to leverage prompts to solve NLP tasks with
sults when k=10.","We
verges.",We also visualize the result of K-       pre-trained language models.,2022-05-25 03:50:23+00:00,Learning a Better Initialization for Soft Prompts via Meta-Learning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yukun Huang'), arxiv.Result.Author('Kun Qian'), arxiv.Result.Author('Zhou Yu')]","Prompt tuning (PT) is an effective approach to adapting pre-trained language
models to downstream tasks. Without a good initialization, prompt tuning
doesn't perform well under few-shot settings. So pre-trained prompt tuning
(PPT) is proposed to initialize prompts by leveraging pre-training data. We
propose MetaPT (Meta-learned Prompt Tuning) to further improve PPT's
initialization by considering latent structure within the pre-training data.
Specifically, we introduce the structure by first clustering pre-training data
into different auxiliary tasks with unsupervised methods. Then we use these
tasks to pre-train prompts with a meta-learning algorithm. Such a process can
make prompts learn a better initialization by discovering commonalities among
these auxiliary tasks. We evaluate our method on seven downstream tasks. Our
MetaPT achieves better and more stable performance than the state-of-the-art
method."
6704,"We run a grid search
In this section we further study whether the routing-  for learning rate (1e-5, 2e-5, 5e-5) and batch size
based multi-task learning induces better few-shot      (2,4,8) for each few-shot sample.","7 Few-shot Adaptation to Novel Tasks                   We train on Dtrain for 1000 steps, and validate
                                                       on Ddev every 100 steps.","Finally, the model
learning performance on unseen tasks.",2022-05-25 11:59:05+00:00,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Qinyuan Ye'), arxiv.Result.Author('Juan Zha'), arxiv.Result.Author('Xiang Ren')]","Recent work suggests that transformer models are capable of multi-task
learning on diverse NLP tasks. However, the potential of these models may be
limited as they use the same set of parameters for all tasks. In contrast,
humans tackle tasks in a more flexible way, by making proper presumptions on
what skills and knowledge are relevant and executing only the necessary
computations. Inspired by this, we propose to use task-level mixture-of-expert
models, which has a collection of transformer layers (i.e., experts) and a
router component to choose among these experts dynamically and flexibly. We
show that the learned routing decisions and experts partially rediscover human
categorization of NLP tasks -- certain experts are strongly associated with
extractive tasks, some with classification tasks, and some with tasks requiring
world knowledge."
6744,"We encourage further research into
integrating more advanced reported speech conver-       3.","(2020) confers to the unsupervised nature        et al., 2021b), SAMSum (Gliwa et al., 2019)
of this paper.","Interview: MediaSum (Zhu et al., 2021)
sion techniques into the abstractive summarization
pipeline.",2022-05-26 02:18:12+00:00,Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Seongmin Park'), arxiv.Result.Author('Jihwa Lee')]","We advance the state-of-the-art in unsupervised abstractive dialogue
summarization by utilizing multi-sentence compression graphs. Starting from
well-founded assumptions about word graphs, we present simple but reliable
path-reranking and topic segmentation schemes. Robustness of our method is
demonstrated on datasets across multiple domains, including meetings,
interviews, movie scripts, and day-to-day conversations. We also identify
possible avenues to augment our heuristic-based system with deep learning. We
open-source our code, to provide a strong, reproducible baseline for future
research into unsupervised dialogue summarization."
6745,"With recent advances in unsuper-
                                                            vised sequence to sequence transduction (Li et al.,
2020; He et al., 2020), we expect further research         International Conference on Computational Linguis-
into more advanced POV conversion techniques               tics, pages 5360–5371, Barcelona, Spain (Online).","POV conversion in particular
                                                            can beneﬁt from deep learning-based approaches
                                                            (Lee et al., 2020).","will improve unsupervised dialogue summariza-              International Committee on Computational Linguis-
tion.",2022-05-26 02:18:12+00:00,Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Seongmin Park'), arxiv.Result.Author('Jihwa Lee')]","We advance the state-of-the-art in unsupervised abstractive dialogue
summarization by utilizing multi-sentence compression graphs. Starting from
well-founded assumptions about word graphs, we present simple but reliable
path-reranking and topic segmentation schemes. Robustness of our method is
demonstrated on datasets across multiple domains, including meetings,
interviews, movie scripts, and day-to-day conversations. We also identify
possible avenues to augment our heuristic-based system with deep learning. We
open-source our code, to provide a strong, reproducible baseline for future
research into unsupervised dialogue summarization."
6751,"Smokey
                                                       paved the way for further research in using classical
   We evaluate our model on two datasets; one          machine learning techniques to exploit the inherent
treats the task of hate speech and offensive lan-      features of Natural Language over a plethora of
guage detection separately (Davidson et al., 2017).","the three classes (f lame, okay or maybe).","tasks such as junk ﬁltering (Sahami et al., 1998),
The other uses a hierarchical classiﬁcation system     opinion mining (Wiebe et al., 2005) etc.",2022-05-26 05:27:50+00:00,Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Divyam Goel'), arxiv.Result.Author('Raksha Sharma')]","The last few years have witnessed an exponential rise in the propagation of
offensive text on social media. Identification of this text with high precision
is crucial for the well-being of society. Most of the existing approaches tend
to give high toxicity scores to innocuous statements (e.g., ""I am a gay man"").
These false positives result from over-generalization on the training data
where specific terms in the statement may have been used in a pejorative sense
(e.g., ""gay""). Emphasis on such words alone can lead to discrimination against
the classes these systems are designed to protect. In this paper, we address
the problem of offensive language detection on Twitter, while also detecting
the type and the target of the offence. We propose a novel approach called
SyLSTM, which integrates syntactic features in the form of the dependency parse
tree of a sentence and semantic features in the form of word embeddings into a
deep learning architecture using a Graph Convolutional Network. Results show
that the proposed approach significantly outperforms the state-of-the-art BERT
model with orders of magnitude fewer number of parameters."
6833,"To further study the correlation between the base model quality and the improvement of NADO, we
conduct experiments on GPT-2 base model.","However, with warmup and
NADO under importance sampling, we show that it is still possible to obtain a powerful model with
the proposed method.","The GPT-2 base model has lower scores with and without
NADO compared with GPT-2 large, while the coverage improvements are similar.",2022-05-27 20:17:53+00:00,Controllable Text Generation with Neurally-Decomposed Oracle,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tao Meng'), arxiv.Result.Author('Sidi Lu'), arxiv.Result.Author('Nanyun Peng'), arxiv.Result.Author('Kai-Wei Chang')]","We propose a general and efficient framework to control auto-regressive
generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained
base language model and a sequence-level boolean oracle function, we propose to
decompose the oracle function into token-level guidance to steer the base model
in text generation. Specifically, the token-level guidance is approximated by a
neural model trained with examples sampled from the base model, demanding no
additional auxiliary labeled data. Based on posterior regularization, we
present the closed-form optimal solution to incorporate the token-level
guidance into the base model for controllable generation. We further provide a
theoretical analysis of how the approximation quality of NADO affects the
controllable generation results. Experiments conducted on two applications: (1)
text generation with lexical constraints and (2) machine translation with
formality control demonstrate that our framework efficiently guides the base
model towards the given oracle while maintaining high generation quality."
6844,"abuse of notation, we will use g to refer to either a
(2021) further study the compositional generaliza-          subgoal sequence or it’s textual representation v(g)
tion ability of such semantic parsers.",(2020) and Herzig et al.,In our work          depending on the context.,2022-05-28 01:03:30+00:00,Few-shot Subgoal Planning with Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Lajanugen Logeswaran'), arxiv.Result.Author('Yao Fu'), arxiv.Result.Author('Moontae Lee'), arxiv.Result.Author('Honglak Lee')]","Pre-trained large language models have shown successful progress in many
language understanding benchmarks. This work explores the capability of these
models to predict actionable plans in real-world environments. Given a text
instruction, we show that language priors encoded in pre-trained language
models allow us to infer fine-grained subgoal sequences. In contrast to recent
methods which make strong assumptions about subgoal supervision, our
experiments show that language models can infer detailed subgoal sequences from
few training sequences without any fine-tuning. We further propose a simple
strategy to re-rank language model predictions based on interaction and
feedback from the environment. Combined with pre-trained navigation and visual
reasoning components, our approach demonstrates competitive performance on
subgoal prediction and task completion in the ALFRED benchmark compared to
prior methods that assume more subgoal supervision."
6853,This dataset and all the models will be useful for further research.,"We also present this dataset’s multipurpose nature, especially on machine translation for Bangla-English
                                        and English-Bangla.","Keywords: Image Captioning, Natural Language Processing, Multilingual, Multimodal, Machine Translation

                                                        1.",2022-05-28 15:39:09+00:00,BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mohammad Faiyaz Khan'), arxiv.Result.Author('S. M. Sadiq-Ur-Rahman Shifath'), arxiv.Result.Author('Md Saiful Islam')]","As computers have become efficient at understanding visual information and
transforming it into a written representation, research interest in tasks like
automatic image captioning has seen a significant leap over the last few years.
While most of the research attention is given to the English language in a
monolingual setting, resource-constrained languages like Bangla remain out of
focus, predominantly due to a lack of standard datasets. Addressing this issue,
we present a new dataset BAN-Cap following the widely used Flickr8k dataset,
where we collect Bangla captions of the images provided by qualified
annotators. Our dataset represents a wider variety of image caption styles
annotated by trained people from different backgrounds. We present a
quantitative and qualitative analysis of the dataset and the baseline
evaluation of the recent models in Bangla image captioning. We investigate the
effect of text augmentation and demonstrate that an adaptive attention-based
model combined with text augmentation using Contextualized Word Replacement
(CWR) outperforms all state-of-the-art models for Bangla image captioning. We
also present this dataset's multipurpose nature, especially on machine
translation for Bangla-English and English-Bangla. This dataset and all the
models will be useful for further research."
6886,"In the future, it is necessary to further study the inﬂuence
                       h = LM (uM ),                   (8)        and challenge of emotion consistence and emotion mutation in

                       eM = M LP (h)),                 (9)

   2) ERC models with both current utterance and dia-
logue history include bcLSTM [64], DialogueRNN [65],
DialogueGCN [66], DialogXL [67] and EmoBERTa [68].","They use sentence-level language        emotion mutation in negative, neutral and positive are 0.225,
model LM to obtain the representation of uM , and then use        0.337 and 0.427 respectively, which makes the ERC task
M LP to predict the emotion eM as follows:                        signiﬁcantly different from other long text emotion recognition
                                                                  tasks.","9

dialogue to ERC task.",2022-05-29 17:45:12+00:00,CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'cs.MM']","[arxiv.Result.Author('Yirong Chen'), arxiv.Result.Author('Weiquan Fan'), arxiv.Result.Author('Xiaofen Xing'), arxiv.Result.Author('Jianxin Pang'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Wenjing Han'), arxiv.Result.Author('Qianfeng Tie'), arxiv.Result.Author('Xiangmin Xu')]","Human language expression is based on the subjective construal of the
situation instead of the objective truth conditions, which means that speakers'
personalities and emotions after cognitive processing have an important
influence on conversation. However, most existing datasets for conversational
AI ignore human personalities and emotions, or only consider part of them. It's
difficult for dialogue systems to understand speakers' personalities and
emotions although large-scale pre-training language models have been widely
used. In order to consider both personalities and emotions in the process of
conversation generation, we propose CPED, a large-scale Chinese personalized
and emotional dialogue dataset, which consists of multi-source knowledge
related to empathy and personal characteristic. These knowledge covers gender,
Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED
contains more than 12K dialogues of 392 speakers from 40 TV shows. We release
the textual dataset with audio features and video features according to the
copyright claims, privacy issues, terms of service of video platforms. We
provide detailed description of the CPED construction process and introduce
three tasks for conversational AI, including personality recognition, emotion
recognition in conversations as well as personalized and emotional conversation
generation. Finally, we provide baseline systems for these tasks and consider
the function of speakers' personalities and emotions on conversation. Our
motivation is to propose a dataset to be widely adopted by the NLP community as
a new open benchmark for conversational AI research. The full dataset is
available at https://github.com/scutcyr/CPED."
6887,"Both the sources have been released
     separately to enable further research in an individual area.","The major chunk of the dataset
     comes from news sources with 212M tokens and the remaining 76.4M tokens
     come from non-news literature work.","The monolingual
     data set can be used to train models using unsupervised language modeling
     tasks.",2022-05-29 17:51:00+00:00,"L3Cube-MahaNLP: Marathi Natural Language Processing Datasets, Models, and Library",cs.CL,"['cs.CL', 'cs.LG']",[arxiv.Result.Author('Raviraj Joshi')],"Despite being the third most popular language in India, the Marathi language
lacks useful NLP resources. Moreover, popular NLP libraries do not have support
for the Marathi language. With L3Cube-MahaNLP, we aim to build resources and a
library for Marathi natural language processing. We present datasets and
transformer models for supervised tasks like sentiment analysis, named entity
recognition, and hate speech detection. We have also published a monolingual
Marathi corpus for unsupervised language modeling tasks. Overall we present
MahaCorpus, MahaSent, MahaNER, and MahaHate datasets and their corresponding
MahaBERT models fine-tuned on these datasets. We aim to move ahead of benchmark
datasets and prepare useful resources for Marathi. The resources are available
at https://github.com/l3cube-pune/MarathiNLP."
6888,"Both the sources have been released
     separately to enable further research in an individual area.","The major chunk of the dataset
     comes from news sources with 212M tokens and the remaining 76.4M tokens
     come from non-news literature work.","The monolingual
     data set can be used to train models using unsupervised language modeling
     tasks.",2022-05-29 17:51:00+00:00,"L3Cube-MahaNLP: Marathi Natural Language Processing Datasets, Models, and Library",cs.CL,"['cs.CL', 'cs.LG']",[arxiv.Result.Author('Raviraj Joshi')],"Despite being the third most popular language in India, the Marathi language
lacks useful NLP resources. Moreover, popular NLP libraries do not have support
for the Marathi language. With L3Cube-MahaNLP, we aim to build resources and a
library for Marathi natural language processing. We present datasets and
transformer models for supervised tasks like sentiment analysis, named entity
recognition, and hate speech detection. We have also published a monolingual
Marathi corpus for unsupervised language modeling tasks. Overall we present
MahaCorpus, MahaSent, MahaNER, and MahaHate datasets and their corresponding
MahaBERT models fine-tuned on these datasets. We aim to move ahead of benchmark
datasets and prepare useful resources for Marathi. The resources are available
at https://github.com/l3cube-pune/MarathiNLP."
6910,"[...]
benchmark to foster further research on this task, we create a new      Introduction: Designing new deep reinforcement learning algo-
multilingual dataset of TLDRs in a variety of different languages       rithms that can efficiently solve across a wide variety of problems
(i.e., German, Italian, Chinese, and Japanese).","ment learning algorithms by searching over the space of com-
                                                                        putational graphs which compute the loss function for a value-
   In order to evaluate the difficulty of CL-TLDR and provide a         based model-free RL agent to optimize.",Our dataset consists    generally requires a tremendous amount of manual effort.,2022-05-30 12:31:28+00:00,X-SCITLDR: Cross-Lingual Extreme Summarization of Scholarly Documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sotaro Takeshita'), arxiv.Result.Author('Tommaso Green'), arxiv.Result.Author('Niklas Friedrich'), arxiv.Result.Author('Kai Eckert'), arxiv.Result.Author('Simone Paolo Ponzetto')]","The number of scientific publications nowadays is rapidly increasing, causing
information overload for researchers and making it hard for scholars to keep up
to date with current trends and lines of work. Consequently, recent work on
applying text mining technologies for scholarly publications has investigated
the application of automatic text summarization technologies, including extreme
summarization, for this domain. However, previous work has concentrated only on
monolingual settings, primarily in English. In this paper, we fill this
research gap and present an abstractive cross-lingual summarization dataset for
four different languages in the scholarly domain, which enables us to train and
evaluate models that process English papers and generate summaries in German,
Italian, Chinese and Japanese. We present our new X-SCITLDR dataset for
multilingual summarization and thoroughly benchmark different models based on a
state-of-the-art multilingual pre-trained model, including a two-stage
`summarize and translate' approach and a direct cross-lingual model. We
additionally explore the benefits of intermediate-stage training using English
monolingual summarization and machine translation as intermediate tasks and
analyze performance in zero- and few-shot scenarios."
6940,"Designing effective
                                                            warm-starting interaction warrants further research
                                                            especially from the HCI perspective.","the user to provide several example utterances in
                                                            the initial calibration stage.","4
NAACL ’22 2nd Workshop on Bridging Human-Computer Interaction and Natural Language Processing

   Beer Logging                                            Name        Type           Score    Review

                                                           Text        Ale | Stout |  Numeric  Text
                                                           Short-form  Lager | Other           Long-form
   User

1 “I drank Guinness Blonde today.”          NLU            Guinness Blonde Lager      ?",2022-05-31 01:58:04+00:00,Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'I.2.7; H.5.1']","[arxiv.Result.Author('Young-Ho Kim'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Minsuk Chang'), arxiv.Result.Author('Sang-Woo Lee')]","Current natural language interaction for self-tracking tools largely depends
on bespoke implementation optimized for a specific tracking theme and data
format, which is neither generalizable nor scalable to a tremendous design
space of self-tracking. However, training machine learning models in the
context of self-tracking is challenging due to the wide variety of tracking
topics and data formats. In this paper, we propose a novel NLP task for
self-tracking that extracts close- and open-ended information from a
retrospective activity log described as a plain text, and a domain-agnostic,
GPT-3-based NLU framework that performs this task. The framework augments the
prompt using synthetic samples to transform the task into 10-shot learning, to
address a cold-start problem in bootstrapping a new tracking topic. Our
preliminary evaluation suggests that our approach significantly outperforms the
baseline QA models. Going further, we discuss future application domains toward
which the NLP and HCI researchers can collaborate."
6941,"Designing effective
                                                            warm-starting interaction warrants further research
                                                            especially from the HCI perspective.","the user to provide several example utterances in
                                                            the initial calibration stage.","4
NAACL ’22 2nd Workshop on Bridging Human-Computer Interaction and Natural Language Processing

   Beer Logging                                            Name        Type           Score    Review

                                                           Text        Ale | Stout |  Numeric  Text
                                                           Short-form  Lager | Other           Long-form
   User

1 “I drank Guinness Blonde today.”          NLU            Guinness Blonde Lager      ?",2022-05-31 01:58:04+00:00,Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'I.2.7; H.5.1']","[arxiv.Result.Author('Young-Ho Kim'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Minsuk Chang'), arxiv.Result.Author('Sang-Woo Lee')]","Current natural language interaction for self-tracking tools largely depends
on bespoke implementation optimized for a specific tracking theme and data
format, which is neither generalizable nor scalable to a tremendous design
space of self-tracking. However, training machine learning models in the
context of self-tracking is challenging due to the wide variety of tracking
topics and data formats. In this paper, we propose a novel NLP task for
self-tracking that extracts close- and open-ended information from a
retrospective activity log described as a plain text, and a domain-agnostic,
GPT-3-based NLU framework that performs this task. The framework augments the
prompt using synthetic samples to transform the task into 10-shot learning, to
address a cold-start problem in bootstrapping a new tracking topic. Our
preliminary evaluation suggests that our approach significantly outperforms the
baseline QA models. Going further, we discuss future application domains toward
which the NLP and HCI researchers can collaborate."
6942,"Designing effective
                                                            warm-starting interaction warrants further research
                                                            especially from the HCI perspective.","the user to provide several example utterances in
                                                            the initial calibration stage.","4
NAACL ’22 2nd Workshop on Bridging Human-Computer Interaction and Natural Language Processing

   Beer Logging                                            Name        Type           Score    Review

                                                           Text        Ale | Stout |  Numeric  Text
                                                           Short-form  Lager | Other           Long-form
   User

1 “I drank Guinness Blonde today.”          NLU            Guinness Blonde Lager      ?",2022-05-31 01:58:04+00:00,Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'I.2.7; H.5.1']","[arxiv.Result.Author('Young-Ho Kim'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Minsuk Chang'), arxiv.Result.Author('Sang-Woo Lee')]","Current natural language interaction for self-tracking tools largely depends
on bespoke implementation optimized for a specific tracking theme and data
format, which is neither generalizable nor scalable to a tremendous design
space of self-tracking. However, training machine learning models in the
context of self-tracking is challenging due to the wide variety of tracking
topics and data formats. In this paper, we propose a novel NLP task for
self-tracking that extracts close- and open-ended information from a
retrospective activity log described as a plain text, and a domain-agnostic,
GPT-3-based NLU framework that performs this task. The framework augments the
prompt using synthetic samples to transform the task into 10-shot learning, to
address a cold-start problem in bootstrapping a new tracking topic. Our
preliminary evaluation suggests that our approach significantly outperforms the
baseline QA models. Going further, we discuss future application domains toward
which the NLP and HCI researchers can collaborate."
6955,"All of them did not enhance performance but represent a suitable
starting point for further research.","Finally, our detailed study of hmBert also includes experiments with a knowledge-based
approach, training an ELECTRA-based language model [8], and addressing a tokenization issue
with the Fraktur typeface.","Our contributions are i) the comprehensive description of
the development of hmBert, ii) the release of hmBert models of different sizes, iii) the release
of the hmBert pretraining code, and iv) extensive experiments using hmBert including detailed
insights for the community.",2022-05-31 07:30:33+00:00,hmBERT: Historical Multilingual Language Models for Named Entity Recognition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Stefan Schweter'), arxiv.Result.Author('Luisa März'), arxiv.Result.Author('Katharina Schmid'), arxiv.Result.Author('Erion Çano')]","Compared to standard Named Entity Recognition (NER), identifying persons,
locations, and organizations in historical texts forms a big challenge. To
obtain machine-readable corpora, the historical text is usually scanned and
optical character recognition (OCR) needs to be performed. As a result, the
historical corpora contain errors. Also, entities like location or organization
can change over time, which poses another challenge. Overall historical texts
come with several peculiarities that differ greatly from modern texts and large
labeled corpora for training a neural tagger are hardly available for this
domain. In this work, we tackle NER for historical German, English, French,
Swedish, and Finnish by training large historical language models. We
circumvent the need for labeled data by using unlabeled data for pretraining a
language model. hmBERT, a historical multilingual BERT-based language model is
proposed, with different sizes of it being publicly released. Furthermore, we
evaluate the capability of hmBERT by solving downstream NER as part of this
year's HIPE-2022 shared task and provide detailed analysis and insights. For
the Multilingual Classical Commentary coarse-grained NER challenge, our tagger
HISTeria outperforms the other teams' models for two out of three languages."
6956,"These additional experiments did not enhance performance
but represent a suitable starting point for further research.","Finally, our detailed study of hmBert also includes
experiments with a knowledge-based approach, training an ELECTRA-based language model [8],
and addressing a tokenization issue.","Our contributions are i) the comprehensive description of the development of hmBert, ii)
the release of hmBert models of different sizes, iii) the release of the hmBert pretraining code,
and iv) extensive experiments using hmBert including detailed insights for the community.",2022-05-31 07:30:33+00:00,hmBERT: Historical Multilingual Language Models for Named Entity Recognition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Stefan Schweter'), arxiv.Result.Author('Luisa März'), arxiv.Result.Author('Katharina Schmid'), arxiv.Result.Author('Erion Çano')]","Compared to standard Named Entity Recognition (NER), identifying persons,
locations, and organizations in historical texts constitutes a big challenge.
To obtain machine-readable corpora, the historical text is usually scanned and
Optical Character Recognition (OCR) needs to be performed. As a result, the
historical corpora contain errors. Also, entities like location or organization
can change over time, which poses another challenge. Overall, historical texts
come with several peculiarities that differ greatly from modern texts and large
labeled corpora for training a neural tagger are hardly available for this
domain. In this work, we tackle NER for historical German, English, French,
Swedish, and Finnish by training large historical language models. We
circumvent the need for large amounts of labeled data by using unlabeled data
for pretraining a language model. We propose hmBERT, a historical multilingual
BERT-based language model, and release the model in several versions of
different sizes. Furthermore, we evaluate the capability of hmBERT by solving
downstream NER as part of this year's HIPE-2022 shared task and provide
detailed analysis and insights. For the Multilingual Classical Commentary
coarse-grained NER challenge, our tagger HISTeria outperforms the other teams'
models for two out of three languages."
6960,"Hence, a broad overview of the techniques
employed gives a good grounding for further research.","The profusion of methods has made it hard to crisply point out the state-of-the-art,
even for fairly general word problem solving settings.","Similarly, understanding the source, set-
tings and relevance of datasets is often important.",2022-05-31 10:51:25+00:00,Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sowmya S Sundaram'), arxiv.Result.Author('Sairam Gurajada'), arxiv.Result.Author('Marco Fisichella'), arxiv.Result.Author('Deepak P'), arxiv.Result.Author('Savitha Sam Abraham')]","From the latter half of the last decade, there has been a growing interest in
developing algorithms for automatically solving mathematical word problems
(MWP). It is a challenging and unique task that demands blending surface level
text pattern recognition with mathematical reasoning. In spite of extensive
research, we are still miles away from building robust representations of
elementary math word problems and effective solutions for the general task. In
this paper, we critically examine the various models that have been developed
for solving word problems, their pros and cons and the challenges ahead. In the
last two years, a lot of deep learning models have recorded competing results
on benchmark datasets, making a critical and conceptual analysis of literature
highly useful at this juncture. We take a step back and analyse why, in spite
of this abundance in scholarly interest, the predominantly used experiment and
dataset designs continue to be a stumbling block. From the vantage point of
having analyzed the literature closely, we also endeavour to provide a road-map
for future math word problem research."
6993,"The national clinical
further research into how bias manifests or does             assessment tool for medical students in emergency
not manifest at different stages of professionals’           medicine (ncat-em).",2017.,"West J Emerg Med, pages 66–
careers, and how we can combine multiple sources             74.
of information together with text to form a wider
view of bias and fairness.",2022-06-01 05:01:36+00:00,Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback,cs.CL,['cs.CL'],"[arxiv.Result.Author('Emmy Liu'), arxiv.Result.Author('Michael Henry Tessler'), arxiv.Result.Author('Nicole Dubosh'), arxiv.Result.Author('Katherine Mosher Hiller'), arxiv.Result.Author('Roger Levy')]","Although approximately 50% of medical school graduates today are women,
female physicians tend to be underrepresented in senior positions, make less
money than their male counterparts and receive fewer promotions. There is a
growing body of literature demonstrating gender bias in various forms of
evaluation in medicine, but this work was mainly conducted by looking for
specific words using fixed dictionaries such as LIWC and focused on
recommendation letters. We use a dataset of written and quantitative
assessments of medical student performance on individual shifts of work,
collected across multiple institutions, to investigate the extent to which
gender bias exists in a day-to-day context for medical students. We investigate
differences in the narrative comments given to male and female students by both
male or female faculty assessors, using a fine-tuned BERT model. This allows us
to examine whether groups are written about in systematically different ways,
without relying on hand-crafted wordlists or topic models. We compare these
results to results from the traditional LIWC method and find that, although we
find no evidence of group-level gender bias in this dataset, terms related to
family and children are used more in feedback given to women."
7003,This aspect would need further study.,"The longer
clippings may also be fuzzier than the shorter ones and contain text from adjacent
text segments.","In the case of the pre-formulated queries, the difference in the effect of Optical
Character Recognition quality on the relevance judgements was statistically
significant (p=0.002, Wilcoxon's signed rank test, Croft et al., 2010) when the
relevance of the individual underlying documents was judged based on two possible
levels of Optical Character Recognition quality.",2022-06-01 10:07:50+00:00,Optical character recognition quality affects perceived usefulness of historical newspaper clippings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kimmo Kettunen'), arxiv.Result.Author('Heikki Keskustalo'), arxiv.Result.Author('Sanna Kumpulainen'), arxiv.Result.Author('Tuula Pääkkönen'), arxiv.Result.Author('Juha Rautiainen')]","Introduction. We study effect of different quality optical character
recognition in interactive information retrieval with a collection of one
digitized historical Finnish newspaper. Method. This study is based on the
simulated interactive information retrieval work task model. Thirty-two users
made searches to an article collection of Finnish newspaper Uusi Suometar
1869-1918 with ca. 1.45 million auto segmented articles. Our article search
database had two versions of each article with different quality optical
character recognition. Each user performed six pre-formulated and six
self-formulated short queries and evaluated subjectively the top-10 results
using graded relevance scale of 0-3 without knowing about the optical character
recognition quality differences of the otherwise identical articles. Analysis.
Analysis of the user evaluations was performed by comparing mean averages of
evaluations scores in user sessions. Differences of query results were detected
by analysing lengths of returned articles in pre-formulated and self-formulated
queries and number of different documents retrieved overall in these two
sessions. Results. The main result of the study is that improved optical
character recognition quality affects perceived usefulness of historical
newspaper articles positively. Conclusions. We were able to show that
improvement in optical character recognition quality of documents leads to
higher mean relevance evaluation scores of query results in our historical
newspaper collection. To the best of our knowledge this simulated interactive
user-task is the first one showing empirically that users' subjective relevance
assessments are affected by a change in the quality of optically read text."
7038,"summarization task of mental health related posts, and
to provide strong baselines for further research, we ex-   4.2.","Experimental setup                            Unlike BERT, BART utilizes a pre-trained encoder-
                                                             decoder framework for language generation task,
To evaluate the quality of MENTSUM dataset for the           summarization being one of them.","Implementation details
plored several baselines.",2022-06-02 03:08:34+00:00,MentSum: A Resource for Exploring Summarization of Mental Health Online Posts,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sajad Sotudeh'), arxiv.Result.Author('Nazli Goharian'), arxiv.Result.Author('Zachary Young')]","Mental health remains a significant challenge of public health worldwide.
With increasing popularity of online platforms, many use the platforms to share
their mental health conditions, express their feelings, and seek help from the
community and counselors. Some of these platforms, such as Reachout, are
dedicated forums where the users register to seek help. Others such as Reddit
provide subreddits where the users publicly but anonymously post their mental
health distress. Although posts are of varying length, it is beneficial to
provide a short, but informative summary for fast processing by the counselors.
To facilitate research in summarization of mental health online posts, we
introduce Mental Health Summarization dataset, MentSum, containing over 24k
carefully selected user posts from Reddit, along with their short user-written
summary (called TLDR) in English from 43 mental health subreddits. This
domain-specific dataset could be of interest not only for generating short
summaries on Reddit, but also for generating summaries of posts on the
dedicated mental health forums such as Reachout. We further evaluate both
extractive and abstractive state-of-the-art summarization baselines in terms of
Rouge scores, and finally conduct an in-depth human evaluation study of both
user-written and system-generated summaries, highlighting challenges in this
research."
7049,"However, due to the limited scope of this research, further study is required to investigate the
e ect of including unethical text in the pretraining process.","This resulted in Spearman ⇢ = 0.036,
meaning that we could not ﬁnd a correlation between using possibly unethical data in pretraining and the performance
of a ﬁne-tuned model.","In addition to containing texts from di erent domains, the corpora also holds di erent amount of data for di erent
languages.",2022-06-02 09:53:15+00:00,Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Juuso Eronen'), arxiv.Result.Author('Michal Ptaszynski'), arxiv.Result.Author('Fumito Masui'), arxiv.Result.Author('Masaki Arata'), arxiv.Result.Author('Gniewosz Leliwa'), arxiv.Result.Author('Michal Wroczynski')]","We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection."
7050,"However, further research is required to investigate the possible bias in pretrained multilingual models
considering the used pretraining corpora sizes for each language.","In the case of our proposed languages, we were not able to notice that the pretraining corpus size inﬂuenced
the results.",6.5.,2022-06-02 09:53:15+00:00,Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Juuso Eronen'), arxiv.Result.Author('Michal Ptaszynski'), arxiv.Result.Author('Fumito Masui'), arxiv.Result.Author('Masaki Arata'), arxiv.Result.Author('Gniewosz Leliwa'), arxiv.Result.Author('Michal Wroczynski')]","We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection."
7093,"We further study how state transitions
mark the backbone of the representation space and encode meaningful phrase constructions (§6.1).","Then we reveal the
mechanism of contextualization by showing how words within states “receive” meaning from their
context and become interpretable after contextualization (§5.2).","Finally, combining the state-word and state-state topology, we reach the latent mechanism about how
sentences are encoded as a traversal over the state network (§6.2).",2022-06-03 11:22:48+00:00,Latent Topology Induction for Understanding Contextualized Representations,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE']","[arxiv.Result.Author('Yao Fu'), arxiv.Result.Author('Mirella Lapata')]","In this work, we study the representation space of contextualized embeddings
and gain insight into the hidden topology of large language models. We show
there exists a network of latent states that summarize linguistic properties of
contextualized representations. Instead of seeking alignments to existing
well-defined annotations, we infer this latent network in a fully unsupervised
way using a structured variational autoencoder. The induced states not only
serve as anchors that mark the topology (neighbors and connectivity) of the
representation manifold but also reveal the internal mechanism of encoding
sentences. With the induced network, we: (1). decompose the representation
space into a spectrum of latent states which encode fine-grained word meanings
with lexical, morphological, syntactic and semantic information; (2). show
state-state transitions encode rich phrase constructions and serve as the
backbones of the latent space. Putting the two together, we show that sentences
are represented as a traversal over the latent network where state-state
transition chains encode syntactic templates and state-word emissions fill in
the content. We demonstrate these insights with extensive experiments and
visualizations."
7098,"In the future, we will further study
voting ensemble in the test phase.","Table 4 shows           based approaches are effective to produce more ro-
the number of models used as experts for span-            bust predictions.","In Table 3, we         how to incorporate a stacking ensemble approach with
outline the ofﬁcial results obtained for our submis-      multiple stages to achieve better performance rather
sions.",2022-06-03 13:00:48+00:00,TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Mohammed ElKomy'), arxiv.Result.Author('Amany M. Sarhan')]","In recent years, we witnessed great progress in different tasks of natural
language understanding using machine learning. Question answering is one of
these tasks which is used by search engines and social media platforms for
improved user experience. Arabic is the language of the Holy Qur'an; the sacred
text for 1.8 billion people across the world. Arabic is a challenging language
for Natural Language Processing (NLP) due to its complex structures. In this
article, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which
is a question answering challenge on the Holy Qur'an in Arabic. We propose an
ensemble learning model based on Arabic variants of BERT models. In addition,
we perform post-processing to enhance the model predictions. Our system
achieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test
set."
7102,"For example, SGs might be categorized in different ways based on learning mechanisms
developed in further research.","However, the categorization given is certainly not the only one possible.","But different SGs are needed to provide for the different connections paths in the HD-NBA
that implement different sentence structures, as illustrated in Figure 2.",2022-05-19 13:58:45+00:00,Sentences as connection paths: A neural language architecture of sentence structure in the brain,cs.CL,"['cs.CL', 'cs.NE', 'q-bio.NC']",[arxiv.Result.Author('Frank van der Velde')],"This article presents a neural language architecture of sentence structure in
the brain, in which sentences are temporal connection paths that interconnect
neural structures underlying their words. Words remain 'in-situ', hence they
are always content-addressable. Arbitrary and novel sentences (with novel
words) can be created with 'neural blackboards' for words and sentences. Hence,
the unlimited productivity of natural language can be achieved with a 'fixed'
small world like network structure. The article focuses on the neural
blackboard for sentences. The architecture uses only one 'connection matrix'
for binding all structural relations between words in sentences. Its ability to
represent arbitrary (English) sentences is discussed in detail, based on a
comprehensive analysis of them. The architecture simulates intra-cranial brain
activity observed during sentence processing and fMRI observations related to
sentence complexity and ambiguity. The simulations indicate that the observed
effects relate to global control over the architecture, not to the sentence
structures involved, which predicts higher activity differences related to
complexity and ambiguity with higher comprehension capacity. Other aspects
discussed are the 'intrinsic' sentence structures provided by connection paths
and their relation to scope and inflection, the use of a dependency parser for
control of binding, long-distance dependencies and gaps, question answering,
ambiguity resolution based on backward processing without explicit
backtracking, garden paths, and performance difficulties related to embeddings."
7110,"Our ﬁndings that existing rele-       that further research into new measures of dialogue
vance metrics generalize poorly to new domains         relevance is required, and that care must be taken
is consistent with previous ﬁndings about metrics      in their evaluation to compare against a number of
of dialogue quality (Lowe, 2019; Yeh et al., 2021).","As such, it is clear
valid responses.",different models in a number of domains.,2022-06-03 21:23:05+00:00,"Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric",cs.CL,['cs.CL'],"[arxiv.Result.Author('Ian Berlot-Attwell'), arxiv.Result.Author('Frank Rudzicz')]","In this work, we evaluate various existing dialogue relevance metrics, find
strong dependency on the dataset, often with poor correlation with human scores
of relevance, and propose modifications to reduce data requirements and domain
sensitivity while improving correlation. Our proposed metric achieves
state-of-the-art performance on the HUMOD dataset while reducing measured
sensitivity to dataset by 37%-66%. We achieve this without fine-tuning a
pretrained language model, and using only 3,750 unannotated human dialogues and
a single negative example. Despite these limitations, we demonstrate
competitive performance on four datasets from different domains. Our code,
including our metric and experiments, is open sourced."
7111,"Interestingly,
the authors found that the FED metric performs            Having said this, it is clear that further research
well on P-DD (reporting a Spearman’s correlation       into what exactly these metrics are measuring, and
of 0.507), however our results demonstrate that        why they fail to generalize, is merited.",datasets we used (P-DD and FED).,"The re-
the components of FED that are meant to mea-           sults are often counter-intuitive; our demonstration
sure relevance (i.e.",2022-06-03 21:23:05+00:00,"Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric",cs.CL,['cs.CL'],"[arxiv.Result.Author('Ian Berlot-Attwell'), arxiv.Result.Author('Frank Rudzicz')]","In this work, we evaluate various existing dialogue relevance metrics, find
strong dependency on the dataset, often with poor correlation with human scores
of relevance, and propose modifications to reduce data requirements and domain
sensitivity while improving correlation. Our proposed metric achieves
state-of-the-art performance on the HUMOD dataset while reducing measured
sensitivity to dataset by 37%-66%. We achieve this without fine-tuning a
pretrained language model, and using only 3,750 unannotated human dialogues and
a single negative example. Despite these limitations, we demonstrate
competitive performance on four datasets from different domains. Our code,
including our metric and experiments, is open sourced."
7112,"Additionally, we hope          ceedings of the ACL Workshop on Intrinsic and Ex-
that our ﬁndings on the domain sensitivity of exist-      trinsic Evaluation Measures for Machine Transla-
ing metrics will spur further research into both the      tion and/or Summarization, pages 65–72, Ann Ar-
cause of – and solutions to – this problem.","In Pro-
and hyperparameter search.","bor, Michigan.",2022-06-03 21:23:05+00:00,"Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric",cs.CL,['cs.CL'],"[arxiv.Result.Author('Ian Berlot-Attwell'), arxiv.Result.Author('Frank Rudzicz')]","In this work, we evaluate various existing dialogue relevance metrics, find
strong dependency on the dataset, often with poor correlation with human scores
of relevance, and propose modifications to reduce data requirements and domain
sensitivity while improving correlation. Our proposed metric achieves
state-of-the-art performance on the HUMOD dataset while reducing measured
sensitivity to dataset by 37%-66%. We achieve this without fine-tuning a
pretrained language model, and using only 3,750 unannotated human dialogues and
a single negative example. Despite these limitations, we demonstrate
competitive performance on four datasets from different domains. Our code,
including our metric and experiments, is open sourced."
7170,"Given the corpus D, we can calculate
the average values of TP, IP and WR of the n-th repetitive sentence sn as

                     1         TP(sn),                             1            IP(sn),                    1                       WR(sn)
           TPn =                                         IPn =                                 WRn =
                    |D| s∈D                                         |D| s∈D                                               |D| s∈D

by enumerating all sentences s in D. To further study the effect of different corpus, we construct three

corpus 1) Drandom: the tokens of sentences are randomly sampled from the vocabulary of the model;
2) Dbook: the sentences are randomly sampled from BookCorpus [36], and 3) Dwiki: the sentences are
randomly sampled from the dev set of Wikitext-103.",4.1.,"The size of all different corpus is 1,000.",2022-06-06 05:51:12+00:00,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jin Xu'), arxiv.Result.Author('Xiaojiang Liu'), arxiv.Result.Author('Jianhao Yan'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Huayang Li'), arxiv.Result.Author('Jian Li')]","While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method."
7171,"We DITTO vs SG                      *63%

further study it when the model is trained with different

methods.","2, the DITTO vs UL-token                   *71%

sentence-level repetition has a self-reinforcement effect, DITTO vs UL-token+seq *62%

which is the core issue of sentence-level repetition.",The settings of experiments follow those in Sec.,2022-06-06 05:51:12+00:00,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jin Xu'), arxiv.Result.Author('Xiaojiang Liu'), arxiv.Result.Author('Jianhao Yan'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Huayang Li'), arxiv.Result.Author('Jian Li')]","While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method."
7172,"Given the corpus D, we can calculate
the average values of TP, IP and WR of the n-th repetitive sentence sn as

                     1         TP(sn),                             1            IP(sn),                    1                       WR(sn)
           TPn =                                         IPn =                                 WRn =
                    |D| s∈D                                         |D| s∈D                                               |D| s∈D

by enumerating all sentences s in D. To further study the effect of different corpus, we construct three

corpus 1) Drandom: the tokens of sentences are randomly sampled from the vocabulary of the model;
2) Dbook: the sentences are randomly sampled from BookCorpus [39], and 3) Dwiki: the sentences are
randomly sampled from the dev set of Wikitext-103.",4.1.,"The size of all different corpus is 1,000.",2022-06-06 05:51:12+00:00,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jin Xu'), arxiv.Result.Author('Xiaojiang Liu'), arxiv.Result.Author('Jianhao Yan'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Huayang Li'), arxiv.Result.Author('Jian Li')]","While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method."
7173,"We       DITTO vs SG                *63%

further study it when the model is trained with different

methods.","2, the          DITTO vs UL-token          *71%
sentence-level repetition has a self-reinforcement effect,     DITTO vs UL-token+seq      *62%
which is the core issue of sentence-level repetition.",The settings of experiments follow those in Sec.,2022-06-06 05:51:12+00:00,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jin Xu'), arxiv.Result.Author('Xiaojiang Liu'), arxiv.Result.Author('Jianhao Yan'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Huayang Li'), arxiv.Result.Author('Jian Li')]","While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method."
7174,"To
further study whether the length of pseudo data has an effect on overcoming the self-reinforcement
effect, we short the maximum input length of the repetitive sequence to 150 tokens for training,
named DITTO-short.","Length of Pseudo Repetitive Data DITTO constructs pseudo repetitive data by repeating sen-
tences until reaching the maximum input sequence length of the model (e.g., 1536 tokens).","Then, we measure the TP, IP, and WR metrics.",2022-06-06 05:51:12+00:00,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jin Xu'), arxiv.Result.Author('Xiaojiang Liu'), arxiv.Result.Author('Jianhao Yan'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Huayang Li'), arxiv.Result.Author('Jian Li')]","While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method."
7201,"model results) to facilitate further research into
                                            realistic clinical QA and QG.","We release this       are by nature brittle and have limited evidence of
                                            dataset (and all code to reproduce baseline         producing questions that medical professionals ask.","1                        Datasets such as emrQA (Pampari et al., 2018)
                                                                                                and emrKBQA (Raghavan et al., 2021) attempt to
                                       1 Introduction                                           simulate physician queries by deﬁning templates
                                                                                                derived from actual questions posed by physicians
                                       Physicians often query electronic health records         and then performing slot-ﬁlling with clinical enti-
                                       (EHR) to make fully informed decisions about pa-         ties.",2022-06-06 15:50:54+00:00,Learning to Ask Like a Physician,cs.CL,['cs.CL'],"[arxiv.Result.Author('Eric Lehman'), arxiv.Result.Author('Vladislav Lialin'), arxiv.Result.Author('Katelyn Y. Legaspi'), arxiv.Result.Author('Anne Janelle R. Sy'), arxiv.Result.Author('Patricia Therese S. Pile'), arxiv.Result.Author('Nicole Rose I. Alberto'), arxiv.Result.Author('Richard Raymund R. Ragasa'), arxiv.Result.Author('Corinna Victoria M. Puyat'), arxiv.Result.Author('Isabelle Rose I. Alberto'), arxiv.Result.Author('Pia Gabrielle I. Alfonso'), arxiv.Result.Author('Marianne Taliño'), arxiv.Result.Author('Dana Moukheiber'), arxiv.Result.Author('Byron C. Wallace'), arxiv.Result.Author('Anna Rumshisky'), arxiv.Result.Author('Jenifer J. Liang'), arxiv.Result.Author('Preethi Raghavan'), arxiv.Result.Author('Leo Anthony Celi'), arxiv.Result.Author('Peter Szolovits')]","Existing question answering (QA) datasets derived from electronic health
records (EHR) are artificially generated and consequently fail to capture
realistic physician information needs. We present Discharge Summary Clinical
Questions (DiSCQ), a newly curated question dataset composed of 2,000+
questions paired with the snippets of text (triggers) that prompted each
question. The questions are generated by medical experts from 100+ MIMIC-III
discharge summaries. We analyze this dataset to characterize the types of
information sought by medical experts. We also train baseline models for
trigger detection and question generation (QG), paired with unsupervised answer
retrieval over EHRs. Our baseline model is able to generate high quality
questions in over 62% of cases when prompted with human selected triggers. We
release this dataset (and all code to reproduce baseline model results) to
facilitate further research into realistic clinical QA and QG:
https://github.com/elehman16/discq."
7202,"heesht Sharma, Taewoon Kim, M Saiful Bari,
We release this dataset and our code to facilitate         Thibault Fevry, Zaid Alyafeai, Manan Dey, An-
further research into realistic clinical question an-      drea Santilli, Zhiqing Sun, Srulik Ben-David, Can-
swering and generation here.","We encourage           bert Webson, Colin Raffel, Nihal V. Nayak, Ab-
the community to improve on our baseline models.","wen Xu, Gunjan Chhablani, Han Wang, Jason Alan
                                                           Fries, Maged S. Al-shaibani, Shanya Sharma, Ur-
10 Acknowledgements                                        mish Thakker, Khalid Almubarak, Xiangru Tang,
                                                           Dragomir Radev, Mike Tian-Jian Jiang, and Alexan-
This work was supported and sponsored by the               der M. Rush.",2022-06-06 15:50:54+00:00,Learning to Ask Like a Physician,cs.CL,['cs.CL'],"[arxiv.Result.Author('Eric Lehman'), arxiv.Result.Author('Vladislav Lialin'), arxiv.Result.Author('Katelyn Y. Legaspi'), arxiv.Result.Author('Anne Janelle R. Sy'), arxiv.Result.Author('Patricia Therese S. Pile'), arxiv.Result.Author('Nicole Rose I. Alberto'), arxiv.Result.Author('Richard Raymund R. Ragasa'), arxiv.Result.Author('Corinna Victoria M. Puyat'), arxiv.Result.Author('Isabelle Rose I. Alberto'), arxiv.Result.Author('Pia Gabrielle I. Alfonso'), arxiv.Result.Author('Marianne Taliño'), arxiv.Result.Author('Dana Moukheiber'), arxiv.Result.Author('Byron C. Wallace'), arxiv.Result.Author('Anna Rumshisky'), arxiv.Result.Author('Jenifer J. Liang'), arxiv.Result.Author('Preethi Raghavan'), arxiv.Result.Author('Leo Anthony Celi'), arxiv.Result.Author('Peter Szolovits')]","Existing question answering (QA) datasets derived from electronic health
records (EHR) are artificially generated and consequently fail to capture
realistic physician information needs. We present Discharge Summary Clinical
Questions (DiSCQ), a newly curated question dataset composed of 2,000+
questions paired with the snippets of text (triggers) that prompted each
question. The questions are generated by medical experts from 100+ MIMIC-III
discharge summaries. We analyze this dataset to characterize the types of
information sought by medical experts. We also train baseline models for
trigger detection and question generation (QG), paired with unsupervised answer
retrieval over EHRs. Our baseline model is able to generate high quality
questions in over 62% of cases when prompted with human selected triggers. We
release this dataset (and all code to reproduce baseline model results) to
facilitate further research into realistic clinical QA and QG:
https://github.com/elehman16/discq."
7258,"We
   Beyond the expectations we may have, it re-                    require it to be sensitive to co-directionality (i.e.,
mains to be seen whether our proposed method-                     whether tt and et have similar directions) and rel-
ology is of actual use, i.e., whether is conducive                ative magnitude (whether tt is a major component
to further research.","To that end, we propose an importance metric
                                                                  to compare one of the terms tt to the total et .",The remainder of this article                of et ).,2022-06-07 18:24:46+00:00,How to Dissect a Muppet: The Structure of Transformer Embedding Spaces,cs.CL,['cs.CL'],"[arxiv.Result.Author('Timothee Mickus'), arxiv.Result.Author('Denis Paperno'), arxiv.Result.Author('Mathieu Constant')]","Pretrained embeddings based on the Transformer architecture have taken the
NLP community by storm. We show that they can mathematically be reframed as a
sum of vector factors and showcase how to use this reframing to study the
impact of each component. We provide evidence that multi-head attentions and
feed-forwards are not equally useful in all downstream applications, as well as
a quantitative overview of the effects of finetuning on the overall embedding
space. This approach allows us to draw connections to a wide range of previous
studies, from vector space anisotropy to attention weights."
7280,"We present a preliminary analysis of these factors and    distributions of two different languages (e.g., en-el), or the distance
how they could affect classification performance, leaving for future    between the shared test label distribution and the training label dis-
work a further study of the factors and how they interact.","estimated by the label distribution alignment between the train and     In both cases, we measure the distance between the training label
test subsets.","tribution of a language (e.g., test-en).",2022-06-08 10:02:11+00:00,Realistic Zero-Shot Cross-Lingual Transfer in Legal Topic Classification,cs.CL,['cs.CL'],"[arxiv.Result.Author('Stratos Xenouleas'), arxiv.Result.Author('Alexia Tsoukara'), arxiv.Result.Author('Giannis Panagiotakis'), arxiv.Result.Author('Ilias Chalkidis'), arxiv.Result.Author('Ion Androutsopoulos')]","We consider zero-shot cross-lingual transfer in legal topic classification
using the recent MultiEURLEX dataset. Since the original dataset contains
parallel documents, which is unrealistic for zero-shot cross-lingual transfer,
we develop a new version of the dataset without parallel documents. We use it
to show that translation-based methods vastly outperform cross-lingual
fine-tuning of multilingually pre-trained models, the best previous zero-shot
transfer method for MultiEURLEX. We also develop a bilingual teacher-student
zero-shot transfer approach, which exploits additional unlabeled documents of
the target language and performs better than a model fine-tuned directly on
labeled target language documents."
7341,"Hence, the discipline classification
results based on the pre-trained model may potentially be used to calculate the interdisciplinary nature
of the literature and the journals, which is worthy of further research.","Therefore, the discipline category of the
abstract in the dataset is an annotation result based on a mapping relationship, which does not mean
that the abstract of the paper is directly related to the discipline.","In terms of structure and functional recognition, the SsciBERT models presented in this paper
improved the state-of-the-art classification of sentences according to the BPMRC paradigm.",2022-06-09 13:49:04+00:00,SsciBERT: A Pre-trained Language Model for Social Science Texts,cs.CL,['cs.CL'],"[arxiv.Result.Author('Si Shen'), arxiv.Result.Author('Jiangfeng Liu'), arxiv.Result.Author('Litao Lin'), arxiv.Result.Author('Ying Huang'), arxiv.Result.Author('Lin Zhang'), arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Yutong Feng'), arxiv.Result.Author('Dongbo Wang')]","The academic literature of social sciences records human civilization and
studies human social problems. With its large-scale growth, the ways to quickly
find existing research on relevant issues have become an urgent demand for
researchers. Previous studies, such as SciBERT, have shown that pre-training
using domain-specific texts can improve the performance of natural language
processing tasks. However, the pre-trained language model for social sciences
is not available so far. In light of this, the present research proposes a
pre-trained model based on the abstracts published in the Social Science
Citation Index (SSCI) journals. The models, which are available on GitHub
(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent
performance on discipline classification, abstract structure-function
recognition, and named entity recognition tasks with the social sciences
literature."
7430,"This can be attributed to the fact that the  6.1 Out-of-Domain Evaluator
distribution of the dataset was skewed by data aug-
mentation, and further study is needed.","of DADE Best vs DE Greedy and DADE Best vs
DE Best show the responses of the DA generator          6 Analysis
were not rated better than the responses of the DE
generator.","Example         In the experiments in Section 5, each evaluator of
responses generated by the proposed system are          DE and DA was trained using the human evalu-
shown in Table 7.                                       ations of the corresponding generator responses
                                                        for each of DE and DA.",2022-06-10 08:22:22+00:00,"Generate, Evaluate, and Select: A Dialogue System with a Response Evaluator for Diversity-Aware Response Generation",cs.CL,['cs.CL'],"[arxiv.Result.Author('Ryoma Sakaeda'), arxiv.Result.Author('Daisuke Kawahara')]","We aim to overcome the lack of diversity in responses of current dialogue
systems and to develop a dialogue system that is engaging as a conversational
partner. We propose a generator-evaluator model that evaluates multiple
responses generated by a response generator and selects the best response by an
evaluator. By generating multiple responses, we obtain diverse responses. We
conduct human evaluations to compare the output of the proposed system with
that of a baseline system. The results of the human evaluations showed that the
proposed system's responses were often judged to be better than the baseline
system's, and indicated the effectiveness of the proposed method."
7456,"This will
facilitate further research on the appraisal generalizability to various domains.","Further, we prepare our implementation
for further use and will make it available as easy-to-use pre-trained models.","We would
like to note that the collected data includes variables that have not all been fully analyzed
in this paper.",2022-06-10 17:20:17+00:00,"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",cs.CL,['cs.CL'],"[arxiv.Result.Author('Enrica Troiano'), arxiv.Result.Author('Laura Oberländer'), arxiv.Result.Author('Roman Klinger')]","The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An important observation
for natural language processing is that emotions can be communicated implicitly
by referring to events alone, appealing to an empathetic, intersubjective
understanding of events, even without explicitly mentioning an emotion name. In
psychology, the class of emotion theories known as appraisal theories aims at
explaining the link between events and emotions. Appraisals can be formalized
as variables that measure a cognitive evaluation by people living through an
event that they consider relevant. They include the assessment if an event is
novel, if the person considers themselves to be responsible, if it is in line
with the own goals, and many others. Such appraisals explain which emotions are
developed based on an event, e.g., that a novel situation can induce surprise
or one with uncertain consequences could evoke fear. We analyze the suitability
of appraisal theories for emotion analysis in text with the goal of
understanding if appraisal concepts can reliably be reconstructed by
annotators, if they can be predicted by text classifiers, and if appraisal
concepts help to identify emotion categories. To achieve that, we compile a
corpus by asking people to textually describe events that triggered particular
emotions and to disclose their appraisals. Then, we ask readers to reconstruct
emotions and appraisals from the text. This setup allows us to measure if
emotions and appraisals can be recovered purely from text and provides a human
baseline to judge model's performance measures. Our comparison of text
classification methods to human annotators shows that both can reliably detect
emotions and appraisals with similar performance. We further show that
appraisal concepts improve the categorization of emotions in text."
7457,"This will
facilitate further research on the appraisal generalizability to various domains.","Further, we prepare our implementation
for further use and will make it available as easy-to-use pre-trained models.","We would
like to note that the collected data includes variables that have not all been fully analyzed
in this paper.",2022-06-10 17:20:17+00:00,"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",cs.CL,['cs.CL'],"[arxiv.Result.Author('Enrica Troiano'), arxiv.Result.Author('Laura Oberländer'), arxiv.Result.Author('Roman Klinger')]","The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An important observation
for natural language processing is that emotions can be communicated implicitly
by referring to events alone, appealing to an empathetic, intersubjective
understanding of events, even without explicitly mentioning an emotion name. In
psychology, the class of emotion theories known as appraisal theories aims at
explaining the link between events and emotions. Appraisals can be formalized
as variables that measure a cognitive evaluation by people living through an
event that they consider relevant. They include the assessment if an event is
novel, if the person considers themselves to be responsible, if it is in line
with the own goals, and many others. Such appraisals explain which emotions are
developed based on an event, e.g., that a novel situation can induce surprise
or one with uncertain consequences could evoke fear. We analyze the suitability
of appraisal theories for emotion analysis in text with the goal of
understanding if appraisal concepts can reliably be reconstructed by
annotators, if they can be predicted by text classifiers, and if appraisal
concepts help to identify emotion categories. To achieve that, we compile a
corpus by asking people to textually describe events that triggered particular
emotions and to disclose their appraisals. Then, we ask readers to reconstruct
emotions and appraisals from the text. This setup allows us to measure if
emotions and appraisals can be recovered purely from text and provides a human
baseline to judge model's performance measures. Our comparison of text
classification methods to human annotators shows that both can reliably detect
emotions and appraisals with similar performance. We further show that
appraisal concepts improve the categorization of emotions in text."
7458,"Overall, appraisal theories proved to be a valid framework
for further research into the modeling of emotions in text.","Finally, we saw that
appraisals help to predict certain emotion categories, as they correct mistakes of a system
relying on text alone (RQ4).",We make crowd-enVENT publicly available.,2022-06-10 17:20:17+00:00,"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",cs.CL,['cs.CL'],"[arxiv.Result.Author('Enrica Troiano'), arxiv.Result.Author('Laura Oberländer'), arxiv.Result.Author('Roman Klinger')]","The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An observation for NLP is
that emotions can be communicated implicitly by referring to events, appealing
to an empathetic, intersubjective understanding of events, even without
explicitly mentioning an emotion name. In psychology, the class of emotion
theories known as appraisal theories aims at explaining the link between events
and emotions. Appraisals can be formalized as variables that measure a
cognitive evaluation by people living through an event that they consider
relevant. They include the assessment if an event is novel, if the person
considers themselves to be responsible, if it is in line with the own goals,
and many others. Such appraisals explain which emotions are developed based on
an event, e.g., that a novel situation can induce surprise or one with
uncertain consequences could evoke fear. We analyze the suitability of
appraisal theories for emotion analysis in text with the goal of understanding
if appraisal concepts can reliably be reconstructed by annotators, if they can
be predicted by text classifiers, and if appraisal concepts help to identify
emotion categories. To achieve that, we compile a corpus by asking people to
textually describe events that triggered particular emotions and to disclose
their appraisals. Then, we ask readers to reconstruct emotions and appraisals
from the text. This setup allows us to measure if emotions and appraisals can
be recovered purely from text and provides a human baseline. Our comparison of
text classification methods to human annotators shows that both can reliably
detect emotions and appraisals with similar performance. Therefore, appraisals
constitute an alternative computational emotion analysis paradigm and further
improve the categorization of emotions in text with joint models."
7459,"Overall, appraisal theories proved to be a valid framework
for further research into the modeling of emotions in text.","Finally, we saw that
appraisals help to predict certain emotion categories, as they correct mistakes of a system
relying on text alone (RQ4).",We make crowd-enVENT publicly available.,2022-06-10 17:20:17+00:00,"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",cs.CL,['cs.CL'],"[arxiv.Result.Author('Enrica Troiano'), arxiv.Result.Author('Laura Oberländer'), arxiv.Result.Author('Roman Klinger')]","The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An observation for NLP is
that emotions can be communicated implicitly by referring to events, appealing
to an empathetic, intersubjective understanding of events, even without
explicitly mentioning an emotion name. In psychology, the class of emotion
theories known as appraisal theories aims at explaining the link between events
and emotions. Appraisals can be formalized as variables that measure a
cognitive evaluation by people living through an event that they consider
relevant. They include the assessment if an event is novel, if the person
considers themselves to be responsible, if it is in line with the own goals,
and many others. Such appraisals explain which emotions are developed based on
an event, e.g., that a novel situation can induce surprise or one with
uncertain consequences could evoke fear. We analyze the suitability of
appraisal theories for emotion analysis in text with the goal of understanding
if appraisal concepts can reliably be reconstructed by annotators, if they can
be predicted by text classifiers, and if appraisal concepts help to identify
emotion categories. To achieve that, we compile a corpus by asking people to
textually describe events that triggered particular emotions and to disclose
their appraisals. Then, we ask readers to reconstruct emotions and appraisals
from the text. This setup allows us to measure if emotions and appraisals can
be recovered purely from text and provides a human baseline. Our comparison of
text classification methods to human annotators shows that both can reliably
detect emotions and appraisals with similar performance. Therefore, appraisals
constitute an alternative computational emotion analysis paradigm and further
improve the categorization of emotions in text with joint models."
7460,"Overall, appraisal theories proved to be a valid framework
for further research into the modeling of emotions in text.","Finally, we saw that
appraisals help to predict certain emotion categories, as they correct mistakes of a system
relying on text alone (RQ4).",We make crowd-enVENT publicly available.,2022-06-10 17:20:17+00:00,"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",cs.CL,['cs.CL'],"[arxiv.Result.Author('Enrica Troiano'), arxiv.Result.Author('Laura Oberländer'), arxiv.Result.Author('Roman Klinger')]","The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An observation for NLP is
that emotions can be communicated implicitly by referring to events, appealing
to an empathetic, intersubjective understanding of events, even without
explicitly mentioning an emotion name. In psychology, the class of emotion
theories known as appraisal theories aims at explaining the link between events
and emotions. Appraisals can be formalized as variables that measure a
cognitive evaluation by people living through an event that they consider
relevant. They include the assessment if an event is novel, if the person
considers themselves to be responsible, if it is in line with the own goals,
and many others. Such appraisals explain which emotions are developed based on
an event, e.g., that a novel situation can induce surprise or one with
uncertain consequences could evoke fear. We analyze the suitability of
appraisal theories for emotion analysis in text with the goal of understanding
if appraisal concepts can reliably be reconstructed by annotators, if they can
be predicted by text classifiers, and if appraisal concepts help to identify
emotion categories. To achieve that, we compile a corpus by asking people to
textually describe events that triggered particular emotions and to disclose
their appraisals. Then, we ask readers to reconstruct emotions and appraisals
from the text. This setup allows us to measure if emotions and appraisals can
be recovered purely from text and provides a human baseline. Our comparison of
text classification methods to human annotators shows that both can reliably
detect emotions and appraisals with similar performance. Therefore, appraisals
constitute an alternative computational emotion analysis paradigm and further
improve the categorization of emotions in text with joint models."
7461,"Overall, appraisal theories proved to be a valid framework
for further research into the modeling of emotions in text.","Finally, we saw that
appraisals help to predict certain emotion categories, as they correct mistakes of a system
relying on text alone (RQ4).",We make crowd-enVENT publicly available.,2022-06-10 17:20:17+00:00,"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction",cs.CL,['cs.CL'],"[arxiv.Result.Author('Enrica Troiano'), arxiv.Result.Author('Laura Oberländer'), arxiv.Result.Author('Roman Klinger')]","The most prominent tasks in emotion analysis are to assign emotions to texts
and to understand how emotions manifest in language. An observation for NLP is
that emotions can be communicated implicitly by referring to events, appealing
to an empathetic, intersubjective understanding of events, even without
explicitly mentioning an emotion name. In psychology, the class of emotion
theories known as appraisal theories aims at explaining the link between events
and emotions. Appraisals can be formalized as variables that measure a
cognitive evaluation by people living through an event that they consider
relevant. They include the assessment if an event is novel, if the person
considers themselves to be responsible, if it is in line with the own goals,
and many others. Such appraisals explain which emotions are developed based on
an event, e.g., that a novel situation can induce surprise or one with
uncertain consequences could evoke fear. We analyze the suitability of
appraisal theories for emotion analysis in text with the goal of understanding
if appraisal concepts can reliably be reconstructed by annotators, if they can
be predicted by text classifiers, and if appraisal concepts help to identify
emotion categories. To achieve that, we compile a corpus by asking people to
textually describe events that triggered particular emotions and to disclose
their appraisals. Then, we ask readers to reconstruct emotions and appraisals
from the text. This setup allows us to measure if emotions and appraisals can
be recovered purely from text and provides a human baseline. Our comparison of
text classification methods to human annotators shows that both can reliably
detect emotions and appraisals with similar performance. Therefore, appraisals
constitute an alternative computational emotion analysis paradigm and further
improve the categorization of emotions in text with joint models."
7505,"A further study [20] reveals
bottom plot zooms in the index interval between 10 and 100.        the sensitivity of BERT ﬁne-tuning to random seeds where the
                                                                   randomness is introduced by the shufﬂe of data order and the
                        VI.",[19].,"RELATED WORK                           random initialization of the task-speciﬁc layer by conducting
                                                                   extensive empirical analysis.",2022-06-12 04:42:49+00:00,Fine-tuning Pre-trained Language Models with Noise Stability Regularization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hang Hua'), arxiv.Result.Author('Xingjian Li'), arxiv.Result.Author('Dejing Dou'), arxiv.Result.Author('Cheng-Zhong Xu'), arxiv.Result.Author('Jiebo Luo')]","The advent of large-scale pre-trained language models has contributed greatly
to the recent progress in natural language processing. Many state-of-the-art
language models are first trained on a large text corpus and then fine-tuned on
downstream tasks. Despite its recent success and wide adoption, fine-tuning a
pre-trained language model often suffers from overfitting, which leads to poor
generalizability due to the extremely high complexity of the model and the
limited training samples from downstream tasks. To address this problem, we
propose a novel and effective fine-tuning framework, named Layerwise Noise
Stability Regularization (LNSR). Specifically, we propose to inject the
standard Gaussian noise or In-manifold noise and regularize hidden
representations of the fine-tuned model. We first provide theoretical analyses
to support the efficacy of our method. We then demonstrate the advantages of
the proposed method over other state-of-the-art algorithms including L2-SP,
Mixout and SMART. While these previous works only verify the effectiveness of
their methods on relatively simple text classification tasks, we also verify
the effectiveness of our method on question answering tasks, where the target
problem is much more difficult and more training examples are available.
Furthermore, extensive experimental results indicate that the proposed
algorithm can not only enhance the in-domain performance of the language models
but also improve the domain generalization performance on out-of-domain data."
7655,"Identifying context-
appropriate and effective disclosure mechanisms for the use of AI in communication is an urgent question
that requires further research (43).","Prior research also
shows that typical notice and consent disclosures are often ignored by users (42).","In some cases, though, our results suggest that it may be possible to create language models that are
self-disclosing by design: Rather than training AI language systems that imitate human language, AI
systems could be optimized to fulfill their specific communicative function while preserving the validity of
human intuitive judgment (34).",2022-06-15 03:18:56+00:00,Human Heuristics for AI-Generated Language Are Flawed,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.HC']","[arxiv.Result.Author('Maurice Jakesch'), arxiv.Result.Author('Jeffrey Hancock'), arxiv.Result.Author('Mor Naaman')]","Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems produce smart replies,
autocompletes, and translations. AI-generated language is often not identified
as such but presented as language written by humans, raising concerns about
novel forms of deception and manipulation. Here, we study how humans discern
whether verbal self-presentations, one of the most personal and consequential
forms of language, were generated by AI. In six experiments, participants (N =
4,600) were unable to detect self-presentations generated by state-of-the-art
AI language models in professional, hospitality, and dating contexts. A
computational analysis of language features shows that human judgments of
AI-generated language are handicapped by intuitive but flawed heuristics such
as associating first-person pronouns, spontaneous wording, or family topics
with human-written language. We experimentally demonstrate that these
heuristics make human judgment of AI-generated language predictable and
manipulable, allowing AI systems to produce language perceived as more human
than human. We discuss solutions, such as AI accents, to reduce the deceptive
potential of language generated by AI, limiting the subversion of human
intuition."
7679,"Moreover, once an ability is discovered, further research may make the ability available for smaller scale
models.","(2022c) that a computationally-eﬃcient continued pre-training stage on a mixture-of-denoisers
objective (Tay et al., 2022a) enabled emergent performance on several BIG-Bench tasks.","Consider the nascent direction of enabling language models to follow natural language instructions
describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022, inter alia).",2022-06-15 17:32:01+00:00,Emergent Abilities of Large Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Yi Tay'), arxiv.Result.Author('Rishi Bommasani'), arxiv.Result.Author('Colin Raffel'), arxiv.Result.Author('Barret Zoph'), arxiv.Result.Author('Sebastian Borgeaud'), arxiv.Result.Author('Dani Yogatama'), arxiv.Result.Author('Maarten Bosma'), arxiv.Result.Author('Denny Zhou'), arxiv.Result.Author('Donald Metzler'), arxiv.Result.Author('Ed H. Chi'), arxiv.Result.Author('Tatsunori Hashimoto'), arxiv.Result.Author('Oriol Vinyals'), arxiv.Result.Author('Percy Liang'), arxiv.Result.Author('Jeff Dean'), arxiv.Result.Author('William Fedus')]","Scaling up language models has been shown to predictably improve performance
and sample efficiency on a wide range of downstream tasks. This paper instead
discusses an unpredictable phenomenon that we refer to as emergent abilities of
large language models. We consider an ability to be emergent if it is not
present in smaller models but is present in larger models. Thus, emergent
abilities cannot be predicted simply by extrapolating the performance of
smaller models. The existence of such emergence implies that additional scaling
could further expand the range of capabilities of language models."
7688,"are publicly available as a research resource for further study in
                                                                         conversation.","The
on conversational data substantially improves personal entity link-      collected annotations and our EL tool with detailed instructions
ing performance (S2E vs. S2Econv results).","4.2 Concept and NE Linking
                                                                         6 https://huggingface.co/bert- base- uncased
   Experimental Setup.",2022-06-15 22:32:29+00:00,"Personal Entity, Concept, and Named Entity Linking in Conversations",cs.CL,"['cs.CL', 'cs.IR', 'H.3']","[arxiv.Result.Author('Hideaki Joko'), arxiv.Result.Author('Faegheh Hasibi')]","Building conversational agents that can have natural and knowledge-grounded
interactions with humans requires understanding user utterances. Entity Linking
(EL) is an effective and widely used method for understanding natural language
text and connecting it to external knowledge. It is, however, shown that
existing EL methods developed for annotating documents are suboptimal for
conversations, where personal entities (e.g., ""my cars"") and concepts are
essential for understanding user utterances. In this paper, we introduce a
collection and a tool for entity linking in conversations. We collect EL
annotations for 1327 conversational utterances, consisting of links to named
entities, concepts, and personal entities. The dataset is used for training our
toolkit for conversational entity linking, CREL. Unlike existing EL methods,
CREL is developed to identify both named entities and concepts. It also
utilizes coreference resolution techniques to identify personal entities and
references to the explicit entity mentions in the conversations. We compare
CREL with state-of-the-art techniques and show that it outperforms all existing
baselines."
7689,"It also indicates that, fine-tuning     annotations and our EL tool with detailed instructions are publicly
on conversational data substantially improves personal entity link-         available as a research resource for further study in conversation.","The collected
datasets outperforms all baselines.",ing performance (S2E vs. S2Econv results).,2022-06-15 22:32:29+00:00,"Personal Entity, Concept, and Named Entity Linking in Conversations",cs.CL,"['cs.CL', 'cs.IR', 'H.3']","[arxiv.Result.Author('Hideaki Joko'), arxiv.Result.Author('Faegheh Hasibi')]","Building conversational agents that can have natural and knowledge-grounded
interactions with humans requires understanding user utterances. Entity Linking
(EL) is an effective and widely used method for understanding natural language
text and connecting it to external knowledge. It is, however, shown that
existing EL methods developed for annotating documents are suboptimal for
conversations, where personal entities (e.g., ""my cars"") and concepts are
essential for understanding user utterances. In this paper, we introduce a
collection and a tool for entity linking in conversations. We collect EL
annotations for 1327 conversational utterances, consisting of links to named
entities, concepts, and personal entities. The dataset is used for training our
toolkit for conversational entity linking, CREL. Unlike existing EL methods,
CREL is developed to identify both named entities and concepts. It also
utilizes coreference resolution techniques to identify personal entities and
references to the explicit entity mentions in the conversations. We compare
CREL with state-of-the-art techniques and show that it outperforms all existing
baselines."
7747,"We publicly release the CookDial dataset, compris-
                                            ing rich annotations of both dialogs and recipe documents, to stimulate
                                            further research on domain-speciﬁc document-grounded dialog systems.","For each
                                            of these tasks, we develop a neural baseline model, which we evaluate on
                                            the CookDial dataset.","Keywords: dialog system, procedural knowledge, neural network modeling

                                                                                       1
                          Springer Nature 2021 LATEX template

2 CookDial: A dataset for task-oriented dialogs grounded in procedural documents

1 Introduction

The last decade has seen a surge of work dedicated to building conversational
agents (CA) via annual challenges (e.g., Dialog System Technology Challenges
[1]) or benchmark datasets (e.g., WoZ 2.0 [2], MultiWoZ [3], SGD [4]).",2022-06-17 12:23:53+00:00,CookDial: A dataset for task-oriented dialogs grounded in procedural documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yiwei Jiang'), arxiv.Result.Author('Klim Zaporojets'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Thomas Demeester'), arxiv.Result.Author('Chris Develder')]","This work presents a new dialog dataset, CookDial, that facilitates research
on task-oriented dialog systems with procedural knowledge understanding. The
corpus contains 260 human-to-human task-oriented dialogs in which an agent,
given a recipe document, guides the user to cook a dish. Dialogs in CookDial
exhibit two unique features: (i) procedural alignment between the dialog flow
and supporting document; (ii) complex agent decision-making that involves
segmenting long sentences, paraphrasing hard instructions and resolving
coreference in the dialog context. In addition, we identify three challenging
(sub)tasks in the assumed task-oriented dialog system: (1) User Question
Understanding, (2) Agent Action Frame Prediction, and (3) Agent Response
Generation. For each of these tasks, we develop a neural baseline model, which
we evaluate on the CookDial dataset. We publicly release the CookDial dataset,
comprising rich annotations of both dialogs and recipe documents, to stimulate
further research on domain-specific document-grounded dialog systems."
7748,"To facilitate further research on procedural assistance
dialog systems, we focus on establishing an exemplary dialog dataset and
baseline solutions for a conversational agent grounded in textual procedural
documents.","[12], which is based on ﬂowcharts
describing the procedure.","Thus, our work relates to aforementioned document-grounded
systems.",2022-06-17 12:23:53+00:00,CookDial: A dataset for task-oriented dialogs grounded in procedural documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yiwei Jiang'), arxiv.Result.Author('Klim Zaporojets'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Thomas Demeester'), arxiv.Result.Author('Chris Develder')]","This work presents a new dialog dataset, CookDial, that facilitates research
on task-oriented dialog systems with procedural knowledge understanding. The
corpus contains 260 human-to-human task-oriented dialogs in which an agent,
given a recipe document, guides the user to cook a dish. Dialogs in CookDial
exhibit two unique features: (i) procedural alignment between the dialog flow
and supporting document; (ii) complex agent decision-making that involves
segmenting long sentences, paraphrasing hard instructions and resolving
coreference in the dialog context. In addition, we identify three challenging
(sub)tasks in the assumed task-oriented dialog system: (1) User Question
Understanding, (2) Agent Action Frame Prediction, and (3) Agent Response
Generation. For each of these tasks, we develop a neural baseline model, which
we evaluate on the CookDial dataset. We publicly release the CookDial dataset,
comprising rich annotations of both dialogs and recipe documents, to stimulate
further research on domain-specific document-grounded dialog systems."
7749,"We thus aim to stimulate further research on procedural conversational agent
systems, as a subﬁeld of document-grounded dialog models.","• We identify three challenging tasks (Section 5) and propose baseline
      models for each (Section 6), which we evaluate on the CookDial dataset:
      User Question Understanding (Task I), Agent Action Frame Prediction
      (Task II) and Agent Response Generation (Task III).","Table 1 Summary of the characteristics of CookDial compared to other
document-grounded dialog datasets.",2022-06-17 12:23:53+00:00,CookDial: A dataset for task-oriented dialogs grounded in procedural documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yiwei Jiang'), arxiv.Result.Author('Klim Zaporojets'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Thomas Demeester'), arxiv.Result.Author('Chris Develder')]","This work presents a new dialog dataset, CookDial, that facilitates research
on task-oriented dialog systems with procedural knowledge understanding. The
corpus contains 260 human-to-human task-oriented dialogs in which an agent,
given a recipe document, guides the user to cook a dish. Dialogs in CookDial
exhibit two unique features: (i) procedural alignment between the dialog flow
and supporting document; (ii) complex agent decision-making that involves
segmenting long sentences, paraphrasing hard instructions and resolving
coreference in the dialog context. In addition, we identify three challenging
(sub)tasks in the assumed task-oriented dialog system: (1) User Question
Understanding, (2) Agent Action Frame Prediction, and (3) Agent Response
Generation. For each of these tasks, we develop a neural baseline model, which
we evaluate on the CookDial dataset. We publicly release the CookDial dataset,
comprising rich annotations of both dialogs and recipe documents, to stimulate
further research on domain-specific document-grounded dialog systems."
7750,"We publicly release the dataset and the baseline models
to spur further research.","From the CA perspective, we identi-
ﬁed three major tasks, for which we also established baseline solutions: (i) user
question understanding, (ii) agent action frame prediction, and (iii) agent
response generation.","In terms of next steps, we highlight three directions
that will guide our future work.",2022-06-17 12:23:53+00:00,CookDial: A dataset for task-oriented dialogs grounded in procedural documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yiwei Jiang'), arxiv.Result.Author('Klim Zaporojets'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Thomas Demeester'), arxiv.Result.Author('Chris Develder')]","This work presents a new dialog dataset, CookDial, that facilitates research
on task-oriented dialog systems with procedural knowledge understanding. The
corpus contains 260 human-to-human task-oriented dialogs in which an agent,
given a recipe document, guides the user to cook a dish. Dialogs in CookDial
exhibit two unique features: (i) procedural alignment between the dialog flow
and supporting document; (ii) complex agent decision-making that involves
segmenting long sentences, paraphrasing hard instructions and resolving
coreference in the dialog context. In addition, we identify three challenging
(sub)tasks in the assumed task-oriented dialog system: (1) User Question
Understanding, (2) Agent Action Frame Prediction, and (3) Agent Response
Generation. For each of these tasks, we develop a neural baseline model, which
we evaluate on the CookDial dataset. We publicly release the CookDial dataset,
comprising rich annotations of both dialogs and recipe documents, to stimulate
further research on domain-specific document-grounded dialog systems."
7763,"This result could
                                                                                                           provide support to further research in automatic behavioural
                                        Research interest in automatic behavioral analysis has been in-    analysis.",Introduction                                  liance is manifested in speech and language.,creasing in the past few years[1].,2022-06-17 15:32:34+00:00,What can Speech and Language Tell us About the Working Alliance in Psychotherapy,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Sebastian P. Bayerl'), arxiv.Result.Author('Gabriel Roccabruna'), arxiv.Result.Author('Shammur Absar Chowdhury'), arxiv.Result.Author('Tommaso Ciulli'), arxiv.Result.Author('Morena Danieli'), arxiv.Result.Author('Korbinian Riedhammer'), arxiv.Result.Author('Giuseppe Riccardi')]","We are interested in the problem of conversational analysis and its
application to the health domain. Cognitive Behavioral Therapy is a structured
approach in psychotherapy, allowing the therapist to help the patient to
identify and modify the malicious thoughts, behavior, or actions. This
cooperative effort can be evaluated using the Working Alliance Inventory
Observer-rated Shortened - a 12 items inventory covering task, goal, and
relationship - which has a relevant influence on therapeutic outcomes. In this
work, we investigate the relation between this alliance inventory and the
spoken conversations (sessions) between the patient and the psychotherapist. We
have delivered eight weeks of e-therapy, collected their audio and video call
sessions, and manually transcribed them. The spoken conversations have been
annotated and evaluated with WAI ratings by professional therapists. We have
investigated speech and language features and their association with WAI items.
The feature types include turn dynamics, lexical entrainment, and
conversational descriptors extracted from the speech and language signals. Our
findings provide strong evidence that a subset of these features are strong
indicators of working alliance. To the best of our knowledge, this is the first
and a novel study to exploit speech and language for characterising working
alliance."
7764,"This result could
                                                                                                           provide support to further research in automatic behavioural
                                        Research interest in automatic behavioral analysis has been in-    analysis.",Introduction                                  liance is manifested in speech and language.,creasing in the past few years[1].,2022-06-17 15:32:34+00:00,What can Speech and Language Tell us About the Working Alliance in Psychotherapy,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Sebastian P. Bayerl'), arxiv.Result.Author('Gabriel Roccabruna'), arxiv.Result.Author('Shammur Absar Chowdhury'), arxiv.Result.Author('Tommaso Ciulli'), arxiv.Result.Author('Morena Danieli'), arxiv.Result.Author('Korbinian Riedhammer'), arxiv.Result.Author('Giuseppe Riccardi')]","We are interested in the problem of conversational analysis and its
application to the health domain. Cognitive Behavioral Therapy is a structured
approach in psychotherapy, allowing the therapist to help the patient to
identify and modify the malicious thoughts, behavior, or actions. This
cooperative effort can be evaluated using the Working Alliance Inventory
Observer-rated Shortened - a 12 items inventory covering task, goal, and
relationship - which has a relevant influence on therapeutic outcomes. In this
work, we investigate the relation between this alliance inventory and the
spoken conversations (sessions) between the patient and the psychotherapist. We
have delivered eight weeks of e-therapy, collected their audio and video call
sessions, and manually transcribed them. The spoken conversations have been
annotated and evaluated with WAI ratings by professional therapists. We have
investigated speech and language features and their association with WAI items.
The feature types include turn dynamics, lexical entrainment, and
conversational descriptors extracted from the speech and language signals. Our
findings provide strong evidence that a subset of these features are strong
indicators of working alliance. To the best of our knowledge, this is the first
and a novel study to exploit speech and language for characterising working
alliance."
7800,"We hope that the
                                        prepared dataset and baselines will help to foster further research on argument mining for the Russian
                                        language.","This system showed the following results: for the stance detection task
                                        an F1-score of 0.6968, for the argument classification task an F1-score of 0.7404.","Keywords: Argumentation Mining, Stance Detection, Premise Classification, COVID-19
                                            DOI: 10.28995/2075-7182-2022-21-333-348

                                                RuArg-2022: соревнование по анализу аргументации

                                                  Евгений Котельников1, Наталья Лукашевич2, Ирина Никишина3, и
                                                                                 Александр Панченко1

                                                                     1Вятский государственный университет,
                                                   2Московский государственный университет им.",2022-06-18 17:13:37+00:00,RuArg-2022: Argument Mining Evaluation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Evgeny Kotelnikov'), arxiv.Result.Author('Natalia Loukachevitch'), arxiv.Result.Author('Irina Nikishina'), arxiv.Result.Author('Alexander Panchenko')]","Argumentation analysis is a field of computational linguistics that studies
methods for extracting arguments from texts and the relationships between them,
as well as building argumentation structure of texts. This paper is a report of
the organizers on the first competition of argumentation analysis systems
dealing with Russian language texts within the framework of the Dialogue
conference. During the competition, the participants were offered two tasks:
stance detection and argument classification. A corpus containing 9,550
sentences (comments on social media posts) on three topics related to the
COVID-19 pandemic (vaccination, quarantine, and wearing masks) was prepared,
annotated, and used for training and testing. The system that won the first
place in both tasks used the NLI (Natural Language Inference) variant of the
BERT architecture, automatic translation into English to apply a specialized
BERT model, retrained on Twitter posts discussing COVID-19, as well as
additional masking of target entities. This system showed the following
results: for the stance detection task an F1-score of 0.6968, for the argument
classification task an F1-score of 0.7404. We hope that the prepared dataset
and baselines will help to foster further research on argument mining for the
Russian language."
7801,"All the data and codes are available online.14 We hope that these materials will help
to foster further research and developments in the area of argument mining for the Russian language.","According to the provided results, we see that the argument mining is a feasible task, especially on the
COVID-19 dataset.","As future work, we see it promising to explore more complex argument mining setups such as sequence
tagging (Chernodub et al., 2019) or information retrieval (Bondarenko et al., 2020).",2022-06-18 17:13:37+00:00,RuArg-2022: Argument Mining Evaluation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Evgeny Kotelnikov'), arxiv.Result.Author('Natalia Loukachevitch'), arxiv.Result.Author('Irina Nikishina'), arxiv.Result.Author('Alexander Panchenko')]","Argumentation analysis is a field of computational linguistics that studies
methods for extracting arguments from texts and the relationships between them,
as well as building argumentation structure of texts. This paper is a report of
the organizers on the first competition of argumentation analysis systems
dealing with Russian language texts within the framework of the Dialogue
conference. During the competition, the participants were offered two tasks:
stance detection and argument classification. A corpus containing 9,550
sentences (comments on social media posts) on three topics related to the
COVID-19 pandemic (vaccination, quarantine, and wearing masks) was prepared,
annotated, and used for training and testing. The system that won the first
place in both tasks used the NLI (Natural Language Inference) variant of the
BERT architecture, automatic translation into English to apply a specialized
BERT model, retrained on Twitter posts discussing COVID-19, as well as
additional masking of target entities. This system showed the following
results: for the stance detection task an F1-score of 0.6968, for the argument
classification task an F1-score of 0.7404. We hope that the prepared dataset
and baselines will help to foster further research on argument mining for the
Russian language."
7802,"The fifth
        section provides conclusions and suggests directions for further research.",In the fourth section the experimental results are presented and discussed.,"2 Previous work

        Language models based on the Transformer architecture [28] have become a key technology for solving
        natural language processing problems, including automatic text summarization [15].",2022-06-18 17:28:04+00:00,Automatic Summarization of Russian Texts: Comparison of Extractive and Abstractive Methods,cs.CL,['cs.CL'],"[arxiv.Result.Author('Valeriya Goloviznina'), arxiv.Result.Author('Evgeny Kotelnikov')]","The development of large and super-large language models, such as GPT-3, T5,
Switch Transformer, ERNIE, etc., has significantly improved the performance of
text generation. One of the important research directions in this area is the
generation of texts with arguments. The solution of this problem can be used in
business meetings, political debates, dialogue systems, for preparation of
student essays. One of the main domains for these applications is the economic
sphere. The key problem of the argument text generation for the Russian
language is the lack of annotated argumentation corpora. In this paper, we use
translated versions of the Argumentative Microtext, Persuasive Essays and UKP
Sentential corpora to fine-tune RuBERT model. Further, this model is used to
annotate the corpus of economic news by argumentation. Then the annotated
corpus is employed to fine-tune the ruGPT-3 model, which generates argument
texts. The results show that this approach improves the accuracy of the
argument generation by more than 20 percentage points (63.2% vs. 42.5%)
compared to the original ruGPT-3 model."
7803,"In further research, we intend to compare the considered methods on the Russian-language part of
the WikiLingva corpus [11], formed on WikiHow articles, which differ in their structure from news
articles.","During the analysis of summaries obtained by different methods, we identified several features:
    • mBART has the lowest level of abstractiveness compared to ruGPT3Large and ruT5-large;
    • ruGPT3Small and ruGPT3Large generate summaries that are closest in length to the reference

         ones, but often does not complete them and makes errors;
    • ruT5-base and ruT5-large summaries are also close to the reference length, rather abstract and

         contain fewer errors than summaries of ruGPT3Small and ruGPT3Large;
    • TextRank more evenly selects sentences from the text;
    • LexRank tends to select sentences from the beginning of the text.","Acknowledgements

This work was supported by Russian Science Foundation, project № 22-21-00885,
https://rscf.ru/en/project/22-21-00885.",2022-06-18 17:28:04+00:00,Automatic Summarization of Russian Texts: Comparison of Extractive and Abstractive Methods,cs.CL,['cs.CL'],"[arxiv.Result.Author('Valeriya Goloviznina'), arxiv.Result.Author('Evgeny Kotelnikov')]","The development of large and super-large language models, such as GPT-3, T5,
Switch Transformer, ERNIE, etc., has significantly improved the performance of
text generation. One of the important research directions in this area is the
generation of texts with arguments. The solution of this problem can be used in
business meetings, political debates, dialogue systems, for preparation of
student essays. One of the main domains for these applications is the economic
sphere. The key problem of the argument text generation for the Russian
language is the lack of annotated argumentation corpora. In this paper, we use
translated versions of the Argumentative Microtext, Persuasive Essays and UKP
Sentential corpora to fine-tune RuBERT model. Further, this model is used to
annotate the corpus of economic news by argumentation. Then the annotated
corpus is employed to fine-tune the ruGPT-3 model, which generates argument
texts. The results show that this approach improves the accuracy of the
argument generation by more than 20 percentage points (63.2% vs. 42.5%)
compared to the original ruGPT-3 model."
7806,"We have tried
to provide adequate pointers to allow further research on this.","Here we have performed some
experiments to understand it and provided insights and detailed discussion on the possible shortcomings.","2 Background

2.1 Knowledge Graphs

As mentioned above, Knowledge graphs are Graphical Database being widely used in Natural Language to improve the
existing models through techniques such as Knowledge infusion Sheth et al.",2022-06-18 18:12:20+00:00,Can Language Models Capture Graph Semantics? From Graphs to Language Model and Vice-Versa,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tarun Garg'), arxiv.Result.Author('Kaushik Roy'), arxiv.Result.Author('Amit Sheth')]","Knowledge Graphs are a great resource to capture semantic knowledge in terms
of entities and relationships between the entities. However, current deep
learning models takes as input distributed representations or vectors. Thus,
the graph is compressed in a vectorized representation. We conduct a study to
examine if the deep learning model can compress a graph and then output the
same graph with most of the semantics intact. Our experiments show that
Transformer models are not able to express the full semantics of the input
knowledge graph. We find that this is due to the disparity between the
directed, relationship and type based information contained in a Knowledge
Graph and the fully connected token-token undirected graphical interpretation
of the Transformer Attention matrix."
7807,"Thus the idea of graph structures from
Language Models needs further research to overcome the limitations prevailing in the current models.","Also in case of incomplete data where there’s no KG triplet, it will tend
to predict unwanted relations as the attention weights will still be signiﬁcant.","Language models
although excellent at GLUE tasks show signiﬁcant limitations in semantic knowledge capture ultimately required for
more intelligent systems, for example, to enable common sense reasoning.",2022-06-18 18:12:20+00:00,Can Language Models Capture Graph Semantics? From Graphs to Language Model and Vice-Versa,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tarun Garg'), arxiv.Result.Author('Kaushik Roy'), arxiv.Result.Author('Amit Sheth')]","Knowledge Graphs are a great resource to capture semantic knowledge in terms
of entities and relationships between the entities. However, current deep
learning models takes as input distributed representations or vectors. Thus,
the graph is compressed in a vectorized representation. We conduct a study to
examine if the deep learning model can compress a graph and then output the
same graph with most of the semantics intact. Our experiments show that
Transformer models are not able to express the full semantics of the input
knowledge graph. We find that this is due to the disparity between the
directed, relationship and type based information contained in a Knowledge
Graph and the fully connected token-token undirected graphical interpretation
of the Transformer Attention matrix."
7925,"We release Multi-LexSum for
                                                   further research in summarization methods as well as to facilitate development of
                                                   applications to assist in the CRLC’s mission.2

                                        1 Introduction

                                        Automatic summarization is a longstanding goal of natural language processing.","We present
                                                   extensive analysis demonstrating that despite the high-quality summaries in the
                                                   training data (adhering to strict content and style guidelines), state-of-the-art sum-
                                                   marization models perform poorly on this task.","Recently, abstractive
                                        summarization methods powered by large pretrained language models have shown impressive results
                                        [36, 60]—raising the question of whether these methods can help real-world summarization workloads
                                        currently performed by human experts.",2022-06-22 07:26:55+00:00,Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities,cs.CL,"['cs.CL', 'cs.CY']","[arxiv.Result.Author('Zejiang Shen'), arxiv.Result.Author('Kyle Lo'), arxiv.Result.Author('Lauren Yu'), arxiv.Result.Author('Nathan Dahlberg'), arxiv.Result.Author('Margo Schlanger'), arxiv.Result.Author('Doug Downey')]","With the advent of large language models, methods for abstractive
summarization have made great strides, creating potential for use in
applications to aid knowledge workers processing unwieldy document collections.
One such setting is the Civil Rights Litigation Clearinghouse (CRLC)
(https://clearinghouse.net),which posts information about large-scale civil
rights lawsuits, serving lawyers, scholars, and the general public. Today,
summarization in the CRLC requires extensive training of lawyers and law
students who spend hours per case understanding multiple relevant documents in
order to produce high-quality summaries of key events and outcomes. Motivated
by this ongoing real-world summarization effort, we introduce Multi-LexSum, a
collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.
Multi-LexSum presents a challenging multi-document summarization task given the
length of the source documents, often exceeding two hundred pages per case.
Furthermore, Multi-LexSum is distinct from other datasets in its multiple
target summaries, each at a different granularity (ranging from one-sentence
""extreme"" summaries to multi-paragraph narrations of over five hundred words).
We present extensive analysis demonstrating that despite the high-quality
summaries in the training data (adhering to strict content and style
guidelines), state-of-the-art summarization models perform poorly on this task.
We release Multi-LexSum for further research in summarization methods as well
as to facilitate development of applications to assist in the CRLC's mission at
https://multilexsum.github.io."
7926,"We release Multi-LexSum for
                                                   further research in summarization methods as well as to facilitate development of
                                                   applications to assist in the CRLC’s mission.2

                                        1 Introduction

                                        Automatic summarization is a longstanding goal of natural language processing.","We present
                                                   extensive analysis demonstrating that despite the high-quality summaries in the
                                                   training data (adhering to strict content and style guidelines), state-of-the-art sum-
                                                   marization models perform poorly on this task.","Recently, abstractive
                                        summarization methods powered by large pretrained language models have shown impressive results
                                        [36, 60]—raising the question of whether these methods can help real-world summarization workloads
                                        currently performed by human experts.",2022-06-22 07:26:55+00:00,Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities,cs.CL,"['cs.CL', 'cs.CY']","[arxiv.Result.Author('Zejiang Shen'), arxiv.Result.Author('Kyle Lo'), arxiv.Result.Author('Lauren Yu'), arxiv.Result.Author('Nathan Dahlberg'), arxiv.Result.Author('Margo Schlanger'), arxiv.Result.Author('Doug Downey')]","With the advent of large language models, methods for abstractive
summarization have made great strides, creating potential for use in
applications to aid knowledge workers processing unwieldy document collections.
One such setting is the Civil Rights Litigation Clearinghouse (CRLC)
(https://clearinghouse.net),which posts information about large-scale civil
rights lawsuits, serving lawyers, scholars, and the general public. Today,
summarization in the CRLC requires extensive training of lawyers and law
students who spend hours per case understanding multiple relevant documents in
order to produce high-quality summaries of key events and outcomes. Motivated
by this ongoing real-world summarization effort, we introduce Multi-LexSum, a
collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.
Multi-LexSum presents a challenging multi-document summarization task given the
length of the source documents, often exceeding two hundred pages per case.
Furthermore, Multi-LexSum is distinct from other datasets in its multiple
target summaries, each at a different granularity (ranging from one-sentence
""extreme"" summaries to multi-paragraph narrations of over five hundred words).
We present extensive analysis demonstrating that despite the high-quality
summaries in the training data (adhering to strict content and style
guidelines), state-of-the-art summarization models perform poorly on this task.
We release Multi-LexSum for further research in summarization methods as well
as to facilitate development of applications to assist in the CRLC's mission at
https://multilexsum.github.io."
7927,"We release Multi-LexSum for
                                                   further research in summarization methods as well as to facilitate development of
                                                   applications to assist in the CRLC’s mission.2

                                        1 Introduction

                                        Automatic summarization is a longstanding goal of natural language processing.","We present
                                                   extensive analysis demonstrating that despite the high-quality summaries in the
                                                   training data (adhering to strict content and style guidelines), state-of-the-art sum-
                                                   marization models perform poorly on this task.","Recently, abstractive
                                        summarization methods powered by large pretrained language models have shown impressive results
                                        [36, 60]—raising the question of whether these methods can help real-world summarization workloads
                                        currently performed by human experts.",2022-06-22 07:26:55+00:00,Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities,cs.CL,"['cs.CL', 'cs.CY']","[arxiv.Result.Author('Zejiang Shen'), arxiv.Result.Author('Kyle Lo'), arxiv.Result.Author('Lauren Yu'), arxiv.Result.Author('Nathan Dahlberg'), arxiv.Result.Author('Margo Schlanger'), arxiv.Result.Author('Doug Downey')]","With the advent of large language models, methods for abstractive
summarization have made great strides, creating potential for use in
applications to aid knowledge workers processing unwieldy document collections.
One such setting is the Civil Rights Litigation Clearinghouse (CRLC)
(https://clearinghouse.net),which posts information about large-scale civil
rights lawsuits, serving lawyers, scholars, and the general public. Today,
summarization in the CRLC requires extensive training of lawyers and law
students who spend hours per case understanding multiple relevant documents in
order to produce high-quality summaries of key events and outcomes. Motivated
by this ongoing real-world summarization effort, we introduce Multi-LexSum, a
collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.
Multi-LexSum presents a challenging multi-document summarization task given the
length of the source documents, often exceeding two hundred pages per case.
Furthermore, Multi-LexSum is distinct from other datasets in its multiple
target summaries, each at a different granularity (ranging from one-sentence
""extreme"" summaries to multi-paragraph narrations of over five hundred words).
We present extensive analysis demonstrating that despite the high-quality
summaries in the training data (adhering to strict content and style
guidelines), state-of-the-art summarization models perform poorly on this task.
We release Multi-LexSum for further research in summarization methods as well
as to facilitate development of applications to assist in the CRLC's mission at
https://multilexsum.github.io."
7995,"We hypothesize that this problem could potentially be alleviated by choosing individual temperature for
each example based on the characteristics of the combined distributions; this is a possible direction for
our further research.","In
contrast, XLNet+embs produced words that are related to the most frequent sense: will, could, cannot, etc.","8 Conclusion

We presented the ﬁrst comparison of a wide range of LMs/MLMs with different target word injection
methods on the tasks of lexical substitution and word sense induction.",2022-06-07 16:16:19+00:00,Always Keep your Target in Mind: Studying Semantics and Improving Performance of Neural Lexical Substitution,cs.CL,['cs.CL'],"[arxiv.Result.Author('Nikolay Arefyev'), arxiv.Result.Author('Boris Sheludko'), arxiv.Result.Author('Alexander Podolskiy'), arxiv.Result.Author('Alexander Panchenko')]","Lexical substitution, i.e. generation of plausible words that can replace a
particular target word in a given context, is an extremely powerful technology
that can be used as a backbone of various NLP applications, including word
sense induction and disambiguation, lexical relation extraction, data
augmentation, etc. In this paper, we present a large-scale comparative study of
lexical substitution methods employing both rather old and most recent language
and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT,
RoBERTa, XLNet. We show that already competitive results achieved by SOTA
LMs/MLMs can be further substantially improved if information about the target
word is injected properly. Several existing and new target word injection
methods are compared for each LM/MLM using both intrinsic evaluation on lexical
substitution datasets and extrinsic evaluation on word sense induction (WSI)
datasets. On two WSI datasets we obtain new SOTA results. Besides, we analyze
the types of semantic relations between target words and their substitutes
generated by different models or given by annotators."
8000,"Based
                                           To deal with label noises, recent works primar-         on such observations, we further study whether the ad-
                                        ily propose loss correction approaches.","means of coping with label noise (Lukasik et al., 2020)
                                                                                                   and preventing memorization (Xie et al., 2016).","These meth-        versarial training methods, which is an indirect label
                                        ods directly correct loss function (e.g., cross-entropy,   smoothing method, are useful at training a robust clas-
                                        mean squared error) or the probabilities used to com-      siﬁer on label noise.",2022-05-29 14:19:49+00:00,Context-based Virtual Adversarial Training for Text Classification with Noisy Labels,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Do-Myoung Lee'), arxiv.Result.Author('Yeachan Kim'), arxiv.Result.Author('Chang-gyun Seo')]","Deep neural networks (DNNs) have a high capacity to completely memorize noisy
labels given sufficient training time, and its memorization, unfortunately,
leads to performance degradation. Recently, virtual adversarial training (VAT)
attracts attention as it could further improve the generalization of DNNs in
semi-supervised learning. The driving force behind VAT is to prevent the models
from overfitting data points by enforcing consistency between the inputs and
the perturbed inputs. This strategy could be helpful in learning from noisy
labels if it prevents neural models from learning noisy samples while
encouraging the models to generalize clean samples. In this paper, we propose
context-based virtual adversarial training (ConVAT) to prevent a text
classifier from overfitting to noisy labels. Unlike the previous works, the
proposed method performs the adversarial training at the context level rather
than the inputs. It makes the classifier not only learn its label but also its
contextual neighbors, which alleviates the learning from noisy labels by
preserving contextual semantics on each data point. We conduct extensive
experiments on four text classification datasets with two types of label
noises. Comprehensive experimental results clearly show that the proposed
method works quite well even with extremely noisy settings."
8013,"Our results (Table 4) reveal that even state-of-the-art ICD-code classiﬁcation methods such
as KeyClass and FasTag have a hard time predicting low-support categories, which may
beneﬁt from further research on classiﬁcation under high class imbalance.","In the rest of the paper, we restrict ourselves to
assigning high-level ICD-9 codes to discharge notes in the publicly available MIMIC-III
database following the same experimental settings as FasTag (Venkataraman et al., 2020).","To the best of our knowledge, all prior work on ICD code assignment utilized fully
supervised ML techniques, most of them relying on vast quantities of labeled training data.",2022-06-24 05:55:49+00:00,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Chufan Gao'), arxiv.Result.Author('Mononito Goswami'), arxiv.Result.Author('Jieshi Chen'), arxiv.Result.Author('Artur Dubrawski')]","Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database."
8014,"We believe that further research is required to extensively analyze these lesser represented
categories to ensure a high quality automated annotation system.","On the contrary, the models report much lower F1 scores for less frequent classes.",6.2.,2022-06-24 05:55:49+00:00,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Chufan Gao'), arxiv.Result.Author('Mononito Goswami'), arxiv.Result.Author('Jieshi Chen'), arxiv.Result.Author('Artur Dubrawski')]","Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database."
8015,"Although further research is necessary to comprehensively validate the
proposed method across a wider range of complex data and use cases, KeyClass’s impressive
performance on a challenging problem which plagues the healthcare industry, demonstrates
its potential in helping broaden the adoption of beneﬁcial machine learning technology in
multiple application domains.","Additional experiments on four standard NLP multiclass
text classiﬁcation problems conﬁrm our proposed model’s competitive position compared to
previous methods.","14
                             KeyClass for Automated Text Classification

References

Julia Adler-Milstein, A Jay Holmgren, Peter Kralovec, Chantal Worzala, Talisha Searcy,
   and Vaishali Patel.",2022-06-24 05:55:49+00:00,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Chufan Gao'), arxiv.Result.Author('Mononito Goswami'), arxiv.Result.Author('Jieshi Chen'), arxiv.Result.Author('Artur Dubrawski')]","Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database."
8016,"Our results (Table 4) reveal that even state-of-the-art ICD-code classiﬁcation methods such
as KeyClass and FasTag have a hard time predicting low-support categories, which may
beneﬁt from further research on classiﬁcation under high class imbalance.","In the rest of the paper, we restrict ourselves to
assigning high-level ICD-9 codes to discharge notes in the publicly available MIMIC-III
database following the same experimental settings as FasTag (Venkataraman et al., 2020).","To the best of our knowledge, all prior work on ICD code assignment utilized fully
supervised ML techniques, most of them relying on vast quantities of labeled training data.",2022-06-24 05:55:49+00:00,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Chufan Gao'), arxiv.Result.Author('Mononito Goswami'), arxiv.Result.Author('Jieshi Chen'), arxiv.Result.Author('Artur Dubrawski')]","Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database."
8017,"We believe that further research is required to extensively analyze these lesser represented
categories to ensure a high quality automated annotation system.","On the contrary, the models report much lower F1 scores for less frequent classes.",6.2.,2022-06-24 05:55:49+00:00,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Chufan Gao'), arxiv.Result.Author('Mononito Goswami'), arxiv.Result.Author('Jieshi Chen'), arxiv.Result.Author('Artur Dubrawski')]","Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database."
8018,"Although further research is necessary to comprehensively validate the
proposed method across a wider range of complex data and use cases, KeyClass’s impressive
performance on a challenging problem which plagues the healthcare industry, demonstrates
its potential in helping broaden the adoption of beneﬁcial machine learning technology in
multiple application domains.","Additional experiments on four standard NLP multiclass
text classiﬁcation problems conﬁrm our proposed model’s competitive position compared to
previous methods.","14
                             KeyClass for Automated Text Classification

Acknowledgments

This work was partially supported by a fellowship from Carnegie Mellon University’s Cen-
ter for Machine Learning and Health to M.G.",2022-06-24 05:55:49+00:00,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Chufan Gao'), arxiv.Result.Author('Mononito Goswami'), arxiv.Result.Author('Jieshi Chen'), arxiv.Result.Author('Artur Dubrawski')]","Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database."
8034,"BERT pre-trained language models with a syntactic dependency model - may out-
                                                                                       perform the alternatives across multiple experimental settings, making a potentially
                                                                                       strong case for further research in the use of heterogeneous text representations in
                                                                                       these and possibly other NLP tasks.","São Paulo, Brazil     Results suggest that certain text representations - in particular, the combined use of
                                        03828-000.","KEYWORDS:
                                                                                       Natural Language Processing, political bias, ideology, political alignment, stance, hyperpartisan news
                                                                                       detection, author proﬁling

                                        1 INTRODUCTION

                                        Inferring politically-charged information from text data is a popular research topic in Natural Language Processing (NLP), with
                                        a wide range of applications to predict individuals’ political views ?",2022-06-24 13:45:36+00:00,Text and author-level political inference using heterogeneous knowledge representations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Samuel Caetano da Silva'), arxiv.Result.Author('Ivandre Paraboni')]","The inference of politically-charged information from text data is a popular
research topic in Natural Language Processing (NLP) at both text- and
author-level. In recent years, studies of this kind have been implemented with
the aid of representations from transformers such as BERT. Despite considerable
success, however, we may ask whether results may be improved even further by
combining transformed-based models with additional knowledge representations.
To shed light on this issue, the present work describes a series of experiments
to compare alternative model configurations for political inference from text
in both English and Portuguese languages. Results suggest that certain text
representations - in particular, the combined use of BERT pre-trained language
models with a syntactic dependency model - may outperform the alternatives
across multiple experimental settings, making a potentially strong case for
further research in the use of heterogeneous text representations in these and
possibly other NLP tasks."
8035,The current work leaves a number of opportunities for further research.,"In particular, it was a
subset of our original CNN architecture combining only BERT and the syntactic dependency model that obtained overall best
results in most tasks.","First, we notice that political bias and ideology are
relatively broad terms that may actually include a wide range of distinct politically-related phenomena, and that future NLP
studies may beneﬁt from more ﬁne grained task deﬁnitions.",2022-06-24 13:45:36+00:00,Text and author-level political inference using heterogeneous knowledge representations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Samuel Caetano da Silva'), arxiv.Result.Author('Ivandre Paraboni')]","The inference of politically-charged information from text data is a popular
research topic in Natural Language Processing (NLP) at both text- and
author-level. In recent years, studies of this kind have been implemented with
the aid of representations from transformers such as BERT. Despite considerable
success, however, we may ask whether results may be improved even further by
combining transformed-based models with additional knowledge representations.
To shed light on this issue, the present work describes a series of experiments
to compare alternative model configurations for political inference from text
in both English and Portuguese languages. Results suggest that certain text
representations - in particular, the combined use of BERT pre-trained language
models with a syntactic dependency model - may outperform the alternatives
across multiple experimental settings, making a potentially strong case for
further research in the use of heterogeneous text representations in these and
possibly other NLP tasks."
8036,"BERT pre-trained language models with a syntactic dependency model - may out-
                                                                                       perform the alternatives across multiple experimental settings, making a potentially
                                                                                       strong case for further research in the use of heterogeneous text representations in
                                                                                       these and possibly other NLP tasks.","São Paulo, Brazil     Results suggest that certain text representations - in particular, the combined use of
                                        03828-000.","KEYWORDS:
                                                                                       Natural Language Processing, political bias, ideology, political alignment, stance, hyperpartisan news
                                                                                       detection, author proﬁling

                                        1 INTRODUCTION

                                        Inferring politically-charged information from text data is a popular research topic in Natural Language Processing (NLP),
                                        with a wide range of applications to predict individuals’ political views 1,2,3 and behaviour 4,5,6.",2022-06-24 13:45:36+00:00,Text and author-level political inference using heterogeneous knowledge representations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Samuel Caetano da Silva'), arxiv.Result.Author('Ivandre Paraboni')]","The inference of politically-charged information from text data is a popular
research topic in Natural Language Processing (NLP) at both text- and
author-level. In recent years, studies of this kind have been implemented with
the aid of representations from transformers such as BERT. Despite considerable
success, however, we may ask whether results may be improved even further by
combining transformed-based models with additional knowledge representations.
To shed light on this issue, the present work describes a series of experiments
to compare alternative model configurations for political inference from text
in both English and Portuguese languages. Results suggest that certain text
representations - in particular, the combined use of BERT pre-trained language
models with a syntactic dependency model - may outperform the alternatives
across multiple experimental settings, making a potentially strong case for
further research in the use of heterogeneous text representations in these and
possibly other NLP tasks."
8037,The current work leaves a number of opportunities for further research.,"In particular, it was a
subset of our original CNN architecture combining only BERT and the syntactic dependency model that obtained overall best
results in most tasks.","First, we notice that political bias and ideology are
relatively broad terms that may actually include a wide range of distinct politically-related phenomena, and that future NLP
studies may beneﬁt from more ﬁne grained task deﬁnitions.",2022-06-24 13:45:36+00:00,Text and author-level political inference using heterogeneous knowledge representations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Samuel Caetano da Silva'), arxiv.Result.Author('Ivandre Paraboni')]","The inference of politically-charged information from text data is a popular
research topic in Natural Language Processing (NLP) at both text- and
author-level. In recent years, studies of this kind have been implemented with
the aid of representations from transformers such as BERT. Despite considerable
success, however, we may ask whether results may be improved even further by
combining transformed-based models with additional knowledge representations.
To shed light on this issue, the present work describes a series of experiments
to compare alternative model configurations for political inference from text
in both English and Portuguese languages. Results suggest that certain text
representations - in particular, the combined use of BERT pre-trained language
models with a syntactic dependency model - may outperform the alternatives
across multiple experimental settings, making a potentially strong case for
further research in the use of heterogeneous text representations in these and
possibly other NLP tasks."
8105,"We further study the word closest to the [mask] token and
                                                                                          present some cases for prompt 𝑃3 in Table 11.","Moreover, for the same stance
                                                                                          label across different targets, our model can identify different top
                                                                                          words for different targets, showing that our model is capable of
                                                                                          capturing the correlation between the target and the stance.","We can see that
                                                                                          the predicted word are consistent with the stance label in most
                                                                                          cases.",2022-06-27 12:04:14+00:00,Few-Shot Stance Detection via Target-Aware Prompt Distillation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yan Jiang'), arxiv.Result.Author('Jinhua Gao'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Xueqi Cheng')]","Stance detection aims to identify whether the author of a text is in favor
of, against, or neutral to a given target. The main challenge of this task
comes two-fold: few-shot learning resulting from the varying targets and the
lack of contextual information of the targets. Existing works mainly focus on
solving the second issue by designing attention-based models or introducing
noisy external knowledge, while the first issue remains under-explored. In this
paper, inspired by the potential capability of pre-trained language models
(PLMs) serving as knowledge bases and few-shot learners, we propose to
introduce prompt-based fine-tuning for stance detection. PLMs can provide
essential contextual information for the targets and enable few-shot learning
via prompts. Considering the crucial role of the target in stance detection
task, we design target-aware prompts and propose a novel verbalizer. Instead of
mapping each label to a concrete word, our verbalizer maps each label to a
vector and picks the label that best captures the correlation between the
stance and the target. Moreover, to alleviate the possible defect of dealing
with varying targets with a single hand-crafted prompt, we propose to distill
the information learned from multiple prompts. Experimental results show the
superior performance of our proposed model in both full-data and few-shot
scenarios."
8166,"In order to further study the aspect absent problem, we    role of supplementing semantics and eliminating ambiguity.","Besides, we find that images also play the
supplement.","The
also build the MACSA-hard dataset as a hard setting that is selected   information expressed by images is more objective than the text.",2022-06-28 12:49:16+00:00,MACSA: A Multimodal Aspect-Category Sentiment Analysis Dataset with Multimodal Fine-grained Aligned Annotations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hao Yang'), arxiv.Result.Author('Yanyan Zhao'), arxiv.Result.Author('Jianwei Liu'), arxiv.Result.Author('Yang Wu'), arxiv.Result.Author('Bing Qin')]","Multimodal fine-grained sentiment analysis has recently attracted increasing
attention due to its broad applications. However, the existing multimodal
fine-grained sentiment datasets most focus on annotating the fine-grained
elements in text but ignore those in images, which leads to the fine-grained
elements in visual content not receiving the full attention they deserve. In
this paper, we propose a new dataset, the Multimodal Aspect-Category Sentiment
Analysis (MACSA) dataset, which contains more than 21K text-image pairs. The
dataset provides fine-grained annotations for both textual and visual content
and firstly uses the aspect category as the pivot to align the fine-grained
elements between the two modalities. Based on our dataset, we propose the
Multimodal ACSA task and a multimodal graph-based aligned model (MGAM), which
adopts a fine-grained cross-modal fusion method. Experimental results show that
our method can facilitate the baseline comparison for future research on this
corpus. We will make the dataset and code publicly available."
8167,"To further study the aspect absent problem in the multimodal
                                                                         ACSA task, we selected the data that there is at least two aspect cat-
   It is noteworthy that we annotate the relevance of images and         egory absent in textual data to construct the MACSA-hard dataset.","manually recognize the sentiments to the related categories with
the sentiment states 1 (positive), 2 (neutral) and 3 (negative).",RoIs to pre-defined aspect categories.,2022-06-28 12:49:16+00:00,MACSA: A Multimodal Aspect-Category Sentiment Analysis Dataset with Multimodal Fine-grained Aligned Annotations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hao Yang'), arxiv.Result.Author('Yanyan Zhao'), arxiv.Result.Author('Jianwei Liu'), arxiv.Result.Author('Yang Wu'), arxiv.Result.Author('Bing Qin')]","Multimodal fine-grained sentiment analysis has recently attracted increasing
attention due to its broad applications. However, the existing multimodal
fine-grained sentiment datasets most focus on annotating the fine-grained
elements in text but ignore those in images, which leads to the fine-grained
elements in visual content not receiving the full attention they deserve. In
this paper, we propose a new dataset, the Multimodal Aspect-Category Sentiment
Analysis (MACSA) dataset, which contains more than 21K text-image pairs. The
dataset provides fine-grained annotations for both textual and visual content
and firstly uses the aspect category as the pivot to align the fine-grained
elements between the two modalities. Based on our dataset, we propose the
Multimodal ACSA task and a multimodal graph-based aligned model (MGAM), which
adopts a fine-grained cross-modal fusion method. Experimental results show that
our method can facilitate the baseline comparison for future research on this
corpus. We will make the dataset and code publicly available."
8174,"which is ready for further research as well as prac-    DF functions (iteratively modifying the implemen-
tical uses.","theoretically represent more complex user requests than the   Rather, it is a general approach; a paradigm, which
FSM approach, rather that it can practically be signiﬁcantly  can have different interpretations and ﬂavours, and
easier to handle complex requests using DF.","tation to ensure new functions can interact con-
                                                        sistently with previously implemented functions).",2022-06-28 16:36:08+00:00,Simplifying Dataflow Dialogue Design,cs.CL,['cs.CL'],[arxiv.Result.Author('Joram Meron')],"In \citep{andreas2020task-oriented}, a dataflow (DF) based dialogue system
was introduced, showing clear advantages compared to many commonly used current
systems. This was accompanied by the release of SMCalFlow, a practically
relevant, manually annotated dataset, more detailed and much larger than any
comparable dialogue dataset. Despite these remarkable contributions, the
community has not shown further interest in this direction. What are the
reasons for this lack of interest? And how can the community be encouraged to
engage in research in this direction?
  One explanation may be the perception that this approach is too complex -
both the the annotation and the system. This paper argues that this perception
is wrong: 1) Suggestions for a simplified format for the annotation of the
dataset are presented, 2) An implementation of the DF execution engine is
released\footnote{https://github.com/telepathylabsai/OpenDF}, which can serve
as a sandbox allowing researchers to easily implement, and experiment with, new
DF dialogue designs. The hope is that these contributions will help engage more
practitioners in exploring new ideas and designs for DF based dialogue systems."
8178,"Finally, we release       the corpus encompasses laws, regulations, and government-issues
                                        the corpus to the research community to promote further study.1        guidelines and recommendations intended to instruct citizens, or-
                                                                                                               ganizations, law enforcement, or lawmakers on actions to protect
                                        KEYWORDS                                                               digital privacy.","We coin the term government
                                        results show the prevalence of common themes in GPIs, such as          privacy instructions, or GPIs to characterize these documents, as
                                        finance, healthcare, and telecommunications.","We include legally binding documents such as laws
                                                                                                               and government-produced non-legally binding documents such as
                                        corpus, text analysis, privacy, law, legal text                        guidelines in the corpus.",2022-06-28 17:36:12+00:00,Creation and Analysis of an International Corpus of Privacy Laws,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sonu Gupta'), arxiv.Result.Author('Ellen Poplavska'), arxiv.Result.Author(""Nora O'Toole""), arxiv.Result.Author('Siddhant Arora'), arxiv.Result.Author('Thomas Norton'), arxiv.Result.Author('Norman Sadeh'), arxiv.Result.Author('Shomir Wilson')]","The landscape of privacy laws and regulations around the world is complex and
ever-changing. National and super-national laws, agreements, decrees, and other
government-issued rules form a patchwork that companies must follow to operate
internationally. To examine the status and evolution of this patchwork, we
introduce the Government Privacy Instructions Corpus, or GPI Corpus, of 1,043
privacy laws, regulations, and guidelines, covering 182 jurisdictions. This
corpus enables a large-scale quantitative and qualitative examination of legal
foci on privacy. We examine the temporal distribution of when GPIs were created
and illustrate the dramatic increase in privacy legislation over the past 50
years, although a finer-grained examination reveals that the rate of increase
varies depending on the personal data types that GPIs address. Our exploration
also demonstrates that most privacy laws respectively address relatively few
personal data types, showing that comprehensive privacy legislation remains
rare. Additionally, topic modeling results show the prevalence of common themes
in GPIs, such as finance, healthcare, and telecommunications. Finally, we
release the corpus to the research community to promote further study."
8192,"The best single model is also shown for comparison

    There were two considerations in the deﬁnition of these metrics that may cause criticism and that
we plan to further research:

   1.","15
Ensemble Choice EM F1 Models Selected (names abbreviated)

Best EM         80.91  83.88  bert-lg-uc
Best 3 by EM    83.17  87.06  bert-lg-uc, bert-lg-cas, roberta/002
Best 3 by F1    83.08  87.30  bert-lg-uc, bert-lg-cas, roberta/001
Best 3 by GRIM  81.11  85.49  roberta, bert-lg-uc, bert/010
Best 5 by EM    83.34  87.21  bert-lg-uc, bert-lg-cas, roberta/002, longformer/011,roberta/001
Best 5 by F1    83.99  87.94  bert-lg-uc, bert-lg-cas, roberta/001, longformer/011, electra/006
Best 5 by GRIM  81.76  85.73  roberta, bert-lg-uc, bert/010, bert-lg-cas, bert/009

Table 7: Ensembles whose members were selected based on highest performance according to best EM,
F1 or GRIM.",Our methodology opted to exclude primary predictions from the calculation of the GRIM.,2022-06-29 01:17:47+00:00,What Can Secondary Predictions Tell Us? An Exploration on Question-Answering with SQuAD-v2.0,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Michael Kamfonas'), arxiv.Result.Author('Gabriel Alon')]","Performance in natural language processing, and specifically for the
question-answer task, is typically measured by comparing a model\'s most
confident (primary) prediction to golden answers (the ground truth). We are
making the case that it is also useful to quantify how close a model came to
predicting a correct answer even for examples that failed. We define the Golden
Rank (GR) of an example as the rank of its most confident prediction that
exactly matches a ground truth, and show why such a match always exists. For
the 16 transformer models we analyzed, the majority of exactly matched golden
answers in secondary prediction space hover very close to the top rank. We
refer to secondary predictions as those ranking above 0 in descending
confidence probability order. We demonstrate how the GR can be used to classify
questions and visualize their spectrum of difficulty, from persistent near
successes to persistent extreme failures. We derive a new aggregate statistic
over entire test sets, named the Golden Rank Interpolated Median (GRIM) that
quantifies the proximity of failed predictions to the top choice made by the
model. To develop some intuition and explore the applicability of these metrics
we use the Stanford Question Answering Dataset (SQuAD-2) and a few popular
transformer models from the Hugging Face hub. We first demonstrate that the
GRIM is not directly correlated with the F1 and exact match (EM) scores. We
then calculate and visualize these scores for various transformer
architectures, probe their applicability in error analysis by clustering failed
predictions, and compare how they relate to other training diagnostics such as
the EM and F1 scores. We finally suggest various research goals, such as
broadening data collection for these metrics and their possible use in
adversarial training."
8193,"There were two considerations in the deﬁnition of these metrics that may cause criticism and that
we plan to further research:

                                  13
   1.","The resulting clusters can, in turn, expose patterns of failure and inconsistencies
to aid the detection of errors and missing or unexpected golden answers.",Our methodology opted to exclude primary predictions from the calculation of the GRIM.,2022-06-29 01:17:47+00:00,What Can Secondary Predictions Tell Us? An Exploration on Question-Answering with SQuAD-v2.0,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Michael Kamfonas'), arxiv.Result.Author('Gabriel Alon')]","Performance in natural language processing, and specifically for the
question-answer task, is typically measured by comparing a model\'s most
confident (primary) prediction to golden answers (the ground truth). We are
making the case that it is also useful to quantify how close a model came to
predicting a correct answer even for examples that failed. We define the Golden
Rank (GR) of an example as the rank of its most confident prediction that
exactly matches a ground truth, and show why such a match always exists. For
the 16 transformer models we analyzed, the majority of exactly matched golden
answers in secondary prediction space hover very close to the top rank. We
refer to secondary predictions as those ranking above 0 in descending
confidence probability order. We demonstrate how the GR can be used to classify
questions and visualize their spectrum of difficulty, from persistent near
successes to persistent extreme failures. We derive a new aggregate statistic
over entire test sets, named the Golden Rank Interpolated Median (GRIM) that
quantifies the proximity of failed predictions to the top choice made by the
model. To develop some intuition and explore the applicability of these metrics
we use the Stanford Question Answering Dataset (SQuAD-2) and a few popular
transformer models from the Hugging Face hub. We first demonstrate that the
GRIM is not directly correlated with the F1 and exact match (EM) scores. We
then calculate and visualize these scores for various transformer
architectures, probe their applicability in error analysis by clustering failed
predictions, and compare how they relate to other training diagnostics such as
the EM and F1 scores. We finally suggest various research goals, such as
broadening data collection for these metrics and their possible use in
adversarial training."
8199,"In experi-
ments, we further study the SememeWSD model on WSD dataset and we also
evaluate our model on semantic similarity.","To
prove its feasibility, we present a SememeWSD-Synonym (SWSDS) model, which
disambiguates polysemous words and represents it by synonym set.","The experiment results show that
our model outperforms the baseline method.",2022-06-29 03:42:03+00:00,Chinese Word Sense Embedding with SememeWSD and Synonym Set,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yangxi Zhou'), arxiv.Result.Author('Junping Du'), arxiv.Result.Author('Zhe Xue'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Zeli Guan')]","Word embedding is a fundamental natural language processing task which can
learn feature of words. However, most word embedding methods assign only one
vector to a word, even if polysemous words have multi-senses. To address this
limitation, we propose SememeWSD Synonym (SWSDS) model to assign a different
vector to every sense of polysemous words with the help of word sense
disambiguation (WSD) and synonym set in OpenHowNet. We use the SememeWSD model,
an unsupervised word sense disambiguation model based on OpenHowNet, to do word
sense disambiguation and annotate the polysemous word with sense id. Then, we
obtain top 10 synonyms of the word sense from OpenHowNet and calculate the
average vector of synonyms as the vector of the word sense. In experiments, We
evaluate the SWSDS model on semantic similarity calculation with Gensim's
wmdistance method. It achieves improvement of accuracy. We also examine the
SememeWSD model on different BERT models to find the more effective model."
8240,"tag-tag dependency vs. word-tag dependency)
and we encourage further study on this topic.","preferences together with the observation in Ap-
pendix E suggest different languages (or datasets)
can prefer different types of dependency modeling
(e.g.","MPoSM (width=1)                D(0)                                MORPH         D(2-4)
-connecting
MPoSM (width=2)            99.50 (95%)                           95.41 (55%)  87.19 (0%)
-connecting                83.37 (0%)                            85.09 (0%)   83.13 (0%)
MPoSM (full)               92.99 (30%)                           90.02 (30%)  87.62 (0%)
-connecting                83.97 (5%)                            84.03 (0%)   83.44 (0%)
                           96.50 (75%)                           93.01 (30%)  95.31 (30%)
Stratos (2019) (width=2)   75.02 (5%)                            73.27 (0%)   82.81 (0%)
Tran et al.",2022-06-30 01:43:05+00:00,Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised POS-tagging?,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Xiang Zhou'), arxiv.Result.Author('Shiyue Zhang'), arxiv.Result.Author('Mohit Bansal')]","Previous Part-Of-Speech (POS) induction models usually assume certain
independence assumptions (e.g., Markov, unidirectional, local dependency) that
do not hold in real languages. For example, the subject-verb agreement can be
both long-term and bidirectional. To facilitate flexible dependency modeling,
we propose a Masked Part-of-Speech Model (MPoSM), inspired by the recent
success of Masked Language Models (MLM). MPoSM can model arbitrary tag
dependency and perform POS induction through the objective of masked POS
reconstruction. We achieve competitive results on both the English Penn WSJ
dataset as well as the universal treebank containing 10 diverse languages.
Though modeling the long-term dependency should ideally help this task, our
ablation study shows mixed trends in different languages. To better understand
this phenomenon, we design a novel synthetic experiment that can specifically
diagnose the model's ability to learn tag agreement. Surprisingly, we find that
even strong baselines fail to solve this problem consistently in a very
simplified setting: the agreement between adjacent words. Nonetheless, MPoSM
achieves overall better performance. Lastly, we conduct a detailed error
analysis to shed light on other remaining challenges. Our code is available at
https://github.com/owenzx/MPoSM"
8243,"Additionally, Bertbase is a relatively small model
compared to other Transformers and further research into the scalability would also prove beneﬁcial.","Further investigation as to the robustness of this
approach to different domains would be valuable.","Finally, since no runtime exists for mixing low-width weights with 8-bit activations, NxMiFormer
relies on FLOP and compression proxies rather than end-to-end results in all cases.",2022-06-30 04:33:50+00:00,Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Connor Holmes'), arxiv.Result.Author('Minjia Zhang'), arxiv.Result.Author('Yuxiong He'), arxiv.Result.Author('Bo Wu')]","In recent years, large pre-trained Transformer networks have demonstrated
dramatic improvements in many natural language understanding tasks. However,
the huge size of these models brings significant challenges to their
fine-tuning and online deployment due to latency and cost constraints. New
hardware supporting both N:M semi-structured sparsity and low-precision integer
computation is a promising solution to boost DNN model serving efficiency.
However, there have been very few studies that systematically investigate to
what extent pre-trained Transformer networks benefit from the combination of
these techniques, as well as how to best compress each component of the
Transformer. We propose a flexible compression framework NxMiFormer that
performs simultaneous sparsification and quantization using ADMM and STE-based
QAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm
that identifies promising heterogeneous compression configurations that meet a
compression ratio constraint. When evaluated across the GLUE suite of NLU
benchmarks, our approach can achieve up to 93% compression of the encoders of a
BERT model while retaining 98.2% of the original model accuracy and taking full
advantage of the hardware's capabilities. Heterogeneous configurations found
the by the search heuristic maintain 99.5% of the baseline accuracy while still
compressing the model by 87.5%."
8244,"datasets and benchmarks is essential before the
                                        further research about QA.","human generated abstractive as questions while
                                                                                              one entities is hided, the stories are used as context
                                           An comprehensive understanding of those            to do the ﬁll-in-the-blank questions.","Therefore, in this pa-        CoQA (Reddy et al., 2019) is the ﬁrst conversa-
                                        per, we investigate some of the most commonly         tional question answering dataset.",2022-06-30 05:53:56+00:00,Modern Question Answering Datasets and Benchmarks: A Survey,cs.CL,['cs.CL'],[arxiv.Result.Author('Zhen Wang')],"Question Answering (QA) is one of the most important natural language
processing (NLP) tasks. It aims using NLP technologies to generate a
corresponding answer to a given question based on the massive unstructured
corpus. With the development of deep learning, more and more challenging QA
datasets are being proposed, and lots of new methods for solving them are also
emerging. In this paper, we investigate influential QA datasets that have been
released in the era of deep learning. Specifically, we begin with introducing
two of the most common QA tasks - textual question answer and visual question
answering - separately, covering the most representative datasets, and then
give some current challenges of QA research."
8250,"A summary of the problems and suggested ways forward
Drawing on the preceding discussion, we identify two speciﬁc problems, brieﬂy suggest ways in which
these problems might be tackled, and then present candidate hypotheses and research questions for
further research.",8.,8.1.,2022-06-30 07:00:11+00:00,"Story-thinking, computational-thinking, programming and software engineering",cs.CL,['cs.CL'],"[arxiv.Result.Author('Austen Rainer'), arxiv.Result.Author('Catherine Menon')]","Working with stories and working with computations require very different
modes of thought. We call the first mode ""story-thinking"" and the second
""computational-thinking"". The aim of this curiosity-driven paper is to explore
the nature of these two modes of thinking, and to do so in relation to
programming, including software engineering as programming-in-the-large. We
suggest that story-thinking and computational-thinking may be understood as two
ways of attending to the world, and that each both contributes and neglects the
world, though in different ways and for different ends. We formulate two
fundamental problems, i.e., the problem of ""neglectful representations"" and the
problem of oppositional ways of thinking. We briefly suggest two ways in which
these problems might be tackled and identify candidate hypotheses about the
current state of the world, one assertion about a possible future state, and
several research questions for future research."
8252,"Dataset Description: The MEDIQA challenge is an ACL-BioNLP 2019 shared task aiming to attract
further research efforts in Natural Language Inference (NLI), Recognizing Question Entailment
(RQE), and their applications in medical Question Answering (QA).","For further information, please visit https://temu.bsc.es/cantemist or send an email to
encargo-pln-life@bsc.es

Homepage: https://temu.bsc.es/cantemist/?p=4338

URL: https://zenodo.org/record/3978041/files/cantemist.zip?download=1

Licensing: Creative Commons Attribution 4.0 International

Languages: Spanish

Tasks: NER, NED, Text Classiﬁcation

Schemas: TEXT, KB, source

Splits: train, validation, test

                                 51
MEDIQA Data Card

                                                         token length

                      test                                                                         train_live_qa_med
             validation                                                                            train_alexa
           train_alexa                                                                             validation
train_live_qa_med                                                                                  test

                                  0      10      20  30  40                      50  60   70   80

                                                     Label Counts by Type: type

factoid

0                                    20      40      60  80            100           120  140

Figure 14: Token frequency distribution by split (top) and frequency of different kind of instances
(bottom).",Mailing List: https://groups.,2022-06-30 07:15:45+00:00,BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jason Alan Fries'), arxiv.Result.Author('Leon Weber'), arxiv.Result.Author('Natasha Seelam'), arxiv.Result.Author('Gabriel Altay'), arxiv.Result.Author('Debajyoti Datta'), arxiv.Result.Author('Samuele Garda'), arxiv.Result.Author('Myungsun Kang'), arxiv.Result.Author('Ruisi Su'), arxiv.Result.Author('Wojciech Kusa'), arxiv.Result.Author('Samuel Cahyawijaya'), arxiv.Result.Author('Fabio Barth'), arxiv.Result.Author('Simon Ott'), arxiv.Result.Author('Matthias Samwald'), arxiv.Result.Author('Stephen Bach'), arxiv.Result.Author('Stella Biderman'), arxiv.Result.Author('Mario Sänger'), arxiv.Result.Author('Bo Wang'), arxiv.Result.Author('Alison Callahan'), arxiv.Result.Author('Daniel León Periñán'), arxiv.Result.Author('Théo Gigant'), arxiv.Result.Author('Patrick Haller'), arxiv.Result.Author('Jenny Chim'), arxiv.Result.Author('Jose David Posada'), arxiv.Result.Author('John Michael Giorgi'), arxiv.Result.Author('Karthik Rangasai Sivaraman'), arxiv.Result.Author('Marc Pàmies'), arxiv.Result.Author('Marianna Nezhurina'), arxiv.Result.Author('Robert Martin'), arxiv.Result.Author('Michael Cullan'), arxiv.Result.Author('Moritz Freidank'), arxiv.Result.Author('Nathan Dahlberg'), arxiv.Result.Author('Shubhanshu Mishra'), arxiv.Result.Author('Shamik Bose'), arxiv.Result.Author('Nicholas Michio Broad'), arxiv.Result.Author('Yanis Labrak'), arxiv.Result.Author('Shlok S Deshmukh'), arxiv.Result.Author('Sid Kiblawi'), arxiv.Result.Author('Ayush Singh'), arxiv.Result.Author('Minh Chien Vu'), arxiv.Result.Author('Trishala Neeraj'), arxiv.Result.Author('Jonas Golde'), arxiv.Result.Author('Albert Villanova del Moral'), arxiv.Result.Author('Benjamin Beilharz')]","Training and evaluating language models increasingly requires the
construction of meta-datasets --diverse collections of curated data with clear
provenance. Natural language prompting has recently lead to improved zero-shot
generalization by transforming existing, supervised datasets into a diversity
of novel pretraining tasks, highlighting the benefits of meta-dataset curation.
While successful in general-domain text, translating these data-centric
approaches to biomedical language modeling remains challenging, as labeled
biomedical datasets are significantly underrepresented in popular data hubs. To
address this challenge, we introduce BigBIO a community library of 126+
biomedical NLP datasets, currently covering 12 task categories and 10+
languages. BigBIO facilitates reproducible meta-dataset curation via
programmatic access to datasets and their metadata, and is compatible with
current platforms for prompt engineering and end-to-end few/zero shot language
model evaluation. We discuss our process for task schema harmonization, data
auditing, contribution guidelines, and outline two illustrative use cases:
zero-shot evaluation of biomedical prompts and large-scale, multi-task
learning. BigBIO is an ongoing community effort and is available at
https://github.com/bigscience-workshop/biomedical"
8320,"We leave for further research an adaptation of
this dataset for T5.","T5 architecture does not allow to use of this dataset in the same way as for MultiBERT since Masking
Language Modeling and T5 generation are different tasks.","5.1 Human language acquisition results

Many of the linguistic features used in probing tasks have been well studied in terms of their promptness
and ease of acquisition by English speakers.",2022-07-01 17:24:11+00:00,Is neural language acquisition similar to natural? A chronological probing study,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ekaterina Voloshina'), arxiv.Result.Author('Oleg Serikov'), arxiv.Result.Author('Tatiana Shavrina')]","The probing methodology allows one to obtain a partial representation of
linguistic phenomena stored in the inner layers of the neural network, using
external classifiers and statistical analysis. Pre-trained transformer-based
language models are widely used both for natural language understanding (NLU)
and natural language generation (NLG) tasks making them most commonly used for
downstream applications. However, little analysis was carried out, whether the
models were pre-trained enough or contained knowledge correlated with
linguistic theory. We are presenting the chronological probing study of
transformer English models such as MultiBERT and T5. We sequentially compare
the information about the language learned by the models in the process of
training on corpora. The results show that 1) linguistic information is
acquired in the early stages of training 2) both language models demonstrate
capabilities to capture various features from various levels of language,
including morphology, syntax, and even discourse, while they also can
inconsistently fail on tasks that are perceived as easy. We also introduce the
open-source framework for chronological probing research, compatible with other
transformer-based models.
https://github.com/EkaterinaVoloshina/chronological_probing"
8351,"Altogether, these results highlight the limited understanding
on the strategies employed by current neural models to summarize long document and the need of
further research to enhance our understanding on this issue.","On the other hand, Manakul and Gales [77] have found that extending window size of efficient
Transformers to increase number of attended tokens per token do not affect the average distance of
attended neighbor, suggesting that local attention to neighboring tokens will be sufficient for the
long document summarization task.","8.2 Summarizer with Automatic Discourse Parsers/Annotator

Our experimental result has demonstrated that the simple unsupervised graph architecture outper-
forms the other unsupervised models when discourse bias of arXiv section information is included.",2022-07-03 02:57:22+00:00,"An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics",cs.CL,['cs.CL'],"[arxiv.Result.Author('Huan Yee Koh'), arxiv.Result.Author('Jiaxin Ju'), arxiv.Result.Author('Ming Liu'), arxiv.Result.Author('Shirui Pan')]","Long documents such as academic articles and business reports have been the
standard format to detail out important issues and complicated subjects that
require extra attention. An automatic summarization system that can effectively
condense long documents into short and concise texts to encapsulate the most
important information would thus be significant in aiding the reader's
comprehension. Recently, with the advent of neural architectures, significant
research efforts have been made to advance automatic text summarization
systems, and numerous studies on the challenges of extending these systems to
the long document domain have emerged. In this survey, we provide a
comprehensive overview of the research on long document summarization and a
systematic evaluation across the three principal components of its research
setting: benchmark datasets, summarization models, and evaluation metrics. For
each component, we organize the literature within the context of long document
summarization and conduct an empirical analysis to broaden the perspective on
current research progress. The empirical analysis includes a study on the
intrinsic characteristics of benchmark datasets, a multi-dimensional analysis
of summarization models, and a review of the summarization evaluation metrics.
Based on the overall findings, we conclude by proposing possible directions for
future exploration in this rapidly growing field."
8425,"ples in the preﬁx and then the model performs vari-      For further study, we hope to develop tools that can
ous tasks by ﬁnishing the sentence; Cui et al.",(2020) put task descriptions and exam-      progression in the model as previously believed.,"(2021)    more accurately and faithfully locate and identify
enumerate every possible text span in a sentence,        the linguistic properties embedded in the model
create a template for each of them, and ﬁne-tune the     and help us understand the way neural language
model to perform named entity recognition (NER).",2022-07-04 22:14:40+00:00,Probing via Prompting,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiaoda Li'), arxiv.Result.Author('Ryan Cotterell'), arxiv.Result.Author('Mrinmaya Sachan')]","Probing is a popular method to discern what linguistic information is
contained in the representations of pre-trained language models. However, the
mechanism of selecting the probe model has recently been subject to intense
debate, as it is not clear if the probes are merely extracting information or
modeling the linguistic property themselves. To address this challenge, this
paper introduces a novel model-free approach to probing, by formulating probing
as a prompting task. We conduct experiments on five probing tasks and show that
our approach is comparable or better at extracting information than diagnostic
probes while learning much less on its own. We further combine the probing via
prompting approach with attention head pruning to analyze where the model
stores the linguistic information in its architecture. We then examine the
usefulness of a specific linguistic property for pre-training by removing the
heads that are essential to that property and evaluating the resulting model's
performance on language modeling."
8435,"We train LMs on larger quantities of ASR output, analyse their
                                                                                                           beneﬁts on speech-speciﬁc tasks, and make them available for
                                            Pretrained LMs, such as BERT [3], have been applied with       further research.",them to speech use cases.,"success to a range of tasks that involve speech: ASR hypothe-
                                       sis rescoring [4], Spoken Language Understanding [5], Dialog           2.",2022-07-05 08:47:51+00:00,ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks,cs.CL,['cs.CL'],"[arxiv.Result.Author('Valentin Pelloin'), arxiv.Result.Author('Franck Dary'), arxiv.Result.Author('Nicolas Herve'), arxiv.Result.Author('Benoit Favre'), arxiv.Result.Author('Nathalie Camelin'), arxiv.Result.Author('Antoine Laurent'), arxiv.Result.Author('Laurent Besacier')]","We aim at improving spoken language modeling (LM) using very large amount of
automatically transcribed speech. We leverage the INA (French National
Audiovisual Institute) collection and obtain 19GB of text after applying ASR on
350,000 hours of diverse TV shows. From this, spoken language models are
trained either by fine-tuning an existing LM (FlauBERT) or through training a
LM from scratch. New models (FlauBERT-Oral) are shared with the community and
evaluated for 3 downstream tasks: spoken language understanding, classification
of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can
be beneficial compared to its initial FlauBERT version demonstrating that,
despite its inherent noisy nature, ASR-generated text can be used to build
spoken language models."
8436,"6 Conclusion and further research

We have shown that the topology of attention graphs contains enough information for classifying
texts by three different attributes: linguistic acceptability, sentiment, and being SPAM or not.",For more information about this see Appendix B.,"Thus,
we see here some degree of universality for distinguishing different text properties.",2022-07-05 09:10:47+00:00,Betti numbers of attention graphs is all you really need,cs.CL,['cs.CL'],"[arxiv.Result.Author('Laida Kushnareva'), arxiv.Result.Author('Dmitri Piontkovski'), arxiv.Result.Author('Irina Piontkovskaya')]","We apply methods of topological analysis to the attention graphs, calculated
on the attention heads of the BERT model ( arXiv:1810.04805v2 ). Our research
shows that the classifier built upon basic persistent topological features
(namely, Betti numbers) of the trained neural network can achieve
classification results on par with the conventional classification method. We
show the relevance of such topological text representation on three text
classification benchmarks. For the best of our knowledge, it is the first
attempt to analyze the topology of an attention-based neural network, widely
used for Natural Language Processing."
8447,"A question for further research is whether
multi-modal embeddings (see, e.g., Kiela, Bulat, & Clark, 2015; Kiela &
Clark, 2017; Shahmohammadi, Lensch, & Baayen, 2021, for olfactory, audi-
tory, and visual grounding of embeddings) will help enhance the modeling of
the conceptualization of plurality in English.","However, these embeddings
are far from perfect (see also Shahmohammadi et al., this volume, and Stu-
pak & Baayen, this volume).","For representing the speech signal of singular and plural nouns, we made use
of the Continuous Frequency Band Summary Features (C-FBSF) proposed by
Shafaei-Bajestan et al.",2022-07-05 10:44:26+00:00,Making sense of spoken plurals,cs.CL,"['cs.CL', 'J.5']","[arxiv.Result.Author('Elnaz Shafaei-Bajestan'), arxiv.Result.Author('Peter Uhrig'), arxiv.Result.Author('R. Harald Baayen')]","Distributional semantics offers new ways to study the semantics of
morphology. This study focuses on the semantics of noun singulars and their
plural inflectional variants in English. Our goal is to compare two models for
the conceptualization of plurality. One model (FRACSS) proposes that all
singular-plural pairs should be taken into account when predicting plural
semantics from singular semantics. The other model (CCA) argues that
conceptualization for plurality depends primarily on the semantic class of the
base word. We compare the two models on the basis of how well the speech signal
of plural tokens in a large corpus of spoken American English aligns with the
semantic vectors predicted by the two models. Two measures are employed: the
performance of a form-to-meaning mapping and the correlations between form
distances and meaning distances. Results converge on a superior alignment for
CCA. Our results suggest that usage-based approaches to pluralization in which
a given word's own semantic neighborhood is given priority outperform theories
according to which pluralization is conceptualized as a process building on
high-level abstraction. We see that what has often been conceived of as a
highly abstract concept, [+plural], is better captured via a family of
mid-level partial generalizations."
8483,"We
      videos, and an additional 200K simulations                                            then ﬁlter this list down to only a subset of con-
      without videos to support further research in                                         crete actions that include objects which exist in
      this area.1                                                                           the Unity asset store, since we use Unity simu-
                                                                                            lated environments to build our training and evalu-
Overall, our ﬁndings suggest a process by which                                             ation data (§2.4).","(2015) use crowdworkers2 to
   • We release the SPATIAL dataset of 9.5K sim-                                            judge noun-verb pairs and includes over 900 verbs
      ulated object interactions and accompanying                                           that are both abstract and concrete in nature.","This results in a list of six affor-
grounded lexical representations–of the type dis-                                           dances (roll, slide, stack, contain,
cussed by Pustejovsky and Krishnaswamy (2014)                                               wrap-grasp, bounce) which are used to as-
and Siskind (2001)–could potentially arise organi-                                          sign binary labels to each of 39 objects from 11
cally.",2022-07-05 19:19:53+00:00,Pretraining on Interactions for Learning Grounded Affordance Representations,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jack Merullo'), arxiv.Result.Author('Dylan Ebert'), arxiv.Result.Author('Carsten Eickhoff'), arxiv.Result.Author('Ellie Pavlick')]","Lexical semantics and cognitive science point to affordances (i.e. the
actions that objects support) as critical for understanding and representing
nouns and verbs. However, study of these semantic features has not yet been
integrated with the ""foundation"" models that currently dominate language
representation research. We hypothesize that predictive modeling of object
state over time will result in representations that encode object affordance
information ""for free"". We train a neural network to predict objects'
trajectories in a simulated interaction and show that our network's latent
representations differentiate between both observed and unobserved affordances.
We find that models trained using 3D simulations from our SPATIAL dataset
outperform conventional 2D computer vision models trained on a similar task,
and, on initial inspection, that differences between concepts correspond to
expected features (e.g., roll entails rotation). Our results suggest a way in
which modern deep learning approaches to grounded language learning can be
integrated with traditional formal semantic notions of lexical representations."
8495,"By further studying the application of PTMs in ABSA tasks, we made
fine-tuning and mechanism enhancement on it.","LCF can compute local
context features.","Therefore, the main contributions of this paper are as follows:
   1.",2022-07-06 03:50:31+00:00,Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Tianyu Zhao'), arxiv.Result.Author('Junping Du'), arxiv.Result.Author('Zhe Xu'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Zeli Guan')]","Text sentiment analysis, also known as opinion mining, is research on the
calculation of people's views, evaluations, attitude and emotions expressed by
entities. Text sentiment analysis can be divided into text-level sentiment
analysis, sen-tence-level sentiment analysis and aspect-level sentiment
analysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the
field of sentiment analysis, which aims to predict the polarity of aspects. The
research of pre-training neural model has significantly improved the
performance of many natural language processing tasks. In recent years, pre
training model (PTM) has been applied in ABSA. Therefore, there has been a
question, which is whether PTMs contain sufficient syntactic information for
ABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced
BERT with disentangled attention) to solve Aspect-Based Sentiment Analysis
problem. DeBERTa is a kind of neural language model based on transformer, which
uses self-supervised learning to pre-train on a large number of original text
corpora. Based on the Local Context Focus (LCF) mechanism, by integrating
DeBERTa model, we purpose a multi-task learning model for aspect-based
sentiment analysis. The experiments result on the most commonly used the laptop
and restaurant datasets of SemEval-2014 and the ACL twitter dataset show that
LCF mechanism with DeBERTa has significant improvement."
8496,"By further studying the application of PTMs in ABSA tasks, we made
fine-tuning and mechanism enhancement on it.","LCF can compute local
context features.","Therefore, the main contributions of this paper are as follows:
   1.",2022-07-06 03:50:31+00:00,Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Tianyu Zhao'), arxiv.Result.Author('Junping Du'), arxiv.Result.Author('Zhe Xue'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Zeli Guan')]","Text sentiment analysis, also known as opinion mining, is research on the
calculation of people's views, evaluations, attitude and emotions expressed by
entities. Text sentiment analysis can be divided into text-level sentiment
analysis, sen-tence-level sentiment analysis and aspect-level sentiment
analysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the
field of sentiment analysis, which aims to predict the polarity of aspects. The
research of pre-training neural model has significantly improved the
performance of many natural language processing tasks. In recent years, pre
training model (PTM) has been applied in ABSA. Therefore, there has been a
question, which is whether PTMs contain sufficient syntactic information for
ABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced
BERT with disentangled attention) to solve Aspect-Based Sentiment Analysis
problem. DeBERTa is a kind of neural language model based on transformer, which
uses self-supervised learning to pre-train on a large number of original text
corpora. Based on the Local Context Focus (LCF) mechanism, by integrating
DeBERTa model, we purpose a multi-task learning model for aspect-based
sentiment analysis. The experiments result on the most commonly used the laptop
and restaurant datasets of SemEval-2014 and the ACL twitter dataset show that
LCF mechanism with DeBERTa has significant improvement."
8502,"how BERT estimates the relevance of a query-document pair; what
features are encoded and which of those are essential for its perfor-        For reproducibility and to encourage further research in this
mance.",Little is known about      and GLUE).,"direction, we make our new order-invariant BOW-BERT model
                                                                          available to the public under https://github.com/davidmrau/ictir22.",2022-07-06 08:54:18+00:00,The Role of Complex NLP in Transformers for Text Ranking?,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('David Rau'), arxiv.Result.Author('Jaap Kamps')]","Even though term-based methods such as BM25 provide strong baselines in
ranking, under certain conditions they are dominated by large pre-trained
masked language models (MLMs) such as BERT. To date, the source of their
effectiveness remains unclear. Is it their ability to truly understand the
meaning through modeling syntactic aspects? We answer this by manipulating the
input order and position information in a way that destroys the natural
sequence order of query and passage and shows that the model still achieves
comparable performance. Overall, our results highlight that syntactic aspects
do not play a critical role in the effectiveness of re-ranking with BERT. We
point to other mechanisms such as query-passage cross-attention and richer
embeddings that capture word meanings based on aggregated context regardless of
the word order for being the main attributions for its superior performance."
8657,"In order to make

-- 11 --
further research possible, we have released the data openly on Zenodo3 for other
researchers to use.","The fact
that our data is parallel makes it possible to conduct more research in the future on
persuasive intent and its realization across different languages.","The multilingual BERT model did not work too well, even when it was trained with
the training data of all of the languages.",2022-07-10 12:38:02+00:00,Multilingual Persuasion Detection: Video Games as an Invaluable Data Source for NLP,cs.CL,['cs.CL'],"[arxiv.Result.Author('Teemu Pöyhönen'), arxiv.Result.Author('Mika Hämäläinen'), arxiv.Result.Author('Khalid Alnajjar')]","Role-playing games (RPGs) have a considerable amount of text in video game
dialogues. Quite often this text is semi-annotated by the game developers. In
this paper, we extract a multilingual dataset of persuasive dialogue from
several RPGs. We show the viability of this data in building a persuasion
detection system using a natural language processing (NLP) model called BERT.
We believe that video games have a lot of unused potential as a datasource for
a variety of NLP tasks. The code and data described in this paper are available
on Zenodo."
8661,"Second, the eﬀects of our method on ‘low-level’ grammatical tasks require
    further study in English, as we focused on GLUE and sentiment analysis.","Addressing this limi-
    tation would require extending our method to support facts that span
    multiple tokens.","The used Dutch benchmarks do cover more tasks and seem to indicate
    favourable reductions.",2022-07-10 21:56:56+00:00,FairDistillation: Mitigating Stereotyping in Language Models,cs.CL,"['cs.CL', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Pieter Delobelle'), arxiv.Result.Author('Bettina Berendt')]","Large pre-trained language models are successfully being used in a variety of
tasks, across many languages. With this ever-increasing usage, the risk of
harmful side effects also rises, for example by reproducing and reinforcing
stereotypes. However, detecting and mitigating these harms is difficult to do
in general and becomes computationally expensive when tackling multiple
languages or when considering different biases. To address this, we present
FairDistillation: a cross-lingual method based on knowledge distillation to
construct smaller language models while controlling for specific biases. We
found that our distillation method does not negatively affect the downstream
performance on most tasks and successfully mitigates stereotyping and
representational harms. We demonstrate that FairDistillation can create fairer
language models at a considerably lower cost than alternative approaches."
8662,"Second, the eﬀects of our method on ‘low-level’ grammatical tasks require
    further study in English, as we focused on GLUE and sentiment analysis.","Addressing this limi-
    tation would require extending our method to support facts that span
    multiple tokens.","The used Dutch benchmarks do cover more tasks and seem to indicate
    favourable reductions.",2022-07-10 21:56:56+00:00,FairDistillation: Mitigating Stereotyping in Language Models,cs.CL,"['cs.CL', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Pieter Delobelle'), arxiv.Result.Author('Bettina Berendt')]","Large pre-trained language models are successfully being used in a variety of
tasks, across many languages. With this ever-increasing usage, the risk of
harmful side effects also rises, for example by reproducing and reinforcing
stereotypes. However, detecting and mitigating these harms is difficult to do
in general and becomes computationally expensive when tackling multiple
languages or when considering different biases. To address this, we present
FairDistillation: a cross-lingual method based on knowledge distillation to
construct smaller language models while controlling for specific biases. We
found that our distillation method does not negatively affect the downstream
performance on most tasks and successfully mitigates stereotyping and
representational harms. We demonstrate that FairDistillation can create fairer
language models at a considerably lower cost than alternative approaches."
8669,"We open source all the benchmarks, data, scripts, and models described in this effort to
support further research.1 In addition, we focus on the practical applicability of our work for
low-resource speaking communities.","Finally, beyond creating these models, we also reflect on the
creation process, analyzing the risks and benefits of our research from a societal standpoint.","We deploy our techniques to provide translation support
to Wikipedia editors, enabling them to create new articles more efficiently for languages
that are not supported by other translation systems.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8670,"The ability to evaluate allows us to compare different approaches and understand
what requires further research and development.","4.1 FLORES-200

A major area of focus in machine translation research has been on the development of high-
quality evaluation datasets, or benchmarks that can be reliably used to assess progress in
the field.","The creation of benchmark datasets
at the yearly Workshop on Machine Translation (Akhbardeh et al., 2021) led to rapid
progress on translation directions such as English to German and English to French.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8671,"To facilitate further research using xsim, we also
open-source this evaluation method to the community.22

End-to-end Encoder Evaluation.","English is encoded by the teacher and the
other language by the LASER3 student.","Once we have identified the best sentence encoder for
each language using the xsim scores, we perform mining, add the mined data to the existing
bitexts, and train a bilingual NMT system.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8672,"We open source all the benchmarks, data, scripts, and models described in this effort to
support further research.1 In addition, we focus on the practical applicability of our work for
low-resource speaking communities.","Finally, beyond creating these models, we also reflect on the
creation process, analyzing the risks and benefits of our research from a societal standpoint.","We deploy our techniques to provide translation support
to Wikipedia editors, enabling them to create new articles more efficiently for languages
that are not supported by other translation systems.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB Team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8673,"The ability to evaluate allows us to compare different approaches and understand
what requires further research and development.","4.1 FLORES-200

A major area of focus in machine translation research has been on the development of high-
quality evaluation datasets, or benchmarks that can be reliably used to assess progress in
the field.","The creation of benchmark datasets
at the yearly Workshop on Machine Translation (Akhbardeh et al., 2021) led to rapid
progress on translation directions such as English to German and English to French.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB Team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8674,"To facilitate further research using xsim, we also
open-source this evaluation method to the community.22

End-to-end Encoder Evaluation.","English is encoded by the teacher and the
other language by the LASER3 student.","Once we have identified the best sentence encoder for
each language using the xsim scores, we perform mining, add the mined data to the existing
bitexts, and train a bilingual NMT system.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB Team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8675,"We open source all the benchmarks, data, scripts, and models described in this effort to
support further research.1 In addition, we focus on the practical applicability of our work for
low-resource speaking communities.","Finally, beyond creating these models, we also reflect on the
creation process, analyzing the risks and benefits of our research from a societal standpoint.","We deploy our techniques to provide translation support
to Wikipedia editors, enabling them to create new articles more efficiently for languages
that are not supported by other translation systems.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB Team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8676,"The ability to evaluate allows us to compare different approaches and understand
what requires further research and development.","4.1 FLORES-200

A major area of focus in machine translation research has been on the development of high-
quality evaluation datasets, or benchmarks that can be reliably used to assess progress in
the field.","The creation of benchmark datasets
at the yearly Workshop on Machine Translation (Akhbardeh et al., 2021) led to rapid
progress on translation directions such as English to German and English to French.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB Team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8677,"To facilitate further research using xsim, we also
open-source this evaluation method to the community.22

End-to-end Encoder Evaluation.","English is encoded by the teacher and the
other language by the LASER3 student.","Once we have identified the best sentence encoder for
each language using the xsim scores, we perform mining, add the mined data to the existing
bitexts, and train a bilingual NMT system.",2022-07-11 07:33:36+00:00,No Language Left Behind: Scaling Human-Centered Machine Translation,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('NLLB Team'), arxiv.Result.Author('Marta R. Costa-jussà'), arxiv.Result.Author('James Cross'), arxiv.Result.Author('Onur Çelebi'), arxiv.Result.Author('Maha Elbayad'), arxiv.Result.Author('Kenneth Heafield'), arxiv.Result.Author('Kevin Heffernan'), arxiv.Result.Author('Elahe Kalbassi'), arxiv.Result.Author('Janice Lam'), arxiv.Result.Author('Daniel Licht'), arxiv.Result.Author('Jean Maillard'), arxiv.Result.Author('Anna Sun'), arxiv.Result.Author('Skyler Wang'), arxiv.Result.Author('Guillaume Wenzek'), arxiv.Result.Author('Al Youngblood'), arxiv.Result.Author('Bapi Akula'), arxiv.Result.Author('Loic Barrault'), arxiv.Result.Author('Gabriel Mejia Gonzalez'), arxiv.Result.Author('Prangthip Hansanti'), arxiv.Result.Author('John Hoffman'), arxiv.Result.Author('Semarley Jarrett'), arxiv.Result.Author('Kaushik Ram Sadagopan'), arxiv.Result.Author('Dirk Rowe'), arxiv.Result.Author('Shannon Spruit'), arxiv.Result.Author('Chau Tran'), arxiv.Result.Author('Pierre Andrews'), arxiv.Result.Author('Necip Fazil Ayan'), arxiv.Result.Author('Shruti Bhosale'), arxiv.Result.Author('Sergey Edunov'), arxiv.Result.Author('Angela Fan'), arxiv.Result.Author('Cynthia Gao'), arxiv.Result.Author('Vedanuj Goswami'), arxiv.Result.Author('Francisco Guzmán'), arxiv.Result.Author('Philipp Koehn'), arxiv.Result.Author('Alexandre Mourachko'), arxiv.Result.Author('Christophe Ropers'), arxiv.Result.Author('Safiyyah Saleem'), arxiv.Result.Author('Holger Schwenk'), arxiv.Result.Author('Jeff Wang')]","Driven by the goal of eradicating language barriers on a global scale,
machine translation has solidified itself as a key focus of artificial
intelligence research today. However, such efforts have coalesced around a
small subset of languages, leaving behind the vast majority of mostly
low-resource languages. What does it take to break the 200 language barrier
while ensuring safe, high quality results, all while keeping ethical
considerations in mind? In No Language Left Behind, we took on this challenge
by first contextualizing the need for low-resource language translation support
through exploratory interviews with native speakers. Then, we created datasets
and models aimed at narrowing the performance gap between low and high-resource
languages. More specifically, we developed a conditional compute model based on
Sparsely Gated Mixture of Experts that is trained on data obtained with novel
and effective data mining techniques tailored for low-resource languages. We
propose multiple architectural and training improvements to counteract
overfitting while training on thousands of tasks. Critically, we evaluated the
performance of over 40,000 different translation directions using a
human-translated benchmark, Flores-200, and combined human evaluation with a
novel toxicity benchmark covering all languages in Flores-200 to assess
translation safety. Our model achieves an improvement of 44% BLEU relative to
the previous state-of-the-art, laying important groundwork towards realizing a
universal translation system. Finally, we open source all contributions
described in this work, accessible at
https://github.com/facebookresearch/fairseq/tree/nllb."
8705,"(6)) (the examination of the excluded part
is left for further research).","Properly contained discourse relations

Properly contained discourse relations are a subtype of fully embedded ones except that some
material is left out (shown with three dots in Ex.","Similar to fully embedded relations, properly contained relations
tend to occur in the patterns where 𝐷𝐶2 is an explicit discourse connective.",2022-07-11 16:57:00+00:00,A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Deniz Zeyrek'), arxiv.Result.Author('Mustafa Erolcan Er')]","We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing."
8706,"The
investigation of such factors is left for further research.",the sense of 𝐷𝐶1 and/or 𝐷𝐶2.,"Acknowledgments

We acknowledge the partial support of Middle East Technical University (BAP-07-04-2017-001)
and thank Salih Fırat Canpolat, Deniz Dilek Bilgiç, Ozan Deniz, Ali Can Serhan Yılmaz, Zeynep
Başer, Özgür Şen Bartan, Aytaç Çeltek and Murathan Kurfalı for their assistance at various
stages of the development of TDB 1.2.",2022-07-11 16:57:00+00:00,A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Deniz Zeyrek'), arxiv.Result.Author('Mustafa Erolcan Er')]","We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing."
8707,"(6)) (the examination of the excluded part
is left for further research).","Properly contained discourse relations

Properly contained discourse relations are a subtype of fully embedded ones except that some
material is left out (shown with three dots in Ex.","Similar to fully embedded relations, properly contained relations
tend to occur in the patterns where 𝐷𝐶2 is an explicit discourse connective.",2022-07-11 16:57:00+00:00,A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Deniz Zeyrek'), arxiv.Result.Author('Mustafa Erolcan Er')]","We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing."
8708,"The
investigation of such factors is left for further research.",the sense of 𝐷𝐶1 and/or 𝐷𝐶2.,"Acknowledgments

We acknowledge the partial support of Middle East Technical University (BAP-07-04-2017-001)
and thank Salih Fırat Canpolat, Deniz Dilek Bilgiç, Ozan Deniz, Ali Can Serhan Yılmaz, Zeynep
Başer, Özgür Şen Bartan, Aytaç Çeltek and Murathan Kurfalı for their assistance at various
stages of the development of TDB 1.2.",2022-07-11 16:57:00+00:00,A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Deniz Zeyrek'), arxiv.Result.Author('Mustafa Erolcan Er')]","We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing."
8856,"This architecture permits us to      releasing our source code as open source (Apache
integrate results from BM25, further improving ac-      2.0 license) to enable further research.","We are
initial retrieval.",curacy.,2022-07-13 15:51:40+00:00,"Re2G: Retrieve, Rerank, Generate",cs.CL,"['cs.CL', 'cs.AI', 'cs.IR']","[arxiv.Result.Author('Michael Glass'), arxiv.Result.Author('Gaetano Rossiello'), arxiv.Result.Author('Md Faisal Mahbub Chowdhury'), arxiv.Result.Author('Ankita Rajaram Naik'), arxiv.Result.Author('Pengshan Cai'), arxiv.Result.Author('Alfio Gliozzo')]","As demonstrated by GPT-3 and T5, transformers grow in capability as parameter
spaces become larger and larger. However, for tasks that require a large amount
of knowledge, non-parametric memory allows models to grow dramatically with a
sub-linear increase in computational cost and GPU memory requirements. Recent
models such as RAG and REALM have introduced retrieval into conditional
generation. These models incorporate neural initial retrieval from a corpus of
passages. We build on this line of research, proposing Re2G, which combines
both neural initial retrieval and reranking into a BART-based
sequence-to-sequence generation. Our reranking approach also permits merging
retrieval results from sources with incomparable scores, enabling an ensemble
of BM25 and neural initial retrieval. To train our system end-to-end, we
introduce a novel variation of knowledge distillation to train the initial
retrieval, reranker, and generation using only ground truth on the target
sequence output. We find large gains in four diverse tasks: zero-shot slot
filling, question answering, fact-checking, and dialog, with relative gains of
9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make
our code available as open source at
https://github.com/IBM/kgi-slot-filling/tree/re2g."
8888,"These results may open a way to further research about the effect
of direct knowledge transfer among languages that use the same script, in particular, Nastalíq.","It is plausible that the combination of a powerful
deep learning model and fine-tuning on a relevant, although somewhat unexpectedly, dataset was
key for the high performance.","25

      https://huggingface.co/Hate-speech-CNERG/dehatebert-mono-arabic
Table 7
Final results and ranking for Subtask B: Threatening language detection

            Team Names                    Private Leaderboard            Public Leaderboard
                                     Rank F1 Score ROC AUC
               hate-alert                                                F1 Score ROC AUC
                 SATLab
          Somnath Banerjee           1  0.545   0.810                    0.489  0.798
 Org’s LogReg v2 sol., out-of-comp.",2022-07-14 07:38:13+00:00,Overview of Abusive and Threatening Language Detection in Urdu at FIRE 2021,cs.CL,['cs.CL'],"[arxiv.Result.Author('Maaz Amjad'), arxiv.Result.Author('Alisa Zhila'), arxiv.Result.Author('Grigori Sidorov'), arxiv.Result.Author('Andrey Labunets'), arxiv.Result.Author('Sabur Butta'), arxiv.Result.Author('Hamza Imam Amjad'), arxiv.Result.Author('Oxana Vitman'), arxiv.Result.Author('Alexander Gelbukh')]","With the growth of social media platform influence, the effect of their
misuse becomes more and more impactful. The importance of automatic detection
of threatening and abusive language can not be overestimated. However, most of
the existing studies and state-of-the-art methods focus on English as the
target language, with limited work on low- and medium-resource languages. In
this paper, we present two shared tasks of abusive and threatening language
detection for the Urdu language which has more than 170 million speakers
worldwide. Both are posed as binary classification tasks where participating
systems are required to classify tweets in Urdu into two classes, namely: (i)
Abusive and Non-Abusive for the first task, and (ii) Threatening and
Non-Threatening for the second. We present two manually annotated datasets
containing tweets labelled as (i) Abusive and Non-Abusive, and (ii) Threatening
and Non-Threatening. The abusive dataset contains 2400 annotated tweets in the
train part and 1100 annotated tweets in the test part. The threatening dataset
contains 6000 annotated tweets in the train part and 3950 annotated tweets in
the test part. We also provide logistic regression and BERT-based baseline
classifiers for both tasks. In this shared task, 21 teams from six countries
registered for participation (India, Pakistan, China, Malaysia, United Arab
Emirates, and Taiwan), 10 teams submitted their runs for Subtask A, which is
Abusive Language Detection and 9 teams submitted their runs for Subtask B,
which is Threatening Language detection, and seven teams submitted their
technical reports. The best performing system achieved an F1-score value of
0.880 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based
transformer model showed the best performance."
8902,"17We do not intend to claim state-of-the-art performance, but to demonstrate that PIXEL is effective in over-
coming the vocabulary bottleneck and to provide a starting point for further research on pixel-based encoding
of language.",16We use BERT weights from https://huggingface.co/bert-base-cased.,"18Pearson correlation r=0.9, p<0.001 for POS tagging, r=0.95, p<0.0001 for dependency parsing.",2022-07-14 15:20:36+00:00,Language Modelling with Pixels,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Phillip Rust'), arxiv.Result.Author('Jonas F. Lotz'), arxiv.Result.Author('Emanuele Bugliarello'), arxiv.Result.Author('Elizabeth Salesky'), arxiv.Result.Author('Miryam de Lhoneux'), arxiv.Result.Author('Desmond Elliott')]","Language models are defined over a finite set of inputs, which creates a
vocabulary bottleneck when we attempt to scale the number of supported
languages. Tackling this bottleneck results in a trade-off between what can be
represented in the embedding matrix and computational issues in the output
layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which
suffers from neither of these issues. PIXEL is a pretrained language model that
renders text as images, making it possible to transfer representations across
languages based on orthographic similarity or the co-activation of pixels.
PIXEL is trained to reconstruct the pixels of masked patches, instead of
predicting a distribution over tokens. We pretrain the 86M parameter PIXEL
model on the same English data as BERT and evaluate on syntactic and semantic
tasks in typologically diverse languages, including various non-Latin scripts.
We find that PIXEL substantially outperforms BERT on syntactic and semantic
processing tasks on scripts that are not found in the pretraining data, but
PIXEL is slightly weaker than BERT when working with Latin scripts.
Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT,
further confirming the benefits of modelling language with pixels."
8910,"investigating similarities between transformer-
based language models and humans at the neu-           Content eﬀects are generally more pronounced
ral processing level (Schrimpf et al., 2021; Gold-  in diﬃcult tasks that require extensive logical rea-
stein et al., 2022; Kumar et al., 2022), and could  soning, and are stronger in children or adults un-
generate exciting hypotheses for further research   der cognitive load (Evans, 1989; Evans and Perry,
on the computational basis of human reasoning.","Finally, our
behavioral results complement other research        an entailment).",1995).,2022-07-14 16:51:09+00:00,Language models show human-like content effects on reasoning,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ishita Dasgupta'), arxiv.Result.Author('Andrew K. Lampinen'), arxiv.Result.Author('Stephanie C. Y. Chan'), arxiv.Result.Author('Antonia Creswell'), arxiv.Result.Author('Dharshan Kumaran'), arxiv.Result.Author('James L. McClelland'), arxiv.Result.Author('Felix Hill')]","Abstract reasoning is a key ability for an intelligent system. Large language
models achieve above-chance performance on abstract reasoning tasks, but
exhibit many imperfections. However, human abstract reasoning is also
imperfect, and depends on our knowledge and beliefs about the content of the
reasoning problem. For example, humans reason much more reliably about logical
rules that are grounded in everyday situations than arbitrary rules about
abstract attributes. The training experiences of language models similarly
endow them with prior expectations that reflect human knowledge and beliefs. We
therefore hypothesized that language models would show human-like content
effects on abstract reasoning problems. We explored this hypothesis across
three logical reasoning tasks: natural language inference, judging the logical
validity of syllogisms, and the Wason selection task (Wason, 1968). We find
that state of the art large language models (with 7 or 70 billion parameters;
Hoffman et al., 2022) reflect many of the same patterns observed in humans
across these tasks -- like humans, models reason more effectively about
believable situations than unrealistic or abstract ones. Our findings have
implications for understanding both these cognitive effects, and the factors
that contribute to language model performance."
8972,Compared to       which requires further research.,"Due to the difference
emotion classiﬁcation accuracy, which can explain     of the biases, the effect of debiasing also varies,
the results in Table 4 and Table 6.","Our model also
the single modal results (Table 4), our MMKET         has a few limitations.",2022-07-17 08:16:49+00:00,A Multibias-mitigated and Sentiment Knowledge Enriched Transformer for Debiasing in Multimodal Conversational Emotion Recognition,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jinglin Wang'), arxiv.Result.Author('Fang Ma'), arxiv.Result.Author('Yazhou Zhang'), arxiv.Result.Author('Dawei Song')]","Multimodal emotion recognition in conversations (mERC) is an active research
topic in natural language processing (NLP), which aims to predict human's
emotional states in communications of multiple modalities, e,g., natural
language and facial gestures. Innumerable implicit prejudices and
preconceptions fill human language and conversations, leading to the question
of whether the current data-driven mERC approaches produce a biased error. For
example, such approaches may offer higher emotional scores on the utterances by
females than males. In addition, the existing debias models mainly focus on
gender or race, where multibias mitigation is still an unexplored task in mERC.
In this work, we take the first step to solve these issues by proposing a
series of approaches to mitigate five typical kinds of bias in textual
utterances (i.e., gender, age, race, religion and LGBTQ+) and visual
representations (i.e, gender and age), followed by a Multibias-Mitigated and
sentiment Knowledge Enriched bi-modal Transformer (MMKET). Comprehensive
experimental results show the effectiveness of the proposed model and prove
that the debias operation has a great impact on the classification performance
for mERC. We hope our study will benefit the development of bias mitigation in
mERC and related emotion studies."
8984,"In a further study, we intend to present check if the dialogue
obtained during the experiment can be used to evaluate and train dialogue models.","We hope that that the
community will be able to beneﬁt from PxSLU which will be distributed with a Attribution 4.0
International (CC BY 4.0) license.","7 Acknowledgements

This work was supported by a CIFRE grant number 2017/1798 from ANRT (National Association for
Research and Technology) and was partially supported by MIAI@Grenoble-Alpes (ANR-19-P3IA-
0003).",2022-07-17 21:18:03+00:00,A Spoken Drug Prescription Dataset in French for Spoken Language Understanding,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ali Can Kocabiyikoglu'), arxiv.Result.Author('François Portet'), arxiv.Result.Author('Prudence Gibert'), arxiv.Result.Author('Hervé Blanchon'), arxiv.Result.Author('Jean-Marc Babouchkine'), arxiv.Result.Author('Gaëtan Gavazzi')]","Spoken medical dialogue systems are increasingly attracting interest to
enhance access to healthcare services and improve quality and traceability of
patient care. In this paper, we focus on medical drug prescriptions acquired on
smartphones through spoken dialogue. Such systems would facilitate the
traceability of care and would free clinicians' time. However, there is a lack
of speech corpora to develop such systems since most of the related corpora are
in text form and in English. To facilitate the research and development of
spoken medical dialogue systems, we present, to the best of our knowledge, the
first spoken medical drug prescriptions corpus, named PxSLU. It contains 4
hours of transcribed and annotated dialogues of drug prescriptions in French
acquired through an experiment with 55 participants experts and non-experts in
prescriptions. We also present some experiments that demonstrate the interest
of this corpus for the evaluation and development of medical dialogue systems."
8986,"Besides, in call center conversations scenarios, the caller (or user) might express their impatience,
complaint and other emotions, which are especially interesting and make DECODA corpus a seldom
corpus for further research related to emotional aspects of dialogue summarization.","3
Previous works on DECODA dialogue summarization reported results of extractive methods and
abstractive results, our hypothesis is that by using pre-trained language models, we could get better
results in this task.","3.2 DECODA Corpus and Data Partitioning

DECODA call center conversations are a kind of task-oriented dialogue.",2022-07-17 21:43:18+00:00,Effectiveness of French Language Models on Abstractive Dialogue Summarization Task,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yongxin Zhou'), arxiv.Result.Author('François Portet'), arxiv.Result.Author('Fabien Ringeval')]","Pre-trained language models have established the state-of-the-art on various
natural language processing tasks, including dialogue summarization, which
allows the reader to quickly access key information from long conversations in
meetings, interviews or phone calls. However, such dialogues are still
difficult to handle with current models because the spontaneity of the language
involves expressions that are rarely present in the corpora used for
pre-training the language models. Moreover, the vast majority of the work
accomplished in this field has been focused on English. In this work, we
present a study on the summarization of spontaneous oral dialogues in French
using several language specific pre-trained models: BARThez, and BelGPT-2, as
well as multilingual pre-trained models: mBART, mBARThez, and mT5. Experiments
were performed on the DECODA (Call Center) dialogue corpus whose task is to
generate abstractive synopses from call center conversations between a caller
and one or several agents depending on the situation. Results show that the
BARThez models offer the best performance far above the previous
state-of-the-art on DECODA. We further discuss the limits of such pre-trained
models and the challenges that must be addressed for summarizing spontaneous
dialogues."
9034,"As another direction of
further research, we would like to analyze emotional attitudes towards diﬀerent
ethically relevant concepts in the works of the same author (sentiment analysis).","As a next step in our research, we are planning to extend the analysis pre-
sented in this paper by considering and comparing the eﬀect of diﬀerent aspects
relevant to the process of authorship analysis, such as style (linguistic features)
and contentual characteristics (semantics and topics).","We also plan to deﬁne a list of text features that the model should take into
consideration in order for it to be applied to other tasks such as authorship
analysis in online harassment scenarios and broaden this set of features to make
it adaptable to use with an artiﬁcial companion.",2022-07-19 05:43:49+00:00,Can You Fool AI by Doing a 180? $\unicode{x2013}$ A Case Study on Authorship Analysis of Texts by Arata Osada,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jagna Nieuwazny'), arxiv.Result.Author('Karol Nowakowski'), arxiv.Result.Author('Michal Ptaszynski'), arxiv.Result.Author('Fumito Masui')]","This paper is our attempt at answering a twofold question covering the areas
of ethics and authorship analysis. Firstly, since the methods used for
performing authorship analysis imply that an author can be recognized by the
content he or she creates, we were interested in finding out whether it would
be possible for an author identification system to correctly attribute works to
authors if in the course of years they have undergone a major psychological
transition. Secondly, and from the point of view of the evolution of an
author's ethical values, we checked what it would mean if the authorship
attribution system encounters difficulties in detecting single authorship. We
set out to answer those questions through performing a binary authorship
analysis task using a text classifier based on a pre-trained transformer model
and a baseline method relying on conventional similarity metrics. For the test
set, we chose works of Arata Osada, a Japanese educator and specialist in the
history of education, with half of them being books written before the World
War II and another half in the 1950s, in between which he underwent a
transformation in terms of political opinions. As a result, we were able to
confirm that in the case of texts authored by Arata Osada in a time span of
more than 10 years, while the classification accuracy drops by a large margin
and is substantially lower than for texts by other non-fiction writers,
confidence scores of the predictions remain at a similar level as in the case
of a shorter time span, indicating that the classifier was in many instances
tricked into deciding that texts written over a time span of multiple years
were actually written by two different people, which in turn leads us to
believe that such a change can affect authorship analysis, and that historical
events have great impact on a person's ethical outlook as expressed in their
writings."
9044,"source constraints we perform a hyperparameter
                                                                     The F1 performances of the FrALBERTbase
    2We share our SQuAD-fr corpus on request and on dataset       model are close to those of the CamemBERTbase
sharing platforms to support further research in this area.","The highest F1 score is 81.2 on
   To address our considerations related to re-                   FQuADdev and 68.1 on PIAFdev.","model, both pre-trained on the French content of
                                                                  Wikipedia (4GB).",2022-07-19 09:46:15+00:00,On the Usability of Transformers-based models for a French Question-Answering task,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('Oralie Cattan'), arxiv.Result.Author('Christophe Servan'), arxiv.Result.Author('Sophie Rosset')]","For many tasks, state-of-the-art results have been achieved with
Transformer-based architectures, resulting in a paradigmatic shift in practices
from the use of task-specific architectures to the fine-tuning of pre-trained
language models. The ongoing trend consists in training models with an
ever-increasing amount of data and parameters, which requires considerable
resources. It leads to a strong search to improve resource efficiency based on
algorithmic and hardware improvements evaluated only for English. This raises
questions about their usability when applied to small-scale learning problems,
for which a limited amount of training data is available, especially for
under-resourced languages tasks. The lack of appropriately sized corpora is a
hindrance to applying data-driven and transfer learning-based approaches with
strong instability cases. In this paper, we establish a state-of-the-art of the
efforts dedicated to the usability of Transformer-based models and propose to
evaluate these improvements on the question-answering performances of French
language which have few resources. We address the instability relating to data
scarcity by investigating various training strategies with data augmentation,
hyperparameters optimization and cross-lingual transfer. We also introduce a
new compact model for French FrALBERT which proves to be competitive in
low-resource settings."
9153,"We
will leave the further study of a more complicated iterative approach for future work.","We conjecture it might be unnecessary to
consider the importance of test cases in this way, or an obviously good test case with a high score
can pass many different solutions without introducing differentiation to rank the code solutions.","As mentioned in section 2.2, a straightforward way to score a code solution is to simply count the
number of test cases it can pass.",2022-07-21 10:18:37+00:00,CodeT: Code Generation with Generated Tests,cs.CL,"['cs.CL', 'cs.AI', 'cs.PL', 'cs.SE']","[arxiv.Result.Author('Bei Chen'), arxiv.Result.Author('Fengji Zhang'), arxiv.Result.Author('Anh Nguyen'), arxiv.Result.Author('Daoguang Zan'), arxiv.Result.Author('Zeqi Lin'), arxiv.Result.Author('Jian-Guang Lou'), arxiv.Result.Author('Weizhu Chen')]","Given a programming problem, pre-trained language models such as Codex have
demonstrated the ability to generate multiple different code solutions via
sampling. However, selecting a correct or best solution from those samples
still remains a challenge. While an easy way to verify the correctness of a
code solution is through executing test cases, producing high-quality test
cases is prohibitively expensive. In this paper, we explore the use of
pre-trained language models to automatically generate test cases, calling our
method CodeT: Code generation with generated Tests. CodeT executes the code
solutions using the generated test cases, and then chooses the best solution
based on a dual execution agreement with both the generated test cases and
other generated solutions. We evaluate CodeT on five different pre-trained
models with both HumanEval and MBPP benchmarks. Extensive experimental results
demonstrate CodeT can achieve significant, consistent, and surprising
improvements over previous methods. For example, CodeT improves the pass@1 on
HumanEval to 65.8%, an increase of absolute 18.8% on the code-davinci-002
model, and an absolute 20+% improvement over previous state-of-the-art results."
9166,"Nonetheless, we believe
mWIKINLI and our training setup offer a useful framework for further research into
multilingual learning with pretrained models.","One possibility is the presence of emergent cross-
lingual structure in mBERT (Conneau et al., 2020b).","4.4 Summary

In this chapter, we described approaches to exploiting various naturally-occurring
structures on Wikipedia.",2022-07-21 17:26:03+00:00,Leveraging Natural Supervision for Language Representation Learning and Generation,cs.CL,['cs.CL'],[arxiv.Result.Author('Mingda Chen')],"Recent breakthroughs in Natural Language Processing (NLP) have been driven by
language models trained on a massive amount of plain text. While powerful,
deriving supervision from textual resources is still an open question. For
example, language model pretraining often neglects the rich, freely-available
structures in textual data. In this thesis, we describe three lines of work
that seek to improve the training and evaluation of neural models using
naturally-occurring supervision.
  We first investigate self-supervised training losses to help enhance the
performance of pretrained language models for various NLP tasks. Specifically,
we alter the sentence prediction loss to make it better suited to other
pretraining losses and more challenging to solve. We design an intermediate
finetuning step that uses self-supervised training to promote models' ability
in cross-task generalization.
  Then we describe methods to leverage the structures in Wikipedia and
paraphrases. In particular, we propose training losses to exploit hyperlinks,
article structures, and article category graphs for entity-, discourse-,
entailment-related knowledge. We propose a framework that uses paraphrase pairs
to disentangle semantics and syntax in sentence representations. We extend the
framework for a novel generation task that controls the syntax of output text
with a sentential exemplar.
  Lastly, we discuss our work on tailoring textual resources for establishing
challenging evaluation tasks. We introduce three datasets by defining novel
tasks using various fan-contributed websites, including a long-form
data-to-text generation dataset, a screenplay summarization dataset, and a
long-form story generation dataset. These datasets have unique characteristics
offering challenges to future work in their respective task settings."
9171,"by contacting authors) datasets, the ones that enable further research as well as
allow to explore the

Figure 4: PRISMA Model for Cyberbullying Detection Research.","Datasets

    Among the publications considered within this survey, we selected all those with publicly available or
otherwise accessible (e.g.","Yi and Zubiaga: Preprint submitted to Elsevier                Page 5 of 22
                                       Session-based Cyberbullying Detection in Social Media
Table 1
Available cyberbullying datasets.",2022-07-14 18:56:54+00:00,Session-based Cyberbullying Detection in Social Media: A Survey,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peiling Yi'), arxiv.Result.Author('Arkaitz Zubiaga')]","Cyberbullying is a pervasive problem in online social media, where a bully
abuses a victim through a social media session. By investigating cyberbullying
perpetrated through social media sessions, recent research has looked into
mining patterns and features for modeling and understanding the two defining
characteristics of cyberbullying: repetitive behavior and power imbalance. In
this survey paper, we define the Session-based Cyberbullying Detection
framework that encapsulates the different steps and challenges of the problem.
Based on this framework, we provide a comprehensive overview of session-based
cyberbullying detection in social media, delving into existing efforts from a
data and methodological perspective. Our review leads us to propose
evidence-based criteria for a set of best practices to create session-based
cyberbullying datasets. In addition, we perform benchmark experiments comparing
the performance of state-of-the-art session-based cyberbullying detection
models as well as large pre-trained language models across two different
datasets. Through our review, we also put forth a set of open challenges as
future research directions."
9172,"This in turn requires creation and release of additional
datasets, ideally from different social media platforms, to further study the generalisability of models
beyond these two platforms.","Still, the differences in performance we
observe across both these datasets call for the implementation of more generalisable models that can
perform well across different platforms and datasets.",7.,2022-07-14 18:56:54+00:00,Session-based Cyberbullying Detection in Social Media: A Survey,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peiling Yi'), arxiv.Result.Author('Arkaitz Zubiaga')]","Cyberbullying is a pervasive problem in online social media, where a bully
abuses a victim through a social media session. By investigating cyberbullying
perpetrated through social media sessions, recent research has looked into
mining patterns and features for modeling and understanding the two defining
characteristics of cyberbullying: repetitive behavior and power imbalance. In
this survey paper, we define the Session-based Cyberbullying Detection
framework that encapsulates the different steps and challenges of the problem.
Based on this framework, we provide a comprehensive overview of session-based
cyberbullying detection in social media, delving into existing efforts from a
data and methodological perspective. Our review leads us to propose
evidence-based criteria for a set of best practices to create session-based
cyberbullying datasets. In addition, we perform benchmark experiments comparing
the performance of state-of-the-art session-based cyberbullying detection
models as well as large pre-trained language models across two different
datasets. Through our review, we also put forth a set of open challenges as
future research directions."
9173,"In order to further research in cyberbullying detection, it is crucial
to enable reproducibility of existing models, so that researchers can build upon and improve existing models.","Not all cyberbullying models are reported with

sufficient details; where the code of these models isn’t published, it also means that they are not replicable
because the level of detail is insufficient.","Likewise, it is also important that research in cyberbullying detection considers more than a single dataset
in their studies, which enables evaluating the generalisability of models demonstrating competitive
performance not only on a single dataset.",2022-07-14 18:56:54+00:00,Session-based Cyberbullying Detection in Social Media: A Survey,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peiling Yi'), arxiv.Result.Author('Arkaitz Zubiaga')]","Cyberbullying is a pervasive problem in online social media, where a bully
abuses a victim through a social media session. By investigating cyberbullying
perpetrated through social media sessions, recent research has looked into
mining patterns and features for modeling and understanding the two defining
characteristics of cyberbullying: repetitive behavior and power imbalance. In
this survey paper, we define the Session-based Cyberbullying Detection
framework that encapsulates the different steps and challenges of the problem.
Based on this framework, we provide a comprehensive overview of session-based
cyberbullying detection in social media, delving into existing efforts from a
data and methodological perspective. Our review leads us to propose
evidence-based criteria for a set of best practices to create session-based
cyberbullying datasets. In addition, we perform benchmark experiments comparing
the performance of state-of-the-art session-based cyberbullying detection
models as well as large pre-trained language models across two different
datasets. Through our review, we also put forth a set of open challenges as
future research directions."
9242,"Lastly, a further study can be conducted by considering more datasets to investigate general capability of
the proposals in this study.",with CAM.,"References

 [1] Nir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and David Lazer.",2022-07-23 17:54:48+00:00,Better Reasoning Behind Classification Predictions with BERT for Fake News Detection,cs.CL,['cs.CL'],[arxiv.Result.Author('Daesoo Lee')],"Fake news detection has become a major task to solve as there has been an
increasing number of fake news on the internet in recent years. Although many
classification models have been proposed based on statistical learning methods
showing good results, reasoning behind the classification performances may not
be enough. In the self-supervised learning studies, it has been highlighted
that a quality of representation (embedding) space matters and directly affects
a downstream task performance. In this study, a quality of the representation
space is analyzed visually and analytically in terms of linear separability for
different classes on a real and fake news dataset. To further add
interpretability to a classification model, a modification of Class Activation
Mapping (CAM) is proposed. The modified CAM provides a CAM score for each word
token, where the CAM score on a word token denotes a level of focus on that
word token to make the prediction. Finally, it is shown that the naive BERT
model topped with a learnable linear layer is enough to achieve robust
performance while being compatible with CAM."
9256,"In order to test the morphological capabilities of the re-annotated BOUN Treebank, a parser that refers to morpholog-
ical information can be implemented in further research.","The aim of this study is to offer linguis-
tically sound solutions to illustrate syntactically relevant morphological features of Turkish such as null morpheme
realizations and derivational processes without diverging signiﬁcantly from the UD framework.",acknowledgments This work was supported by Bog˘azic¸i University Research Fund Grant Number 16909.,2022-07-24 17:56:27+00:00,Enhancements to the BOUN Treebank Reflecting the Agglutinative Nature of Turkish,cs.CL,['cs.CL'],"[arxiv.Result.Author('Büşra Marşan'), arxiv.Result.Author('Salih Furkan Akkurt'), arxiv.Result.Author('Muhammet Şen'), arxiv.Result.Author('Merve Gürbüz'), arxiv.Result.Author('Onur Güngör'), arxiv.Result.Author('Şaziye Betül Özateş'), arxiv.Result.Author('Suzan Üsküdarlı'), arxiv.Result.Author('Arzucan Özgür'), arxiv.Result.Author('Tunga Güngör'), arxiv.Result.Author('Balkız Öztürk')]","In this study, we aim to offer linguistically motivated solutions to resolve
the issues of the lack of representation of null morphemes, highly productive
derivational processes, and syncretic morphemes of Turkish in the BOUN Treebank
without diverging from the Universal Dependencies framework.
  In order to tackle these issues, new annotation conventions were introduced
by splitting certain lemmas and employing the MISC (miscellaneous) tab in the
UD framework to denote derivation. Representational capabilities of the
re-annotated treebank were tested on a LSTM-based dependency parser and an
updated version of the BoAT Tool is introduced."
9266,The explanation of this fact is a question for further research.,"However, the
baseline approach with character bi-grams and Logistic Regression achieved the second position
in the shared task with just 1.1 % difference in F1-macro from BERT 4EVER, which is quite an
unexpected result.",Table 4 presents the best results of the submitted systems.,2022-07-25 03:41:32+00:00,Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2020,cs.CL,['cs.CL'],"[arxiv.Result.Author('Maaz Amjad'), arxiv.Result.Author('Grigori Sidorov'), arxiv.Result.Author('Alisa Zhila'), arxiv.Result.Author('Alexander Gelbukh'), arxiv.Result.Author('Paolo Rosso')]","This overview paper describes the first shared task on fake news detection in
Urdu language. The task was posed as a binary classification task, in which the
goal is to differentiate between real and fake news. We provided a dataset
divided into 900 annotated news articles for training and 400 news articles for
testing. The dataset contained news in five domains: (i) Health, (ii) Sports,
(iii) Showbiz, (iv) Technology, and (v) Business. 42 teams from 6 different
countries (India, China, Egypt, Germany, Pakistan, and the UK) registered for
the task. 9 teams submitted their experimental results. The participants used
various machine learning methods ranging from feature-based traditional machine
learning to neural networks techniques. The best performing system achieved an
F-score value of 0.90, showing that the BERT-based approach outperforms other
machine learning techniques"
9309,"Bloomberg News generates a third of           for this survey, followed by a structured examination of
                                        its content with Cyborg, their in-house automation system          approaches, benchmark datasets, and evaluation protocols
                                        that can dissect tedious ﬁnancial reports and churn out            that constitute the DTG landscape with the intent to outline
                                                                                                           promising avenues for further research.","With our scope deﬁned, below we outline the rationale
                                        journalism [8], [9].",1.,2022-07-25 23:21:48+00:00,Innovations in Neural Data-to-text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mandar Sharma'), arxiv.Result.Author('Ajay Gogineni'), arxiv.Result.Author('Naren Ramakrishnan')]","The neural boom that has sparked natural language processing (NLP) research
through the last decade has similarly led to significant innovations in
data-to-text generation (DTG). This survey offers a consolidated view into the
neural DTG paradigm with a structured examination of the approaches, benchmark
datasets, and evaluation protocols. This survey draws boundaries separating DTG
from the rest of the natural language generation (NLG) landscape, encompassing
an up-to-date synthesis of the literature, and highlighting the stages of
technological adoption from within and outside the greater NLG umbrella. With
this holistic view, we highlight promising avenues for DTG research that not
only focus on the design of linguistically capable systems but also systems
that exhibit fairness and accountability."
9403,"While these are rich resources,
ing on claims arising from social media in the form    which enable further research against misinforma-
of posts (Zubiaga et al., 2016; Ma et al., 2017) and   tion, they are insufﬁcient for training generalisable
(2) focusing on manually formulated claims, either     models as they solely focus on one topic.","2.1 Existing Veracity Classiﬁcation Datasets           One of the most relevant work to ours is COAID
                                                       (Cui and Lee, 2020), a large-scale dataset contain-
Among existing datasets for veracity classiﬁcation     ing COVID-19 related news articles as well as so-
we can broadly discern two categories: (1) focus-      cial media posts.","created speciﬁcally for a task (Thorne et al., 2018),
or consisting of titles from news or debunking web-       In this work we have augmented the PHEME
sites (Wang, 2017; Alhindi et al., 2018; Lim et al.,   dataset, a popular benchmark dataset for social
2019; Ahmadi et al., 2019).",2022-07-28 09:21:05+00:00,PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('John Dougrez-Lewis'), arxiv.Result.Author('Elena Kochkina'), arxiv.Result.Author('M. Arana-Catania'), arxiv.Result.Author('Maria Liakata'), arxiv.Result.Author('Yulan He')]","Work on social media rumour verification utilises signals from posts, their
propagation and users involved. Other lines of work target identifying and
fact-checking claims based on information from Wikipedia, or trustworthy news
articles without considering social media context. However works combining the
information from social media with external evidence from the wider web are
lacking. To facilitate research in this direction, we release a novel dataset,
PHEMEPlus, an extension of the PHEME benchmark, which contains social media
conversations as well as relevant external evidence for each rumour. We
demonstrate the effectiveness of incorporating such evidence in improving
rumour verification models. Additionally, as part of the evidence collection,
we evaluate various ways of query formulation to identify the most effective
method."
9439,"Moreover, such a pruned model with sparse fusion still beats                         g) Comparison with Ensemble System: To further study
the model with 48 encoder layers trained from scratch using                    the effectiveness of our method, we present the comparison of
the dense fusion.",obtained when we select the 48 bottom layers of the encoder.,our method with a two-model ensemble system in Figure 10.,2022-07-29 04:10:36+00:00,GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jian Yang'), arxiv.Result.Author('Yuwei Yin'), arxiv.Result.Author('Shuming Ma'), arxiv.Result.Author('Haoyang Huang'), arxiv.Result.Author('Dongdong Zhang'), arxiv.Result.Author('Furu Wei'), arxiv.Result.Author('Zhoujun Li')]","Transformer structure, stacked by a sequence of encoder and decoder network
layers, achieves significant development in neural machine translation.
However, vanilla Transformer mainly exploits the top-layer representation,
assuming the lower layers provide trivial or redundant information and thus
ignoring the bottom-layer feature that is potentially valuable. In this work,
we propose the Group-Transformer model (GTrans) that flexibly divides
multi-layer representations of both encoder and decoder into different groups
and then fuses these group features to generate target words. To corroborate
the effectiveness of the proposed method, extensive experiments and analytic
experiments are conducted on three bilingual translation benchmarks and two
multilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14
and OPUS-100 benchmark. Experimental and analytical results demonstrate that
our model outperforms its Transformer counterparts by a consistent gain.
Furthermore, it can be successfully scaled up to 60 encoder layers and 36
decoder layers."
9440,"∂LMT N dp ∂LM (i)T M e ∂hej
      i) Comparison with Ensemble System: To further study                  ∂hel = i=1 wi ∂hef j=m wi ∂hel (8)
the effectiveness of our method, we present the comparison of
our method with a two-model ensemble system in Figure 11.","It shows that              of LMT with respect to the l-th layer in the m-th group can
deeper layers have a greater effect on the translation but the             be calculated by:
last representation may not be the largest one.","Decoder  Encoder
Two independent Transformer_big models are trained
with different settings, which are denoted as Big1 and Big2.",2022-07-29 04:10:36+00:00,GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jian Yang'), arxiv.Result.Author('Yuwei Yin'), arxiv.Result.Author('Liqun Yang'), arxiv.Result.Author('Shuming Ma'), arxiv.Result.Author('Haoyang Huang'), arxiv.Result.Author('Dongdong Zhang'), arxiv.Result.Author('Furu Wei'), arxiv.Result.Author('Zhoujun Li')]","Transformer structure, stacked by a sequence of encoder and decoder network
layers, achieves significant development in neural machine translation.
However, vanilla Transformer mainly exploits the top-layer representation,
assuming the lower layers provide trivial or redundant information and thus
ignoring the bottom-layer feature that is potentially valuable. In this work,
we propose the Group-Transformer model (GTrans) that flexibly divides
multi-layer representations of both encoder and decoder into different groups
and then fuses these group features to generate target words. To corroborate
the effectiveness of the proposed method, extensive experiments and analytic
experiments are conducted on three bilingual translation benchmarks and two
multilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14
and OPUS-100 benchmark. Experimental and analytical results demonstrate that
our model outperforms its Transformer counterparts by a consistent gain.
Furthermore, it can be successfully scaled up to 60 encoder layers and 36
decoder layers."
9528,"This
corpus is made public in order to motivate further research in this direction4.","This was done by holding interviews where the following data is gathered:
(1) demographics through a questionnaire, (2) personality traits through the Big
Five Personality test [28], and (3) CS frequency from interview transcriptions.",The participants included 65 Egyptian Arabic-English bilingual speakers.,2022-07-31 13:47:35+00:00,The Who in Code-Switching: A Case Study for Predicting Egyptian Arabic-English Code-Switching Levels based on Character Profiles,cs.CL,['cs.CL'],"[arxiv.Result.Author('Injy Hamed'), arxiv.Result.Author('Alia El Bolock'), arxiv.Result.Author('Cornelia Herbert'), arxiv.Result.Author('Slim Abdennadher'), arxiv.Result.Author('Ngoc Thang Vu')]","Code-switching (CS) is a common linguistic phenomenon exhibited by
multilingual individuals, where they tend to alternate between languages within
one single conversation. CS is a complex phenomenon that not only encompasses
linguistic challenges, but also contains a great deal of complexity in terms of
its dynamic behaviour across speakers. Given that the factors giving rise to CS
vary from one country to the other, as well as from one person to the other, CS
is found to be a speaker-dependant behaviour, where the frequency by which the
foreign language is embedded differs across speakers. While several researchers
have looked into predicting CS behaviour from a linguistic point of view,
research is still lacking in the task of predicting user CS behaviour from
sociological and psychological perspectives. We provide an empirical user
study, where we investigate the correlations between users' CS levels and
character traits. We conduct interviews with bilinguals and gather information
on their profiles, including their demographics, personality traits, and
traveling experiences. We then use machine learning (ML) to predict users' CS
levels based on their profiles, where we identify the main influential factors
in the modeling process. We experiment with both classification as well as
regression tasks. Our results show that the CS behaviour is affected by the
relation between speakers, travel experiences as well as Neuroticism and
Extraversion personality traits."
9577,"(2022b) perform language adaptation through in-                                        6 Reproducibility
termediate MLM in the target languages with ﬁl-
tered text corpora, demonstrating substantial gains                                    To ensure full reproducibility of our results and fuel
in downstream zero-shot cross-lingual transfer for                                     further research on sociodemographic adaptation
abusive language detection and for dialog tasks,                                       in NLP, we release our code and data, which make
respectively.",(2020) and Hung et al.,Hung et al.,2022-08-01 17:58:02+00:00,On the Limitations of Sociodemographic Adaptation with Transformers,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chia-Chien Hung'), arxiv.Result.Author('Anne Lauscher'), arxiv.Result.Author('Dirk Hovy'), arxiv.Result.Author('Simone Paolo Ponzetto'), arxiv.Result.Author('Goran Glavaš')]","Sociodemographic factors (e.g., gender or age) shape our language. Previous
work showed that incorporating specific sociodemographic factors can
consistently improve performance for various NLP tasks in traditional NLP
models. We investigate whether these previous findings still hold with
state-of-the-art pretrained Transformers. We use three common specialization
methods proven effective for incorporating external knowledge into pretrained
Transformers (e.g., domain-specific or geographic knowledge). We adapt the
language representations for the sociodemographic dimensions of gender and age,
using continuous language modeling and dynamic multi-task learning for
adaptation, where we couple language modeling with the prediction of a
sociodemographic class. Our results when employing a multilingual model show
substantial performance gains across four languages (English, German, French,
and Danish). These findings are in line with the results of previous work and
hold promise for successful sociodemographic specialization. However,
controlling for confounding factors like domain and language shows that, while
sociodemographic adaptation does improve downstream performance, the gains do
not always solely stem from sociodemographic knowledge. Our results indicate
that sociodemographic specialization, while very important, is still an
unresolved problem in NLP."
9740,"We hope that this work stim-
tion, as Google has supported these two languages for a number         ulates further research on ASR for these and other languages of
of years.","Other novel ASR modeling techniques which could
yo   –     45.6 62.4  60.1                                             help include federated learning and personalization, zero-shot
                                                                       learning, data augmentation using synthesized speech (though
zu   23.8  23.8 15.0  13.1                                             text-to-speech is generally not available for these languages ei-
                                                                       ther), and adding external LMs which would also enable tech-
icons and language models have been subject to greater atten-          niques like second pass rescoring.",Africa.,2022-08-05 09:54:19+00:00,Large vocabulary speech recognition for languages of Africa: multilingual modeling and self-supervised learning,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Sandy Ritchie'), arxiv.Result.Author('You-Chi Cheng'), arxiv.Result.Author('Mingqing Chen'), arxiv.Result.Author('Rajiv Mathews'), arxiv.Result.Author('Daan van Esch'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Khe Chai Sim')]","Almost none of the 2,000+ languages spoken in Africa have widely available
automatic speech recognition systems, and the required data is also only
available for a few languages. We have experimented with two techniques which
may provide pathways to large vocabulary speech recognition for African
languages: multilingual modeling and self-supervised learning. We gathered
available open source data and collected data for 15 languages, and trained
experimental models using these techniques. Our results show that pooling the
small amounts of data available in multilingual end-to-end models, and
pre-training on unsupervised data can help improve speech recognition quality
for many African languages."
9741,"We hope that this work stim-
tion, as Google has supported these two languages for a number         ulates further research on ASR for these and other languages of
of years.","Other novel ASR modeling techniques which could
yo   –     45.6 62.4  60.1                                             help include federated learning and personalization, zero-shot
                                                                       learning, data augmentation using synthesized speech (though
zu   23.8  23.8 15.0  13.1                                             text-to-speech is generally not available for these languages ei-
                                                                       ther), and adding external LMs which would also enable tech-
icons and language models have been subject to greater atten-          niques like second pass rescoring.",Africa.,2022-08-05 09:54:19+00:00,Large vocabulary speech recognition for languages of Africa: multilingual modeling and self-supervised learning,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Sandy Ritchie'), arxiv.Result.Author('You-Chi Cheng'), arxiv.Result.Author('Mingqing Chen'), arxiv.Result.Author('Rajiv Mathews'), arxiv.Result.Author('Daan van Esch'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Khe Chai Sim')]","Almost none of the 2,000+ languages spoken in Africa have widely available
automatic speech recognition systems, and the required data is also only
available for a few languages. We have experimented with two techniques which
may provide pathways to large vocabulary speech recognition for African
languages: multilingual modeling and self-supervised learning. We gathered
available open source data and collected data for 15 languages, and trained
experimental models using these techniques. Our results show that pooling the
small amounts of data available in multilingual end-to-end models, and
pre-training on unsupervised data can help improve speech recognition quality
for many African languages."
9748,"Search (FITS), is made publicly available for repro-
                                                     ducible experiments and further research.","The resulting dataset that is
5.1 What’s the best method to learn from             collected, called Feedback on Interactive Talk &
      feedback?","In a companion paper (Xu et al., 2022b) a study
is conducted of how to improve dialogue models
Input: What’s happening in                                             LM Head            Class Head                           Next token probabilities
F1 these days?",2022-08-05 14:20:46+00:00,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Eric Michael Smith'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Moya Chen'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Joshua Lane'), arxiv.Result.Author('Morteza Behrooz'), arxiv.Result.Author('William Ngan'), arxiv.Result.Author('Spencer Poff'), arxiv.Result.Author('Naman Goyal'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Melanie Kambadur'), arxiv.Result.Author('Jason Weston')]","We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction."
9749,"community
as well as to further research into improving these
models through interaction.","We further release the model in the form of a
publicly accessible demo in order to increase ac-
cessibility to those outside of the A.I.","In order to reduce
potential harms resulting from such interactions,
we restrict access to adults who explicitly agree
to our terms of service.",2022-08-05 14:20:46+00:00,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Eric Michael Smith'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Moya Chen'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Joshua Lane'), arxiv.Result.Author('Morteza Behrooz'), arxiv.Result.Author('William Ngan'), arxiv.Result.Author('Spencer Poff'), arxiv.Result.Author('Naman Goyal'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Melanie Kambadur'), arxiv.Result.Author('Jason Weston')]","We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction."
9750,"Search (FITS), is made publicly available for repro-
                                                     ducible experiments and further research.","The resulting dataset that is
5.1 What’s the best method to learn from             collected, called Feedback on Interactive Talk &
      feedback?","In a companion paper (Xu et al., 2022b) a study
is conducted of how to improve dialogue models
Input: What’s happening in                                             LM Head            Class Head                           Next token probabilities
F1 these days?",2022-08-05 14:20:46+00:00,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Eric Michael Smith'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Moya Chen'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Joshua Lane'), arxiv.Result.Author('Morteza Behrooz'), arxiv.Result.Author('William Ngan'), arxiv.Result.Author('Spencer Poff'), arxiv.Result.Author('Naman Goyal'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Melanie Kambadur'), arxiv.Result.Author('Jason Weston')]","We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction."
9751,"community
as well as to further research into improving these
models through interaction.","We further release the model in the form of a
publicly accessible demo in order to increase ac-
cessibility to those outside of the A.I.","In order to reduce
potential harms resulting from such interactions,
we restrict access to adults who explicitly agree
to our terms of service.",2022-08-05 14:20:46+00:00,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Eric Michael Smith'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Moya Chen'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Joshua Lane'), arxiv.Result.Author('Morteza Behrooz'), arxiv.Result.Author('William Ngan'), arxiv.Result.Author('Spencer Poff'), arxiv.Result.Author('Naman Goyal'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Melanie Kambadur'), arxiv.Result.Author('Jason Weston')]","We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction."
9752,"Search (FITS), is made publicly available for repro-
                                                     ducible experiments and further research.","The resulting dataset that is
5.1 What’s the best method to learn from             collected, called Feedback on Interactive Talk &
      feedback?","In a companion paper (Xu et al., 2022b) a study
is conducted of how to improve dialogue models
Input: What’s happening in                                             LM Head            Class Head                           Next token probabilities
F1 these days?",2022-08-05 14:20:46+00:00,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Eric Michael Smith'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Moya Chen'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Joshua Lane'), arxiv.Result.Author('Morteza Behrooz'), arxiv.Result.Author('William Ngan'), arxiv.Result.Author('Spencer Poff'), arxiv.Result.Author('Naman Goyal'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Melanie Kambadur'), arxiv.Result.Author('Jason Weston')]","We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction."
9753,"community
as well as to further research into improving these
models through interaction.","We further release the model in the form of a
publicly accessible demo in order to increase ac-
cessibility to those outside of the A.I.","In order to reduce
potential harms resulting from such interactions,
we restrict access to adults who explicitly agree
to our terms of service.",2022-08-05 14:20:46+00:00,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kurt Shuster'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Eric Michael Smith'), arxiv.Result.Author('Stephen Roller'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Moya Chen'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Joshua Lane'), arxiv.Result.Author('Morteza Behrooz'), arxiv.Result.Author('William Ngan'), arxiv.Result.Author('Spencer Poff'), arxiv.Result.Author('Naman Goyal'), arxiv.Result.Author('Arthur Szlam'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Melanie Kambadur'), arxiv.Result.Author('Jason Weston')]","We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction."
9754,"We make our data and code publicly available for
                                                         further research.",test seen v1 and the unseen set).,"Very large models beneﬁt from feedback from
smaller models OPT-175B, either in zero-shot             7 Limitations and Discussion
or few-shot variants is only pre-trained on dialogue
data, and not ﬁne-tuned on our task, and performs        All of our experiments have taken place by deploy-
reasonably – but not better than smaller models that     ing conversational agents on Amazon Mechanical
are ﬁne-tuned.",2022-08-05 16:41:46+00:00,Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Jason Weston')]","Frozen models trained to mimic static datasets can never improve their
performance. Models that can employ internet-retrieval for up-to-date
information and obtain feedback from humans during deployment provide the
promise of both adapting to new information, and improving their performance.
In this work we study how to improve internet-driven conversational skills in
such a learning framework. We collect deployment data, which we make publicly
available, of human interactions, and collect various types of human feedback
-- including binary quality measurements, free-form text feedback, and
fine-grained reasons for failure. We then study various algorithms for
improving from such feedback, including standard supervised learning, rejection
sampling, model-guiding and reward-based learning, in order to make
recommendations on which type of feedback and algorithms work best. We find the
recently introduced Director model (Arora et al., '22) shows significant
improvements over other existing approaches."
9755,"We make our data and code publicly available for
                                                         further research.",test seen v1 and the unseen set).,"Very large models beneﬁt from feedback from
smaller models OPT-175B, either in zero-shot             7 Limitations and Discussion
or few-shot variants is only pre-trained on dialogue
data, and not ﬁne-tuned on our task, and performs        All of our experiments have taken place by deploy-
reasonably – but not better than smaller models that     ing conversational agents on Amazon Mechanical
are ﬁne-tuned.",2022-08-05 16:41:46+00:00,Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Megan Ung'), arxiv.Result.Author('Mojtaba Komeili'), arxiv.Result.Author('Kushal Arora'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Jason Weston')]","Frozen models trained to mimic static datasets can never improve their
performance. Models that can employ internet-retrieval for up-to-date
information and obtain feedback from humans during deployment provide the
promise of both adapting to new information, and improving their performance.
In this work we study how to improve internet-driven conversational skills in
such a learning framework. We collect deployment data, which we make publicly
available, of human interactions, and collect various types of human feedback
-- including binary quality measurements, free-form text feedback, and
fine-grained reasons for failure. We then study various algorithms for
improving from such feedback, including standard supervised learning, rejection
sampling, model-guiding and reward-based learning, in order to make
recommendations on which type of feedback and algorithms work best. We find the
recently introduced Director model (Arora et al., '22) shows significant
improvements over other existing approaches."
9759,"We thus make the           robust loss functions (e.g., (Ghosh et al., 2017;
code of our experiments and our benchmark Safet-        Liu and Guo, 2020)), robust regularization (e.g.,
yMix publicly available for further research1.","Therefore, we expect further improvements       solutions into four categories: robust architectures
to come with advances in learning techniques be-        (e.g., (Sukhbaatar et al., 2014; Xiao et al., 2015)),
yond the ones proposed here.","(Jenni and Favaro, 2018; Goodfellow et al., 2014))
                                                        and sample selection methods (e.g., (Malach
2 Related Work                                          and Shalev-Shwartz, 2017; Shen and Sanghavi,
                                                        2019)).",2022-08-05 17:33:33+00:00,Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring the trolls,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Jason Weston')]","The promise of interaction between intelligent conversational agents and
humans is that models can learn from such feedback in order to improve.
Unfortunately, such exchanges in the wild will not always involve human
utterances that are benign or of high quality, and will include a mixture of
engaged (helpers) and unengaged or even malicious users (trolls). In this work
we study how to perform robust learning in such an environment. We introduce a
benchmark evaluation, SafetyMix, which can evaluate methods that learn safe vs.
toxic language in a variety of adversarial settings to test their robustness.
We propose and analyze several mitigating learning algorithms that identify
trolls either at the example or at the user level. Our main finding is that
user-based methods, that take into account that troll users will exhibit
adversarial behavior across multiple examples, work best in a variety of
settings on our benchmark. We then test these methods in a further real-life
setting of conversations collected during deployment, with similar results."
9760,"All these different approaches might
be exploitable by trolls in different ways, thus re-
quiring further study.","Finally, one
could make use of more sophisticated UIs, for ex-
ample stack ranking potential responses (Ouyang
et al., 2022).","A completely different way of dealing with trolls
is to rely on other humans (helpers) to police them
directly, which is effectively done for example by
the editors of Wikipedia when dealing with article
vandalism (Shachaf and Hara, 2010).",2022-08-05 17:33:33+00:00,Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring the trolls,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Da Ju'), arxiv.Result.Author('Jing Xu'), arxiv.Result.Author('Y-Lan Boureau'), arxiv.Result.Author('Jason Weston')]","The promise of interaction between intelligent conversational agents and
humans is that models can learn from such feedback in order to improve.
Unfortunately, such exchanges in the wild will not always involve human
utterances that are benign or of high quality, and will include a mixture of
engaged (helpers) and unengaged or even malicious users (trolls). In this work
we study how to perform robust learning in such an environment. We introduce a
benchmark evaluation, SafetyMix, which can evaluate methods that learn safe vs.
toxic language in a variety of adversarial settings to test their robustness.
We propose and analyze several mitigating learning algorithms that identify
trolls either at the example or at the user level. Our main finding is that
user-based methods, that take into account that troll users will exhibit
adversarial behavior across multiple examples, work best in a variety of
settings on our benchmark. We then test these methods in a further real-life
setting of conversations collected during deployment, with similar results."
9919,"This calls for further research
et al.","(2022)), such poor
heuristic with the bottom scored translations ac-      performance for completely inadequate translations
cording to a quality ﬁlter was introduced in Raunak    is highly unexpected.",(2021).,2022-08-10 12:44:13+00:00,Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Nuno M. Guerreiro'), arxiv.Result.Author('Elena Voita'), arxiv.Result.Author('André F. T. Martins')]","Although the problem of hallucinations in neural machine translation (NMT)
has received some attention, research on this highly pathological phenomenon
lacks solid ground. Previous work has been limited in several ways: it often
resorts to artificial settings where the problem is amplified, it disregards
some (common) types of hallucinations, and it does not validate adequacy of
detection heuristics. In this paper, we set foundations for the study of NMT
hallucinations. First, we work in a natural setting, i.e., in-domain data
without artificial noise neither in training nor in inference. Next, we
annotate a dataset of over 3.4k sentences indicating different kinds of
critical errors and hallucinations. Then, we turn to detection methods and both
revisit methods used previously and propose using glass-box uncertainty-based
detectors. Overall, we show that for preventive settings, (i) previously used
methods are largely inadequate, (ii) sequence log-probability works best and
performs on par with reference-based methods. Finally, we propose
DeHallucinator, a simple method for alleviating hallucinations at test time
that significantly reduces the hallucinatory rate. To ease future research, we
release our annotated dataset for WMT18 German-English data, along with the
model, training data, and code."
9929,"The ﬁnd-
ings and resources presented in this dissertation have been released to the com-
munity to facilitate further research and knowledge transfer.","I have addressed existing gaps in the research ﬁeld,
posed new research questions, and explored novel research directions.","Resumen

En esta tesis se exploran los aspectos lingüísticos y computacionales de las rela-
ciones semánticas que puede haber entre dos o más expresiones lingüísticas com-
plejas (sintagmas, cláusulas, oraciones, párrafos).",2022-08-10 15:07:49+00:00,"Paraphrasing, textual entailment, and semantic similarity above word level",cs.CL,['cs.CL'],[arxiv.Result.Author('Venelin Kovatchev')],"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity."
9930,"We
show the advantages of a qualitative evaluation framework and emphasize the
need to further research and improve the PI task.","Our ﬁndings demonstrate the limitations of the traditional PI task deﬁnition
and datasets and the way PI systems are typically interpreted and evaluated.","The “performance proﬁle”
also enables the direct empirical comparison of related phenomena such as “same
polarity substitution (habitual)” and “(contextual)” or “contains negation” and
“negation switching”.",2022-08-10 15:07:49+00:00,"Paraphrasing, textual entailment, and semantic similarity above word level",cs.CL,['cs.CL'],[arxiv.Result.Author('Venelin Kovatchev')],"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity."
9931,"CONCLUSIONS AND FUTURE WORK  111

systems; and 3) It identiﬁes phenomena which are easy/hard to solve for multiple
systems and may require further research.","Our methodology
has clear advantages over using simple quantitative measures (Accuracy and F1
Score): 1) It allows for a better interpretation and error analysis on the individ-
ual systems; 2) It allows for a better qualitative comparison between the different

     4https://github.com/JavierBJ/paraphrase_eval
6.7.","We demonstrate the methodology by evaluating and comparing several of the
state-of-the-art systems in PI.",2022-08-10 15:07:49+00:00,"Paraphrasing, textual entailment, and semantic similarity above word level",cs.CL,['cs.CL'],[arxiv.Result.Author('Venelin Kovatchev')],"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity."
9932,"This indicates that speciﬁcity
cannot be automatically predicted using the other meaning relations and requires
further study.","It also shows no clear trend on the similarity scale and no correlation with the
difference in word length between the sentences.","In the examples that we discuss, we focus on interesting cases, which are
complicated and unexpected (ex.",2022-08-10 15:07:49+00:00,"Paraphrasing, textual entailment, and semantic similarity above word level",cs.CL,['cs.CL'],[arxiv.Result.Author('Venelin Kovatchev')],"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity."
9933,"The corpus can be used to further study relation interactions or
as a more challenging dataset for detecting the different relations automatically10.","We release a new
corpus that contains all relations of interest and the corpus creation methodology
to the community.","Some of our most important ﬁndings are:

   1) there is a strong correlation between paraphrasing and entailment and most
       paraphrases include at least uni-directional entailment;

   2) paraphrases and bi-directional entailment are not equivalent in practical set-
       tings;

   3) speciﬁcity relation does not correlate strongly with the other relations and
       requires further study;

   4) contradictions (in our dataset) are perceived as dis-similar.",2022-08-10 15:07:49+00:00,"Paraphrasing, textual entailment, and semantic similarity above word level",cs.CL,['cs.CL'],[arxiv.Result.Author('Venelin Kovatchev')],"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity."
9934,"Some of our most important ﬁndings are:

   1) there is a strong correlation between paraphrasing and entailment and most
       paraphrases include at least uni-directional entailment;

   2) paraphrases and bi-directional entailment are not equivalent in practical set-
       tings;

   3) speciﬁcity relation does not correlate strongly with the other relations and
       requires further study;

   4) contradictions (in our dataset) are perceived as dis-similar.","The corpus can be used to further study relation interactions or
as a more challenging dataset for detecting the different relations automatically10.","As a future work, we plan to: 1) study the speciﬁcity relation in a different
setting; 2) use a linguistic annotation to determine more ﬁne-grained distinctions
between the relations; 3) and annotate the rest of the 11,000 sentences in a semi-
automated way.",2022-08-10 15:07:49+00:00,"Paraphrasing, textual entailment, and semantic similarity above word level",cs.CL,['cs.CL'],[arxiv.Result.Author('Venelin Kovatchev')],"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity."
9935,"It would, nevertheless, require a further research and richer cor-
pora to empirically determine the importance of these phenomena for the different
meaning relations.","ANALYSIS OF THE RESULTS  149

ShaRel typology.","Summary The similarities and common tendencies between paraphrases, en-
tailment, and contradiction clearly indicate that these relations belong within the
same conceptual framework and should be studied and compared together.",2022-08-10 15:07:49+00:00,"Paraphrasing, textual entailment, and semantic similarity above word level",cs.CL,['cs.CL'],[arxiv.Result.Author('Venelin Kovatchev')],"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity."
9947,"To facil-
MFTC dataset to have the same number of sam-            itate further research in this domain, here we in-
ples for each label as MFRC.","Additionally, we downsampled the           for which training data plays a vital role.","The results are pre-       troduced the MFRC, a collection of 16,123 Reddit
sented in Table 11.",2022-08-10 20:08:10+00:00,The Moral Foundations Reddit Corpus,cs.CL,"['cs.CL', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Jackson Trager'), arxiv.Result.Author('Alireza S. Ziabari'), arxiv.Result.Author('Aida Mostafazadeh Davani'), arxiv.Result.Author('Preni Golazazian'), arxiv.Result.Author('Farzan Karimi-Malekabadi'), arxiv.Result.Author('Ali Omrani'), arxiv.Result.Author('Zhihe Li'), arxiv.Result.Author('Brendan Kennedy'), arxiv.Result.Author('Nils Karl Reimer'), arxiv.Result.Author('Melissa Reyes'), arxiv.Result.Author('Kelsey Cheng'), arxiv.Result.Author('Mellow Wei'), arxiv.Result.Author('Christina Merrifield'), arxiv.Result.Author('Arta Khosravi'), arxiv.Result.Author('Evans Alvarez'), arxiv.Result.Author('Morteza Dehghani')]","Moral framing and sentiment can affect a variety of online and offline
behaviors, including donation, pro-environmental action, political engagement,
and even participation in violent protests. Various computational methods in
Natural Language Processing (NLP) have been used to detect moral sentiment from
textual data, but in order to achieve better performances in such subjective
tasks, large sets of hand-annotated training data are needed. Previous corpora
annotated for moral sentiment have proven valuable, and have generated new
insights both within NLP and across the social sciences, but have been limited
to Twitter. To facilitate improving our understanding of the role of moral
rhetoric, we present the Moral Foundations Reddit Corpus, a collection of
16,123 Reddit comments that have been curated from 12 distinct subreddits,
hand-annotated by at least three trained annotators for 8 categories of moral
sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty,
Thin Morality, Implicit/Explicit Morality) based on the updated Moral
Foundations Theory (MFT) framework. We use a range of methodologies to provide
baseline moral-sentiment classification results for this new corpus, e.g.,
cross-domain classification and knowledge transfer."
9948,"To facil-
MFTC dataset to have the same number of sam-            itate further research in this domain, here we in-
ples for each label as MFRC.","Additionally, we downsampled the           for which training data plays a vital role.","The results are pre-       troduced the MFRC, a collection of 16,123 Reddit
sented in Table 11.",2022-08-10 20:08:10+00:00,The Moral Foundations Reddit Corpus,cs.CL,"['cs.CL', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Jackson Trager'), arxiv.Result.Author('Alireza S. Ziabari'), arxiv.Result.Author('Aida Mostafazadeh Davani'), arxiv.Result.Author('Preni Golazizian'), arxiv.Result.Author('Farzan Karimi-Malekabadi'), arxiv.Result.Author('Ali Omrani'), arxiv.Result.Author('Zhihe Li'), arxiv.Result.Author('Brendan Kennedy'), arxiv.Result.Author('Nils Karl Reimer'), arxiv.Result.Author('Melissa Reyes'), arxiv.Result.Author('Kelsey Cheng'), arxiv.Result.Author('Mellow Wei'), arxiv.Result.Author('Christina Merrifield'), arxiv.Result.Author('Arta Khosravi'), arxiv.Result.Author('Evans Alvarez'), arxiv.Result.Author('Morteza Dehghani')]","Moral framing and sentiment can affect a variety of online and offline
behaviors, including donation, pro-environmental action, political engagement,
and even participation in violent protests. Various computational methods in
Natural Language Processing (NLP) have been used to detect moral sentiment from
textual data, but in order to achieve better performances in such subjective
tasks, large sets of hand-annotated training data are needed. Previous corpora
annotated for moral sentiment have proven valuable, and have generated new
insights both within NLP and across the social sciences, but have been limited
to Twitter. To facilitate improving our understanding of the role of moral
rhetoric, we present the Moral Foundations Reddit Corpus, a collection of
16,123 Reddit comments that have been curated from 12 distinct subreddits,
hand-annotated by at least three trained annotators for 8 categories of moral
sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty,
Thin Morality, Implicit/Explicit Morality) based on the updated Moral
Foundations Theory (MFT) framework. We use a range of methodologies to provide
baseline moral-sentiment classification results for this new corpus, e.g.,
cross-domain classification and knowledge transfer."
9957,"Moreover, further study has found, perhaps unsur-              also has the advantage of generally requiring less training
prisingly, that there appears to be little overlap between the          data than previous supervised methods.","of identity: the generated text is being disguised as human-            Beyond the increase in performance, the ﬁne-tuning approach
written.","Despite this, ﬁne-
cues typically associated with human-crafted deception, and             tuning approaches do still appear to be limited in their ability
the cues of deception that denote a machine-generated text.",2022-08-11 11:27:38+00:00,A Comprehensive Survey of Natural Language Generation Advances from the Perspective of Digital Deception,cs.CL,['cs.CL'],"[arxiv.Result.Author('Keenan Jones'), arxiv.Result.Author('Enes Altuncu'), arxiv.Result.Author('Virginia N. L. Franqueira'), arxiv.Result.Author('Yichao Wang'), arxiv.Result.Author('Shujun Li')]","In recent years there has been substantial growth in the capabilities of
systems designed to generate text that mimics the fluency and coherence of
human language. From this, there has been considerable research aimed at
examining the potential uses of these natural language generators (NLG) towards
a wide number of tasks. The increasing capabilities of powerful text generators
to mimic human writing convincingly raises the potential for deception and
other forms of dangerous misuse. As these systems improve, and it becomes ever
harder to distinguish between human-written and machine-generated text,
malicious actors could leverage these powerful NLG systems to a wide variety of
ends, including the creation of fake news and misinformation, the generation of
fake online product reviews, or via chatbots as means of convincing users to
divulge private information. In this paper, we provide an overview of the NLG
field via the identification and examination of 119 survey-like papers focused
on NLG research. From these identified papers, we outline a proposed high-level
taxonomy of the central concepts that constitute NLG, including the methods
used to develop generalised NLG systems, the means by which these systems are
evaluated, and the popular NLG tasks and subtasks that exist. In turn, we
provide an overview and discussion of each of these items with respect to
current research and offer an examination of the potential roles of NLG in
deception and detection systems to counteract these threats. Moreover, we
discuss the broader challenges of NLG, including the risks of bias that are
often exhibited by existing text generation systems. This work offers a broad
overview of the field of NLG with respect to its potential for misuse, aiming
to provide a high-level understanding of this rapidly developing area of
research."
9967,"While the aforementioned approaches prove to be helpful in certain scenarios of domain
adaptation, we believe there is a need for further research in this area to address current chal-
lenges of in-domain data scarcity and synthetic data creation.","Similarly, some works have inves-
tigated retrieving similar translations (fuzzy matches) from bilingual datasets, and then apply-
ing on-the-ﬂy domain adaptation through ﬁne-tuning the baseline model at translation time
(Farajian et al., 2017), or integrating them into NMT training (Bulte and Tezcan, 2019; Xu et al.,
2020).","Some approaches, such as on-the-
ﬂy domain adaptation, require using GPUs synchronously at translation time, which presents a
challenge for some institutions due to the lack of resources.",2022-08-11 16:22:16+00:00,Domain-Specific Text Generation for Machine Translation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yasmin Moslem'), arxiv.Result.Author('Rejwanul Haque'), arxiv.Result.Author('John D. Kelleher'), arxiv.Result.Author('Andy Way')]","Preservation of domain knowledge from the source to target is crucial in any
translation workflow. It is common in the translation industry to receive
highly specialized projects, where there is hardly any parallel in-domain data.
In such scenarios where there is insufficient in-domain data to fine-tune
Machine Translation (MT) models, producing translations that are consistent
with the relevant context is challenging. In this work, we propose a novel
approach to domain adaptation leveraging state-of-the-art pretrained language
models (LMs) for domain-specific data augmentation for MT, simulating the
domain characteristics of either (a) a small bilingual dataset, or (b) the
monolingual source text to be translated. Combining this idea with
back-translation, we can generate huge amounts of synthetic bilingual in-domain
data for both use cases. For our investigation, we use the state-of-the-art
Transformer architecture. We employ mixed fine-tuning to train models that
significantly improve translation of in-domain texts. More specifically, in
both scenarios, our proposed methods achieve improvements of approximately 5-6
BLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic
language pairs. Furthermore, the outcome of human evaluation corroborates the
automatic evaluation results."
10000,"Finally, we present an in-depth analysis of
already available high-quality large-scale knowl-                               cases where our proposed approach gives incorrect
edge bases such as YAGO and Wikidata; (ii) not                                  answers paving the way for further research in this
all the knowledge needs to be packed in the pa-                                 direction (Section 4.7).","This approach offers                            CORE on two knowledge-intensive downstream
several potential advantages – (i) we can utilize the                           tasks.","rameters of the model resulting in lighter, smaller
and greener models; and (iii) as new knowledge                                  2 Related Work
becomes available, the knowledge base can be up-
dated independently of the language model.",2022-08-12 18:59:37+00:00,LM-CORE: Language Models with Contextually Relevant External Knowledge,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jivat Neet Kaur'), arxiv.Result.Author('Sumit Bhatia'), arxiv.Result.Author('Milan Aggarwal'), arxiv.Result.Author('Rachit Bansal'), arxiv.Result.Author('Balaji Krishnamurthy')]","Large transformer-based pre-trained language models have achieved impressive
performance on a variety of knowledge-intensive tasks and can capture factual
knowledge in their parameters. We argue that storing large amounts of knowledge
in the model parameters is sub-optimal given the ever-growing amounts of
knowledge and resource requirements. We posit that a more efficient alternative
is to provide explicit access to contextually relevant structured knowledge to
the model and train it to use that knowledge. We present LM-CORE -- a general
framework to achieve this -- that allows \textit{decoupling} of the language
model training from the external knowledge source and allows the latter to be
updated without affecting the already trained model. Experimental results show
that LM-CORE, having access to external knowledge, achieves significant and
robust outperformance over state-of-the-art knowledge-enhanced language models
on knowledge probing tasks; can effectively handle knowledge updates; and
performs well on two downstream tasks. We also present a thorough error
analysis highlighting the successes and failures of LM-CORE."
10020,"Interest-
                                                                        ingly, all models perform worse in the “Soccer” domain; this occurs
   Based on our dataset, further research in the community is ex-       due to similar terms (e.g.","and T5, have been pretrained for several generation tasks.","“plays for"") it shares with other domains
pected to focus on developing end-to-end systems covering both          (e.g.",2022-08-13 21:21:28+00:00,An Answer Verbalization Dataset for Conversational Question Answerings over Knowledge Graphs,cs.CL,['cs.CL'],"[arxiv.Result.Author('Endri Kacupaj'), arxiv.Result.Author('Kuldeep Singh'), arxiv.Result.Author('Maria Maleshkova'), arxiv.Result.Author('Jens Lehmann')]","We introduce a new dataset for conversational question answering over
Knowledge Graphs (KGs) with verbalized answers. Question answering over KGs is
currently focused on answer generation for single-turn questions (KGQA) or
multiple-tun conversational question answering (ConvQA). However, in a
real-world scenario (e.g., voice assistants such as Siri, Alexa, and Google
Assistant), users prefer verbalized answers. This paper contributes to the
state-of-the-art by extending an existing ConvQA dataset with multiple
paraphrased verbalized answers. We perform experiments with five
sequence-to-sequence models on generating answer responses while maintaining
grammatical correctness. We additionally perform an error analysis that details
the rates of models' mispredictions in specified categories. Our proposed
dataset extended with answer verbalization is publicly available with detailed
documentation on its usage for wider utility."
10041,"7 Future Work

Directions for further research on the topic of TOD systems include testing our proposals on bigger backbone
models to evaluate their effectiveness against overﬁtting, experimenting with additional auxiliary tasks for
the current baseline, and introducing data augmentations.","It employs techniques for
overcoming backpropagation issues with the discrete token representations of a generated response sequence‡.","Also, whether our classiﬁer heads could be used
during inference to perform real-time response selection should be explored.",2022-08-15 09:59:44+00:00,Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Radostin Cholakov'), arxiv.Result.Author('Todor Kolev')]","The adoption of pre-trained language models in task-oriented dialogue systems
has resulted in significant enhancements of their text generation abilities.
However, these architectures are slow to use because of the large number of
trainable parameters and can sometimes fail to generate diverse responses. To
address these limitations, we propose two models with auxiliary tasks for
response selection - (1) distinguishing distractors from ground truth responses
and (2) distinguishing synthetic responses from ground truth labels. They
achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined
scores of 107.5 and 108.3 and outperform a baseline with three times more
parameters. We publish reproducible code and checkpoints and discuss the
effects of applying auxiliary tasks to T5-based architectures."
10099,"Their methodology provides a
foundation for further research examining terminological drift across longer time
periods and with more robust full-text datasets.","Toepfer and
Seifert explored temporal concept drift over decades, with a dataset comprised of
document titles and author-provided keywords.","Foundational work here illustrates the
notion of KOSs capturing the zeitgeist, and supports the solution of infrastructure
inversion, as discussed below.",2022-08-16 16:37:17+00:00,Temporal Concept Drift and Alignment: An empirical approach to comparing Knowledge Organization Systems over time,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sam Grabus'), arxiv.Result.Author('Peter Melville Logan'), arxiv.Result.Author('Jane Greenberg')]","This research explores temporal concept drift and temporal alignment in
knowledge organization systems (KOS). A comparative analysis is pursued using
the 1910 Library of Congress Subject Headings, 2020 FAST Topical, and automatic
indexing. The use case involves a sample of 90 nineteenth-century Encyclopedia
Britannica entries. The entries were indexed using two approaches: 1) full-text
indexing; 2) Named Entity Recognition was performed upon the entries with
Stanza, Stanford's NLP toolkit, and entities were automatically indexed with
the Helping Interdisciplinary Vocabulary application (HIVE), using both 1910
LCSH and FAST Topical. The analysis focused on three goals: 1) identifying
results that were exclusive to the 1910 LCSH output; 2) identifying terms in
the exclusive set that have been deprecated from the contemporary LCSH,
demonstrating temporal concept drift; and 3) exploring the historical
significance of these deprecated terms. Results confirm that historical
vocabularies can be used to generate anachronistic subject headings
representing conceptual drift across time in KOS and historical resources. A
methodological contribution is made demonstrating how to study changes in KOS
over time and improve the contextualization of historical humanities resources."
10100,"The research reviewed above
8

points to the need for further research on automatic indexing in the humanities, and
informs the goals and questions for the research presented in this paper.","Their bibliographic analysis discovered that only 13.1% of the
humanities articles were indexed using a controlled vocabulary, none of which were
humanities-specific or even multidisciplinary, and a need for highly-granular indexing
terms for humanities resources was identified (1206).","3 Goals and Research Questions

This research presents a methodology for examining how the temporal alignment of
controlled vocabularies can be used to highlight temporal concept drift in our KOSs.",2022-08-16 16:37:17+00:00,Temporal Concept Drift and Alignment: An empirical approach to comparing Knowledge Organization Systems over time,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sam Grabus'), arxiv.Result.Author('Peter Melville Logan'), arxiv.Result.Author('Jane Greenberg')]","This research explores temporal concept drift and temporal alignment in
knowledge organization systems (KOS). A comparative analysis is pursued using
the 1910 Library of Congress Subject Headings, 2020 FAST Topical, and automatic
indexing. The use case involves a sample of 90 nineteenth-century Encyclopedia
Britannica entries. The entries were indexed using two approaches: 1) full-text
indexing; 2) Named Entity Recognition was performed upon the entries with
Stanza, Stanford's NLP toolkit, and entities were automatically indexed with
the Helping Interdisciplinary Vocabulary application (HIVE), using both 1910
LCSH and FAST Topical. The analysis focused on three goals: 1) identifying
results that were exclusive to the 1910 LCSH output; 2) identifying terms in
the exclusive set that have been deprecated from the contemporary LCSH,
demonstrating temporal concept drift; and 3) exploring the historical
significance of these deprecated terms. Results confirm that historical
vocabularies can be used to generate anachronistic subject headings
representing conceptual drift across time in KOS and historical resources. A
methodological contribution is made demonstrating how to study changes in KOS
over time and improve the contextualization of historical humanities resources."
10109,"works for dialogue attributes other than relevance
requires further study.","CoRR, abs/1902.00098.","Nouha Dziri, Ehsan Kamalloo, Kory Wallace Mathew-
                                                             son, and Osmar R. Zaïane.",2022-08-17 06:12:09+00:00,SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Longxuan Ma'), arxiv.Result.Author('Ziyu Zhuang'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Mingda Li'), arxiv.Result.Author('Ting Liu')]","This paper introduces a novel Self-supervised Fine-grained Dialogue
Evaluation framework (SelF-Eval). The core idea is to model the correlation
between turn quality and the entire dialogue quality. We first propose a novel
automatic data construction method that can automatically assign fine-grained
scores for arbitrarily dialogue data. Then we train \textbf{SelF-Eval} with a
multi-level contrastive learning schema which helps to distinguish different
score levels. Experimental results on multiple benchmarks show that SelF-Eval
is highly consistent with human evaluations and better than the
state-of-the-art models. We give a detailed analysis of the experiments in this
paper. Our code and data will be published on GitHub."
10110,"works for dialogue attributes other than relevance
requires further study.","However, whether this penalizing            tional Linguistics.","Emily Dinan, Varvara Logacheva, Valentin Malykh,
                                                             Alexander H. Miller, Kurt Shuster, Jack Urbanek,
6 Conclusion                                                 Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
                                                             Lowe, Shrimai Prabhumoye, Alan W. Black, Alexan-
We propose to measure dialogue quality by mod-               der I. Rudnicky, Jason Williams, Joelle Pineau,
eling the ﬁne-grained correlations between turns             Mikhail S. Burtsev, and Jason Weston.",2022-08-17 06:12:09+00:00,SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Longxuan Ma'), arxiv.Result.Author('Ziyu Zhuang'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Mingda Li'), arxiv.Result.Author('Ting Liu')]","This paper introduces a novel Self-supervised Fine-grained Dialogue
Evaluation framework (SelF-Eval). The core idea is to model the correlation
between turn quality and the entire dialogue quality. We first propose a novel
automatic data construction method that can automatically assign fine-grained
scores for arbitrarily dialogue data. Then we train \textbf{SelF-Eval} with a
multi-level contrastive learning schema which helps to distinguish different
score levels. Experimental results on multiple benchmarks show that SelF-Eval
is highly consistent with human evaluations and better than the
state-of-the-art models. We give a detailed analysis of the experiments in this
paper. Our code and data will be published on GitHub."
10111,"works for dialogue attributes other than relevance
requires further study.","However, whether this penalizing            tional Linguistics.","Emily Dinan, Varvara Logacheva, Valentin Malykh,
                                                             Alexander H. Miller, Kurt Shuster, Jack Urbanek,
6 Conclusion                                                 Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
                                                             Lowe, Shrimai Prabhumoye, Alan W. Black, Alexan-
We propose to measure dialogue quality by mod-               der I. Rudnicky, Jason Williams, Joelle Pineau,
eling the ﬁne-grained correlations between turns             Mikhail S. Burtsev, and Jason Weston.",2022-08-17 06:12:09+00:00,SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Longxuan Ma'), arxiv.Result.Author('Ziyu Zhuang'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Mingda Li'), arxiv.Result.Author('Ting Liu')]","This paper introduces a novel Self-supervised Fine-grained Dialogue
Evaluation framework (SelF-Eval). The core idea is to model the correlation
between turn quality and the entire dialogue quality. We first propose a novel
automatic data construction method that can automatically assign fine-grained
scores for arbitrarily dialogue data. Then we train \textbf{SelF-Eval} with a
multi-level contrastive learning schema which helps to distinguish different
score levels. Experimental results on multiple benchmarks show that SelF-Eval
is highly consistent with human evaluations and better than the
state-of-the-art models. We give a detailed analysis of the experiments in this
paper. Our code is available on GitHub."
10112,"However, whether this penalizing
works for dialogue attributes other than relevance     Emily Dinan, Varvara Logacheva, Valentin Malykh,
requires further study.",are too extreme.,"Alexander H. Miller, Kurt Shuster, Jack Urbanek,
                                                          Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
6 Conclusion                                              Lowe, Shrimai Prabhumoye, Alan W. Black, Alexan-
                                                          der I. Rudnicky, Jason Williams, Joelle Pineau,
We propose to measure dialogue quality by mod-            Mikhail S. Burtsev, and Jason Weston.",2022-08-17 06:12:09+00:00,SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Longxuan Ma'), arxiv.Result.Author('Ziyu Zhuang'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Mingda Li'), arxiv.Result.Author('Ting Liu')]","This paper introduces a novel Self-supervised Fine-grained Dialogue
Evaluation framework (SelF-Eval). The core idea is to model the correlation
between turn quality and the entire dialogue quality. We first propose a novel
automatic data construction method that can automatically assign fine-grained
scores for arbitrarily dialogue data. Then we train \textbf{SelF-Eval} with a
multi-level contrastive learning schema which helps to distinguish different
score levels. Experimental results on multiple benchmarks show that SelF-Eval
is highly consistent with human evaluations and better than the
state-of-the-art models. We give a detailed analysis of the experiments in this
paper. Our code is available on GitHub."
10113,"The
works for dialogue attributes other than relevance              second conversational intelligence challenge (con-
requires further study.",2019.,vai2).,2022-08-17 06:12:09+00:00,SelF-Eval: Self-supervised Fine-grained Dialogue Evaluation,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Longxuan Ma'), arxiv.Result.Author('Ziyu Zhuang'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Mingda Li'), arxiv.Result.Author('Ting Liu')]","This paper introduces a novel Self-supervised Fine-grained Dialogue
Evaluation framework (SelF-Eval). The core idea is to model the correlation
between turn quality and the entire dialogue quality. We first propose a novel
automatic data construction method that can automatically assign fine-grained
scores for arbitrarily dialogue data. Then we train \textbf{SelF-Eval} with a
multi-level contrastive learning schema which helps to distinguish different
score levels. Experimental results on multiple benchmarks show that SelF-Eval
is highly consistent with human evaluations and better than the
state-of-the-art models. We give a detailed analysis of the experiments in this
paper. Our code is available on GitHub."
10118,"Technologies, demanding further research.","Neurocomputing, 384.","Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
Acknowledgements                                              Kos, and Dawn Song.",2022-08-17 08:15:44+00:00,Differential Privacy in Natural Language Processing: The Story So Far,cs.CL,['cs.CL'],"[arxiv.Result.Author('Oleksandra Klymenko'), arxiv.Result.Author('Stephen Meisenbacher'), arxiv.Result.Author('Florian Matthes')]","As the tide of Big Data continues to influence the landscape of Natural
Language Processing (NLP), the utilization of modern NLP methods has grounded
itself in this data, in order to tackle a variety of text-based tasks. These
methods without a doubt can include private or otherwise personally
identifiable information. As such, the question of privacy in NLP has gained
fervor in recent years, coinciding with the development of new
Privacy-Enhancing Technologies (PETs). Among these PETs, Differential Privacy
boasts several desirable qualities in the conversation surrounding data
privacy. Naturally, the question becomes whether Differential Privacy is
applicable in the largely unstructured realm of NLP. This topic has sparked
novel research, which is unified in one basic goal: how can one adapt
Differential Privacy to NLP methods? This paper aims to summarize the
vulnerabilities addressed by Differential Privacy, the current thinking, and
above all, the crucial next steps that must be considered."
10122,"However, our upper bound over the separation rank of mlp architectures is not
necessarily tight, so we did not claim it but we leave this possibility open for further research.","See Appendix H.

Remark 5.2 The last (H.1) proposition leaves open the possibility that if someone can scale mlp-
architectures 1.58 deeper than transformer architectures while using the same budget, then it
may be possible that the mlp-architectures would have a higher ability to model multi-variable
dependencies.","6 Experiments

To assess our theory we derived a few predictions from it and asses them in experiments as shown
below.",2022-08-17 09:59:22+00:00,Transformer Vs. MLP-Mixer Exponential Expressive Gap For NLP Problems,cs.CL,"['cs.CL', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Dan Navon'), arxiv.Result.Author('Alex M. Bronstein')]","Vision-Transformers are widely used in various vision tasks. Meanwhile, there
is another line of works starting with the MLP-mixer trying to achieve similar
performance using mlp-based architectures. Interestingly, until now none
reported using them for NLP tasks, additionally until now non of those
mlp-based architectures claimed to achieve state-of-the-art in vision tasks. In
this paper, we analyze the expressive power of mlp-based architectures in
modeling dependencies between multiple different inputs simultaneously, and
show an exponential gap between the attention and the mlp-based mechanisms. Our
results suggest a theoretical explanation for the mlp inability to compete with
attention-based mechanisms in NLP problems, they also suggest that the
performance gap in vision tasks may be due to the mlp relative weakness in
modeling dependencies between multiple different locations, and that combining
smart input permutations to the mlp architectures may not suffice alone to
close the performance gap."
10123,"However, our upper bound over the
separation rank of mlp architectures is not necessarily tight,                      1<  p       <2                       (21)
so we did not claim it but we leave this possibility open for
further research.","the same budget, then it may be possible that the mlp-
architectures would have a higher ability to model multi-            As we can see (1) the pick performance is obtained for
variable dependencies.","log2 d

                                                                     and note that

                                                                     p              p                              p
                                                                     log3 d = log2 d · log2 3 ≈ 1.58 · log2 d (22)

                                                                     hence for the transformer it just holds that

6.",2022-08-17 09:59:22+00:00,Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems,cs.CL,"['cs.CL', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Dan Navon'), arxiv.Result.Author('Alex M. Bronstein')]","Vision-Transformers are widely used in various vision tasks. Meanwhile, there
is another line of works starting with the MLP-mixer trying to achieve similar
performance using mlp-based architectures. Interestingly, until now those
mlp-based architectures have not been adapted for NLP tasks. Additionally,
until now, mlp-based architectures have failed to achieve state-of-the-art
performance in vision tasks. In this paper, we analyze the expressive power of
mlp-based architectures in modeling dependencies between multiple different
inputs simultaneously, and show an exponential gap between the attention and
the mlp-based mechanisms. Our results suggest a theoretical explanation for the
mlp inability to compete with attention-based mechanisms in NLP problems, they
also suggest that the performance gap in vision tasks may be due to the mlp
relative weakness in modeling dependencies between multiple different
locations, and that combining smart input permutations with mlp architectures
may not be enough to close the performance gap alone."
10124,"However, our upper bound over the
separation rank of mlp architectures is not necessarily tight,                      1<  p       <2                       (21)
so we did not claim it but we leave this possibility open for
further research.","the same budget, then it may be possible that the mlp-
architectures would have a higher ability to model multi-            As we can see (1) the pick performance is obtained for
variable dependencies.","log2 d

                                                                     and note that

                                                                     p              p                              p
                                                                     log3 d = log2 d · log2 3 ≈ 1.58 · log2 d (22)

                                                                     hence for the transformer it just holds that

6.",2022-08-17 09:59:22+00:00,Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems,cs.CL,"['cs.CL', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Dan Navon'), arxiv.Result.Author('Alex M. Bronstein')]","Vision-Transformers are widely used in various vision tasks. Meanwhile, there
is another line of works starting with the MLP-mixer trying to achieve similar
performance using mlp-based architectures. Interestingly, until now those
mlp-based architectures have not been adapted for NLP tasks. Additionally,
until now, mlp-based architectures have failed to achieve state-of-the-art
performance in vision tasks. In this paper, we analyze the expressive power of
mlp-based architectures in modeling dependencies between multiple different
inputs simultaneously, and show an exponential gap between the attention and
the mlp-based mechanisms. Our results suggest a theoretical explanation for the
mlp inability to compete with attention-based mechanisms in NLP problems, they
also suggest that the performance gap in vision tasks may be due to the mlp
relative weakness in modeling dependencies between multiple different
locations, and that combining smart input permutations with mlp architectures
may not be enough to close the performance gap alone."
10164,"Background noise: Background noise is an interesting
data                                                                                                     direction for further study, which perhaps maybe a more
                                                                                                         realistic kind of noise.",Experiment (E2B)- Evolution of the activations on noisy                                       1.,"To generate realistic background
In our next experiment, we compare how the model inference                                               noise, some works, e.g.",2022-08-17 20:00:54+00:00,Analyzing Robustness of End-to-End Neural Models for Automatic Speech Recognition,cs.CL,"['cs.CL', 'cs.LG', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Goutham Rajendran'), arxiv.Result.Author('Wei Zou')]","We investigate robustness properties of pre-trained neural models for
automatic speech recognition. Real life data in machine learning is usually
very noisy and almost never clean, which can be attributed to various factors
depending on the domain, e.g. outliers, random noise and adversarial noise.
Therefore, the models we develop for various tasks should be robust to such
kinds of noisy data, which led to the thriving field of robust machine
learning. We consider this important issue in the setting of automatic speech
recognition. With the increasing popularity of pre-trained models, it's an
important question to analyze and understand the robustness of such models to
noise. In this work, we perform a robustness analysis of the pre-trained neural
models wav2vec2, HuBERT and DistilHuBERT on the LibriSpeech and TIMIT datasets.
We use different kinds of noising mechanisms and measure the model performances
as quantified by the inference time and the standard Word Error Rate metric. We
also do an in-depth layer-wise analysis of the wav2vec2 model when injecting
noise in between layers, enabling us to predict at a high level what each layer
learns. Finally for this model, we visualize the propagation of errors across
the layers and compare how it behaves on clean versus noisy data. Our
experiments conform the predictions of Pasad et al. [2021] and also raise
interesting directions for future work."
10165,"So this
where out(i1) and out(i2) are the activations of layer i on inputs                                       is a deeply fascinating area for further research.",[19].,"x(1) and x(2) respectively, and di is the number of neurons in
                                                                      NoAisveg=L20.l1o,ssWvEsRla=ye0r.05                                                    Acoustics, Speech and Signal Processing (ICASSP).",2022-08-17 20:00:54+00:00,Analyzing Robustness of End-to-End Neural Models for Automatic Speech Recognition,cs.CL,"['cs.CL', 'cs.LG', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Goutham Rajendran'), arxiv.Result.Author('Wei Zou')]","We investigate robustness properties of pre-trained neural models for
automatic speech recognition. Real life data in machine learning is usually
very noisy and almost never clean, which can be attributed to various factors
depending on the domain, e.g. outliers, random noise and adversarial noise.
Therefore, the models we develop for various tasks should be robust to such
kinds of noisy data, which led to the thriving field of robust machine
learning. We consider this important issue in the setting of automatic speech
recognition. With the increasing popularity of pre-trained models, it's an
important question to analyze and understand the robustness of such models to
noise. In this work, we perform a robustness analysis of the pre-trained neural
models wav2vec2, HuBERT and DistilHuBERT on the LibriSpeech and TIMIT datasets.
We use different kinds of noising mechanisms and measure the model performances
as quantified by the inference time and the standard Word Error Rate metric. We
also do an in-depth layer-wise analysis of the wav2vec2 model when injecting
noise in between layers, enabling us to predict at a high level what each layer
learns. Finally for this model, we visualize the propagation of errors across
the layers and compare how it behaves on clean versus noisy data. Our
experiments conform the predictions of Pasad et al. [2021] and also raise
interesting directions for future work."
10171,"To further research in this
      tisation through active learning for few-shot     direction, we propose Active PETs, a model that
      claim veriﬁcation;                                incorporates active learning capabilities into PET
                                                        (Schick and Schütze, 2021a,b).","To
following novel contributions:                          the best of our knowledge, however, no work has
                                                        investigated the use of active learning in the con-
   • we are the ﬁrst to study data annotation priori-   text of claim veriﬁcation.","PET has shown
   • we are the ﬁrst to study the extensibility of      competitive performance in a range of NLP clas-
      PET to enable active learning, by proposing       siﬁcation tasks, but its adaptation to the context
      Active PETs, a novel ensemble-based cold-         of automated fact-checking and/or active learning
      start active learning strategy that enables mul-  settings has not been studied.",2022-08-18 10:11:36+00:00,Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xia Zeng'), arxiv.Result.Author('Arkaitz Zubiaga')]","To mitigate the impact of the scarcity of labelled data on fact-checking
systems, we focus on few-shot claim verification. Despite recent work on
few-shot classification by proposing advanced language models, there is a
dearth of research in data annotation prioritisation that improves the
selection of the few shots to be labelled for optimal model performance. We
propose Active PETs, a novel weighted approach that utilises an ensemble of
Pattern Exploiting Training (PET) models based on various language models, to
actively select unlabelled data as candidates for annotation. Using Active PETs
for few-shot data selection shows consistent improvement over the baseline
methods, on two technical fact-checking datasets and using six different
pretrained language models. We show further improvement with Active PETs-o,
which further integrates an oversampling strategy. Our approach enables
effective selection of instances to be labelled where unlabelled data is
abundant but resources for labelling are limited, leading to consistently
improved few-shot claim verification performance. Our code is available."
10207,"We further study ﬁtting models with different
amounts of word order information, and ﬁnd that encoding excessive or insufﬁcient word
orders into the model results in inferior quality of the task knowledge transfer.","Experiments show that ORT consistently outperforms the order-sensitive mod-
els on multiple cross-lingual NLU tasks.","We collect CrossNER, a fully-labeled collection of named entity recognition (NER) data
spanning ﬁve diverse domains with specialized entity categories, to catalyze the cross-

                                                         102
domain NER research.",2022-08-19 06:59:00+00:00,Effective Transfer Learning for Low-Resource Natural Language Understanding,cs.CL,"['cs.CL', 'cs.AI']",[arxiv.Result.Author('Zihan Liu')],"Natural language understanding (NLU) is the task of semantic decoding of
human languages by machines. NLU models rely heavily on large training data to
ensure good performance. However, substantial languages and domains have very
few data resources and domain experts. It is necessary to overcome the data
scarcity challenge, when very few or even zero training samples are available.
In this thesis, we focus on developing cross-lingual and cross-domain methods
to tackle the low-resource issues. First, we propose to improve the model's
cross-lingual ability by focusing on the task-related keywords, enhancing the
model's robustness and regularizing the representations. We find that the
representations for low-resource languages can be easily and greatly improved
by focusing on just the keywords. Second, we present Order-Reduced Modeling
methods for the cross-lingual adaptation, and find that modeling partial word
orders instead of the whole sequence can improve the robustness of the model
against word order differences between languages and task knowledge transfer to
low-resource languages. Third, we propose to leverage different levels of
domain-related corpora and additional masking of data in the pre-training for
the cross-domain adaptation, and discover that more challenging pre-training
can better address the domain discrepancy issue in the task knowledge transfer.
Finally, we introduce a coarse-to-fine framework, Coach, and a cross-lingual
and cross-domain parsing framework, X2Parser. Coach decomposes the
representation learning process into a coarse-grained and a fine-grained
feature learning, and X2Parser simplifies the hierarchical task structures into
flattened ones. We observe that simplifying task structures makes the
representation learning more effective for low-resource languages and domains."
10217,"All of them seem to lead to an interaction between text-to-image
generators and machine translation, opening an interesting topic for further research.","Based on these results, we brieﬂy discuss how language support can be achieved
in text-to-image generators exploring different options.",The rest of the paper is organized as follows.,2022-08-19 13:24:56+00:00,Text to Image Generation: Leaving no Language Behind,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Pedro Reviriego'), arxiv.Result.Author('Elena Merino-Gómez')]","One of the latest applications of Artificial Intelligence (AI) is to generate
images from natural language descriptions. These generators are now becoming
available and achieve impressive results that have been used for example in the
front cover of magazines. As the input to the generators is in the form of a
natural language text, a question that arises immediately is how these models
behave when the input is written in different languages. In this paper we
perform an initial exploration of how the performance of three popular
text-to-image generators depends on the language. The results show that there
is a significant performance degradation when using languages other than
English, especially for languages that are not widely used. This observation
leads us to discuss different alternatives on how text-to-image generators can
be improved so that performance is consistent across different languages. This
is fundamental to ensure that this new technology can be used by non-native
English speakers and to preserve linguistic diversity."
10218,"All of them seem to lead to an interaction between text-to-image
generators and machine translation, opening an interesting topic for further research.","Based on these results, we brieﬂy discuss how language support can be achieved
in text-to-image generators exploring different options.",The rest of the paper is organized as follows.,2022-08-19 13:24:56+00:00,Text to Image Generation: Leaving no Language Behind,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Pedro Reviriego'), arxiv.Result.Author('Elena Merino-Gómez')]","One of the latest applications of Artificial Intelligence (AI) is to generate
images from natural language descriptions. These generators are now becoming
available and achieve impressive results that have been used for example in the
front cover of magazines. As the input to the generators is in the form of a
natural language text, a question that arises immediately is how these models
behave when the input is written in different languages. In this paper we
perform an initial exploration of how the performance of three popular
text-to-image generators depends on the language. The results show that there
is a significant performance degradation when using languages other than
English, especially for languages that are not widely used. This observation
leads us to discuss different alternatives on how text-to-image generators can
be improved so that performance is consistent across different languages. This
is fundamental to ensure that this new technology can be used by non-native
English speakers and to preserve linguistic diversity."
10235,"After further research, it was found that the model hub contains     5.1.1 Data Layout.",Figure 6: The data hierarchical layout of gBuilder.,"The data layout in gBuilder is defined as four
an RE model named BERTRE (6) that specializes in identifying            levels, as shown in the Figure 6.
relations between company and person, as well as an RE model            • Corpora contains all documents of an entire project.",2022-08-20 15:07:06+00:00,gBuilder: A Scalable Knowledge Graph Construction System for Unstructured Corpus,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yanzeng Li'), arxiv.Result.Author('Lei Zou')]","We design a user-friendly and scalable knowledge graph construction (KGC)
system for extracting structured knowledge from the unstructured corpus.
Different from existing KGC systems, gBuilder provides a flexible and
user-defined pipeline to embracing the rapid development of IE models. More
built-in template-based or heuristic operators and programmable operators are
available for adapting to data from different domains. Furthermore, we also
design a cloud-based self-adaptive task scheduling for gBuilder to ensure its
scalability on large-scale knowledge graph construction. Experimental
evaluation not only demonstrates the ability of gBuilder to organize multiple
information extraction models for knowledge graph construction in a uniform
platform, and also confirms its high scalability on large-scale KGC task."
10236,"• Task Column is a set of data columns related to a single task,

   After further research, it was found that the model hub con-              which only contains the required fields.",which meets the requirements of the pre-defined ontology.,"We split the data into
tains an RE model named BERTRE (6) that specializes in identify-             columns and then group/transmit the necessities for successor
ing relations between company and person, as well as an RE model             tasks to reduce the intermediate transmission data amount (e.g.,
named LSTMRE (7) that can identify attributes relation of company            a single entity_type_filter task only requires the input of
founded date.",2022-08-20 15:07:06+00:00,gBuilder: A Scalable Knowledge Graph Construction System for Unstructured Corpus,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yanzeng Li'), arxiv.Result.Author('Lei Zou')]","We design a user-friendly and scalable knowledge graph construction (KGC)
system for extracting structured knowledge from the unstructured corpus.
Different from existing KGC systems, gBuilder provides a flexible and
user-defined pipeline to embrace the rapid development of IE models. More
built-in template-based or heuristic operators and programmable operators are
available for adapting to data from different domains. Furthermore, we also
design a cloud-based self-adaptive task scheduling for gBuilder to ensure its
scalability on large-scale knowledge graph construction. Experimental
evaluation demonstrates the ability of gBuilder to organize multiple
information extraction models for knowledge graph construction in a uniform
platform, and confirms its high scalability on large-scale KGC tasks."
10248,"In the future, further research will be made on the hierarchical relationship of
knowledge points, linking mathematical entities, and expanding other datasets.","Besides, the math text has few features and is easy to overﬁt.","References

 [1] Somnath Banerjee, Krishnan Ramanathan, and Ajay Gupta.",2022-08-21 11:11:30+00:00,Automatic tagging of knowledge points for K12 math problems,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xiaolu Wang'), arxiv.Result.Author('Ziqi Ding'), arxiv.Result.Author('Liangyu Chen')]","Automatic tagging of knowledge points for practice problems is the basis for
managing question bases and improving the automation and intelligence of
education. Therefore, it is of great practical significance to study the
automatic tagging technology for practice problems. However, there are few
studies on the automatic tagging of knowledge points for math problems. Math
texts have more complex structures and semantics compared with general texts
because they contain unique elements such as symbols and formulas. Therefore,
it is difficult to meet the accuracy requirement of knowledge point prediction
by directly applying the text classification techniques in general domains. In
this paper, K12 math problems taken as the research object, the LABS model
based on label-semantic attention and multi-label smoothing combining textual
features is proposed to improve the automatic tagging of knowledge points for
math problems. The model combines the text classification techniques in general
domains and the unique features of math texts. The results show that the models
using label-semantic attention or multi-label smoothing perform better on
precision, recall, and F1-score metrics than the traditional BiLSTM model,
while the LABS model using both performs best. It can be seen that label
information can guide the neural networks to extract meaningful information
from the problem text, which improves the text classification performance of
the model. Moreover, multi-label smoothing combining textual features can fully
explore the relationship between text and labels, improve the model's
prediction ability for new data and improve the model's classification
accuracy."
10275,"We believe that the incorporation of external knowledge into
explanation methods is a promising direction for further research as the prediction may not be intuitively explainable
to humans in terms of only input components.","The proposed explanation methods for pharmacology commonly
use a connection to an external knowledge source.","In addition, external human-curated knowledge may naturally be more
intuitive to end-users.",2022-08-22 12:10:27+00:00,"Survey of NLP in Pharmacology: Methodology, Tasks, Resources, Knowledge, and Tools",cs.CL,"['cs.CL', 'cs.LG', 'q-bio.BM', 'J.3; A.1']","[arxiv.Result.Author('Dimitar Trajanov'), arxiv.Result.Author('Vangel Trajkovski'), arxiv.Result.Author('Makedonka Dimitrieva'), arxiv.Result.Author('Jovana Dobreva'), arxiv.Result.Author('Milos Jovanovik'), arxiv.Result.Author('Matej Klemen'), arxiv.Result.Author('Aleš Žagar'), arxiv.Result.Author('Marko Robnik-Šikonja')]","Natural language processing (NLP) is an area of artificial intelligence that
applies information technologies to process the human language, understand it
to a certain degree, and use it in various applications. This area has rapidly
developed in the last few years and now employs modern variants of deep neural
networks to extract relevant patterns from large text corpora. The main
objective of this work is to survey the recent use of NLP in the field of
pharmacology. As our work shows, NLP is a highly relevant information
extraction and processing approach for pharmacology. It has been used
extensively, from intelligent searches through thousands of medical documents
to finding traces of adversarial drug interactions in social media. We split
our coverage into five categories to survey modern NLP methodology, commonly
addressed tasks, relevant textual data, knowledge bases, and useful programming
libraries. We split each of the five categories into appropriate subcategories,
describe their main properties and ideas, and summarize them in a tabular form.
The resulting survey presents a comprehensive overview of the area, useful to
practitioners and interested observers."
10283,"It has been claimed that there is an even stronger law which relates the length of a word with
its context of occurrence (Piantadosi et al., 2011), but further research has demonstrated that this new
law is weaker and supported in fewer languages than expected (Koplenig et al., 2022; Levshina, 2022;
Meylan and Griﬃths, 2021).","One of the most robust patterns is Zipf’s
law of abbreviation: more frequent words tend to be shorter (Bentz and Ferrer-i-Cancho, 2016; Zipf,
1949).","Therefore, Zipf’s law of abbreviation is, at present, one of the strongest
laws of language in terms of theoretical understanding (Ferrer-i-Cancho et al., 2019; Ferrer-i-Cancho,
Debowski, et al., 2013; Kanwal et al., 2017), and in terms of empirical support both across languages
(Bentz and Ferrer-i-Cancho, 2016) and in other species (Semple et al., 2022).",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10284,"However, that issue should be the subject of further research.","We suspect that the low values of Ω𝜌 and Ω𝜏 could originate from some
similarity between 𝜏 and 𝜌 (e.g., both are rank correlation scores and then both may yield low values of
Ω) or the fact that the deﬁnition of Ω𝜌 and that of Ω𝜏 is the same except for the choice of the correlation
coeﬃcient).",Question 5.,2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10285,"Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","Future research

In this article, we have introduced two new scores that require further theoretical and empirical research.","We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10286,"We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.","Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","For simplicity, we have investigated only the weak recoding problem.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10287,"It has been claimed that there is an even stronger law which relates the length of a word with
its context of occurrence (Piantadosi et al., 2011), but further research has demonstrated that this new
law is weaker and supported in fewer languages than expected (Koplenig et al., 2022; Levshina, 2022;
Meylan and Griﬃths, 2021).","One of the most robust patterns is Zipf’s
law of abbreviation: more frequent words tend to be shorter (Bentz and Ferrer-i-Cancho, 2016; Zipf,
1949).","Therefore, Zipf’s law of abbreviation is, at present, one of the strongest
laws of language in terms of theoretical understanding (Ferrer-i-Cancho et al., 2019; Ferrer-i-Cancho,
Debowski, et al., 2013; Kanwal et al., 2017), and in terms of empirical support both across languages
(Bentz and Ferrer-i-Cancho, 2016) and in other species (Semple et al., 2022).",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10288,"However, that issue
should be the subject of further research.","We suspect that the low values of Ω𝜌 and Ω𝜏 could originate from some similarity between 𝜏 and 𝜌 (e.g.,
both are rank correlation scores and then both may yield low values of Ω) or the fact that the deﬁnition
of Ω𝜌 and that of Ω𝜏 is the same except for the choice of the correlation coeﬃcient).",Question 5.,2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10289,"Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","Future research

In this article, we have introduced two new scores that require further theoretical and empirical research.","We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10290,"We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.","Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","For simplicity, we have investigated only the weak recoding problem.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical advantages and
disadvantages of these and other scores. Harnessing the best score, we quantify
for the first time the degree of optimality of word lengths in languages. This
indicates that languages are optimized to 62 or 67 percent on average
(depending on the source) when word lengths are measured in characters, and to
65 percent on average when word lengths are measured in time. In general,
spoken word durations are more optimized than written word lengths in
characters. Beyond the analyses reported here, our work paves the way to
measure the degree of optimality of the vocalizations or gestures of other
species, and to compare them against written, spoken, or signed human
languages."
10291,"It has been claimed that there is an even stronger law which relates the length of a word with
its context of occurrence (Piantadosi et al., 2011), but further research has demonstrated that this new
law is weaker and supported in fewer languages than expected (Koplenig et al., 2022; Levshina, 2022;
Meylan and Griﬃths, 2021).","One of the most robust patterns is Zipf’s
law of abbreviation: more frequent words tend to be shorter (Bentz and Ferrer-i-Cancho, 2016; Zipf,
1949).","Therefore, Zipf’s law of abbreviation is, at present, one of the strongest
laws of language in terms of theoretical understanding (Ferrer-i-Cancho et al., 2019; Ferrer-i-Cancho,
Debowski, et al., 2013; Kanwal et al., 2017), and in terms of empirical support both across languages
(Bentz and Ferrer-i-Cancho, 2016) and in other species (Semple et al., 2022).",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10292,"However, that issue should be the subject of further research.","both are rank correlation scores and then both may yield low values of Ω) or the fact that
the template of the deﬁnition of Ω𝜌 and that of Ω𝜏 is the same (Ω𝜌 and Ω𝜏 diﬀer only in the choice of
the correlation coeﬃcient).",Question 5.,2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10293,"Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","Future research

In this article, we have introduced two new scores that require further theoretical and empirical research.","We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10294,"We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.","Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","For simplicity, we have investigated only the weak recoding problem.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10295,"It has been claimed that there is an even stronger law which relates the length of a word with
its context of occurrence (Piantadosi et al., 2011), but further research has demonstrated that this new
law is weaker and supported in fewer languages than expected (Koplenig et al., 2022; Levshina, 2022;
Meylan and Griﬃths, 2021).","One of the most robust patterns is Zipf’s
law of abbreviation: more frequent words tend to be shorter (Bentz and Ferrer-i-Cancho, 2016; Zipf,
1949).","Therefore, Zipf’s law of abbreviation is, at present, one of the strongest
laws of language in terms of theoretical understanding (Ferrer-i-Cancho et al., 2019; Ferrer-i-Cancho,
Debowski, et al., 2013; Kanwal et al., 2017), and in terms of empirical support both across languages
(Bentz and Ferrer-i-Cancho, 2016) and in other species (Semple et al., 2022).",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10296,"However, that issue should be the subject of further research.","both are rank correlation scores and then both may yield low values of Ω) or the fact that
the template of the deﬁnition of Ω𝜌 and that of Ω𝜏 is the same (Ω𝜌 and Ω𝜏 diﬀer only in the choice of
the correlation coeﬃcient).",Question 5.,2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10297,"Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","Future research

In this article, we have introduced two new scores that require further theoretical and empirical research.","We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10298,"We have made a signiﬁcant contribution to the problem of the best score for cross-linguistic
research, but further research is necessary.","Although we have focused on Ψ for the sake of simplicity, further research on Ω and its variants is
necessary.","For simplicity, we have investigated only the weak recoding problem.",2022-08-22 15:03:31+00:00,The optimality of word lengths. Theoretical foundations and an empirical study,cs.CL,"['cs.CL', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Sonia Petrini'), arxiv.Result.Author('Antoni Casas-i-Muñoz'), arxiv.Result.Author('Jordi Cluet-i-Martinell'), arxiv.Result.Author('Mengxue Wang'), arxiv.Result.Author('Christian Bentz'), arxiv.Result.Author('Ramon Ferrer-i-Cancho')]","One of the most robust patterns found in human languages is Zipf's law of
abbreviation, that is, the tendency of more frequent words to be shorter. Since
Zipf's pioneering research, this law has been viewed as a manifestation of
compression, i.e. the minimization of the length of forms - a universal
principle of natural communication. Although the claim that languages are
optimized has become trendy, attempts to measure the degree of optimization of
languages have been rather scarce. Here we demonstrate that compression
manifests itself in a wide sample of languages without exceptions, and
independently of the unit of measurement. It is detectable for both word
lengths in characters of written language as well as durations in time in
spoken language. Moreover, to measure the degree of optimization, we derive a
simple formula for a random baseline and present two scores that are dualy
normalized, namely, they are normalized with respect to both the minimum and
the random baseline. We analyze the theoretical and statistical pros and cons
of these and other scores. Harnessing the best score, we quantify for the first
time the degree of optimality of word lengths in languages. This indicates that
languages are optimized to 62 or 67 percent on average (depending on the
source) when word lengths are measured in characters, and to 65 percent on
average when word lengths are measured in time. In general, spoken word
durations are more optimized than written word lengths in characters. Beyond
the analyses reported here, our work paves the way to measure the degree of
optimality of the vocalizations or gestures of other species, and to compare
them against written, spoken, or signed human languages."
10306,"We believe      derstand these concepts in context (Young et al.,
                                             our results warrant further research towards on-  2013).","We also demonstrate that         amount of dialogue data labelled with concepts
                                             each of the features is responsible for discov-   from that ontology in order to recognize and un-
                                             ering different kinds of content.","This manual annotation is again challeng-
                                             tology induction, and continued harnessing of     ing, time-consuming and expensive (Budzianowski
                                             topological data analysis for dialogue and nat-   et al., 2018).",2022-08-22 17:04:04+00:00,Dialogue Term Extraction using Transfer Learning and Topological Data Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Renato Vukovic'), arxiv.Result.Author('Michael Heck'), arxiv.Result.Author('Benjamin Matthias Ruppik'), arxiv.Result.Author('Carel van Niekerk'), arxiv.Result.Author('Marcus Zibrowius'), arxiv.Result.Author('Milica Gašić')]","Goal oriented dialogue systems were originally designed as a natural language
interface to a fixed data-set of entities that users might inquire about,
further described by domain, slots, and values. As we move towards adaptable
dialogue systems where knowledge about domains, slots, and values may change,
there is an increasing need to automatically extract these terms from raw
dialogues or related non-dialogue data on a large scale. In this paper, we take
an important step in this direction by exploring different features that can
enable systems to discover realizations of domains, slots, and values in
dialogues in a purely data-driven fashion. The features that we examine stem
from word embeddings, language modelling features, as well as topological
features of the word embedding space. To examine the utility of each feature
set, we train a seed model based on the widely used MultiWOZ data-set. Then, we
apply this model to a different corpus, the Schema-Guided Dialogue data-set.
Our method outperforms the previously proposed approach that relies solely on
word embeddings. We also demonstrate that each of the features is responsible
for discovering different kinds of content. We believe our results warrant
further research towards ontology induction, and continued harnessing of
topological data analysis for dialogue and natural language processing
research."
10307,"This would be an     funding provided by the Alexander von Humboldt
interesting direction for further research.","CVN and MH are supported by
utilized to increase the precision.","Foundation in the framework of the Sofja Ko-
                                                         valevskaja Award endowed by the Federal Min-
   Some simpler options for improvement are more         istry of Education and Research.",2022-08-22 17:04:04+00:00,Dialogue Term Extraction using Transfer Learning and Topological Data Analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Renato Vukovic'), arxiv.Result.Author('Michael Heck'), arxiv.Result.Author('Benjamin Matthias Ruppik'), arxiv.Result.Author('Carel van Niekerk'), arxiv.Result.Author('Marcus Zibrowius'), arxiv.Result.Author('Milica Gašić')]","Goal oriented dialogue systems were originally designed as a natural language
interface to a fixed data-set of entities that users might inquire about,
further described by domain, slots, and values. As we move towards adaptable
dialogue systems where knowledge about domains, slots, and values may change,
there is an increasing need to automatically extract these terms from raw
dialogues or related non-dialogue data on a large scale. In this paper, we take
an important step in this direction by exploring different features that can
enable systems to discover realizations of domains, slots, and values in
dialogues in a purely data-driven fashion. The features that we examine stem
from word embeddings, language modelling features, as well as topological
features of the word embedding space. To examine the utility of each feature
set, we train a seed model based on the widely used MultiWOZ data-set. Then, we
apply this model to a different corpus, the Schema-Guided Dialogue data-set.
Our method outperforms the previously proposed approach that relies solely on
word embeddings. We also demonstrate that each of the features is responsible
for discovering different kinds of content. We believe our results warrant
further research towards ontology induction, and continued harnessing of
topological data analysis for dialogue and natural language processing
research."
10323,"It should be       7 ACKNOWLEDGMENTS
noted that althought we determine agent’s personas as “married”,
it is still possible for agents to fabricate personas about “gender”,   This work was supported in part by the National Natural Science
which is a potential problem for further research.","Transformer are coherent at both personality and semantic levels
when other methods give wrong or irrelevant answers.","Foundation of China under Grant No.61602197, Grant No.L1924068,
                                                                        Grant No.61772076, in part by CCF-AFSG Research Fund under
   For the third case: The persona retrieved by PRM is related to       Grant No.RF20210005, and in part by the fund of Joint Laboratory
the query and strongly entails all the predefined personas.",2022-08-23 09:00:58+00:00,Improving Personality Consistency in Conversation by Persona Extending,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yifan Liu'), arxiv.Result.Author('Wei Wei'), arxiv.Result.Author('Jiayi Liu'), arxiv.Result.Author('Xianling Mao'), arxiv.Result.Author('Rui Fang'), arxiv.Result.Author('Dangyang Chen')]","Endowing chatbots with a consistent personality plays a vital role for agents
to deliver human-like interactions. However, existing personalized approaches
commonly generate responses in light of static predefined personas depicted
with textual description, which may severely restrict the interactivity of
human and the chatbot, especially when the agent needs to answer the query
excluded in the predefined personas, which is so-called out-of-predefined
persona problem (named OOP for simplicity). To alleviate the problem, in this
paper we propose a novel retrieval-to-prediction paradigm consisting of two
subcomponents, namely, (1) Persona Retrieval Model (PRM), it retrieves a
persona from a global collection based on a Natural Language Inference (NLI)
model, the inferred persona is consistent with the predefined personas; and (2)
Posterior-scored Transformer (PS-Transformer), it adopts a persona posterior
distribution that further considers the actual personas used in the ground
response, maximally mitigating the gap between training and inferring.
Furthermore, we present a dataset called IT-ConvAI2 that first highlights the
OOP problem in personalized dialogue. Extensive experiments on both IT-ConvAI2
and ConvAI2 demonstrate that our proposed model yields considerable
improvements in both automatic metrics and human evaluations."
10371,"Note that
                                              datasets, in order to stimulate further research into       this paper investigates the potential to predict bankruptcy
                                              the task.","This pa-           that end, scripts to reconstruct the benchmark and repro-
                                              per introduces such a benchmark for the unstruc-            duce the presented results are available at https://github.com/
                                              tured data scenario, based on novel and established         henriarnoUG/BankruptcyBenchmarkBaselines.",We describe and evaluate several classi-          from textual disclosures only.,2022-08-24 07:11:49+00:00,Next-Year Bankruptcy Prediction from Textual Data: Benchmark and Baselines,cs.CL,"['cs.CL', 'q-fin.CP']","[arxiv.Result.Author('Henri Arno'), arxiv.Result.Author('Klaas Mulier'), arxiv.Result.Author('Joke Baeck'), arxiv.Result.Author('Thomas Demeester')]","Models for bankruptcy prediction are useful in several real-world scenarios,
and multiple research contributions have been devoted to the task, based on
structured (numerical) as well as unstructured (textual) data. However, the
lack of a common benchmark dataset and evaluation strategy impedes the
objective comparison between models. This paper introduces such a benchmark for
the unstructured data scenario, based on novel and established datasets, in
order to stimulate further research into the task. We describe and evaluate
several classical and neural baseline models, and discuss benefits and flaws of
different strategies. In particular, we find that a lightweight bag-of-words
model based on static in-domain word representations obtains surprisingly good
results, especially when taking textual data from several years into account.
These results are critically assessed, and discussed in light of particular
aspects of the data and the task. All code to replicate the data and
experimental results will be released."
10384,"This will enable us further research in identifying
evidence of the relationship between ACEs and the subsequent developments of mental illness (e.g., addictions) in
large-scale and longitudinal free-text EHRs, which has previously not been possible.","We are currently developing a tool that would use NLP
techniques to assist us in surfacing ACEs from clinical notes.","Introduction

Adverse childhood experiences (ACEs) encompass a broad variety of early traumas that occur in childhood, including
both direct (e.g., abuse and neglect) and indirect (e.g., family mental illness and domestic violence) types [1, 2].",2022-08-24 12:17:32+00:00,Adverse Childhood Experiences Identification from Clinical Notes with Ontologies and NLP,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jinge Wu'), arxiv.Result.Author('Rowena Smith'), arxiv.Result.Author('Honghan Wu')]","Adverse Childhood Experiences (ACEs) are defined as a collection of highly
stressful, and potentially traumatic, events or circumstances that occur
throughout childhood and/or adolescence. They have been shown to be associated
with increased risks of mental health diseases or other abnormal behaviours in
later lives. However, the identification of ACEs from free-text Electronic
Health Records (EHRs) with Natural Language Processing (NLP) is challenging
because (a) there is no NLP ready ACE ontologies; (b) there are limited cases
available for machine learning, necessitating the data annotation from clinical
experts. We are currently developing a tool that would use NLP techniques to
assist us in surfacing ACEs from clinical notes. This will enable us further
research in identifying evidence of the relationship between ACEs and the
subsequent developments of mental illness (e.g., addictions) in large-scale and
longitudinal free-text EHRs, which has previously not been possible."
10406,"realized by an edit, which the model can explain
both in form of a textual comment and by pointing            • To facilitate further research on collaborative
to references used; this is enabled by augmenting               LMs, we release a variety of PEER models as
each input text with retrieved passages containing              well as the data and code used to train them.",This plan is then            strong performance.,potentially relevant background information.,2022-08-24 16:56:47+00:00,PEER: A Collaborative Language Model,cs.CL,['cs.CL'],"[arxiv.Result.Author('Timo Schick'), arxiv.Result.Author('Jane Dwivedi-Yu'), arxiv.Result.Author('Zhengbao Jiang'), arxiv.Result.Author('Fabio Petroni'), arxiv.Result.Author('Patrick Lewis'), arxiv.Result.Author('Gautier Izacard'), arxiv.Result.Author('Qingfei You'), arxiv.Result.Author('Christoforos Nalmpantis'), arxiv.Result.Author('Edouard Grave'), arxiv.Result.Author('Sebastian Riedel')]","Textual content is often the output of a collaborative writing process: We
start with an initial draft, ask for suggestions, and repeatedly make changes.
Agnostic of this process, today's language models are trained to generate only
the final result. As a consequence, they lack several abilities crucial for
collaborative writing: They are unable to update existing texts, difficult to
control and incapable of verbally planning or explaining their actions. To
address these shortcomings, we introduce PEER, a collaborative language model
that is trained to imitate the entire writing process itself: PEER can write
drafts, add suggestions, propose edits and provide explanations for its
actions. Crucially, we train multiple instances of PEER able to infill various
parts of the writing process, enabling the use of self-training techniques for
increasing the quality, amount and diversity of training data. This unlocks
PEER's full potential by making it applicable in domains for which no edit
histories are available and improving its ability to follow instructions, to
write useful comments, and to explain its actions. We show that PEER achieves
strong performance across various domains and editing tasks."
10416,"In this section, we discuss the challenges       why existing mitigation methods have only a lim-
that deserve further research from the community.","This could explain
lenges.",ited improvement in generalization.,2022-08-25 03:51:39+00:00,Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Mengnan Du'), arxiv.Result.Author('Fengxiang He'), arxiv.Result.Author('Na Zou'), arxiv.Result.Author('Dacheng Tao'), arxiv.Result.Author('Xia Hu')]","Large language models (LLMs) have achieved state-of-the-art performance on a
series of natural language understanding tasks. However, these LLMs might rely
on dataset bias and artifacts as shortcuts for prediction. This has
significantly hurt their Out-of-Distribution (OOD) generalization and
adversarial robustness. In this paper, we provide a review of recent
developments that address the robustness challenge of LLMs. We first introduce
the concepts and robustness challenge of LLMs. We then introduce methods to
identify shortcut learning behavior in LLMs, characterize the reasons for
shortcut learning, as well as introduce mitigation solutions. Finally, we
identify key challenges and introduce the connections of this line of research
to other directions."
10420,points out areas of further research.,"It is        Finally, Section 6 provides the conclusion and
a language with 17 sub-linguistic dialects i.e.","Lulogoli, Luisukha, Luitakho, Lutiriki, Lubukusu,
Lutachoni, Lunyore, Lumarachi, Lukhayo,                  2  Related work
Lusamia, Lunyala, Lumarama, Lushisa, Luwanga,
Lutiriki, Lutsotso and Lukabras (Lubangah, 2018).",2022-08-25 13:27:14+00:00,"Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks",cs.CL,['cs.CL'],"[arxiv.Result.Author('Barack Wanjawa'), arxiv.Result.Author('Lilian Wanzare'), arxiv.Result.Author('Florence Indede'), arxiv.Result.Author('Owen McOnyango'), arxiv.Result.Author('Edward Ombui'), arxiv.Result.Author('Lawrence Muchemi')]","Indigenous African languages are categorized as under-served in Artificial
Intelligence and suffer poor digital inclusivity and information access. The
challenge has been how to use machine learning and deep learning models without
the requisite data. Kencorpus is a Kenyan Language corpus that intends to
bridge the gap on how to collect, and store text and speech data that is good
enough to enable data-driven solutions in applications such as machine
translation, question answering and transcription in multilingual communities.
Kencorpus is a corpus (text and speech) for three languages predominantly
spoken in Kenya: Swahili, Dholuo and Luhya (dialects Lumarachi, Lulogooli and
Lubukusu). This corpus intends to fill the gap of developing a dataset that can
be used for Natural Language Processing and Machine Learning tasks for
low-resource languages. Each of these languages contributed text and speech
data for the language corpus. Data collection was done by researchers from
communities, schools and collaborating partners (media, publishers). Kencorpus
has a collection of 5,594 items, being 4,442 texts (5.6million words) and 1,152
speech files (177hrs). Based on this data, other datasets were also developed
e.g POS tagging sets for Dholuo and Luhya (50,000 and 93,000 words tagged
respectively), Question-Answer pairs from Swahili texts (7,537 QA pairs) and
Translation of texts into Swahili (12,400 sentences). The datasets are useful
for machine learning tasks such as text processing, annotation and translation.
The project also undertook proof of concept systems in speech to text and
machine learning for QA task, with initial results confirming the usability of
the Kencorpus to the machine learning community. Kencorpus is the first such
corpus of its kind for these low resource languages and forms a basis of
learning and sharing experiences for similar works."
10538,"We make our dataset
and the pre-trained models available for further research.","Preliminary experiments with state-of-the-art techniques suggest
that classiﬁers that consume both the headline and the target improve their
performance for the task over those that only consume the headline, giving
indications that both sources of information are useful.","2 Previous work

Sentiment analysis and opinion mining have been one of the most popular ap-
plications of NLP, and several workshops were dedicated to this topic [4, 5].",2022-08-30 01:30:30+00:00,A Spanish dataset for Targeted Sentiment Analysis of political headlines,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tomás Alves Salgueiro'), arxiv.Result.Author('Emilio Recart Zapata'), arxiv.Result.Author('Damián Furman'), arxiv.Result.Author('Juan Manuel Pérez'), arxiv.Result.Author('Pablo Nicolás Fernández Larrosa')]","Subjective texts have been studied by several works as they can induce
certain behaviours in their users. Most work focuses on user-generated texts in
social networks, but some other texts also comprise opinions on certain topics
and could influence judgement criteria during political decisions. In this
work, we address the task of Targeted Sentiment Analysis for the domain of news
headlines, published by the main outlets during the 2019 Argentinean
Presidential Elections. For this purpose, we present a polarity dataset of
1,976 headlines mentioning candidates in the 2019 elections at the target
level. Preliminary experiments with state-of-the-art classification algorithms
based on pre-trained linguistic models suggest that target information is
helpful for this task. We make our data and pre-trained models publicly
available."
10539,To further study           REFUTES samples.,"FEVER
                                                           score requests that fact veriﬁcation label is cor-
   Moreover, for a claim, there exist several groups       rectly predicted, and at least one complete group
of evidences, and each group itself is enough to           of evidence sentences is found for SUPPORTS and
independently verify the claim.","The second important metric
the impact of multi-view contextual information,           is label accuracy.",2022-08-30 05:57:34+00:00,IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hao Wang'), arxiv.Result.Author('Yangguang Li'), arxiv.Result.Author('Zhen Huang'), arxiv.Result.Author('Yong Dou')]","With the rapid development of automatic fake news detection technology, fact
extraction and verification (FEVER) has been attracting more attention. The
task aims to extract the most related fact evidences from millions of
open-domain Wikipedia documents and then verify the credibility of
corresponding claims. Although several strong models have been proposed for the
task and they have made great progress, we argue that they fail to utilize
multi-view contextual information and thus cannot obtain better performance. In
this paper, we propose to integrate multi-view contextual information (IMCI)
for fact extraction and verification. For each evidence sentence, we define two
kinds of context, i.e. intra-document context and inter-document context}.
Intra-document context consists of the document title and all the other
sentences from the same document. Inter-document context consists of all other
evidences which may come from different documents. Then we integrate the
multi-view contextual information to encode the evidence sentences to handle
the task. Our experimental results on FEVER 1.0 shared task show that our IMCI
framework makes great progress on both fact extraction and verification, and
achieves state-of-the-art performance with a winning FEVER score of 72.97% and
label accuracy of 75.84% on the online blind test set. We also conduct ablation
study to detect the impact of multi-view contextual information. Our codes will
be released at https://github.com/phoenixsecularbird/IMCI."
10558,"Finally, section 6
concludes with limitations and suggestions for further research directions.","Section 5
demonstrates the use of four functions in WikiLink and presents a design case with WikiLink.",2.,2022-08-30 15:48:36+00:00,WikiLink: an encyclopedia-based semantic network for design innovation,cs.CL,"['cs.CL', 'cs.DL']","[arxiv.Result.Author('Haoyu Zuo'), arxiv.Result.Author('Qianzhi Jing'), arxiv.Result.Author('Tianqi Song'), arxiv.Result.Author('Huiting Liu'), arxiv.Result.Author('Lingyun Sun'), arxiv.Result.Author('Peter Childs'), arxiv.Result.Author('Liuqing Chen')]","Data-driven design and innovation is a process to reuse and provide valuable
and useful information. However, existing semantic networks for design
innovation is built on data source restricted to technological and scientific
information. Besides, existing studies build the edges of a semantic network
only on either statistical or semantic relationships, which is less likely to
make full use of the benefits from both types of relationships and discover
implicit knowledge for design innovation. Therefore, we constructed WikiLink, a
semantic network based on Wikipedia. Combined weight which fuses both the
statistic and semantic weights between concepts is introduced in WikiLink, and
four algorithms are developed for inspiring new ideas. Evaluation experiments
are undertaken and results show that the network is characterised by high
coverage of terms, relationships and disciplines, which proves the network's
effectiveness and usefulness. Then a demonstration and case study results
indicate that WikiLink can serve as an idea generation tool for innovation in
conceptual design. The source code of WikiLink and the backend data are
provided open-source for more users to explore and build on."
10575,"MULTICONER
                                             poses challenges even for large pre-trained        • Memorization effects, due to large overlap of
                                             language models, and we believe that it can          entities between the train and test sets also in-
                                             help further research in building robust NER         creases performance.",ment of macro-F1=+30%).,systems.,2022-08-30 20:45:54+00:00,MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity Recognition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shervin Malmasi'), arxiv.Result.Author('Anjie Fang'), arxiv.Result.Author('Besnik Fetahu'), arxiv.Result.Author('Sudipta Kar'), arxiv.Result.Author('Oleg Rokhlenko')]","We present MultiCoNER, a large multilingual dataset for Named Entity
Recognition that covers 3 domains (Wiki sentences, questions, and search
queries) across 11 languages, as well as multilingual and code-mixing subsets.
This dataset is designed to represent contemporary challenges in NER, including
low-context scenarios (short and uncased text), syntactically complex entities
like movie titles, and long-tail entity distributions. The 26M token dataset is
compiled from public resources using techniques such as heuristic-based
sentence sampling, template extraction and slotting, and machine translation.
We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a
state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves
moderate performance (macro-F1=54%), highlighting the difficulty of our data.
GEMNET, which uses gazetteers, improvement significantly (average improvement
of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained
language models, and we believe that it can help further research in building
robust NER systems. MultiCoNER is publicly available at
https://registry.opendata.aws/multiconer/ and we hope that this resource will
help advance research in various aspects of NER."
10576,"It is our
                                                               hope that this resource will help further research
   10For ZH, the tokenization is done at the character level.","This result is expected, given                pre-trained language models fail to achieve good
that the test data contains highly complex entities            performance without external resources.",for building better NER systems.,2022-08-30 20:45:54+00:00,MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity Recognition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shervin Malmasi'), arxiv.Result.Author('Anjie Fang'), arxiv.Result.Author('Besnik Fetahu'), arxiv.Result.Author('Sudipta Kar'), arxiv.Result.Author('Oleg Rokhlenko')]","We present MultiCoNER, a large multilingual dataset for Named Entity
Recognition that covers 3 domains (Wiki sentences, questions, and search
queries) across 11 languages, as well as multilingual and code-mixing subsets.
This dataset is designed to represent contemporary challenges in NER, including
low-context scenarios (short and uncased text), syntactically complex entities
like movie titles, and long-tail entity distributions. The 26M token dataset is
compiled from public resources using techniques such as heuristic-based
sentence sampling, template extraction and slotting, and machine translation.
We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a
state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves
moderate performance (macro-F1=54%), highlighting the difficulty of our data.
GEMNET, which uses gazetteers, improvement significantly (average improvement
of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained
language models, and we believe that it can help further research in building
robust NER systems. MultiCoNER is publicly available at
https://registry.opendata.aws/multiconer/ and we hope that this resource will
help advance research in various aspects of NER."
10577,"gressive models continue to out-perform masked          (2022) show that a zero pronoun anaphora reso-
language models as the training set increases in        lution system based on XLM-R performs better
size is a question for further research.","Whether or not autore-             Finally, as discussed in Section 2, Yang et al.","Overall,       than one based on multilingual BERT (Song et al.,
then, we see that in our sample of models, autore-      2020b).",2022-08-30 22:06:07+00:00,Do language models make human-like predictions about the coreferents of Italian anaphoric zero pronouns?,cs.CL,"['cs.CL', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT']","[arxiv.Result.Author('James A. Michaelov'), arxiv.Result.Author('Benjamin K. Bergen')]","Some languages allow arguments to be omitted in certain contexts. Yet human
language comprehenders reliably infer the intended referents of these zero
pronouns, in part because they construct expectations about which referents are
more likely. We ask whether Neural Language Models also extract the same
expectations. We test whether 12 contemporary language models display
expectations that reflect human behavior when exposed to sentences with zero
pronouns from five behavioral experiments conducted in Italian by Carminati
(2005). We find that three models - XGLM 2.9B, 4.5B, and 7.5B - capture the
human behavior from all the experiments, with others successfully modeling some
of the results. This result suggests that human expectations about coreference
can be derived from exposure to language, and also indicates features of
language models that allow them to better reflect human behavior."
10578,"gressive models continue to out-perform masked          (2022) show that a zero pronoun anaphora reso-
language models as the training set increases in        lution system based on XLM-R performs better
size is a question for further research.","Whether or not autore-             Finally, as discussed in Section 2, Yang et al.","Overall,       than one based on multilingual BERT (Song et al.,
then, we see that in our sample of models, autore-      2020b).",2022-08-30 22:06:07+00:00,Do language models make human-like predictions about the coreferents of Italian anaphoric zero pronouns?,cs.CL,"['cs.CL', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT']","[arxiv.Result.Author('James A. Michaelov'), arxiv.Result.Author('Benjamin K. Bergen')]","Some languages allow arguments to be omitted in certain contexts. Yet human
language comprehenders reliably infer the intended referents of these zero
pronouns, in part because they construct expectations about which referents are
more likely. We ask whether Neural Language Models also extract the same
expectations. We test whether 12 contemporary language models display
expectations that reflect human behavior when exposed to sentences with zero
pronouns from five behavioral experiments conducted in Italian by Carminati
(2005). We find that three models - XGLM 2.9B, 4.5B, and 7.5B - capture the
human behavior from all the experiments, with others successfully modeling some
of the results. This result suggests that human expectations about coreference
can be derived from exposure to language, and also indicates features of
language models that allow them to better reflect human behavior."
10579,We further study         grate it into a self-training framework.,"(2021) introduce a
our method obtains a 1.5% absolute improvement        calibrated conﬁdence estimation method and inte-
in F1 on the BC5CDR dataset.","Without re-
the impact of various choices of components in our    plying on sophisticated de-noising designs, our bi-
method, and conduct breakdown analysis at entity      encoder framework can be directly used in distant
type level and token level, which suggests further    supervision.",2022-08-30 23:19:04+00:00,Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sheng Zhang'), arxiv.Result.Author('Hao Cheng'), arxiv.Result.Author('Jianfeng Gao'), arxiv.Result.Author('Hoifung Poon')]","We present an efficient bi-encoder framework for named entity recognition
(NER), which applies contrastive learning to map candidate text spans and
entity types into the same vector representation space. Prior work
predominantly approaches NER as sequence labeling or span classification. We
instead frame NER as a metric learning problem that maximizes the similarity
between the vector representations of an entity mention and its type. This
makes it easy to handle nested and flat NER alike, and can better leverage
noisy self-supervision signals. A major challenge to this bi-encoder
formulation for NER lies in separating non-entity spans from entity mentions.
Instead of explicitly labeling all non-entity spans as the same class Outside
(O) as in most prior methods, we introduce a novel dynamic thresholding loss,
which is learned in conjunction with the standard contrastive loss. Experiments
show that our method performs well in both supervised and distantly supervised
settings, for nested and flat NER alike, establishing new state of the art
across standard datasets in the general domain (e.g., ACE2004, ACE2005) and
high-value verticals such as biomedicine (e.g., GENIA, NCBI, BC5CDR, JNLPBA)."
10619,"The dataset, repository, models,     or styles (e.g., to have some banana ﬂavor), or the
                                             and demo will be publicly available to facili-   user has already completed some steps but would
                                             tate further research on this new task.","Take
                                             cepts as prompt outperforms the baseline sig-    “Make Eggless Cupcakes” as an example, the user
                                             niﬁcantly, further conﬁrming the need for        might want to make it with different equipment
                                             task-speciﬁc knowledge in goal-oriented script   (e.g., by using a rice cooker instead of an oven)
                                             completion.","like to seek information on what to proceed next; a
                                                                                              machine learning model trained on goal-to-script
                                        1 Introduction                                        tasks as done in the existing work cannot adaptively
                                                                                              provide solutions under such situations.",2022-08-31 18:55:22+00:00,Incorporating Task-specific Concept Knowledge into Script Learning,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Chenkai Sun'), arxiv.Result.Author('Tie Xu'), arxiv.Result.Author('ChengXiang Zhai'), arxiv.Result.Author('Heng ji')]","In this paper, we present Tetris, a new task of Goal-Oriented Script
Completion. Unlike previous work, it considers a more realistic and more
general setting, where the input includes not only the goal but also additional
user context, including preferences and history. To address the problem using a
knowledge-based approach, we introduce Task Concept Graph, an automatically
constructed knowledge base from instructional websites. Different from
Commonsense Knowledge Base like ConceptNet, the task concept graph schema
introduces various types of noun phrases-based nodes specifically for
accomplishing a task. To integrate such graphs into script learning, we devise
two methods that acquire concepts from the knowledge base as prompts to
downstream script completion. On our WikiHow-based dataset, we find that
incorporating concepts from the Task Concept Graph consistently improves
performance, demonstrating the benefit of Task Concept Graph for this task.
Furthermore, the model with gold-standard concepts as prompt outperforms the
baseline significantly, further confirming the need for task-specific knowledge
in goal-oriented script completion. The dataset, repository, models, and demo
will be publicly available to facilitate further research on this new task."
10673,"Given the sparse availability of NLP solutions
for the Dutch clinical domain, we hope that our ﬁndings and provided implementations of the models
will facilitate further research and the development of data-driven applications in healthcare.","However, the three
assessed methods show a good performance on predicting negations in the Dutch Clinical Corpus, with
the machine learning methods producing the best results.","Acknowledgements

We’d like to express our thanks to Jan Kors from the Biosemantics group at ErasmusMC for providing
us with the Dutch Clinical Corpus.",2022-09-01 14:00:13+00:00,Negation detection in Dutch clinical texts: an evaluation of rule-based and machine learning methods,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG', 'stat.ML', '68T50, 68P20', 'I.2.7; J.3; H.3.3']","[arxiv.Result.Author('Bram van Es'), arxiv.Result.Author('Leon C. Reteig'), arxiv.Result.Author('Sander C. Tan'), arxiv.Result.Author('Marijn Schraagen'), arxiv.Result.Author('Myrthe M. Hemker'), arxiv.Result.Author('Sebastiaan R. S. Arends'), arxiv.Result.Author('Miguel A. R. Rios'), arxiv.Result.Author('Saskia Haitjema')]","As structured data are often insufficient, labels need to be extracted from
free text in electronic health records when developing models for clinical
information retrieval and decision support systems. One of the most important
contextual properties in clinical text is negation, which indicates the absence
of findings. We aimed to improve large scale extraction of labels by comparing
three methods for negation detection in Dutch clinical notes. We used the
Erasmus Medical Center Dutch Clinical Corpus to compare a rule-based method
based on ContextD, a biLSTM model using MedCAT and (finetuned) RoBERTa-based
models. We found that both the biLSTM and RoBERTa models consistently
outperform the rule-based model in terms of F1 score, precision and recall. In
addition, we systematically categorized the classification errors for each
model, which can be used to further improve model performance in particular
applications. Combining the three models naively was not beneficial in terms of
performance. We conclude that the biLSTM and RoBERTa-based models in particular
are highly accurate accurate in detecting clinical negations, but that
ultimately all three approaches can be viable depending on the use case at
hand."
10717,worth further studying in the future.,"The exact theoretical reasons behind are
averages the statistics of the previous ﬁve plots.","5 Discussion and Conclusion                                               Thoroughly evaluating a DA approach for NLP
                                                                       is not easy.",2022-09-02 03:03:51+00:00,"Random Text Perturbations Work, but not Always",cs.CL,['cs.CL'],[arxiv.Result.Author('Zhengxiang Wang')],"We present three large-scale experiments on binary text matching
classification task both in Chinese and English to evaluate the effectiveness
and generalizability of random text perturbations as a data augmentation
approach for NLP. It is found that the augmentation can bring both negative and
positive effects to the test set performance of three neural classification
models, depending on whether the models train on enough original training
examples. This remains true no matter whether five random text editing
operations, used to augment text, are applied together or separately. Our study
demonstrates with strong implication that the effectiveness of random text
perturbations is task specific and not generally positive."
10719,"further study on how the relative distance-derived
Therefore, a critical question naturally arises: Is     structural adapter overwhelms its alternatives.","We also conduct a
quire increased latency to achieve external parsing.","The
structural bias still a necessity for ASTE in the       results demonstrate that the structural adapter is
context of PLMs?",2022-09-02 05:02:18+00:00,Structural Bias for Aspect Sentiment Triplet Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chen Zhang'), arxiv.Result.Author('Lei Ren'), arxiv.Result.Author('Fang Ma'), arxiv.Result.Author('Jingang Wang'), arxiv.Result.Author('Wei Wu'), arxiv.Result.Author('Dawei Song')]","Structural bias has recently been exploited for aspect sentiment triplet
extraction (ASTE) and led to improved performance. On the other hand, it is
recognized that explicitly incorporating structural bias would have a negative
impact on efficiency, whereas pretrained language models (PLMs) can already
capture implicit structures. Thus, a natural question arises: Is structural
bias still a necessity in the context of PLMs? To answer the question, we
propose to address the efficiency issues by using an adapter to integrate
structural bias in the PLM and using a cheap-to-compute relative position
structure in place of the syntactic dependency structure. Benchmarking
evaluation is conducted on the SemEval datasets. The results show that our
proposed structural adapter is beneficial to PLMs and achieves state-of-the-art
performance over a range of strong baselines, yet with a light parameter demand
and low latency. Meanwhile, we give rise to the concern that the current
evaluation default with data of small scale is under-confident. Consequently,
we release a large-scale dataset for ASTE. The results on the new dataset hint
that the structural adapter is confidently effective and efficient to a large
scale. Overall, we draw the conclusion that structural bias shall still be a
necessity even with PLMs."
10722,"We hope our ﬁndings will encourage       References
further research on this setup, which we believe is
common for real-world applications and should get     Parmida Atighehchian, Frédéric Branchaud-Charron,
more attention from the research community.",performance.,and Alexandre Lacoste.,2022-09-02 05:55:09+00:00,Domain Adaptation from Scratch,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Eyal Ben-David'), arxiv.Result.Author('Yftah Ziser'), arxiv.Result.Author('Roi Reichart')]","Natural language processing (NLP) algorithms are rapidly improving but often
struggle when applied to out-of-distribution examples. A prominent approach to
mitigate the domain gap is domain adaptation, where a model trained on a source
domain is adapted to a new target domain. We present a new learning setup,
``domain adaptation from scratch'', which we believe to be crucial for
extending the reach of NLP to sensitive domains in a privacy-preserving manner.
In this setup, we aim to efficiently annotate data from a set of source domains
such that the trained model performs well on a sensitive target domain from
which data is unavailable for annotation. Our study compares several approaches
for this challenging setup, ranging from data selection and domain adaptation
algorithms to active learning paradigms, on two NLP tasks: sentiment analysis
and Named Entity Recognition. Our results suggest that using the abovementioned
approaches eases the domain gap, and combining them further improves the
results."
10723,GPT-NeoX-20B and            avenue for further study in this direction.,"The result with with GPT-3 curie is        more complex reasoning chains, and suggests an
slightly better than random.",Codex code-davinci-001 achieve similar results.,2022-09-02 06:50:11+00:00,FOLIO: Natural Language Reasoning with First-Order Logic,cs.CL,['cs.CL'],"[arxiv.Result.Author('Simeng Han'), arxiv.Result.Author('Hailey Schoelkopf'), arxiv.Result.Author('Yilun Zhao'), arxiv.Result.Author('Zhenting Qi'), arxiv.Result.Author('Martin Riddell'), arxiv.Result.Author('Luke Benson'), arxiv.Result.Author('Lucy Sun'), arxiv.Result.Author('Ekaterina Zubova'), arxiv.Result.Author('Yujie Qiao'), arxiv.Result.Author('Matthew Burtell'), arxiv.Result.Author('David Peng'), arxiv.Result.Author('Jonathan Fan'), arxiv.Result.Author('Yixin Liu'), arxiv.Result.Author('Brian Wong'), arxiv.Result.Author('Malcolm Sailor'), arxiv.Result.Author('Ansong Ni'), arxiv.Result.Author('Linyong Nan'), arxiv.Result.Author('Jungo Kasai'), arxiv.Result.Author('Tao Yu'), arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Shafiq Joty'), arxiv.Result.Author('Alexander R. Fabbri'), arxiv.Result.Author('Wojciech Kryscinski'), arxiv.Result.Author('Xi Victoria Lin'), arxiv.Result.Author('Caiming Xiong'), arxiv.Result.Author('Dragomir Radev')]","We present FOLIO, a human-annotated, open-domain, and logically complex and
diverse dataset for reasoning in natural language (NL), equipped with first
order logic (FOL) annotations. FOLIO consists of 1,435 examples (unique
conclusions), each paired with one of 487 sets of premises which serve as rules
to be used to deductively reason for the validity of each conclusion. The
logical correctness of premises and conclusions is ensured by their parallel
FOL annotations, which are automatically verified by our FOL inference engine.
In addition to the main NL reasoning task, NL-FOL pairs in FOLIO automatically
constitute a new NL-FOL translation dataset using FOL as the logical form. Our
experiments on FOLIO systematically evaluate the FOL reasoning ability of
supervised fine-tuning on medium-sized language models (BERT, RoBERTa) and
few-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). For
NL-FOL translation, we experiment with GPT-3 and Codex. Our results show that
one of the most capable Large Language Model (LLM) publicly available, GPT-3
davinci, achieves only slightly better than random results with few-shot
prompting on a subset of FOLIO, and the model is especially bad at predicting
the correct truth values for False and Unknown conclusions. Our dataset and
code are available at https://github.com/Yale-LILY/FOLIO."
10753,"Intuitively, there should be
8500                                                       3.9 Experiments with Random Data Samples
6800                                                             on Re-TACRED
5100
3400                                                       In order to further study the stability of our ap-
1700                                                       proach, we conduct extra experiments on Re-
                                                           TACRED which contains much more data than
     0                                                     SemEval.","Our method of modeling ambiguous data under
                                                        negative training helps the model compress the
                                                        scope of answers.","For each experiment, we randomly sam-
          1 3 5 7 9 11 13 15 17 19 >20                     ple a new split of training and validation sets for
                                                           the low-resource setting.",2022-09-03 14:16:12+00:00,STAD: Self-Training with Ambiguous Data for Low-Resource Relation Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Junjie Yu'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Jiangjiang Zhao'), arxiv.Result.Author('Chunjie Yang'), arxiv.Result.Author('Wenliang Chen')]","We present a simple yet effective self-training approach, named as STAD, for
low-resource relation extraction. The approach first classifies the
auto-annotated instances into two groups: confident instances and uncertain
instances, according to the probabilities predicted by a teacher model. In
contrast to most previous studies, which mainly only use the confident
instances for self-training, we make use of the uncertain instances. To this
end, we propose a method to identify ambiguous but useful instances from the
uncertain instances and then divide the relations into candidate-label set and
negative-label set for each ambiguous instance. Next, we propose a set-negative
training method on the negative-label sets for the ambiguous instances and a
positive training method for the confident instances. Finally, a joint-training
method is proposed to build the final relation extraction system on all data.
Experimental results on two widely used datasets SemEval2010 Task-8 and
Re-TACRED with low-resource settings demonstrate that this new self-training
approach indeed achieves significant and consistent improvements when comparing
to several competitive self-training systems. Code is publicly available at
https://github.com/jjyunlp/STAD"
10754,"Intuitively, there should be
8500                                                       3.9 Experiments with Random Data Samples
6800                                                             on Re-TACRED
5100
3400                                                       In order to further study the stability of our ap-
1700                                                       proach, we conduct extra experiments on Re-
                                                           TACRED which contains much more data than
     0                                                     SemEval.","Our method of modeling ambiguous data under
                                                        negative training helps the model compress the
                                                        scope of answers.","For each experiment, we randomly sam-
          1 3 5 7 9 11 13 15 17 19 >20                     ple a new split of training and validation sets for
                                                           the low-resource setting.",2022-09-03 14:16:12+00:00,STAD: Self-Training with Ambiguous Data for Low-Resource Relation Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Junjie Yu'), arxiv.Result.Author('Xing Wang'), arxiv.Result.Author('Jiangjiang Zhao'), arxiv.Result.Author('Chunjie Yang'), arxiv.Result.Author('Wenliang Chen')]","We present a simple yet effective self-training approach, named as STAD, for
low-resource relation extraction. The approach first classifies the
auto-annotated instances into two groups: confident instances and uncertain
instances, according to the probabilities predicted by a teacher model. In
contrast to most previous studies, which mainly only use the confident
instances for self-training, we make use of the uncertain instances. To this
end, we propose a method to identify ambiguous but useful instances from the
uncertain instances and then divide the relations into candidate-label set and
negative-label set for each ambiguous instance. Next, we propose a set-negative
training method on the negative-label sets for the ambiguous instances and a
positive training method for the confident instances. Finally, a joint-training
method is proposed to build the final relation extraction system on all data.
Experimental results on two widely used datasets SemEval2010 Task-8 and
Re-TACRED with low-resource settings demonstrate that this new self-training
approach indeed achieves significant and consistent improvements when comparing
to several competitive self-training systems. Code is publicly available at
https://github.com/jjyunlp/STAD"
10807,"The current results show that
due to the memory limits, we leave more investigation for         document-level few-shot event argument extraction is chal-
further research.","However,           argument feature extractor.","Besides, how to efﬁciently adapt NNShot         lenging due to the long document and limited examples, as
to long documents is also an open challenge.",2022-09-06 03:57:23+00:00,Few-Shot Document-Level Event Argument Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xianjun Yang'), arxiv.Result.Author('Yujie Lu'), arxiv.Result.Author('Linda Petzold')]","Event argument extraction (EAE) has been well studied at the sentence level
but under-explored at the document level. In this paper, we study to capture
event arguments that actually spread across sentences in documents. Prior works
mainly assume full access to rich document supervision, ignoring the fact that
the argument supervision is limited in documents. To fill this gap, we present
FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based
on the largest document-level event extraction dataset DocEE. We first define
the new problem and reconstruct the corpus by a novel N-Way-D-Doc sampling
instead of the traditional N-Way-K-Shot strategy. Then we adjust the advanced
document-level neural models into the few-shot setting to provide baseline
results under in- and cross-domain settings. Since the argument extraction
depends on the context from multiple sentences and the learning process is
limited to very few examples, we find the task to be very challenging with
substantively low performance. Considering FewDocAE is closely related to
practical use under low-resource regimes, we hope this benchmark encourages
more research in this direction. Our data and codes will be available online."
10938,"This
implies that there exists a linearly aligned common         We hope that these initial steps towards inter-
core between the three languages (vector spaces)         lingual visual grounding inspire further research.",form the textual embeddings signiﬁcantly.,"which as observed in section 6.3), yielded the low-      Low-resourced languages might beneﬁt from joint
est variance and more pure vector space.",2022-09-08 11:18:39+00:00,Visual Grounding of Inter-lingual Word-Embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wafaa Mohammed'), arxiv.Result.Author('Hassan Shahmohammadi'), arxiv.Result.Author('Hendrik P. A. Lensch'), arxiv.Result.Author('R. Harald Baayen')]","Visual grounding of Language aims at enriching textual representations of
language with multiple sources of visual knowledge such as images and videos.
Although visual grounding is an area of intense research, inter-lingual aspects
of visual grounding have not received much attention. The present study
investigates the inter-lingual visual grounding of word embeddings. We propose
an implicit alignment technique between the two spaces of vision and language
in which inter-lingual textual information interacts in order to enrich
pre-trained textual word embeddings. We focus on three languages in our
experiments, namely, English, Arabic, and German. We obtained visually grounded
vector representations for these languages and studied whether visual grounding
on one or multiple languages improved the performance of embeddings on word
similarity and categorization benchmarks. Our experiments suggest that
inter-lingual knowledge improves the performance of grounded embeddings in
similar languages such as German and English. However, inter-lingual grounding
of German or English with Arabic led to a slight degradation in performance on
word similarity benchmarks. On the other hand, we observed an opposite trend on
categorization benchmarks where Arabic had the most improvement on English. In
the discussion section, several reasons for those findings are laid out. We
hope that our experiments provide a baseline for further research on
inter-lingual visual grounding."
10939,lingual visual grounding inspire further research.,IEEE.,Low-resourced languages might beneﬁt from joint        Abdulrahman Almuhareb and Massimo Poesio.,2022-09-08 11:18:39+00:00,Visual Grounding of Inter-lingual Word-Embeddings,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wafaa Mohammed'), arxiv.Result.Author('Hassan Shahmohammadi'), arxiv.Result.Author('Hendrik P. A. Lensch'), arxiv.Result.Author('R. Harald Baayen')]","Visual grounding of Language aims at enriching textual representations of
language with multiple sources of visual knowledge such as images and videos.
Although visual grounding is an area of intense research, inter-lingual aspects
of visual grounding have not received much attention. The present study
investigates the inter-lingual visual grounding of word embeddings. We propose
an implicit alignment technique between the two spaces of vision and language
in which inter-lingual textual information interacts in order to enrich
pre-trained textual word embeddings. We focus on three languages in our
experiments, namely, English, Arabic, and German. We obtained visually grounded
vector representations for these languages and studied whether visual grounding
on one or multiple languages improved the performance of embeddings on word
similarity and categorization benchmarks. Our experiments suggest that
inter-lingual knowledge improves the performance of grounded embeddings in
similar languages such as German and English. However, inter-lingual grounding
of German or English with Arabic led to a slight degradation in performance on
word similarity benchmarks. On the other hand, we observed an opposite trend on
categorization benchmarks where Arabic had the most improvement on English. In
the discussion section, several reasons for those findings are laid out. We
hope that our experiments provide a baseline for further research on
inter-lingual visual grounding."
10953,"BERT-wwm) uses whole word masking strategy for span masking prediction, we leave
these conﬁgurations for further study.","Some
baselines (e.g.","6 Analysis of Computational Efﬁciency

Inference Speed.",2022-09-08 14:12:15+00:00,Pre-Training a Graph Recurrent Network for Language Representation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yile Wang'), arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Zhiyang Teng'), arxiv.Result.Author('Ming Zhou'), arxiv.Result.Author('Yue Zhang')]","Transformer-based pre-trained models have gained much advance in recent
years, becoming one of the most important backbones in natural language
processing. Recent work shows that the attention mechanism inside Transformer
may not be necessary, both convolutional neural networks and multi-layer
perceptron based models have also been investigated as Transformer
alternatives. In this paper, we consider a graph recurrent network for language
model pre-training, which builds a graph structure for each sequence with local
token-level communications, together with a sentence-level representation
decoupled from other tokens. The original model performs well in
domain-specific text classification under supervised training, however, its
potential in learning transfer knowledge by self-supervised way has not been
fully exploited. We fill this gap by optimizing the architecture and verifying
its effectiveness in more general language understanding tasks, for both
English and Chinese languages. As for model efficiency, instead of the
quadratic complexity in Transformer-based models, our model has linear
complexity and performs more efficiently during inference. Moreover, we find
that our model can generate more diverse outputs with less contextualized
feature redundancy than existing attention-based models."
10954,"To further study the advantages of linear complexity in our model, We
compared our model with Transformer variants, including Longformer [50], Reformer [49], Lin-
former [51], and Performer [52], which aims to optimize the self-attention mechanism for better
dealing with long sequences.",Extra Long Sequences.,"As standard autoencoder and seq2seq PTMs, we also include De-
BERTa [8] and T5 [7] for reference.",2022-09-08 14:12:15+00:00,Pre-Training a Graph Recurrent Network for Language Representation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yile Wang'), arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Zhiyang Teng'), arxiv.Result.Author('Ming Zhou'), arxiv.Result.Author('Yue Zhang')]","Transformer-based pre-trained models have gained much advance in recent
years, becoming one of the most important backbones in natural language
processing. Recent work shows that the attention mechanism inside Transformer
may not be necessary, both convolutional neural networks and multi-layer
perceptron based models have also been investigated as Transformer
alternatives. In this paper, we consider a graph recurrent network for language
model pre-training, which builds a graph structure for each sequence with local
token-level communications, together with a sentence-level representation
decoupled from other tokens. The original model performs well in
domain-specific text classification under supervised training, however, its
potential in learning transfer knowledge by self-supervised way has not been
fully exploited. We fill this gap by optimizing the architecture and verifying
its effectiveness in more general language understanding tasks, for both
English and Chinese languages. As for model efficiency, instead of the
quadratic complexity in Transformer-based models, our model has linear
complexity and performs more efficiently during inference. Moreover, we find
that our model can generate more diverse outputs with less contextualized
feature redundancy than existing attention-based models."
10955,"BERT-wwm) uses whole word masking strategy for span masking prediction, we leave
these conﬁgurations for further study.","Some
baselines (e.g.","6 Analysis of Computational Efﬁciency

Inference Speed.",2022-09-08 14:12:15+00:00,Pre-Training a Graph Recurrent Network for Language Representation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yile Wang'), arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Zhiyang Teng'), arxiv.Result.Author('Ming Zhou'), arxiv.Result.Author('Yue Zhang')]","Transformer-based pre-trained models have gained much advance in recent
years, becoming one of the most important backbones in natural language
processing. Recent work shows that the attention mechanism inside Transformer
may not be necessary, both convolutional neural networks and multi-layer
perceptron based models have also been investigated as Transformer
alternatives. In this paper, we consider a graph recurrent network for language
model pre-training, which builds a graph structure for each sequence with local
token-level communications, together with a sentence-level representation
decoupled from other tokens. The original model performs well in
domain-specific text classification under supervised training, however, its
potential in learning transfer knowledge by self-supervised way has not been
fully exploited. We fill this gap by optimizing the architecture and verifying
its effectiveness in more general language understanding tasks, for both
English and Chinese languages. As for model efficiency, instead of the
quadratic complexity in Transformer-based models, our model has linear
complexity and performs more efficiently during inference. Moreover, we find
that our model can generate more diverse outputs with less contextualized
feature redundancy than existing attention-based models."
10956,"To further study the advantages of linear complexity in our model, We
compared our model with Transformer variants, including Longformer [50], Reformer [49], Lin-
former [51], and Performer [52], which aims to optimize the self-attention mechanism for better
dealing with long sequences.",Extra Long Sequences.,"As standard autoencoder and seq2seq PTMs, we also include De-
BERTa [8] and T5 [7] for reference.",2022-09-08 14:12:15+00:00,Pre-Training a Graph Recurrent Network for Language Representation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yile Wang'), arxiv.Result.Author('Linyi Yang'), arxiv.Result.Author('Zhiyang Teng'), arxiv.Result.Author('Ming Zhou'), arxiv.Result.Author('Yue Zhang')]","Transformer-based pre-trained models have gained much advance in recent
years, becoming one of the most important backbones in natural language
processing. Recent work shows that the attention mechanism inside Transformer
may not be necessary, both convolutional neural networks and multi-layer
perceptron based models have also been investigated as Transformer
alternatives. In this paper, we consider a graph recurrent network for language
model pre-training, which builds a graph structure for each sequence with local
token-level communications, together with a sentence-level representation
decoupled from other tokens. The original model performs well in
domain-specific text classification under supervised training, however, its
potential in learning transfer knowledge by self-supervised way has not been
fully exploited. We fill this gap by optimizing the architecture and verifying
its effectiveness in more general language understanding tasks, for both
English and Chinese languages. As for model efficiency, instead of the
quadratic complexity in Transformer-based models, our model has linear
complexity and performs more efficiently during inference. Moreover, we find
that our model can generate more diverse outputs with less contextualized
feature redundancy than existing attention-based models."
11012,"In this paper,
employing the quantitative analysis, we attempted to explore Chinese news texts collected from five Malay Archipelago nations,
namely Indonesia, Malaysia, Singapore, Philippines and Brunei to enrich the quantitative research on Chinese differences,
contributing to a better understanding of Chinese in overseas Chinese newspapers and opening up new ideas for further research.","Nevertheless, few scholars have systematically
figured out the differences with the texts written in modern standard Chinese, from the perspective of metrology.","Related work

In the existing studies, some scholars have studied the differences in Chinese in different regions based on news texts,
focusing mainly on the differences of Chinese used in Taiwan and Macao and Mandarin in the Mainland.",2022-09-10 07:29:25+00:00,An Analysis of the Differences Among Regional Varieties of Chinese in Malay Archipelago,cs.CL,['cs.CL'],"[arxiv.Result.Author('Nankai Lin'), arxiv.Result.Author('Sihui Fu'), arxiv.Result.Author('Hongyan Wu'), arxiv.Result.Author('Shengyi Jiang')]","Chinese features prominently in the Chinese communities located in the
nations of Malay Archipelago. In these countries, Chinese has undergone the
process of adjustment to the local languages and cultures, which leads to the
occurrence of a Chinese variant in each country. In this paper, we conducted a
quantitative analysis on Chinese news texts collected from five Malay
Archipelago nations, namely Indonesia, Malaysia, Singapore, Philippines and
Brunei, trying to figure out their differences with the texts written in modern
standard Chinese from a lexical and syntactic perspective. The statistical
results show that the Chinese variants used in these five nations are quite
different, diverging from their modern Chinese mainland counterpart. Meanwhile,
we managed to extract and classify several featured Chinese words used in each
nation. All these discrepancies reflect how Chinese evolves overseas, and
demonstrate the profound impact rom local societies and cultures on the
development of Chinese."
11013,"Though not relevant to this
work, nor a part of the ﬁnal dataset, we collect and
annotate this data as well for further research.","nology, f) Food, g) Religion, h) Sports, i) Health,
j) Education, k) Business, l) Environment, and m)
Other (miscellaneous).","We observe an inter-annotator agreement of 92%
within the annotations provided by both.",2022-09-10 07:32:36+00:00,Harnessing Abstractive Summarization for Fact-Checked Claim Detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Varad Bhatnagar'), arxiv.Result.Author('Diptesh Kanojia'), arxiv.Result.Author('Kameswari Chebrolu')]","Social media platforms have become new battlegrounds for anti-social
elements, with misinformation being the weapon of choice. Fact-checking
organizations try to debunk as many claims as possible while staying true to
their journalistic processes but cannot cope with its rapid dissemination. We
believe that the solution lies in partial automation of the fact-checking life
cycle, saving human time for tasks which require high cognition. We propose a
new workflow for efficiently detecting previously fact-checked claims that uses
abstractive summarization to generate crisp queries. These queries can then be
executed on a general-purpose retrieval system associated with a collection of
previously fact-checked claims. We curate an abstractive text summarization
dataset comprising noisy claims from Twitter and their gold summaries. It is
shown that retrieval performance improves 2x by using popular out-of-the-box
summarization models and 3x by fine-tuning them on the accompanying dataset
compared to verbatim querying. Our approach achieves Recall@5 and MRR of 35%
and 0.3, compared to baseline values of 10% and 0.1, respectively. Our dataset,
code, and models are available publicly:
https://github.com/varadhbhatnagar/FC-Claim-Det/"
11014,"Though not relevant to this
work, nor a part of the ﬁnal dataset, we collect and
annotate this data as well for further research.","nology, f) Food, g) Religion, h) Sports, i) Health,
j) Education, k) Business, l) Environment, and m)
Other (miscellaneous).","We observe an inter-annotator agreement of 92%
within the annotations provided by both.",2022-09-10 07:32:36+00:00,Harnessing Abstractive Summarization for Fact-Checked Claim Detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Varad Bhatnagar'), arxiv.Result.Author('Diptesh Kanojia'), arxiv.Result.Author('Kameswari Chebrolu')]","Social media platforms have become new battlegrounds for anti-social
elements, with misinformation being the weapon of choice. Fact-checking
organizations try to debunk as many claims as possible while staying true to
their journalistic processes but cannot cope with its rapid dissemination. We
believe that the solution lies in partial automation of the fact-checking life
cycle, saving human time for tasks which require high cognition. We propose a
new workflow for efficiently detecting previously fact-checked claims that uses
abstractive summarization to generate crisp queries. These queries can then be
executed on a general-purpose retrieval system associated with a collection of
previously fact-checked claims. We curate an abstractive text summarization
dataset comprising noisy claims from Twitter and their gold summaries. It is
shown that retrieval performance improves 2x by using popular out-of-the-box
summarization models and 3x by fine-tuning them on the accompanying dataset
compared to verbatim querying. Our approach achieves Recall@5 and MRR of 35%
and 0.3, compared to baseline values of 10% and 0.1, respectively. Our dataset,
code, and models are available publicly:
https://github.com/varadhbhatnagar/FC-Claim-Det/"
11021,We release the code used to facilitate further research.,"When learning multiple
                                                     hyper-parameters concurrently, we show that the global learning rate can follow a schedule
                                                     over training that improves performance and is not explainable by the ‘short-horizon bias’
                                                     of greedy methods (Wu et al., 2018).","1 Introduction

                                           Finding good hyper-parameter values is critical to achieving good performance across machine
                                           learning domains; this has inspired much work into hyper-parameter optimization (HPO) (see
                                           Feurer & Hutter (2019)).",2022-09-10 14:52:41+00:00,Simple and Effective Gradient-Based Tuning of Sequence-to-Sequence Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jared Lichtarge'), arxiv.Result.Author('Chris Alberti'), arxiv.Result.Author('Shankar Kumar')]","Recent trends towards training ever-larger language models have substantially
improved machine learning performance across linguistic tasks. However, the
huge cost of training larger models can make tuning them prohibitively
expensive, motivating the study of more efficient methods. Gradient-based
hyper-parameter optimization offers the capacity to tune hyper-parameters
during training, yet has not previously been studied in a sequence-to-sequence
setting. We apply a simple and general gradient-based hyperparameter
optimization method to sequence-to-sequence tasks for the first time,
demonstrating both efficiency and performance gains over strong baselines for
both Neural Machine Translation and Natural Language Understanding (NLU) tasks
(via T5 pretraining). For translation, we show the method generalizes across
language pairs, is more efficient than Bayesian hyper-parameter optimization,
and that learned schedules for some hyper-parameters can out-perform even
optimal constant-valued tuning. For T5, we show that learning hyper-parameters
during pretraining can improve performance across downstream NLU tasks. When
learning multiple hyper-parameters concurrently, we show that the global
learning rate can follow a schedule over training that improves performance and
is not explainable by the `short-horizon bias' of greedy methods
\citep{wu2018}. We release the code used to facilitate further research."
11123,"We release the benchmark dataset for
                                                        research purposes to stimulate further research on the task.","We introduce a manually annotated evaluation benchmark
                                                        for skill extraction based on the ESCO taxonomy, on which we validate our models.","Keywords

                                                        Skill Extraction, Information Extraction, Distant Supervision, Extreme Multi-Label Classification

                                        1.",2022-09-13 13:37:06+00:00,Design of Negative Sampling Strategies for Distantly Supervised Skill Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jens-Joris Decorte'), arxiv.Result.Author('Jeroen Van Hautte'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Chris Develder'), arxiv.Result.Author('Thomas Demeester')]","Skills play a central role in the job market and many human resources (HR)
processes. In the wake of other digital experiences, today's online job market
has candidates expecting to see the right opportunities based on their skill
set. Similarly, enterprises increasingly need to use data to guarantee that the
skills within their workforce remain future-proof. However, structured
information about skills is often missing, and processes building on self- or
manager-assessment have shown to struggle with issues around adoption,
completeness, and freshness of the resulting data. Extracting skills is a
highly challenging task, given the many thousands of possible skill labels
mentioned either explicitly or merely described implicitly and the lack of
finely annotated training corpora. Previous work on skill extraction overly
simplifies the task to an explicit entity detection task or builds on manually
annotated training data that would be infeasible if applied to a complete
vocabulary of skills. We propose an end-to-end system for skill extraction,
based on distant supervision through literal matching. We propose and evaluate
several negative sampling strategies, tuned on a small validation dataset, to
improve the generalization of skill extraction towards implicitly mentioned
skills, despite the lack of such implicit skills in the distantly supervised
data. We observe that using the ESCO taxonomy to select negative examples from
related skills yields the biggest improvements, and combining three different
strategies in one model further increases the performance, up to 8 percentage
points in RP@5. We introduce a manually annotated evaluation benchmark for
skill extraction based on the ESCO taxonomy, on which we validate our models.
We release the benchmark dataset for research purposes to stimulate further
research on the task."
11124,"Kan, Re-
and validation dataset for skill extraction to stimulate            trieving skills from job descriptions: A lan-
further research on the task.","Finally, we release our hand-labeled test   [5] A. Bhola, K. Halder, A. Prasad, M.-Y.","guage model based extreme multi-label clas-
                                                                    sification framework, in: Proceedings of
   Future work could entail a more extensive investiga-             the 28th International Conference on Com-
tion of other hyper-parameters, such as the number of               putational Linguistics, International Commit-
negatives per positive sentence (𝑘), which was fixed to             tee on Computational Linguistics, Barcelona,
10 in this work.",2022-09-13 13:37:06+00:00,Design of Negative Sampling Strategies for Distantly Supervised Skill Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jens-Joris Decorte'), arxiv.Result.Author('Jeroen Van Hautte'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Chris Develder'), arxiv.Result.Author('Thomas Demeester')]","Skills play a central role in the job market and many human resources (HR)
processes. In the wake of other digital experiences, today's online job market
has candidates expecting to see the right opportunities based on their skill
set. Similarly, enterprises increasingly need to use data to guarantee that the
skills within their workforce remain future-proof. However, structured
information about skills is often missing, and processes building on self- or
manager-assessment have shown to struggle with issues around adoption,
completeness, and freshness of the resulting data. Extracting skills is a
highly challenging task, given the many thousands of possible skill labels
mentioned either explicitly or merely described implicitly and the lack of
finely annotated training corpora. Previous work on skill extraction overly
simplifies the task to an explicit entity detection task or builds on manually
annotated training data that would be infeasible if applied to a complete
vocabulary of skills. We propose an end-to-end system for skill extraction,
based on distant supervision through literal matching. We propose and evaluate
several negative sampling strategies, tuned on a small validation dataset, to
improve the generalization of skill extraction towards implicitly mentioned
skills, despite the lack of such implicit skills in the distantly supervised
data. We observe that using the ESCO taxonomy to select negative examples from
related skills yields the biggest improvements, and combining three different
strategies in one model further increases the performance, up to 8 percentage
points in RP@5. We introduce a manually annotated evaluation benchmark for
skill extraction based on the ESCO taxonomy, on which we validate our models.
We release the benchmark dataset for research purposes to stimulate further
research on the task."
11136,"Where very different results are reported on different
corpora (such as the WebNLG and WSJ corpus in Same, Chen, and Van Deemter (2022)),
further research into the causes of the divergence are in order.","When judging the direct support for a model, younger models
tend to be harder to judge, because a younger model cannot be expected to have
been subjected to as much scrutiny as an older one, limiting its opportunities for both
negative and positive support.","A further wrinkle in assessing direct support is that models are moving targets:
when a model is examined for the second or third time, it is often a modiﬁed version
of that initial model.",2022-09-13 17:19:04+00:00,The Role of Explanatory Value in Natural Language Processing,cs.CL,['cs.CL'],[arxiv.Result.Author('Kees van Deemter')],"A key aim of science is explanation, yet the idea of explaining language
phenomena has taken a backseat in mainstream Natural Language Processing (NLP)
and many other areas of Artificial Intelligence. I argue that explanation of
linguistic behaviour should be a main goal of NLP, and that this is not the
same as making NLP models explainable. To illustrate these ideas, some recent
models of human language production are compared with each other. I conclude by
asking what it would mean for NLP research and institutional policies if our
community took explanatory value seriously, while heeding some possible
pitfalls."
11164,"0.3 - 0.5       176    17.55%
       0.5 - 0.7       118    11.76%                       To further study the relevance between image
       0.7 - 0.9        66     6.58%                    content type and persuasion mode, we report their
         ≥ 0.9          75     7.48%                    correlated distributions in charts.","10 (c), images in supporting gun con-
       0.0 - 0.1       336    33.50%                    trol stance uses more Logos and Pathos but less
       0.1 - 0.3       232    23.13%                    Ethos than those in the opposing stance.",Fig.,2022-09-14 05:03:10+00:00,ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining,cs.CL,"['cs.CL', 'cs.AI', 'cs.MM']","[arxiv.Result.Author('Zhexiong Liu'), arxiv.Result.Author('Meiqi Guo'), arxiv.Result.Author('Yue Dai'), arxiv.Result.Author('Diane Litman')]","The growing interest in developing corpora of persuasive texts has promoted
applications in automated systems, e.g., debating and essay scoring systems;
however, there is little prior work mining image persuasiveness from an
argumentative perspective. To expand persuasiveness mining into a multi-modal
realm, we present a multi-modal dataset, ImageArg, consisting of annotations of
image persuasiveness in tweets. The annotations are based on a persuasion
taxonomy we developed to explore image functionalities and the means of
persuasion. We benchmark image persuasiveness tasks on ImageArg using
widely-used multi-modal learning methods. The experimental results show that
our dataset offers a useful resource for this rich and challenging topic, and
there is ample room for modeling improvement."
11165,"between different annotation dimensions and raise
                                                        demands for further study.","These correlations imply mutual inﬂuences
Ethos.","Additionally, we show how the stance impacts
image persuasiveness, content type, and persuasion      5 Experiments
mode.",2022-09-14 05:03:10+00:00,ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining,cs.CL,"['cs.CL', 'cs.AI', 'cs.MM']","[arxiv.Result.Author('Zhexiong Liu'), arxiv.Result.Author('Meiqi Guo'), arxiv.Result.Author('Yue Dai'), arxiv.Result.Author('Diane Litman')]","The growing interest in developing corpora of persuasive texts has promoted
applications in automated systems, e.g., debating and essay scoring systems;
however, there is little prior work mining image persuasiveness from an
argumentative perspective. To expand persuasiveness mining into a multi-modal
realm, we present a multi-modal dataset, ImageArg, consisting of annotations of
image persuasiveness in tweets. The annotations are based on a persuasion
taxonomy we developed to explore image functionalities and the means of
persuasion. We benchmark image persuasiveness tasks on ImageArg using
widely-used multi-modal learning methods. The experimental results show that
our dataset offers a useful resource for this rich and challenging topic, and
there is ample room for modeling improvement."
11182,"For our proposed Bayesian approxima-
further study.","We also         set to calculate the detection method’s threshold
compare our method on two feature extractors for         adaptively.","tion, we set the dropout probability to 0.7, and the
                                                         dropout sampling times to 100.",2022-09-14 13:04:09+00:00,Distribution Calibration for Out-of-Domain Detection with Bayesian Approximation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yanan Wu'), arxiv.Result.Author('Zhiyuan Zeng'), arxiv.Result.Author('Keqing He'), arxiv.Result.Author('Yutao Mou'), arxiv.Result.Author('Pei Wang'), arxiv.Result.Author('Weiran Xu')]","Out-of-Domain (OOD) detection is a key component in a task-oriented dialog
system, which aims to identify whether a query falls outside the predefined
supported intent set. Previous softmax-based detection algorithms are proved to
be overconfident for OOD samples. In this paper, we analyze overconfident OOD
comes from distribution uncertainty due to the mismatch between the training
and test distributions, which makes the model can't confidently make
predictions thus probably causing abnormal softmax scores. We propose a
Bayesian OOD detection framework to calibrate distribution uncertainty using
Monte-Carlo Dropout. Our method is flexible and easily pluggable into existing
softmax-based baselines and gains 33.33\% OOD F1 improvements with increasing
only 0.41\% inference time compared to MSP. Further analyses show the
effectiveness of Bayesian learning for OOD detection."
11190,"Apart from the dataset, we also propose var-
                                              benchmark results for further research through      ious baseline models to evaluate our dataset’s qual-
                                              commonly used metrics such as BLEU, ME-             ity.","Along with the dataset, we set up         construct a dataset to help build a question answer-
                                              various deep learning models as baseline to         ing system based on CQA about COVID-19 in Viet-
                                              assess the quality of our dataset and initiate the  namese.","This paper has three main contributions summa-
                                              TEOR, and ROUGE-L. We also illustrate               rized as follows:
                                              the positive effects of having multiple para-
                                              phrased answers experimented on these mod-            1.",2022-09-14 14:24:23+00:00,UIT-ViCoV19QA: A Dataset for COVID-19 Community-based Question Answering on Vietnamese Language,cs.CL,['cs.CL'],"[arxiv.Result.Author('Triet Minh Thai'), arxiv.Result.Author('Ngan Ha-Thao Chu'), arxiv.Result.Author('Anh Tuan Vo'), arxiv.Result.Author('Son T. Luu')]","For the last two years, from 2020 to 2021, COVID-19 has broken disease
prevention measures in many countries, including Vietnam, and negatively
impacted various aspects of human life and the social community. Besides, the
misleading information in the community and fake news about the pandemic are
also serious situations. Therefore, we present the first Vietnamese
community-based question answering dataset for developing question answering
systems for COVID-19 called UIT-ViCoV19QA. The dataset comprises 4,500
question-answer pairs collected from trusted medical sources, with at least one
answer and at most four unique paraphrased answers per question. Along with the
dataset, we set up various deep learning models as baseline to assess the
quality of our dataset and initiate the benchmark results for further research
through commonly used metrics such as BLEU, METEOR, and ROUGE-L. We also
illustrate the positive effects of having multiple paraphrased answers
experimented on these models, especially on Transformer - a dominant
architecture in the field of study."
11230,"To perform a com-                              ing (Med-VQA), medical image-text classification (Med-ITC), and
                                        prehensive evaluation and facilitate further research, we construct                          medical image-text retrieval (Med-ITR)), which can be crucial for
                                        a medical vision-and-language benchmark including three tasks.","Third, we guide the model to                             a vision-and-language model to address a wide range of medical
                                        put emphasis on the most critical information in images and texts                            vision-and-language tasks (e.g., medical visual question answer-
                                        by designing knowledge-induced pretext tasks.",alleviating the data scarcity problem in the medical field.,2022-09-15 08:00:01+00:00,"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge",cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Author('Zhihong Chen'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Xiang Wan')]","Medical vision-and-language pre-training (Med-VLP) has received considerable
attention owing to its applicability to extracting generic vision-and-language
representations from medical images and texts. Most existing methods mainly
contain three elements: uni-modal encoders (i.e., a vision encoder and a
language encoder), a multi-modal fusion module, and pretext tasks, with few
studies considering the importance of medical domain expert knowledge and
explicitly exploiting such knowledge to facilitate Med-VLP. Although there
exist knowledge-enhanced vision-and-language pre-training (VLP) methods in the
general domain, most require off-the-shelf toolkits (e.g., object detectors and
scene graph parsers), which are unavailable in the medical domain. In this
paper, we propose a systematic and effective approach to enhance Med-VLP by
structured medical knowledge from three perspectives. First, considering
knowledge can be regarded as the intermediate medium between vision and
language, we align the representations of the vision encoder and the language
encoder through knowledge. Second, we inject knowledge into the multi-modal
fusion model to enable the model to perform reasoning using knowledge as the
supplementation of the input image and text. Third, we guide the model to put
emphasis on the most critical information in images and texts by designing
knowledge-induced pretext tasks. To perform a comprehensive evaluation and
facilitate further research, we construct a medical vision-and-language
benchmark including three tasks. Experimental results illustrate the
effectiveness of our approach, where state-of-the-art performance is achieved
on all downstream tasks. Further analyses explore the effects of different
components of our approach and various settings of pre-training."
11231,"To verify the effectiveness of our approach         enhanced methods and propose to inject knowledge from three
and facilitate further research, we construct a medical vision-and-     VLP-specific perspectives without requiring object detectors or
language understanding benchmark including three tasks (i.e., Med-      scene graph parsers, which are unavailable in the medical domain.","Different from them, we revisit existing knowledge-
and MIMIC-CXR [21].","VQA, Med-ITC, and Med-ITR).",2022-09-15 08:00:01+00:00,"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge",cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Author('Zhihong Chen'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Xiang Wan')]","Medical vision-and-language pre-training (Med-VLP) has received considerable
attention owing to its applicability to extracting generic vision-and-language
representations from medical images and texts. Most existing methods mainly
contain three elements: uni-modal encoders (i.e., a vision encoder and a
language encoder), a multi-modal fusion module, and pretext tasks, with few
studies considering the importance of medical domain expert knowledge and
explicitly exploiting such knowledge to facilitate Med-VLP. Although there
exist knowledge-enhanced vision-and-language pre-training (VLP) methods in the
general domain, most require off-the-shelf toolkits (e.g., object detectors and
scene graph parsers), which are unavailable in the medical domain. In this
paper, we propose a systematic and effective approach to enhance Med-VLP by
structured medical knowledge from three perspectives. First, considering
knowledge can be regarded as the intermediate medium between vision and
language, we align the representations of the vision encoder and the language
encoder through knowledge. Second, we inject knowledge into the multi-modal
fusion model to enable the model to perform reasoning using knowledge as the
supplementation of the input image and text. Third, we guide the model to put
emphasis on the most critical information in images and texts by designing
knowledge-induced pretext tasks. To perform a comprehensive evaluation and
facilitate further research, we construct a medical vision-and-language
benchmark including three tasks. Experimental results illustrate the
effectiveness of our approach, where state-of-the-art performance is achieved
on all downstream tasks. Further analyses explore the effects of different
components of our approach and various settings of pre-training."
11232,"To perform a comprehensive
sub-figure 3(O)) since the knowledge-induced pretext task guides          evaluation and facilitate further research, we construct a medical
the model to put more emphasis on the medical knowledge-related           vision-and-language understanding benchmark, including three
information.","For “learning from knowledge”, the masked medical entities         sophisticated pretext tasks to guide the model put more emphasis
are correctly recovered by the pre-trained model (as shown in the         on the critical medical information.","In addition, the masked and recovered images are also        tasks (i.e., Med-VQA, Med-ITC, and Med-ITR).",2022-09-15 08:00:01+00:00,"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge",cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Author('Zhihong Chen'), arxiv.Result.Author('Guanbin Li'), arxiv.Result.Author('Xiang Wan')]","Medical vision-and-language pre-training (Med-VLP) has received considerable
attention owing to its applicability to extracting generic vision-and-language
representations from medical images and texts. Most existing methods mainly
contain three elements: uni-modal encoders (i.e., a vision encoder and a
language encoder), a multi-modal fusion module, and pretext tasks, with few
studies considering the importance of medical domain expert knowledge and
explicitly exploiting such knowledge to facilitate Med-VLP. Although there
exist knowledge-enhanced vision-and-language pre-training (VLP) methods in the
general domain, most require off-the-shelf toolkits (e.g., object detectors and
scene graph parsers), which are unavailable in the medical domain. In this
paper, we propose a systematic and effective approach to enhance Med-VLP by
structured medical knowledge from three perspectives. First, considering
knowledge can be regarded as the intermediate medium between vision and
language, we align the representations of the vision encoder and the language
encoder through knowledge. Second, we inject knowledge into the multi-modal
fusion model to enable the model to perform reasoning using knowledge as the
supplementation of the input image and text. Third, we guide the model to put
emphasis on the most critical information in images and texts by designing
knowledge-induced pretext tasks. To perform a comprehensive evaluation and
facilitate further research, we construct a medical vision-and-language
benchmark including three tasks. Experimental results illustrate the
effectiveness of our approach, where state-of-the-art performance is achieved
on all downstream tasks. Further analyses explore the effects of different
components of our approach and various settings of pre-training."
11258,"Our proposed methods represent a
step toward a more robust evaluation paradigm, and we hope that our results, code, and data (To be
released soon), will inspire further research into this exciting area of conditional language generation.","It remains exciting and
necessary for future work to explore how a wider range of existing generation techniques and models
perform under this new paradigm, and, as data availability expands, to understand the implications of
distribution-aware evaluation in ﬁelds beyond visual description.","10
References

 [1] A. Agarwal and A. Lavie.",2022-09-15 17:58:13+00:00,Distribution Aware Metrics for Conditional Natural Language Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('David M Chan'), arxiv.Result.Author('Yiming Ni'), arxiv.Result.Author('Austin Myers'), arxiv.Result.Author('Sudheendra Vijayanarasimhan'), arxiv.Result.Author('David A Ross'), arxiv.Result.Author('John Canny')]","Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity."
11259,"We hope that releasing our code, along with the JSON
ﬁles containing test-set predictions for the models in question will help inspire further research and
examination into the evaluation of models for visual description.","A.1 Code

We make all code/data publicly available for use at https://s3.us-west-1.wasabisys.com/
anon-neurips2022/neurips.tar.gz.","A.2 Datasets

MSR-VTT Dataset: The MSR-VTT dataset [44] is a dataset for video description consisting of
10,000 videos, with 20 reference ground truth descriptions for each video.",2022-09-15 17:58:13+00:00,Distribution Aware Metrics for Conditional Natural Language Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('David M Chan'), arxiv.Result.Author('Yiming Ni'), arxiv.Result.Author('Austin Myers'), arxiv.Result.Author('Sudheendra Vijayanarasimhan'), arxiv.Result.Author('David A Ross'), arxiv.Result.Author('John Canny')]","Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity."
11260,"Our proposed methods represent a
step toward a more robust evaluation paradigm, and we hope that our results, code, and data (To be
released soon), will inspire further research into this exciting area of conditional language generation.","It remains exciting and
necessary for future work to explore how a wider range of existing generation techniques and models
perform under this new paradigm, and, as data availability expands, to understand the implications of
distribution-aware evaluation in ﬁelds beyond visual description.","10
References

 [1] A. Agarwal and A. Lavie.",2022-09-15 17:58:13+00:00,Distribution Aware Metrics for Conditional Natural Language Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('David M Chan'), arxiv.Result.Author('Yiming Ni'), arxiv.Result.Author('David A Ross'), arxiv.Result.Author('Sudheendra Vijayanarasimhan'), arxiv.Result.Author('Austin Myers'), arxiv.Result.Author('John Canny')]","Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity."
11261,"We hope that releasing our code, along with the JSON
ﬁles containing test-set predictions for the models in question will help inspire further research and
examination into the evaluation of models for visual description.","A.1 Code

We make all code/data publicly available for use at https://s3.us-west-1.wasabisys.com/
anon-neurips2022/neurips.tar.gz.","A.2 Datasets

MSR-VTT Dataset: The MSR-VTT dataset [44] is a dataset for video description consisting of
10,000 videos, with 20 reference ground truth descriptions for each video.",2022-09-15 17:58:13+00:00,Distribution Aware Metrics for Conditional Natural Language Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('David M Chan'), arxiv.Result.Author('Yiming Ni'), arxiv.Result.Author('David A Ross'), arxiv.Result.Author('Sudheendra Vijayanarasimhan'), arxiv.Result.Author('Austin Myers'), arxiv.Result.Author('John Canny')]","Traditional automated metrics for evaluating conditional natural language
generation use pairwise comparisons between a single generated text and the
best-matching gold-standard ground truth text. When multiple ground truths are
available, scores are aggregated using an average or max operation across
references. While this approach works well when diversity in the ground truth
data (i.e. dispersion of the distribution of conditional texts) can be ascribed
to noise, such as in automated speech recognition, it does not allow for robust
evaluation in the case where diversity in the ground truths represents signal
for the model. In this work we argue that existing metrics are not appropriate
for domains such as visual description or summarization where ground truths are
semantically diverse, and where the diversity in those captions captures useful
additional information about the context. We propose a novel paradigm for
multi-candidate evaluation of conditional language generation models, and a new
family of metrics that compare the distributions of reference and
model-generated caption sets using small sample sets of each. We demonstrate
the utility of our approach with a case study in visual description: where we
show that existing models optimize for single-description quality over
diversity, and gain some insights into how sampling methods and temperature
impact description quality and diversity."
11267,"We release this model to the
emerged as a valuable tool for transferring in-        academic community to further research in social
formation from relational data to other tasks (El-     media NLP.","performs previously released language models on
                                                       not only semantic tasks, but also on social engage-
Network Embedding: Network embedding has               ment prediction tasks.","Kishky et al., 2022a).",2022-09-15 19:01:21+00:00,TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xinyang Zhang'), arxiv.Result.Author('Yury Malkov'), arxiv.Result.Author('Omar Florez'), arxiv.Result.Author('Serim Park'), arxiv.Result.Author('Brian McWilliams'), arxiv.Result.Author('Jiawei Han'), arxiv.Result.Author('Ahmed El-Kishky')]","We present TwHIN-BERT, a multilingual language model trained on in-domain
data from the popular social network Twitter. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on a variety of multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We will freely open-source
TwHIN-BERT and our curated hashtag prediction and social engagement benchmark
datasets to the research community."
11268,These meth-          to further research in social media NLP.,"We re-
from an XLM-R (Conneau et al., 2020) checkpoint          lease TwHIN-BERT45 to the academic community
on 198 million multilingual Tweets.","ods mostly replicate existing general domain PLM
methods and simply substitute the training data          4http://huggingface.co/Twitter/twhin-ber
with Tweets.",2022-09-15 19:01:21+00:00,TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xinyang Zhang'), arxiv.Result.Author('Yury Malkov'), arxiv.Result.Author('Omar Florez'), arxiv.Result.Author('Serim Park'), arxiv.Result.Author('Brian McWilliams'), arxiv.Result.Author('Jiawei Han'), arxiv.Result.Author('Ahmed El-Kishky')]","We present TwHIN-BERT, a multilingual language model trained on in-domain
data from the popular social network Twitter. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on a variety of multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We will freely open-source
TwHIN-BERT and our curated hashtag prediction and social engagement benchmark
datasets to the research community."
11281,"The scale of
                                        these models, combined with further research into their underpinnings, may unlock robust reasoning capabilities with
                                        revolutionary applications.","However, there is a silver lining.","˚Work done when Aman Madaan was a student researcher at Google Research, Brain Team.",2022-09-16 02:54:00+00:00,"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Aman Madaan'), arxiv.Result.Author('Amir Yazdanbakhsh')]","Reasoning is a key pillar of human cognition and intelligence. In the past
decade, we witnessed dramatic gains in natural language processing and
unprecedented scaling of large language models. Recent work has characterized
the capability of few-shot prompting techniques such as chain of thought to
emulate human reasoning in large language models. This hallmark feature of
few-shot prompting, combined with ever scaling language models, opened a vista
of possibilities to solve various tasks, such as math word problems, code
completion, and commonsense reasoning. Chain of thought (CoT) prompting further
pushes the performance of models in a few-shot setup, by supplying intermediate
steps and urging the model to follow the same process. Despite its compelling
performance, the genesis of reasoning capability in these models is less
explored. This work initiates the preliminary steps towards a deeper
understanding of reasoning mechanisms in large language models. Our work
centers around querying the model while controlling for all but one of the
components in a prompt: symbols, patterns, and text. We then analyze the
performance divergence across the queries. Our results suggest the presence of
factual patterns in a prompt is not necessary for the success of CoT.
Nonetheless, we empirically show that relying solely on patterns is also
insufficient for high quality results. We posit that text imbues patterns with
commonsense knowledge and meaning. Our exhaustive empirical analysis provides
qualitative examples of the symbiotic relationship between text and patterns.
Such systematic understanding of CoT enables us to devise concise chain of
thought, dubbed as CCoT, where text and patterns are pruned to only retain
their key roles, while delivering on par or slightly higher solve task rate."
11282,"We encourage further research and engagement from the community into this intriguing
line of work to enable additional applications for chain of thought and related prompting techniques.","We hope this work
serves as the initial step towards explaining the mechanisms underpinning chain of thought and its success
and failure modes.","ACKNOWLEDGEMENTS

We would like to extend our gratitude towards Kathy Meier-Hellstern, Denny Zhou, Victor Veitch, Saleem
Abdulrasool, Shruthi Sukumar, Milad Hashemi, Douglas Eck, Christian Szegedy, and Stella Aslibekyan.",2022-09-16 02:54:00+00:00,"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Aman Madaan'), arxiv.Result.Author('Amir Yazdanbakhsh')]","Reasoning is a key pillar of human cognition and intelligence. In the past
decade, we witnessed dramatic gains in natural language processing and
unprecedented scaling of large language models. Recent work has characterized
the capability of few-shot prompting techniques such as chain of thought to
emulate human reasoning in large language models. This hallmark feature of
few-shot prompting, combined with ever scaling language models, opened a vista
of possibilities to solve various tasks, such as math word problems, code
completion, and commonsense reasoning. Chain of thought (CoT) prompting further
pushes the performance of models in a few-shot setup, by supplying intermediate
steps and urging the model to follow the same process. Despite its compelling
performance, the genesis of reasoning capability in these models is less
explored. This work initiates the preliminary steps towards a deeper
understanding of reasoning mechanisms in large language models. Our work
centers around querying the model while controlling for all but one of the
components in a prompt: symbols, patterns, and text. We then analyze the
performance divergence across the queries. Our results suggest the presence of
factual patterns in a prompt is not necessary for the success of CoT.
Nonetheless, we empirically show that relying solely on patterns is also
insufficient for high quality results. We posit that text imbues patterns with
commonsense knowledge and meaning. Our exhaustive empirical analysis provides
qualitative examples of the symbiotic relationship between text and patterns.
Such systematic understanding of CoT enables us to devise concise chain of
thought, dubbed as CCoT, where text and patterns are pruned to only retain
their key roles, while delivering on par or slightly higher solve task rate."
11283,"For numerical reasoning             phrase representations using rnn encoder-decoder
type questions, we will do further research on TAT-         for statistical machine translation.","Learning
ing types of questions.","arXiv preprint
HQA (Li et al., 2022) and TAT-DQA (Zhu et al.,              arXiv:1406.1078.",2022-09-16 03:15:12+00:00,Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder,cs.CL,['cs.CL'],"[arxiv.Result.Author('Fangyu Lei'), arxiv.Result.Author('Shizhu He'), arxiv.Result.Author('Xiang Li'), arxiv.Result.Author('Jun Zhao'), arxiv.Result.Author('Kang Liu')]","In the real-world question answering scenarios, hybrid form combining both
tabular and textual contents has attracted more and more attention, among which
numerical reasoning problem is one of the most typical and challenging
problems. Existing methods usually adopt encoder-decoder framework to represent
hybrid contents and generate answers. However, it can not capture the rich
relationship among numerical value, table schema, and text information on the
encoder side. The decoder uses a simple predefined operator classifier which is
not flexible enough to handle numerical reasoning processes with diverse
expressions. To address these problems, this paper proposes a
\textbf{Re}lational \textbf{G}raph enhanced \textbf{H}ybrid table-text
\textbf{N}umerical reasoning model with \textbf{T}ree decoder
(\textbf{RegHNT}). It models the numerical question answering over table-text
hybrid contents as an expression tree generation task. Moreover, we propose a
novel relational graph modeling method, which models alignment between
questions, tables, and paragraphs. We validated our model on the publicly
available table-text hybrid QA benchmark (TAT-QA). The proposed RegHNT
significantly outperform the baseline model and achieve state-of-the-art
results\footnote{We openly released the source code and data
at~\url{https://github.com/lfy79001/RegHNT}}~(2022-05-05)."
11284,"For numerical reasoning             phrase representations using rnn encoder-decoder
type questions, we will do further research on TAT-         for statistical machine translation.","Learning
ing types of questions.","arXiv preprint
HQA (Li et al., 2022) and TAT-DQA (Zhu et al.,              arXiv:1406.1078.",2022-09-16 03:15:12+00:00,Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder,cs.CL,['cs.CL'],"[arxiv.Result.Author('Fangyu Lei'), arxiv.Result.Author('Shizhu He'), arxiv.Result.Author('Xiang Li'), arxiv.Result.Author('Jun Zhao'), arxiv.Result.Author('Kang Liu')]","In the real-world question answering scenarios, hybrid form combining both
tabular and textual contents has attracted more and more attention, among which
numerical reasoning problem is one of the most typical and challenging
problems. Existing methods usually adopt encoder-decoder framework to represent
hybrid contents and generate answers. However, it can not capture the rich
relationship among numerical value, table schema, and text information on the
encoder side. The decoder uses a simple predefined operator classifier which is
not flexible enough to handle numerical reasoning processes with diverse
expressions. To address these problems, this paper proposes a
\textbf{Re}lational \textbf{G}raph enhanced \textbf{H}ybrid table-text
\textbf{N}umerical reasoning model with \textbf{T}ree decoder
(\textbf{RegHNT}). It models the numerical question answering over table-text
hybrid contents as an expression tree generation task. Moreover, we propose a
novel relational graph modeling method, which models alignment between
questions, tables, and paragraphs. We validated our model on the publicly
available table-text hybrid QA benchmark (TAT-QA). The proposed RegHNT
significantly outperform the baseline model and achieve state-of-the-art
results. We openly released the source code and data at
https://github.com/lfy79001/RegHNT (2022-05-05)."
11316,"Thus, there is a need for further research    Katherine Atwell, Anthony Sicilia, Seong Jae Hwang,
(and additional annotated corpora) on discourse re-         and Malihe Alikhani.","corpora are composed of news texts over a short
time span.",2022.,2022-09-17 00:50:24+00:00,APPDIA: A Discourse-aware Transformer-based Style Transfer Model for Offensive Social Media Conversations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Katherine Atwell'), arxiv.Result.Author('Sabit Hassan'), arxiv.Result.Author('Malihe Alikhani')]","Using style-transfer models to reduce offensiveness of social media comments
can help foster a more inclusive environment. However, there are no sizable
datasets that contain offensive texts and their inoffensive counterparts, and
fine-tuning pretrained models with limited labeled data can lead to the loss of
original meaning in the style-transferred text. To address this issue, we
provide two major contributions. First, we release the first
publicly-available, parallel corpus of offensive Reddit comments and their
style-transferred counterparts annotated by expert sociolinguists. Then, we
introduce the first discourse-aware style-transfer models that can effectively
reduce offensiveness in Reddit text while preserving the meaning of the
original text. These models are the first to examine inferential links between
the comment and the text it is replying to when transferring the style of
offensive Reddit text. We propose two different methods of integrating
discourse relations with pretrained transformer models and evaluate them on our
dataset of offensive comments from Reddit and their inoffensive counterparts.
Improvements over the baseline with respect to both automatic metrics and human
evaluation indicate that our discourse-aware models are better at preserving
meaning in style-transferred text when compared to the state-of-the-art
discourse-agnostic models."
11327,"A further study could explore the state-of-the
art-in detecting automatically generated papers for       Anna Glazkova.","In Proceedings of the First Workshop on
                                                             Scholarly Document Processing, pages 356–361.",2021.,2022-09-17 08:43:25+00:00,Detecting Generated Scientific Papers using an Ensemble of Transformer Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', '68T50', 'I.2.7; I.7.m; H.3.3']","[arxiv.Result.Author('Anna Glazkova'), arxiv.Result.Author('Maksim Glazkov')]","The paper describes neural models developed for the DAGPap22 shared task
hosted at the Third Workshop on Scholarly Document Processing. This shared task
targets the automatic detection of generated scientific papers. Our work
focuses on comparing different transformer-based models as well as using
additional datasets and techniques to deal with imbalanced classes. As a final
submission, we utilized an ensemble of SciBERT, RoBERTa, and DeBERTa fine-tuned
using random oversampling technique. Our model achieved 99.24% in terms of
F1-score. The official evaluation results have put our system at the third
place."
11342,"So it needs further research to improve the                                                                                                    64

29 Error Analysis We summarize that the models generalization ability of the model in this regard.",model.,"64
    often fail to predict correct speakers in the follow- Task Difﬁculty Analysis As we can see, this task
30                                                                                                                                                                                                                                                                                                          64
task can also leverage other research on recognition in multi-  Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
person dialogue story scenes.",2022-09-18 10:19:04+00:00,A Benchmark for Understanding and Generating Dialogue between Characters in Stories,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jianzhu Yao'), arxiv.Result.Author('Ziqi Liu'), arxiv.Result.Author('Jian Guan'), arxiv.Result.Author('Minlie Huang')]","Many classical fairy tales, fiction, and screenplays leverage dialogue to
advance story plots and establish characters. We present the first study to
explore whether machines can understand and generate dialogue in stories, which
requires capturing traits of different characters and the relationships between
them. To this end, we propose two new tasks including Masked Dialogue
Generation and Dialogue Speaker Recognition, i.e., generating missing dialogue
turns and predicting speakers for specified dialogue turns, respectively. We
build a new dataset DialStory, which consists of 105k Chinese stories with a
large amount of dialogue weaved into the plots to support the evaluation. We
show the difficulty of the proposed tasks by testing existing models with
automatic and manual evaluation on DialStory. Furthermore, we propose to learn
explicit character representations to improve performance on these tasks.
Extensive experiments and case studies show that our approach can generate more
coherent and informative dialogue, and achieve higher speaker recognition
accuracy than strong baselines."
11354,"These
                                                             results should motivate further research on domain
Table 3: Change in attributions with Dom-spec (DL).","The results demonstrated con-
a sammich not sexist lol  a sammich not sexist lol           sistent improvements on the target domain.","adaptation in hate-speech and building classiﬁers
                                                             that can generalize well to the concept of hate.",2022-09-18 23:52:22+00:00,Domain Classification-based Source-specific Term Penalization for Domain Adaptation in Hate-speech Detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tulika Bose'), arxiv.Result.Author('Nikolaos Aletras'), arxiv.Result.Author('Irina Illina'), arxiv.Result.Author('Dominique Fohr')]","State-of-the-art approaches for hate-speech detection usually exhibit poor
performance in out-of-domain settings. This occurs, typically, due to
classifiers overemphasizing source-specific information that negatively impacts
its domain invariance. Prior work has attempted to penalize terms related to
hate-speech from manually curated lists using feature attribution methods,
which quantify the importance assigned to input terms by the classifier when
making a prediction. We, instead, propose a domain adaptation approach that
automatically extracts and penalizes source-specific terms using a domain
classifier, which learns to differentiate between domains, and
feature-attribution scores for hate-speech classes, yielding consistent
improvements in cross-domain evaluation."
11359,"We hope our results will foster
tions in the loss, which provides more ﬂexibility in   further research in this direction.","Besides, we introduce corrupted rela-        derstanding tasks.",incorporating the KG structure.,2022-09-19 02:41:02+00:00,Joint Language Semantic and Structure Embedding for Knowledge Graph Completion,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Jianhao Shen'), arxiv.Result.Author('Chenguang Wang'), arxiv.Result.Author('Linyuan Gong'), arxiv.Result.Author('Dawn Song')]","The task of completing knowledge triplets has broad downstream applications.
Both structural and semantic information plays an important role in knowledge
graph completion. Unlike previous approaches that rely on either the structures
or semantics of the knowledge graphs, we propose to jointly embed the semantics
in the natural language description of the knowledge triplets with their
structure information. Our method embeds knowledge graphs for the completion
task via fine-tuning pre-trained language models with respect to a
probabilistic structured loss, where the forward pass of the language models
captures semantics and the loss reconstructs structures. Our extensive
experiments on a variety of knowledge graph benchmarks have demonstrated the
state-of-the-art performance of our method. We also show that our method can
significantly improve the performance in a low-resource regime, thanks to the
better use of semantics. The code and datasets are available at
https://github.com/pkusjh/LASS."
11432,"We further study if language models can generate reasonable
explanations and then beneﬁt the reasoning ability.","We establish various baselines, including recent VQA models and large
language models on SCIENCEQA.","Experiments show that the UniﬁedQA with the
chain of thought can achieve an improvement of 3.99% and few-shot GPT-3 via chain-of-thought
(CoT) prompting can obtain a satisfactory accuracy of 75.17% on SCIENCEQA.",2022-09-20 07:04:24+00:00,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Pan Lu'), arxiv.Result.Author('Swaroop Mishra'), arxiv.Result.Author('Tony Xia'), arxiv.Result.Author('Liang Qiu'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Song-Chun Zhu'), arxiv.Result.Author('Oyvind Tafjord'), arxiv.Result.Author('Peter Clark'), arxiv.Result.Author('Ashwin Kalyan')]","When answering a question, humans utilize the information available across
different modalities to synthesize a consistent and complete chain of thought
(CoT). This process is normally a black box in the case of deep learning models
like large-scale language models. Recently, science question benchmarks have
been used to diagnose the multi-hop reasoning ability and interpretability of
an AI system. However, existing datasets fail to provide annotations for the
answers, or are restricted to the textual-only modality, small scales, and
limited domain diversity. To this end, we present Science Question Answering
(SQA), a new benchmark that consists of ~21k multimodal multiple choice
questions with a diverse set of science topics and annotations of their answers
with corresponding lectures and explanations. We further design language models
to learn to generate lectures and explanations as the chain of thought (CoT) to
mimic the multi-hop reasoning process when answering SQA questions. SQA
demonstrates the utility of CoT in language models, as CoT improves the
question answering performance by 1.20% in few-shot GPT-3 and 3.99% in
fine-tuned UnifiedQA. We also explore the upper bound for models to leverage
explanations by feeding those in the input; we observe that it improves the
few-shot performance of GPT-3 by 18.96%. Our analysis further shows that
language models, similar to humans, benefit from explanations to learn from
fewer data and achieve the same performance with just 40% of the data."
11433,"We further study if language models can generate reasonable
explanations and then beneﬁt the reasoning ability.","We establish various baselines, including recent VQA models and large
language models on SCIENCEQA.","Experiments show that UniﬁedQA with the chain
of thought can achieve an improvement of 3.99% and few-shot GPT-3 via chain-of-thought (CoT)
prompting can obtain a satisfactory accuracy of 75.17% on SCIENCEQA.",2022-09-20 07:04:24+00:00,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG', 'cs.MM']","[arxiv.Result.Author('Pan Lu'), arxiv.Result.Author('Swaroop Mishra'), arxiv.Result.Author('Tony Xia'), arxiv.Result.Author('Liang Qiu'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Song-Chun Zhu'), arxiv.Result.Author('Oyvind Tafjord'), arxiv.Result.Author('Peter Clark'), arxiv.Result.Author('Ashwin Kalyan')]","When answering a question, humans utilize the information available across
different modalities to synthesize a consistent and complete chain of thought
(CoT). This process is normally a black box in the case of deep learning models
like large-scale language models. Recently, science question benchmarks have
been used to diagnose the multi-hop reasoning ability and interpretability of
an AI system. However, existing datasets fail to provide annotations for the
answers, or are restricted to the textual-only modality, small scales, and
limited domain diversity. To this end, we present Science Question Answering
(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice
questions with a diverse set of science topics and annotations of their answers
with corresponding lectures and explanations. We further design language models
to learn to generate lectures and explanations as the chain of thought (CoT) to
mimic the multi-hop reasoning process when answering ScienceQA questions.
ScienceQA demonstrates the utility of CoT in language models, as CoT improves
the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in
fine-tuned UnifiedQA. We also explore the upper bound for models to leverage
explanations by feeding those in the input; we observe that it improves the
few-shot performance of GPT-3 by 18.96%. Our analysis further shows that
language models, similar to humans, benefit from explanations to learn from
fewer data and achieve the same performance with just 40% of the data. The data
and code are available at https://scienceqa.github.io."
11460,"To fa-
sis are given in Appendix A.                                 cilitate further research, we explore different
                                                             patch construction methods for the acoustic
   To be more speciﬁc, we adopt a simple two-pass            and visual raw data and analyze their effects.",More details about the complexity analy-              modal emotion recognition framework.,"strategy to capture the tri-modal feature interac-
tions in the proposed progressively tri-modal at-         • We evaluate ME2ET on two public datasets
tention.",2022-09-20 14:51:38+00:00,An Efficient End-to-End Transformer with Progressive Tri-modal Attention for Multi-modal Emotion Recognition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yang Wu'), arxiv.Result.Author('Pai Peng'), arxiv.Result.Author('Zhenyu Zhang'), arxiv.Result.Author('Yanyan Zhao'), arxiv.Result.Author('Bing Qin')]","Recent works on multi-modal emotion recognition move towards end-to-end
models, which can extract the task-specific features supervised by the target
task compared with the two-phase pipeline. However, previous methods only model
the feature interactions between the textual and either acoustic and visual
modalities, ignoring capturing the feature interactions between the acoustic
and visual modalities. In this paper, we propose the multi-modal end-to-end
transformer (ME2ET), which can effectively model the tri-modal features
interaction among the textual, acoustic, and visual modalities at the low-level
and high-level. At the low-level, we propose the progressive tri-modal
attention, which can model the tri-modal feature interactions by adopting a
two-pass strategy and can further leverage such interactions to significantly
reduce the computation and memory complexity through reducing the input token
length. At the high-level, we introduce the tri-modal feature fusion layer to
explicitly aggregate the semantic representations of three modalities. The
experimental results on the CMU-MOSEI and IEMOCAP datasets show that ME2ET
achieves the state-of-the-art performance. The further in-depth analysis
demonstrates the effectiveness, efficiency, and interpretability of the
proposed progressive tri-modal attention, which can help our model to achieve
better performance while significantly reducing the computation and memory
cost. Our code will be publicly available."
11473,"The agreement rate between the participants of the segmentation task and an
                    author is 0.82, which is sufficiently high to be used for further study.","The total
                    number of segments in the corpus was 3,816, the average number of segments per
                    sentence was 2.18, and the average number of segment boundaries per sentence was
                    1.18.","The agreement
                    rate is the accuracy of the workers’ labels for the correct boundaries annotated by an
                    author.",2022-09-20 23:26:02+00:00,Exploring Optimal Granularity for Extractive Summarization of Unstructured Health Records: Analysis of the Largest Multi-Institutional Archive of Health Records in Japan,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kenichiro Ando'), arxiv.Result.Author('Takashi OkumuraID'), arxiv.Result.Author('Mamoru Komachi'), arxiv.Result.Author('Hiromasa Horiguchi'), arxiv.Result.Author('Yuji Matsumoto')]","Automated summarization of clinical texts can reduce the burden of medical
professionals. ""Discharge summaries"" are one promising application of the
summarization, because they can be generated from daily inpatient records. Our
preliminary experiment suggests that 20-31% of the descriptions in discharge
summaries overlap with the content of the inpatient records. However, it
remains unclear how the summaries should be generated from the unstructured
source. To decompose the physician's summarization process, this study aimed to
identify the optimal granularity in summarization. We first defined three types
of summarization units with different granularities to compare the performance
of the discharge summary generation: whole sentences, clinical segments, and
clauses. We defined clinical segments in this study, aiming to express the
smallest medically meaningful concepts. To obtain the clinical segments, it was
necessary to automatically split the texts in the first stage of the pipeline.
Accordingly, we compared rule-based methods and a machine learning method, and
the latter outperformed the formers with an F1 score of 0.846 in the splitting
task. Next, we experimentally measured the accuracy of extractive summarization
using the three types of units, based on the ROUGE-1 metric, on a
multi-institutional national archive of health records in Japan. The measured
accuracies of extractive summarization using whole sentences, clinical
segments, and clauses were 31.91, 36.15, and 25.18, respectively. We found that
the clinical segments yielded higher accuracy than sentences and clauses. This
result indicates that summarization of inpatient records demands finer
granularity than sentence-oriented processing. Although we used only Japanese
health records, it can be interpreted as follows: physicians extract ""concepts
of medical significance"" from patient records and recombine them ..."
11474,"The agreement rate between the participants of the segmentation task and an
                   author is 0.82, which is sufficiently high to be used for further study.","The total
                   number of segments in the corpus was 3,816, the average number of segments per
                   sentence was 2.18, and the average number of segment boundaries per sentence was
                   1.18.","The agreement
                   rate is the accuracy of the workers’ labels for the correct boundaries annotated by an
                   author.",2022-09-20 23:26:02+00:00,Exploring Optimal Granularity for Extractive Summarization of Unstructured Health Records: Analysis of the Largest Multi-Institutional Archive of Health Records in Japan,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kenichiro Ando'), arxiv.Result.Author('Takashi Okumura'), arxiv.Result.Author('Mamoru Komachi'), arxiv.Result.Author('Hiromasa Horiguchi'), arxiv.Result.Author('Yuji Matsumoto')]","Automated summarization of clinical texts can reduce the burden of medical
professionals. ""Discharge summaries"" are one promising application of the
summarization, because they can be generated from daily inpatient records. Our
preliminary experiment suggests that 20-31% of the descriptions in discharge
summaries overlap with the content of the inpatient records. However, it
remains unclear how the summaries should be generated from the unstructured
source. To decompose the physician's summarization process, this study aimed to
identify the optimal granularity in summarization. We first defined three types
of summarization units with different granularities to compare the performance
of the discharge summary generation: whole sentences, clinical segments, and
clauses. We defined clinical segments in this study, aiming to express the
smallest medically meaningful concepts. To obtain the clinical segments, it was
necessary to automatically split the texts in the first stage of the pipeline.
Accordingly, we compared rule-based methods and a machine learning method, and
the latter outperformed the formers with an F1 score of 0.846 in the splitting
task. Next, we experimentally measured the accuracy of extractive summarization
using the three types of units, based on the ROUGE-1 metric, on a
multi-institutional national archive of health records in Japan. The measured
accuracies of extractive summarization using whole sentences, clinical
segments, and clauses were 31.91, 36.15, and 25.18, respectively. We found that
the clinical segments yielded higher accuracy than sentences and clauses. This
result indicates that summarization of inpatient records demands finer
granularity than sentence-oriented processing. Although we used only Japanese
health records, it can be interpreted as follows: physicians extract ""concepts
of medical significance"" from patient records and recombine them ..."
11533,"To help further research on the topic,
                                             we released 10 monolingual byte-level mod-           Perhaps even more surprising is the perfor-
                                             els1 rigorously pretrained under the same con-    mance of monolingual models in a cross-lingual
                                             ﬁguration with a large compute budget (equiv-     setting.",models.,"For instance, models pretrained and ﬁne-
                                             alent to 420 days on a V100) and corpora that     tuned only on English can perform well in French
                                             are 4 times larger than the original BERT’s.",2022-09-22 14:32:48+00:00,MonoByte: A Pool of Monolingual Byte-level Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hugo Abonizio'), arxiv.Result.Author('Leandro Rodrigues de Souza'), arxiv.Result.Author('Roberto Lotufo'), arxiv.Result.Author('Rodrigo Nogueira')]","The zero-shot cross-lingual ability of models pretrained on multilingual and
even monolingual corpora has spurred many hypotheses to explain this intriguing
empirical result. However, due to the costs of pretraining, most research uses
public models whose pretraining methodology, such as the choice of
tokenization, corpus size, and computational budget, might differ drastically.
When researchers pretrain their own models, they often do so under a
constrained budget, and the resulting models might underperform significantly
compared to SOTA models. These experimental differences led to various
inconsistent conclusions about the nature of the cross-lingual ability of these
models. To help further research on the topic, we released 10 monolingual
byte-level models rigorously pretrained under the same configuration with a
large compute budget (equivalent to 420 days on a V100) and corpora that are 4
times larger than the original BERT's. Because they are tokenizer-free, the
problem of unseen token embeddings is eliminated, thus allowing researchers to
try a wider range of cross-lingual experiments in languages with different
scripts. Additionally, we release two models pretrained on non-natural language
texts that can be used in sanity-check experiments. Experiments on QA and NLI
tasks show that our monolingual models achieve competitive performance to the
multilingual one, and hence can be served to strengthen our understanding of
cross-lingual transferability in language models."
11534,"To help further research on the topic,
                                             we released 10 monolingual byte-level mod-           Perhaps even more surprising is the perfor-
                                             els1 rigorously pretrained under the same con-    mance of monolingual models in a cross-lingual
                                             ﬁguration with a large compute budget (equiv-     setting.",models.,"For instance, models pretrained and ﬁne-
                                             alent to 420 days on a V100) and corpora that     tuned only on English can perform well in French
                                             are 4 times larger than the original BERT’s.",2022-09-22 14:32:48+00:00,MonoByte: A Pool of Monolingual Byte-level Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hugo Abonizio'), arxiv.Result.Author('Leandro Rodrigues de Souza'), arxiv.Result.Author('Roberto Lotufo'), arxiv.Result.Author('Rodrigo Nogueira')]","The zero-shot cross-lingual ability of models pretrained on multilingual and
even monolingual corpora has spurred many hypotheses to explain this intriguing
empirical result. However, due to the costs of pretraining, most research uses
public models whose pretraining methodology, such as the choice of
tokenization, corpus size, and computational budget, might differ drastically.
When researchers pretrain their own models, they often do so under a
constrained budget, and the resulting models might underperform significantly
compared to SOTA models. These experimental differences led to various
inconsistent conclusions about the nature of the cross-lingual ability of these
models. To help further research on the topic, we released 10 monolingual
byte-level models rigorously pretrained under the same configuration with a
large compute budget (equivalent to 420 days on a V100) and corpora that are 4
times larger than the original BERT's. Because they are tokenizer-free, the
problem of unseen token embeddings is eliminated, thus allowing researchers to
try a wider range of cross-lingual experiments in languages with different
scripts. Additionally, we release two models pretrained on non-natural language
texts that can be used in sanity-check experiments. Experiments on QA and NLI
tasks show that our monolingual models achieve competitive performance to the
multilingual one, and hence can be served to strengthen our understanding of
cross-lingual transferability in language models."
11563,"We make our code, dataset and model publicly available1, and
                                                 hope that this will help advance further research in this critical area.","Our extensive exper-
                                                 iments show that a multi-lingual mT5 model which uses fact-aware embeddings
                                                 with structure-aware input encoding leads to best results on average across the
                                                 twelve languages.","1 Introduction

                                        Fact-to-text (F2T) generation [41] is the task of transforming structured data (like fact
                                        triples) into natural language.",2022-09-22 18:01:27+00:00,XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shivprasad Sagare'), arxiv.Result.Author('Tushar Abhishek'), arxiv.Result.Author('Bhavyajeet Singh'), arxiv.Result.Author('Anubhav Sharma'), arxiv.Result.Author('Manish Gupta'), arxiv.Result.Author('Vasudeva Varma')]","Multiple business scenarios require an automated generation of descriptive
human-readable text from structured input data. Hence, fact-to-text generation
systems have been developed for various downstream tasks like generating soccer
reports, weather and financial reports, medical reports, person biographies,
etc. Unfortunately, previous work on fact-to-text (F2T) generation has focused
primarily on English mainly due to the high availability of relevant datasets.
Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed
for generation across multiple languages alongwith a dataset, XALIGN for eight
languages. However, there has been no rigorous work on the actual XF2T
generation problem. We extend XALIGN dataset with annotated data for four more
languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive
study using popular Transformer-based text generation models on our extended
multi-lingual dataset, which we call XALIGNV2. Further, we investigate the
performance of different text generation strategies: multiple variations of
pretraining, fact-aware embeddings and structure-aware input encoding. Our
extensive experiments show that a multi-lingual mT5 model which uses fact-aware
embeddings with structure-aware input encoding leads to best results on average
across the twelve languages. We make our code, dataset and model publicly
available, and hope that this will help advance further research in this
critical area."
11629,"This is a
valuable and new addition to previously computed semantic structures, which allows for multiple further researches.","At the same time, it contains information about conceptual and ﬁgurative semantic relations.","Acknowledgments

The authors thank Eugenio Llanos from Corporación Scio for providing the Lisp packages to compute ties in proximity
and cluster contrast to carry out Hierarchical Cluster Analysis.",2022-09-25 14:39:09+00:00,Corpus-based Metaphor Analysis through Graph Theoretical Methods,cs.CL,"['cs.CL', 'cs.DM', '91F20, 05C80, 05C90, 05C07, 91C20, 94C15, 68T50', 'J.5; I.2.7; I.5.3; I.5.1']","[arxiv.Result.Author('Marie Teich'), arxiv.Result.Author('Wilmer Leal'), arxiv.Result.Author('Juergen Jost')]","As a contribution to metaphor analysis, we introduce a statistical,
data-based investigation with empirical analysis of long-standing conjectures
and a first-ever empirical exploration of the systematic features of metaphors.
Conversely, this also makes metaphor theory available as a basis of meaning
emergence that can be quantitatively explored and integrated into the framework
of NLP."
11636,"marizing data from a new source domain or pro-
                                             To support further research, we release: (a)        ducing a summary in a different style.",prompting.,"The suc-
                                             a corpus of 10K generated summaries from            cess of prompt-based models (GPT-3 (Brown et al.,
                                             ﬁne-tuned and zero-shot models across 4             2020), T0 (Sanh et al., 2022), PaLM (Chowdhery
                                             standard summarization benchmarks, (b) 1K           et al., 2022), etc.)",2022-09-26 01:04:52+00:00,News Summarization and Evaluation in the Era of GPT-3,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tanya Goyal'), arxiv.Result.Author('Junyi Jessy Li'), arxiv.Result.Author('Greg Durrett')]","The recent success of zero- and few-shot prompting with models like GPT-3 has
led to a paradigm shift in NLP research. In this paper, we study its impact on
text summarization, focusing on the classic benchmark domain of news
summarization. First, we investigate how zero-shot GPT-3 compares against
fine-tuned models trained on large summarization datasets. We show that not
only do humans overwhelmingly prefer GPT-3 summaries, but these also do not
suffer from common dataset-specific issues such as poor factuality. Next, we
study what this means for evaluation, particularly the role of gold standard
test sets. Our experiments show that both reference-based and reference-free
automatic metrics, e.g. recently proposed QA- or entailment-based factuality
approaches, cannot reliably evaluate zero-shot summaries. Finally, we discuss
future research challenges beyond generic summarization, specifically, keyword-
and aspect-based summarization, showing how dominant fine-tuning approaches
compare to zero-shot prompting.
  To support further research, we release: (a) a corpus of 10K generated
summaries from fine-tuned and zero-shot models across 4 standard summarization
benchmarks, (b) 1K human preference judgments and rationales comparing
different systems for generic- and keyword-based summarization."
11694,"In Section 6, we
                                             and are averaged to perform the classiﬁcation     conclude and give an outlook to further research.","In Section 5,
                                             distances are then evaluated for multiple exe-    the classiﬁcation rates of Lex2Sent are compared
                                             cutions of Doc2Vec on resampled documents         to traditional lexicon methods.",task.,2022-09-26 20:49:18+00:00,Lex2Sent: A bagging approach to unsupervised sentiment analysis,cs.CL,['cs.CL'],"[arxiv.Result.Author('Kai-Robin Lange'), arxiv.Result.Author('Jonas Rieger'), arxiv.Result.Author('Carsten Jentsch')]","Unsupervised sentiment analysis is traditionally performed by counting those
words in a text that are stored in a sentiment lexicon and then assigning a
label depending on the proportion of positive and negative words registered.
While these ""counting"" methods are considered to be beneficial as they rate a
text deterministically, their classification rates decrease when the analyzed
texts are short or the vocabulary differs from what the lexicon considers
default. The model proposed in this paper, called Lex2Sent, is an unsupervised
sentiment analysis method to improve the classification of sentiment lexicon
methods. For this purpose, a Doc2Vec-model is trained to determine the
distances between document embeddings and the embeddings of the positive and
negative part of a sentiment lexicon. These distances are then evaluated for
multiple executions of Doc2Vec on resampled documents and are averaged to
perform the classification task. For three benchmark datasets considered in
this paper, the proposed Lex2Sent outperforms every evaluated lexicon,
including state-of-the-art lexica like VADER or the Opinion Lexicon in terms of
classification rate."
11700,"Overall, style perfor-
and /v/television could be because of style similarities at the        mance drops are less for style (7.3% in accuracy and 8.83%
medium level, an aspect to probe in further research.",The style confusion between /v/travel           about even across the two measures.,"As an            in F-score) than for content (9.09% accuracy and 9.91% F-
aside, the politics communities are highly distinguishable             score).",2022-09-27 02:08:34+00:00,Style Matters! Investigating Linguistic Style in Online Communities,cs.CL,['cs.CL'],"[arxiv.Result.Author('Osama Khalid'), arxiv.Result.Author('Padmini Srinivasan')]","Content has historically been the primary lens used to study language in
online communities. This paper instead focuses on the linguistic style of
communities. While we know that individuals have distinguishable styles, here
we ask whether communities have distinguishable styles. Additionally, while
prior work has relied on a narrow definition of style, we employ a broad
definition involving 262 features to analyze the linguistic style of 9 online
communities from 3 social media platforms discussing politics, television and
travel. We find that communities indeed have distinct styles. Also, style is an
excellent predictor of group membership (F-score 0.952 and Accuracy 96.09%).
While on average it is statistically equivalent to predictions using content
alone, it is more resilient to reductions in training data."
11741,"We leave further study of this         6.2 Task 2 Results
problem for future work.","relevant input from the user and is thus unable to
respond properly.","In this work, we perform
some prepossessing on the data to reduce the noise       Metrics In order to measure the performance of
brought by the three cases.",2022-09-27 15:30:43+00:00,Information Extraction and Human-Robot Dialogue towards Real-life Tasks: A Baseline Study with the MobileCS Dataset,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Hong Liu'), arxiv.Result.Author('Hao Peng'), arxiv.Result.Author('Zhijian Ou'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Yi Huang'), arxiv.Result.Author('Junlan Feng')]","Recently, there have merged a class of task-oriented dialogue (TOD) datasets
collected through Wizard-of-Oz simulated games. However, the Wizard-of-Oz data
are in fact simulated data and thus are fundamentally different from real-life
conversations, which are more noisy and casual. Recently, the SereTOD challenge
is organized and releases the MobileCS dataset, which consists of real-world
dialog transcripts between real users and customer-service staffs from China
Mobile. Based on the MobileCS dataset, the SereTOD challenge has two tasks, not
only evaluating the construction of the dialogue system itself, but also
examining information extraction from dialog transcripts, which is crucial for
building the knowledge base for TOD. This paper mainly presents a baseline
study of the two tasks with the MobileCS dataset. We introduce how the two
baselines are constructed, the problems encountered, and the results. We
anticipate that the baselines can facilitate exciting future research to build
human-robot dialogue systems for real-life tasks."
11742,We leave further study of this         labels before calculating the metric for slot ﬁlling.,"Thus, the system in textual dialogues receives no
relevant input from the user and is thus unable to       entity matching between predictions and golden
respond properly.",problem for future work.,2022-09-27 15:30:43+00:00,Information Extraction and Human-Robot Dialogue towards Real-life Tasks: A Baseline Study with the MobileCS Dataset,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Hong Liu'), arxiv.Result.Author('Hao Peng'), arxiv.Result.Author('Zhijian Ou'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Yi Huang'), arxiv.Result.Author('Junlan Feng')]","Recently, there have merged a class of task-oriented dialogue (TOD) datasets
collected through Wizard-of-Oz simulated games. However, the Wizard-of-Oz data
are in fact simulated data and thus are fundamentally different from real-life
conversations, which are more noisy and casual. Recently, the SereTOD challenge
is organized and releases the MobileCS dataset, which consists of real-world
dialog transcripts between real users and customer-service staffs from China
Mobile. Based on the MobileCS dataset, the SereTOD challenge has two tasks, not
only evaluating the construction of the dialogue system itself, but also
examining information extraction from dialog transcripts, which is crucial for
building the knowledge base for TOD. This paper mainly presents a baseline
study of the two tasks with the MobileCS dataset. We introduce how the two
baselines are constructed, the problems encountered, and the results. We
anticipate that the baselines can facilitate exciting future research to build
human-robot dialogue systems for real-life tasks."
11820,"We believe that the COM-
                                             PILING dataset will beneﬁt further research in complexity controllable deﬁnition generation.","We select various representative generation methods as baselines for
                                             this task and conduct evaluations, which illustrates that our dataset plays an outstanding role in
                                             assisting models in generating different complexity-level deﬁnitions.","1 Introduction

                                        Deﬁnition Generation (DG) is the task of describing the meaning that a word takes in a speciﬁc con-
                                        text.",2022-09-29 08:17:53+00:00,COMPILING: A Benchmark Dataset for Chinese Complexity Controllable Definition Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiaxin Yuan'), arxiv.Result.Author('Cunliang Kong'), arxiv.Result.Author('Chenhui Xie'), arxiv.Result.Author('Liner Yang'), arxiv.Result.Author('Erhong Yang')]","The definition generation task aims to generate a word's definition within a
specific context automatically. However, owing to the lack of datasets for
different complexities, the definitions produced by models tend to keep the
same complexity level. This paper proposes a novel task of generating
definitions for a word with controllable complexity levels. Correspondingly, we
introduce COMPILING, a dataset given detailed information about Chinese
definitions, and each definition is labeled with its complexity levels. The
COMPILING dataset includes 74,303 words and 106,882 definitions. To the best of
our knowledge, it is the largest dataset of the Chinese definition generation
task. We select various representative generation methods as baselines for this
task and conduct evaluations, which illustrates that our dataset plays an
outstanding role in assisting models in generating different complexity-level
definitions. We believe that the COMPILING dataset will benefit further
research in complexity controllable definition generation."
11821,"Differently, we focus on building the benchmark dataset for different Chinese deﬁnition generation
tasks and hope it could be beneﬁcial for further research.","Since the lack of a deﬁnition dataset with different complexities, they managed to generate both
complex and simple deﬁnitions in an unsupervised way.","2.2 Controllable Generation

Controllable generation is widely adapted in kinds of language modeling tasks.",2022-09-29 08:17:53+00:00,COMPILING: A Benchmark Dataset for Chinese Complexity Controllable Definition Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiaxin Yuan'), arxiv.Result.Author('Cunliang Kong'), arxiv.Result.Author('Chenhui Xie'), arxiv.Result.Author('Liner Yang'), arxiv.Result.Author('Erhong Yang')]","The definition generation task aims to generate a word's definition within a
specific context automatically. However, owing to the lack of datasets for
different complexities, the definitions produced by models tend to keep the
same complexity level. This paper proposes a novel task of generating
definitions for a word with controllable complexity levels. Correspondingly, we
introduce COMPILING, a dataset given detailed information about Chinese
definitions, and each definition is labeled with its complexity levels. The
COMPILING dataset includes 74,303 words and 106,882 definitions. To the best of
our knowledge, it is the largest dataset of the Chinese definition generation
task. We select various representative generation methods as baselines for this
task and conduct evaluations, which illustrates that our dataset plays an
outstanding role in assisting models in generating different complexity-level
definitions. We believe that the COMPILING dataset will benefit further
research in complexity controllable definition generation."
11839,tool for further research experiments.,"This             • Provide a new dataset for both tasks (term
                                        paper describes the study on entity recognition and           recognition and relation extraction) for Rus-
                                        relation extraction from scientiﬁc texts on computer          sian scientiﬁc texts and develop a TERMinator
                                        science in Russian.","Currently, there are a number of datasets with          • Study of inﬂuence of language models (with-
                                        annotations of entities and relations in a general            out additional information, with heuristics and
                                        domain (Doddington et al., 2004; Roth and Yih,                dictionaries) on term extraction.",2022-09-29 15:14:42+00:00,TERMinator: A system for scientific texts processing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Elena Bruches'), arxiv.Result.Author('Olga Tikhobaeva'), arxiv.Result.Author('Yana Dementyeva'), arxiv.Result.Author('Tatiana Batura')]","This paper is devoted to the extraction of entities and semantic relations
between them from scientific texts, where we consider scientific terms as
entities. In this paper, we present a dataset that includes annotations for two
tasks and develop a system called TERMinator for the study of the influence of
language models on term recognition and comparison of different approaches for
relation extraction. Experiments show that language models pre-trained on the
target language are not always show the best performance. Also adding some
heuristic approaches may improve the overall quality of the particular task.
The developed tool and the annotated corpus are publicly available at
https://github.com/iis-research-team/terminator and may be useful for other
researchers."
11857,"2019),
further research should be undertaken to differentiate the NNEs-derived distributional
similarity from taxonomic similarity.","2018, Shi et al.","Overall, the aim of our work is to extend current
knowledge of both taxonomic similarity in WordNet and distributional similarity in NNEs.",2022-09-30 02:54:21+00:00,Evaluation of taxonomic and neural embedding methods for calculating semantic similarity,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Dongqiang Yang'), arxiv.Result.Author('Yanqin Yin')]","Modelling semantic similarity plays a fundamental role in lexical semantic
applications. A natural way of calculating semantic similarity is to access
handcrafted semantic networks, but similarity prediction can also be
anticipated in a distributional vector space. Similarity calculation continues
to be a challenging task, even with the latest breakthroughs in deep neural
language models. We first examined popular methodologies in measuring taxonomic
similarity, including edge-counting that solely employs semantic relations in a
taxonomy, as well as the complex methods that estimate concept specificity. We
further extrapolated three weighting factors in modelling taxonomic similarity.
To study the distinct mechanisms between taxonomic and distributional
similarity measures, we ran head-to-head comparisons of each measure with human
similarity judgements from the perspectives of word frequency, polysemy degree
and similarity intensity. Our findings suggest that without fine-tuning the
uniform distance, taxonomic similarity measures can depend on the shortest path
length as a prime factor to predict semantic similarity; in contrast to
distributional semantics, edge-counting is free from sense distribution bias in
use and can measure word similarity both literally and metaphorically; the
synergy of retrofitting neural embeddings with concept relations in similarity
prediction may indicate a new trend to leverage knowledge bases on transfer
learning. It appears that a large gap still exists on computing semantic
similarity among different ranges of word frequency, polysemous degree and
similarity intensity."
11858,"We further study the effectiveness of
injecting human-compiled semantic knowledge into neural embeddings on computing distributional
similarity.","Moreover, using the same training hyperparameters
and corpora, we study typical neural embeddings in the evaluation.","Our results show that the syntactically conditioned contexts can interpret lexical semantics
better than the unconditioned ones, whereas retrofitting neural embeddings with semantic knowledge can
significantly improve synonym detection.",2022-09-30 03:16:41+00:00,Synonym Detection Using Syntactic Dependency And Neural Embeddings,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Dongqiang Yang'), arxiv.Result.Author('Pikun Wang'), arxiv.Result.Author('Xiaodong Sun'), arxiv.Result.Author('Ning Li')]","Recent advances on the Vector Space Model have significantly improved some
NLP applications such as neural machine translation and natural language
generation. Although word co-occurrences in context have been widely used in
counting-/predicting-based distributional models, the role of syntactic
dependencies in deriving distributional semantics has not yet been thoroughly
investigated. By comparing various Vector Space Models in detecting synonyms in
TOEFL, we systematically study the salience of syntactic dependencies in
accounting for distributional similarity. We separate syntactic dependencies
into different groups according to their various grammatical roles and then use
context-counting to construct their corresponding raw and SVD-compressed
matrices. Moreover, using the same training hyperparameters and corpora, we
study typical neural embeddings in the evaluation. We further study the
effectiveness of injecting human-compiled semantic knowledge into neural
embeddings on computing distributional similarity. Our results show that the
syntactically conditioned contexts can interpret lexical semantics better than
the unconditioned ones, whereas retrofitting neural embeddings with semantic
knowledge can significantly improve synonym detection."
11866,"further research.1
                                                                                                 A growing number of approaches attempt to en-
                                        1 Introduction                                        hance the processing of consumer health questions
                                                                                              – or medical question understanding.","We release our code to encourage   (Ben Abacha and Demner-Fushman, 2019a).","These ap-
                                        Motivation.",2022-09-30 08:20:32+00:00,Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision,cs.CL,['cs.CL'],"[arxiv.Result.Author('Khalil Mrini'), arxiv.Result.Author('Harpreet Singh'), arxiv.Result.Author('Franck Dernoncourt'), arxiv.Result.Author('Seunghyun Yoon'), arxiv.Result.Author('Trung Bui'), arxiv.Result.Author('Walter Chang'), arxiv.Result.Author('Emilia Farcas'), arxiv.Result.Author('Ndapa Nakashole')]","Current medical question answering systems have difficulty processing long,
detailed and informally worded questions submitted by patients, called Consumer
Health Questions (CHQs). To address this issue, we introduce a medical question
understanding and answering system with knowledge grounding and semantic
self-supervision. Our system is a pipeline that first summarizes a long,
medical, user-written question, using a supervised summarization loss. Then,
our system performs a two-step retrieval to return answers. The system first
matches the summarized user question with an FAQ from a trusted medical
knowledge base, and then retrieves a fixed number of relevant sentences from
the corresponding answer document. In the absence of labels for question
matching or answer relevance, we design 3 novel, self-supervised and
semantically-guided losses. We evaluate our model against two strong
retrieval-based question answering baselines. Evaluators ask their own
questions and rate the answers retrieved by our baselines and own system
according to their relevance. They find that our system retrieves more relevant
answers, while achieving speeds 20 times faster. Our self-supervised losses
also help the summarizer achieve higher scores in ROUGE, as well as in human
evaluation metrics. We release our code to encourage further research."
11867,"We release
our code and model to encourage further research.","How-             ural Language Processing (EMNLP), pages 3122–
ever, we ﬁnd that this task remains challenging and         3137.
that there is still room for improvement.","Asma Ben Abacha, Eugene Agichtein, Yuval Pinter,
                                                            and Dina Demner-Fushman.",2022-09-30 08:20:32+00:00,Medical Question Understanding and Answering with Knowledge Grounding and Semantic Self-Supervision,cs.CL,['cs.CL'],"[arxiv.Result.Author('Khalil Mrini'), arxiv.Result.Author('Harpreet Singh'), arxiv.Result.Author('Franck Dernoncourt'), arxiv.Result.Author('Seunghyun Yoon'), arxiv.Result.Author('Trung Bui'), arxiv.Result.Author('Walter Chang'), arxiv.Result.Author('Emilia Farcas'), arxiv.Result.Author('Ndapa Nakashole')]","Current medical question answering systems have difficulty processing long,
detailed and informally worded questions submitted by patients, called Consumer
Health Questions (CHQs). To address this issue, we introduce a medical question
understanding and answering system with knowledge grounding and semantic
self-supervision. Our system is a pipeline that first summarizes a long,
medical, user-written question, using a supervised summarization loss. Then,
our system performs a two-step retrieval to return answers. The system first
matches the summarized user question with an FAQ from a trusted medical
knowledge base, and then retrieves a fixed number of relevant sentences from
the corresponding answer document. In the absence of labels for question
matching or answer relevance, we design 3 novel, self-supervised and
semantically-guided losses. We evaluate our model against two strong
retrieval-based question answering baselines. Evaluators ask their own
questions and rate the answers retrieved by our baselines and own system
according to their relevance. They find that our system retrieves more relevant
answers, while achieving speeds 20 times faster. Our self-supervised losses
also help the summarizer achieve higher scores in ROUGE, as well as in human
evaluation metrics. We release our code to encourage further research."
11904,"Our study lays the
                                                      grounds for further research in this direction.","However, we observed a lack of sec-
                                                      ondary research and evaluations in practice, both
                                                      of which are crucial to reﬂect the major scientiﬁc
                                                      progress of the ﬁeld as a whole.","Acknowledgements                                          Xieling Chen, Haoran Xie, Zongxi Li, and Gary Cheng.",2022-09-30 21:53:57+00:00,A Decade of Knowledge Graphs in Natural Language Processing: A Survey,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Phillip Schneider'), arxiv.Result.Author('Tim Schopf'), arxiv.Result.Author('Juraj Vladika'), arxiv.Result.Author('Mikhail Galkin'), arxiv.Result.Author('Elena Simperl'), arxiv.Result.Author('Florian Matthes')]","In pace with developments in the research field of artificial intelligence,
knowledge graphs (KGs) have attracted a surge of interest from both academia
and industry. As a representation of semantic relations between entities, KGs
have proven to be particularly relevant for natural language processing (NLP),
experiencing a rapid spread and wide adoption within recent years. Given the
increasing amount of research work in this area, several KG-related approaches
have been surveyed in the NLP research community. However, a comprehensive
study that categorizes established topics and reviews the maturity of
individual research streams remains absent to this day. Contributing to closing
this gap, we systematically analyzed 507 papers from the literature on KGs in
NLP. Our survey encompasses a multifaceted review of tasks, research types, and
contributions. As a result, we present a structured overview of the research
landscape, provide a taxonomy of tasks, summarize our findings, and highlight
directions for future work."
11909,"For further research, we believe our work complements that of (Vig et al., 2020), which uses causal
mediation analysis to intervene on LLMs at the individual attention head and neuron level to provide
insights into the model’s internal causal mechanisms mediating gender bias.","While not a universal solution, we hope our work can lead to the
development of targeted heuristics, using our method to determine when the model is uncertain
about a prediction task, and thus an alternate value should be returned (for example ‘they’ in the
case of gender pronoun resolution).","Despite the more lim-
ited nature of our non-invasive investigation, our surprising empirical ﬁndings about the additivity
of unrelated spurious associations is consistent with their surprising ﬁnding that gender bias appears
decomposable between the elements of the direct and indirect effect within the model.",2022-09-30 23:10:11+00:00,Selection Induced Collider Bias: A Gender Pronoun Uncertainty Case Study,cs.CL,"['cs.CL', 'cs.AI']",[arxiv.Result.Author('Emily McMilin')],"In this paper, we cast the problem of task underspecification in causal
terms, and develop a method for empirical measurement of spurious associations
between gender and gender-neutral entities for unmodified large language
models, detecting previously unreported spurious correlations. We then describe
a lightweight method to exploit the resulting spurious associations for
prediction task uncertainty classification, achieving over 90% accuracy on a
Winogender Schemas challenge set. Finally, we generalize our approach to
address a wider range of prediction tasks and provide open-source demos for
each method described here."
11926,"sentiment analyses can depict intertemporal dynam-
                                       ics on social media platforms, what challenges are           Furthermore, sentiment analyses have a suﬃcient
                                       inherent and how further research could beneﬁt from a        inter-rater-reliability with less time requirements.","to classify anger, fear, anticipation, trust, surprise,
                                       This discussion paper demonstrates how longitudinal          sadness, joy, and disgust as well.","The
                                       longitudinal perspective.",2022-10-01 18:30:00+00:00,Longitudinal Sentiment Analyses for Radicalization Research: Intertemporal Dynamics on Social Media Platforms and their Implications,cs.CL,"['cs.CL', 'cs.CV', 'stat.AP', 'D.1.5; D.3.0; J.4; K.4.1']",[arxiv.Result.Author('Dennis Klinkhammer')],"This discussion paper demonstrates how longitudinal sentiment analyses can
depict intertemporal dynamics on social media platforms, what challenges are
inherent and how further research could benefit from a longitudinal
perspective. Furthermore and since tools for sentiment analyses shall simplify
and accelerate the analytical process regarding qualitative data at acceptable
inter-rater reliability, their applicability in the context of radicalization
research will be examined regarding the Tweets collected on January 6th 2021,
the day of the storming of the U.S. Capitol in Washington. Therefore, a total
of 49,350 Tweets will be analyzed evenly distributed within three different
sequences: before, during and after the U.S. Capitol in Washington was stormed.
These sequences highlight the intertemporal dynamics within comments on social
media platforms as well as the possible benefits of a longitudinal perspective
when using conditional means and conditional variances. Limitations regarding
the identification of supporters of such events and associated hate speech as
well as common application errors will be demonstrated as well. As a result,
only under certain conditions a longitudinal sentiment analysis can increase
the accuracy of evidence based predictions in the context of radicalization
research."
11934,"We make our code,
                                       models, and corpus available for further research.","Classiﬁcation experiments
                                       using state-of-the-art techniques show evidence that adding contextual information improves hate speech
                                       detection performance for two proposed tasks (binary and multi-label prediction).","INDEX TERMS NLP, Hate Speech detection, COVID-19 Hate Speech

I.",2022-10-02 09:04:47+00:00,Assessing the impact of contextual information in hate speech detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Juan Manuel Pérez'), arxiv.Result.Author('Franco Luque'), arxiv.Result.Author('Demian Zayat'), arxiv.Result.Author('Martín Kondratzky'), arxiv.Result.Author('Agustín Moro'), arxiv.Result.Author('Pablo Serrati'), arxiv.Result.Author('Joaquín Zajac'), arxiv.Result.Author('Paula Miguel'), arxiv.Result.Author('Natalia Debandi'), arxiv.Result.Author('Agustín Gravano'), arxiv.Result.Author('Viviana Cotik')]","In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research."
11935,"From a Natural Language Processing perspective, hate
speech detection can be thought of as a text classiﬁcation           3) We make our code, models, and the annotated corpus
task: given a document generated by a user (a post in a social           available for further research.1
network), predict whether or not it contains hateful content.",forms.,"Additionally, it may be of interest to predict other features:    II.",2022-10-02 09:04:47+00:00,Assessing the impact of contextual information in hate speech detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Juan Manuel Pérez'), arxiv.Result.Author('Franco Luque'), arxiv.Result.Author('Demian Zayat'), arxiv.Result.Author('Martín Kondratzky'), arxiv.Result.Author('Agustín Moro'), arxiv.Result.Author('Pablo Serrati'), arxiv.Result.Author('Joaquín Zajac'), arxiv.Result.Author('Paula Miguel'), arxiv.Result.Author('Natalia Debandi'), arxiv.Result.Author('Agustín Gravano'), arxiv.Result.Author('Viviana Cotik')]","In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research."
11936,"We make our code,
                                       models, and corpus available for further research.","Classiﬁcation experiments
                                       using state-of-the-art techniques show evidence that adding contextual information improves hate speech
                                       detection performance for two proposed tasks (binary and multi-label prediction).","INDEX TERMS NLP, Hate Speech detection, COVID-19 Hate Speech

I.",2022-10-02 09:04:47+00:00,Assessing the impact of contextual information in hate speech detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Juan Manuel Pérez'), arxiv.Result.Author('Franco Luque'), arxiv.Result.Author('Demian Zayat'), arxiv.Result.Author('Martín Kondratzky'), arxiv.Result.Author('Agustín Moro'), arxiv.Result.Author('Pablo Serrati'), arxiv.Result.Author('Joaquín Zajac'), arxiv.Result.Author('Paula Miguel'), arxiv.Result.Author('Natalia Debandi'), arxiv.Result.Author('Agustín Gravano'), arxiv.Result.Author('Viviana Cotik')]","In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research."
11937,"From a Natural Language Processing perspective, hate
speech detection can be thought of as a text classiﬁcation           3) We make our code, models, and the annotated corpus
task: given a document generated by a user (a post in a social           available for further research.1
network), predict whether or not it contains hateful content.",forms.,"Additionally, it may be of interest to predict other features:       The rest of the paper is organized as follows: Section II
for example, if the text contains a call to take some possibly    reviews previous work for automatic hate speech detection.",2022-10-02 09:04:47+00:00,Assessing the impact of contextual information in hate speech detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Juan Manuel Pérez'), arxiv.Result.Author('Franco Luque'), arxiv.Result.Author('Demian Zayat'), arxiv.Result.Author('Martín Kondratzky'), arxiv.Result.Author('Agustín Moro'), arxiv.Result.Author('Pablo Serrati'), arxiv.Result.Author('Joaquín Zajac'), arxiv.Result.Author('Paula Miguel'), arxiv.Result.Author('Natalia Debandi'), arxiv.Result.Author('Agustín Gravano'), arxiv.Result.Author('Viviana Cotik')]","In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research."
11938,"Rep.,
sake of reproducibility and also for further research, we will           2019.
release the anonymized annotations (as suggested by Basile
[55]) in addition to the aggregated dataset.","For the              amenaza a los derechos humanos,” CELE, Tech.","The annotation        [13] CIDH, “Discurso de odio y la incitación a la violencia
criteria will be publicly available upon publication of this             contra las personas lesbianas, gays, bisexuales, trans e
paper.",2022-10-02 09:04:47+00:00,Assessing the impact of contextual information in hate speech detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Juan Manuel Pérez'), arxiv.Result.Author('Franco Luque'), arxiv.Result.Author('Demian Zayat'), arxiv.Result.Author('Martín Kondratzky'), arxiv.Result.Author('Agustín Moro'), arxiv.Result.Author('Pablo Serrati'), arxiv.Result.Author('Joaquín Zajac'), arxiv.Result.Author('Paula Miguel'), arxiv.Result.Author('Natalia Debandi'), arxiv.Result.Author('Agustín Gravano'), arxiv.Result.Author('Viviana Cotik')]","In recent years, hate speech has gained great relevance in social networks
and other virtual media because of its intensity and its relationship with
violent acts against members of protected groups. Due to the great amount of
content generated by users, great effort has been made in the research and
development of automatic tools to aid the analysis and moderation of this
speech, at least in its most threatening forms. One of the limitations of
current approaches to automatic hate speech detection is the lack of context.
Most studies and resources are performed on data without context; that is,
isolated messages without any type of conversational context or the topic being
discussed. This restricts the available information to define if a post on a
social network is hateful or not. In this work, we provide a novel corpus for
contextualized hate speech detection based on user responses to news posts from
media outlets on Twitter. This corpus was collected in the Rioplatense
dialectal variety of Spanish and focuses on hate speech associated with the
COVID-19 pandemic. Classification experiments using state-of-the-art techniques
show evidence that adding contextual information improves hate speech detection
performance for two proposed tasks (binary and multi-label prediction). We make
our code, models, and corpus available for further research."
11944,"Hence the observed overlap in communities from diﬀerent layers might be
   due to a potential cognitive beneﬁt, worthy of further research.","Recent investigations indicated that
   semantic and phonological connections are systematically better than ran-
   dom links in decreasing the average network distance between words [65].","4.3 Finding hidden interactions between two or more
      layers: Mediation, suppression and other
      layer-interaction mechanisms

Given that multiplex networks encode diﬀerent aspects of the mental lexicon
in diﬀerent layers, one might ask: “How similar are layers of a given multiplex
network?”.",2022-10-02 12:22:53+00:00,"Cognitive modelling with multilayer networks: Insights, advancements and future challenges",cs.CL,"['cs.CL', 'cs.AI', 'cs.SI', 'physics.data-an', 'q-bio.NC']","[arxiv.Result.Author('Massimo Stella'), arxiv.Result.Author('Salvatore Citraro'), arxiv.Result.Author('Giulio Rossetti'), arxiv.Result.Author('Daniele Marinazzo'), arxiv.Result.Author('Yoed N. Kenett'), arxiv.Result.Author('Michael S. Vitevitch')]","The mental lexicon is a complex cognitive system representing information
about the words/concepts that one knows. Decades of psychological experiments
have shown that conceptual associations across multiple, interactive cognitive
levels can greatly influence word acquisition, storage, and processing. How can
semantic, phonological, syntactic, and other types of conceptual associations
be mapped within a coherent mathematical framework to study how the mental
lexicon works? We here review cognitive multilayer networks as a promising
quantitative and interpretative framework for investigating the mental lexicon.
Cognitive multilayer networks can map multiple types of information at once,
thus capturing how different layers of associations might co-exist within the
mental lexicon and influence cognitive processing. This review starts with a
gentle introduction to the structure and formalism of multilayer networks. We
then discuss quantitative mechanisms of psychological phenomena that could not
be observed in single-layer networks and were only unveiled by combining
multiple layers of the lexicon: (i) multiplex viability highlights language
kernels and facilitative effects of knowledge processing in healthy and
clinical populations; (ii) multilayer community detection enables contextual
meaning reconstruction depending on psycholinguistic features; (iii) layer
analysis can mediate latent interactions of mediation, suppression and
facilitation for lexical access. By outlining novel quantitative perspectives
where multilayer networks can shed light on cognitive knowledge
representations, also in next-generation brain/mind models, we discuss key
limitations and promising directions for cutting-edge future research."
11953,"While the confirmation of this is left for

further research, the lack of superior performance

in text summarization encoders’ modeling of

quantitative values is clear.","This would be

expected to lead the decoder to generate

numerical values not necessarily represented in

the encoder’s output to compensate to match the

target.","5 Related work

5.1 Abstractive summarization and
hallucination

Abstractive summarization has exhibited a degree
of hallucinated facts in its generated summaries
(Cao et al., 2018; Falke et al., 2019; Maynez et al.,
2020; Zhao et al., 2020a).",2022-10-03 00:59:50+00:00,Probing of Quantitative Values in Abstractive Summarization Models,cs.CL,['cs.CL'],[arxiv.Result.Author('Nathan M. White')],"Abstractive text summarization has recently become a popular approach, but
data hallucination remains a serious problem, including with quantitative data.
We propose a set of probing tests to evaluate the efficacy of abstract
summarization models' modeling of quantitative values found in the input text.
Our results show that in most cases, the encoders of recent SOTA-performing
models struggle to provide embeddings that adequately represent quantitative
values in the input compared to baselines, and in particular, they outperform
random representations in some, but surprisingly not all, cases. Under our
assumptions, this suggests that the encoder's performance contributes to the
quantity hallucination problem. One model type in particular, DistilBART-CDM,
was observed to underperform randomly initialized representations for several
experiments, and performance versus BERT suggests that standard pretraining and
fine-tuning approaches for the summarization task may play a role in
underperformance for some encoders."
11960,We leave further study on connecting classical parsing and CoT to future work.,"From our perspective, we tend to view chain-of-thoughts as
ﬂexible, language model styled “logical forms” which are “executed” by the language model itself.","3 COMPLEXITY-BASED PROMPTING

We study multi-step reasoning tasks, and use math word problems, mathematical problems ex-
pressed in natural language, as our testbed.",2022-10-03 05:33:27+00:00,Complexity-Based Prompting for Multi-Step Reasoning,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yao Fu'), arxiv.Result.Author('Hao Peng'), arxiv.Result.Author('Ashish Sabharwal'), arxiv.Result.Author('Peter Clark'), arxiv.Result.Author('Tushar Khot')]","We study the task of prompting large-scale language models to perform
multi-step reasoning. Existing work shows that when prompted with a chain of
thoughts (CoT), sequences of short sentences describing intermediate reasoning
steps towards a final answer, large language models can generate new reasoning
chains and predict answers for new inputs. A central question is which
reasoning examples make the most effective prompts. In this work, we propose
complexity-based prompting, a simple and effective example selection scheme for
multi-step reasoning. We show that prompts with higher reasoning complexity,
i.e., chains with more reasoning steps, achieve substantially better
performance on math word reasoning tasks over strong baselines. We further
extend our complexity-based criteria from prompting (selecting inputs) to
decoding (selecting outputs), where we sample multiple reasoning chains from
the model, then choose the majority of generated answers from complex reasoning
chains (over simple chains). When used to prompt GPT-3, our approach
substantially improves multi-step reasoning accuracy, with an 8.6% absolute
improvement on GSM8K, and 6.4% on MathQA. Compared with existing example
selection schemes like manual tuning or retrieval-based selection, selection
based on reasoning complexity is intuitive, easy to implement, and
annotation-efficient. Further results demonstrate the robustness of our methods
under format perturbation and distribution shift."
12082,"Thus, we’ll further study the pre-trained
The Effect of Cloze Model The performance of            models employed in the cloze model in the future.","on reliable knowledge-related words when doing
                                                        cloze.","QA-based metrics depends on both QG and QA
Limitations                                                   poor performance in factual consistency evaluation.",2022-10-06 10:30:53+00:00,Just ClozE! A Fast and Simple Method for Evaluating the Factual Consistency in Abstractive Summarization,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yiyang Li'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Qing Yang'), arxiv.Result.Author('Marina Litvak'), arxiv.Result.Author('Natalia Vanetik'), arxiv.Result.Author('Dingxin Hu'), arxiv.Result.Author('Yuze Li'), arxiv.Result.Author('Yanquan Zhou'), arxiv.Result.Author('Dongliang Xu'), arxiv.Result.Author('Xuanyu Zhang')]","The issue of factual consistency in abstractive summarization has attracted
much attention in recent years, and the evaluation of factual consistency
between summary and document has become an important and urgent task. Most of
the current evaluation metrics are adopted from the question answering (QA).
However, the application of QA-based metrics is extremely time-consuming in
practice, causing the iteration cycle of abstractive summarization research to
be severely prolonged. In this paper, we propose a new method called ClozE to
evaluate factual consistency by cloze model, instantiated based on masked
language model(MLM), with strong interpretability and substantially higher
speed. We demonstrate that ClozE can reduce the evaluation time by nearly
96$\%$ relative to QA-based metrics while retaining their interpretability and
performance through experiments on six human-annotated datasets and a
meta-evaluation benchmark GO FIGURE \citep{gabriel2020go}. We also implement
experiments to further demonstrate more characteristics of ClozE in terms of
performance and speed. In addition, we conduct an experimental analysis of the
limitations of ClozE, which suggests future research directions. The code and
models for ClozE will be released upon the paper acceptance."
12087,"To further study how biases are learnt during the
ﬁne-tuning process, we track the extrinsic bias                             Furthermore, according to the intrinsic evalua-
score of an MLM (and its debiased variants) over                         tions in Figure 1, the biases were reduced in both
the number of ﬁne-tuning training iterations on a                        roberta-l and bert-bu by all debiasing methods.","This shows that
                                                                         careful evaluation using various evaluation datasets
5.1 Re-learning Biases via Fine-Tuning                                   and MLMs is necessary when verifying the effec-
                                                                         tiveness of debiasing methods.",particular downstream task.,2022-10-06 14:08:57+00:00,Debiasing isn't enough! -- On the Effectiveness of Debiasing MLMs and their Social Biases in Downstream Tasks,cs.CL,['cs.CL'],"[arxiv.Result.Author('Masahiro Kaneko'), arxiv.Result.Author('Danushka Bollegala'), arxiv.Result.Author('Naoaki Okazaki')]","We study the relationship between task-agnostic intrinsic and task-specific
extrinsic social bias evaluation measures for Masked Language Models (MLMs),
and find that there exists only a weak correlation between these two types of
evaluation measures. Moreover, we find that MLMs debiased using different
methods still re-learn social biases during fine-tuning on downstream tasks. We
identify the social biases in both training instances as well as their assigned
labels as reasons for the discrepancy between intrinsic and extrinsic bias
evaluation measurements. Overall, our findings highlight the limitations of
existing MLM bias evaluation measures and raise concerns on the deployment of
MLMs in downstream applications using those measures."
12090,"sources settings, encourage further research aiming
to address the problem of large action space for TG   Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri
in richer data settings by adapting and extending        Abend.","ArXiv, abs/1902.00183.",2019.,2022-10-06 16:58:27+00:00,Reinforcement Learning with Large Action Spaces for Neural Machine Translation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Asaf Yehudai'), arxiv.Result.Author('Leshem Choshen'), arxiv.Result.Author('Lior Fox'), arxiv.Result.Author('Omri Abend')]","Applying Reinforcement learning (RL) following maximum likelihood estimation
(MLE) pre-training is a versatile method for enhancing neural machine
translation (NMT) performance. However, recent work has argued that the gains
produced by RL for NMT are mostly due to promoting tokens that have already
received a fairly high probability in pre-training. We hypothesize that the
large action space is a main obstacle to RL's effectiveness in MT, and conduct
two sets of experiments that lend support to our hypothesis. First, we find
that reducing the size of the vocabulary improves RL's effectiveness. Second,
we find that effectively reducing the dimension of the action space without
changing the vocabulary also yields notable improvement as evaluated by BLEU,
semantic similarity, and human evaluation. Indeed, by initializing the
network's final fully connected layer (that maps the network's internal
dimension to the vocabulary dimension), with a layer that generalizes over
similar actions, we obtain a substantial improvement in RL performance: 1.5
BLEU points on average."
12097,"level 2 or level 3 shaking, or “weak shaking.” … (307 more words)
                                                                                               Classiﬁer accuracy ablation We further study
Method Ctrl Headline                                                                           how the generation performance depends on the
                                                                                               classiﬁer accuracy.","What was actually      noisy samples, whose attributes contradict with the
felt Thursday in Los Angeles County, while seemingly scary, was actually not that bad either   control codes, would hurt controllability.","We replace the strong clas-
Reference  short  ShakeAlertLA Did Not Fail Its Duty                                           siﬁers (Roberta-base for MReD and BERT-base-
BART+CTRL  long   Find out before the 6.4 magnitude quake why LA's early warning system        uncased for Search Ads) with weaker classiﬁers: l2-
           short  did not send an alert                                                        regularized logistic regression using bag-of-words
           long   Did Los Angeles' ShakeAlertLA app fail to warn of earthquakes?",2022-10-06 19:00:51+00:00,FAST: Improving Controllability for Text Generation with Feedback Aware Self-Training,cs.CL,['cs.CL'],"[arxiv.Result.Author('Junyi Chai'), arxiv.Result.Author('Reid Pryzant'), arxiv.Result.Author('Victor Ye Dong'), arxiv.Result.Author('Konstantin Golobokov'), arxiv.Result.Author('Chenguang Zhu'), arxiv.Result.Author('Yi Liu')]","Controllable text generation systems often leverage control codes to direct
various properties of the output like style and length. Inspired by recent work
on causal inference for NLP, this paper reveals a previously overlooked flaw in
these control code-based conditional text generation algorithms. Spurious
correlations in the training data can lead models to incorrectly rely on parts
of the input other than the control code for attribute selection, significantly
undermining downstream generation quality and controllability. We demonstrate
the severity of this issue with a series of case studies and then propose two
simple techniques to reduce these correlations in training sets. The first
technique is based on resampling the data according to an example's propensity
towards each linguistic attribute (IPS). The second produces multiple
counterfactual versions of each example and then uses an additional feedback
mechanism to remove noisy examples (feedback aware self-training, FAST). We
evaluate on 3 tasks -- news headline, meta review, and search ads generation --
and demonstrate that FAST can significantly improve the controllability and
language quality of generated outputs when compared to state-of-the-art
controllable text generation approaches."
12102,"We believe that MULTIPIT will
facilitate further research in both paraphrase identi-  Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-
ﬁcation and paraphrase generation.",phrase datasets.,"Gazpio, and Lucia Specia.",2022-10-06 22:00:56+00:00,Improving Large-scale Paraphrase Acquisition and Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yao Dou'), arxiv.Result.Author('Chao Jiang'), arxiv.Result.Author('Wei Xu')]","This paper addresses the quality issues in existing Twitter-based paraphrase
datasets, and discusses the necessity of using two separate definitions of
paraphrase for identification and generation tasks. We present a new
Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of
130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert
(MultiPIT_expert) annotations using two different paraphrase definitions for
paraphrase identification, in addition to a multi-reference test set
(MultiPIT_NMR) and a large automatically constructed training set
(MultiPIT_Auto) for paraphrase generation. With improved data annotation
quality and task-specific paraphrase definition, the best pre-trained language
model fine-tuned on our dataset achieves the state-of-the-art performance of
84.2 F1 for automatic paraphrase identification. Furthermore, our empirical
results also demonstrate that the paraphrase generation models trained on
MultiPIT_Auto generate more diverse and high-quality paraphrases compared to
their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and
ParaNMT."
12103,"We believe that MULTIPIT will             Clemens Winter, Chris Hesse, Mark Chen, Eric
facilitate further research in both paraphrase identi-     Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
ﬁcation and paraphrase generation.","Our paraphrase generation evaluation         Arvind Neelakantan, Pranav Shyam, Girish Sastry,
shows that models trained on our corpus have better        Amanda Askell, Sandhini Agarwal, Ariel Herbert-
generation quality and generalizability compared           Voss, Gretchen Krueger, Tom Henighan, Rewon
to models ﬁne-tuned on existing widely-used para-          Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
phrase datasets.","Jack Clark, Christopher Berner, Sam McCandlish,
                                                           Alec Radford, Ilya Sutskever, and Dario Amodei.",2022-10-06 22:00:56+00:00,Improving Large-scale Paraphrase Acquisition and Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yao Dou'), arxiv.Result.Author('Chao Jiang'), arxiv.Result.Author('Wei Xu')]","This paper addresses the quality issues in existing Twitter-based paraphrase
datasets, and discusses the necessity of using two separate definitions of
paraphrase for identification and generation tasks. We present a new
Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of
130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert
(MultiPIT_expert) annotations using two different paraphrase definitions for
paraphrase identification, in addition to a multi-reference test set
(MultiPIT_NMR) and a large automatically constructed training set
(MultiPIT_Auto) for paraphrase generation. With improved data annotation
quality and task-specific paraphrase definition, the best pre-trained language
model fine-tuned on our dataset achieves the state-of-the-art performance of
84.2 F1 for automatic paraphrase identification. Furthermore, our empirical
results also demonstrate that the paraphrase generation models trained on
MultiPIT_Auto generate more diverse and high-quality paraphrases compared to
their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and
ParaNMT."
12104,"The views and conclusions con-
facilitate further research in both paraphrase identi-  tained herein are those of the authors and should
ﬁcation and paraphrase generation.",We believe that MULTIPIT will          eryone Award.,"not be interpreted as necessarily representing the of-
                                                        ﬁcial policies, either expressed or implied, of NSF,
Limitations                                             ODNI, IARPA, or the U.S. Government.",2022-10-06 22:00:56+00:00,Improving Large-scale Paraphrase Acquisition and Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yao Dou'), arxiv.Result.Author('Chao Jiang'), arxiv.Result.Author('Wei Xu')]","This paper addresses the quality issues in existing Twitter-based paraphrase
datasets, and discusses the necessity of using two separate definitions of
paraphrase for identification and generation tasks. We present a new
Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of
130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert
(MultiPIT_expert) annotations using two different paraphrase definitions for
paraphrase identification, in addition to a multi-reference test set
(MultiPIT_NMR) and a large automatically constructed training set
(MultiPIT_Auto) for paraphrase generation. With improved data annotation
quality and task-specific paraphrase definition, the best pre-trained language
model fine-tuned on our dataset achieves the state-of-the-art performance of
84.2 F1 for automatic paraphrase identification. Furthermore, our empirical
results also demonstrate that the paraphrase generation models trained on
MultiPIT_Auto generate more diverse and high-quality paraphrases compared to
their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and
ParaNMT."
12106,Understanding online health in-            agenda for further research.,"On-
Elisabeth Beaunoyer, Marianne Arsenault,               line health information seeking by parents
   Anna M Lomanowska, and Matthieu J                   for their children: systematic review and
   Guitton.","Journal of
   formation: Evaluation, tools, and strate-           Medical Internet Research, 22(8):e19985,
   gies.",2022-10-06 23:18:24+00:00,HealthE: Classifying Entities in Online Textual Health Advice,cs.CL,['cs.CL'],"[arxiv.Result.Author('Joseph Gatto'), arxiv.Result.Author('Parker Seegmiller'), arxiv.Result.Author('Garrett Johnston'), arxiv.Result.Author('Sarah M. Preum')]","The processing of entities in natural language is essential to many medical
NLP systems. Unfortunately, existing datasets vastly under-represent the
entities required to model public health relevant texts such as health advice
often found on sites like WebMD. People rely on such information for personal
health management and clinically relevant decision making. In this work, we
release a new annotated dataset, HealthE, consisting of 6,756 health advice.
HealthE has a more granular label space compared to existing medical NER
corpora and contains annotation for diverse health phrases. Additionally, we
introduce a new health entity classification model, EP S-BERT, which leverages
textual context patterns in the classification of entity classes. EP S-BERT
provides a 4-point increase in F1 score over the nearest baseline and a
34-point increase in F1 when compared to off-the-shelf medical NER tools
trained to extract disease and medication mentions from clinical texts. All
code and data are publicly available on Github."
12127,"Also, our datasets have at most      Ioana Croitoru, Simion-Vlad Bogolin, Marius
9 languages, and it will take further research to         Leordeanu, Hailin Jin, Andrew Zisserman, Samuel
develop massively multilingual text-video retrieval.","machine translation models and multilingual text
encoders improve.","Albanie, and Yang Liu.",2022-10-07 15:30:24+00:00,C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval,cs.CL,"['cs.CL', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Andrew Rouditchenko'), arxiv.Result.Author('Yung-Sung Chuang'), arxiv.Result.Author('Nina Shvetsova'), arxiv.Result.Author('Samuel Thomas'), arxiv.Result.Author('Rogerio Feris'), arxiv.Result.Author('Brian Kingsbury'), arxiv.Result.Author('Leonid Karlinsky'), arxiv.Result.Author('David Harwath'), arxiv.Result.Author('Hilde Kuehne'), arxiv.Result.Author('James Glass')]","Multilingual text-video retrieval methods have improved significantly in
recent years, but the performance for other languages lags behind English. We
propose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve
multilingual text-video retrieval. Inspired by the fact that English text-video
retrieval outperforms other languages, we train a student model using input
text in different languages to match the cross-modal predictions from teacher
models using input text in English. We propose a cross entropy based objective
which forces the distribution over the student's text-video similarity scores
to be similar to those of the teacher models. We introduce a new multilingual
video dataset, Multi-YouCook2, by translating the English captions in the
YouCook2 video dataset to 8 other languages. Our method improves multilingual
text-video retrieval performance on Multi-YouCook2 and several other datasets
such as Multi-MSRVTT and VATEX. We also conducted an analysis on the
effectiveness of different multilingual text models as teachers."
12129,"However, the
latter still needs further research for its systematic application in a produc-
tion environment.","to inject structured
knowledge about entities and the relations between them from a pre-existing
knowledge base into a pre-trained language model (Peters et al., 2019; Wang
et al., 2021b), enabling domain adaptation at a limited cost.","Many NLP tasks can be solved equally well by using a machine learning-
based approach or a symbolic approach.",2022-10-07 15:50:17+00:00,Artificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('José Manuel Gómez-Pérez'), arxiv.Result.Author('Andrés García-Silva'), arxiv.Result.Author('Rosemarie Leone'), arxiv.Result.Author('Mirko Albani'), arxiv.Result.Author('Moritz Fontaine'), arxiv.Result.Author('Charles Poncet'), arxiv.Result.Author('Leopold Summerer'), arxiv.Result.Author('Alessandro Donati'), arxiv.Result.Author('Ilaria Roma'), arxiv.Result.Author('Stefano Scaglioni')]","The European Space Agency is well known as a powerful force for scientific
discovery in numerous areas related to Space. The amount and depth of the
knowledge produced throughout the different missions carried out by ESA and
their contribution to scientific progress is enormous, involving large
collections of documents like scientific publications, feasibility studies,
technical reports, and quality management procedures, among many others.
Through initiatives like the Open Space Innovation Platform, ESA also acts as a
hub for new ideas coming from the wider community across different challenges,
contributing to a virtuous circle of scientific discovery and innovation.
Handling such wealth of information, of which large part is unstructured text,
is a colossal task that goes beyond human capabilities, hence requiring
automation. In this paper, we present a methodological framework based on
artificial intelligence and natural language processing and understanding to
automatically extract information from Space documents, generating value from
it, and illustrate such framework through several case studies implemented
across different functional areas of ESA, including Mission Design, Quality
Assurance, Long-Term Data Preservation, and the Open Space Innovation Platform.
In doing so, we demonstrate the value of these technologies in several tasks
ranging from effortlessly searching and recommending Space information to
automatically determining how innovative an idea can be, answering questions
about Space, and generating quizzes regarding quality procedures. Each of these
accomplishments represents a step forward in the application of increasingly
intelligent AI systems in Space, from structuring and facilitating information
access to intelligent systems capable to understand and reason with such
information."
12153,"We commit to make our
code and data publicly available upon acceptance to facilitate reproduction and further research.","D.8 SCIENTIFIC ARTIFACT DETAILS

KALM is built with the help of many existing scientiﬁc artifacts, including TagMe (Ferragina &
Scaiella, 2011), pytorch (Paszke et al., 2019), pytorch lightning (Falcon & The PyTorch Lightning
team, 2019), transformers (Wolf et al., 2020), pytorch geometric (Fey & Lenssen, 2019), sklearn
(Pedregosa et al., 2011), numpy (Harris et al., 2020), nltk (Bird et al., 2009), and the three adopted
knowledge graphs (Feng et al., 2021a; Hu et al., 2021; Speer et al., 2017).",20,2022-10-08 20:51:02+00:00,"KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",cs.CL,['cs.CL'],"[arxiv.Result.Author('Shangbin Feng'), arxiv.Result.Author('Zhaoxuan Tan'), arxiv.Result.Author('Wenqian Zhang'), arxiv.Result.Author('Zhenyu Lei'), arxiv.Result.Author('Yulia Tsvetkov')]","With the advent of pre-trained language models (LMs), increasing research
efforts have been focusing on infusing commonsense and domain-specific
knowledge to prepare LMs for downstream tasks. These works attempt to leverage
knowledge graphs, the de facto standard of symbolic knowledge representation,
along with pre-trained LMs. While existing approaches leverage external
knowledge, it remains an open question how to jointly incorporate knowledge
graphs representing varying contexts, from local (e.g., sentence), to
document-level, to global knowledge, to enable knowledge-rich and interpretable
exchange across these contexts. Such rich contextualization can be especially
beneficial for long document understanding tasks since standard pre-trained LMs
are typically bounded by the input sequence length. In light of these
challenges, we propose KALM, a Knowledge-Aware Language Model that jointly
leverages knowledge in local, document-level, and global contexts for long
document understanding. KALM first encodes long documents and knowledge graphs
into the three knowledge-aware context representations. It then processes each
context with context-specific layers, followed by a context fusion layer that
facilitates interpretable knowledge exchange to derive an overarching document
representation. Extensive experiments demonstrate that KALM achieves
state-of-the-art performance on three long document understanding tasks across
6 datasets/settings. Further analyses reveal that the three knowledge-aware
contexts are complementary and they all contribute to model performance, while
the importance and information exchange patterns of different contexts vary
with respect to different tasks and datasets."
12167,We note that this doesn’t neces-      requires further research for improvement.,"Interestingly, in the NO_SRC          Overall, this highlights some limitations of the
setting, the largest model has comparable perfor-      InstructGPT model for analogical reasoning, which
mance to humans.","sarily mean that those models are creative or have
commonsense reasoning skills as they could have        5.4.3 Error Analysis
simply memorized those analogies, which a known
problem of such models (Bender et al., 2021).",2022-10-09 06:35:14+00:00,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bhavya Bhavya'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Chengxiang Zhai')]","We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
12168,"It       The annotators were also asked to explain their an-
requires further research to test whether the models   swer choice (i.e, meaningful analogy or not).","sarily mean that those models are creative or have
commonsense reasoning skills as they could have        5.4.3 Error Analysis
simply memorized those analogies, which a known
problem of such models (Bender et al., 2021).","By in-
generate novel analogies unseen during training.",2022-10-09 06:35:14+00:00,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bhavya Bhavya'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Chengxiang Zhai')]","We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
12169,We note that this doesn’t neces-      requires further research for improvement.,"Interestingly, in the NO_SRC          Overall, this highlights some limitations of the
setting, the largest model has comparable perfor-      InstructGPT model for analogical reasoning, which
mance to humans.","sarily mean that those models are creative or have
commonsense reasoning skills as they could have        5.4.3 Error Analysis
simply memorized those analogies, which a known
problem of such models (Bender et al., 2021).",2022-10-09 06:35:14+00:00,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bhavya Bhavya'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Chengxiang Zhai')]","We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
12170,"It       The annotators were also asked to explain their an-
requires further research to test whether the models   swer choice (i.e, meaningful analogy or not).","sarily mean that those models are creative or have
commonsense reasoning skills as they could have        5.4.3 Error Analysis
simply memorized those analogies, which a known
problem of such models (Bender et al., 2021).","By in-
generate novel analogies unseen during training.",2022-10-09 06:35:14+00:00,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bhavya Bhavya'), arxiv.Result.Author('Jinjun Xiong'), arxiv.Result.Author('Chengxiang Zhai')]","We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task."
12174,do further research and exploration.,"In general, the above
limitations are of practical meaning and need us to     Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.",2019.,2022-10-09 10:37:43+00:00,Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Fei Zhao'), arxiv.Result.Author('Yuchen Shen'), arxiv.Result.Author('Zhen Wu'), arxiv.Result.Author('Xinyu Dai')]","Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of
aspect-based sentiment analysis, which aims to detect aspect categories
accurately with limited training instances. Recently, dominant works use the
prototypical network to accomplish this task, and employ the attention
mechanism to extract keywords of aspect category from the sentences to produce
the prototype for each aspect. However, they still suffer from serious noise
problems: (1) due to lack of sufficient supervised data, the previous methods
easily catch noisy words irrelevant to the current aspect category, which
largely affects the quality of the generated prototype; (2) the
semantically-close aspect categories usually generate similar prototypes, which
are mutually noisy and confuse the classifier seriously. In this paper, we
resort to the label information of each aspect to tackle the above problems,
along with proposing a novel Label-Driven Denoising Framework (LDF). Extensive
experimental results show that our framework achieves better performance than
other state-of-the-art methods."
12176,"The
                                               public release of our NEWS-COPY de-duplication dataset will facilitate further research and applications.","We show that the bi-encoder
                                               scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours.","1 Introduction

                                       Robust identiﬁcation of near-duplicate texts in large, noisy corpora is important for a variety of applications.",2022-10-09 13:30:42+00:00,Noise-Robust De-Duplication at Scale,cs.CL,['cs.CL'],"[arxiv.Result.Author('Emily Silcock'), arxiv.Result.Author(""Luca D'Amico-Wong""), arxiv.Result.Author('Jinglin Yang'), arxiv.Result.Author('Melissa Dell')]","Identifying near duplicates within large, noisy text corpora has a myriad of
applications that range from de-duplicating training datasets, reducing privacy
risk, and evaluating test set leakage, to identifying reproduced news articles
and literature within large corpora. Across these diverse applications, the
overwhelming majority of work relies on N-grams. Limited efforts have been made
to evaluate how well N-gram methods perform, in part because it is unclear how
one could create an unbiased evaluation dataset for a massive corpus. This
study uses the unique timeliness of historical news wires to create a 27,210
document dataset, with 122,876 positive duplicate pairs, for studying
noise-robust de-duplication. The time-sensitivity of news makes comprehensive
hand labelling feasible - despite the massive overall size of the corpus - as
duplicates occur within a narrow date range. The study then develops and
evaluates a range of de-duplication methods: hashing and N-gram overlap (which
predominate in the literature), a contrastively trained bi-encoder, and a
re-rank style approach combining a bi- and cross-encoder. The neural approaches
significantly outperform hashing and N-gram overlap. We show that the
bi-encoder scales well, de-duplicating a 10 million article corpus on a single
GPU card in a matter of hours. The public release of our NEWS-COPY
de-duplication dataset will facilitate further research and applications."
12177,"The resulting
public NEWS-COPY dataset - which contains 27,210 articles, comprising 122,876 positive duplicate pairs - aims
to encourage further study of robust de-duplication.","Additional data, spanning the period from 1920 to 1977, were compiled for model training.","In the absence of evaluation data, the literature has largely assumed that text de-duplication is suﬃciently
simple that neural methods are not required.",2022-10-09 13:30:42+00:00,Noise-Robust De-Duplication at Scale,cs.CL,['cs.CL'],"[arxiv.Result.Author('Emily Silcock'), arxiv.Result.Author(""Luca D'Amico-Wong""), arxiv.Result.Author('Jinglin Yang'), arxiv.Result.Author('Melissa Dell')]","Identifying near duplicates within large, noisy text corpora has a myriad of
applications that range from de-duplicating training datasets, reducing privacy
risk, and evaluating test set leakage, to identifying reproduced news articles
and literature within large corpora. Across these diverse applications, the
overwhelming majority of work relies on N-grams. Limited efforts have been made
to evaluate how well N-gram methods perform, in part because it is unclear how
one could create an unbiased evaluation dataset for a massive corpus. This
study uses the unique timeliness of historical news wires to create a 27,210
document dataset, with 122,876 positive duplicate pairs, for studying
noise-robust de-duplication. The time-sensitivity of news makes comprehensive
hand labelling feasible - despite the massive overall size of the corpus - as
duplicates occur within a narrow date range. The study then develops and
evaluates a range of de-duplication methods: hashing and N-gram overlap (which
predominate in the literature), a contrastively trained bi-encoder, and a
re-rank style approach combining a bi- and cross-encoder. The neural approaches
significantly outperform hashing and N-gram overlap. We show that the
bi-encoder scales well, de-duplicating a 10 million article corpus on a single
GPU card in a matter of hours. The public release of our NEWS-COPY
de-duplication dataset will facilitate further research and applications."
12184,"Additionally, topological
principles of these diagrams and many potential avenues for further research are
proposed.","The germane
differences between the English and Japanese languages are emphasized to help address
English language bias in the current body of research.",Why is this endeavor important?,2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12185,"assert that studying other languages is an important area of further research in
QNLP (2021).","The present study differs from prior research in that it not only aims to
contribute to QNLP research, but also intends to cover blind spots in the literature that
may exist because of English language bias.2 To achieve this, diagrammatic reasoning,
which is based on category theory, will be used throughout the work to contribute new

2 Abbaszade et al.","9
QUANTUM SELF-MOVE AND OTHER-MOVE IN JAPANESE  10

diagrams to the field to aid further research while specifying general topological
principles for said diagrams.",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12186,"9
QUANTUM SELF-MOVE AND OTHER-MOVE IN JAPANESE  10

diagrams to the field to aid further research while specifying general topological
principles for said diagrams.","assert that studying other languages is an important area of further research in
QNLP (2021).","Background: Category Theory

         All categories are made up of two types of entities, “objects and morphisms”
(van de Wetering, 2020).",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12187,This is an avenue for further research.,"It is possible that CCG approaches to grammars that allow for
planar graphs could also be applied in Japanese to address these utterances (see Figure
10).","b The term “perfective” here is used in widely the same way as past tense is used in
English, and for the purposes of this work that understanding is sufficient.",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12188,"Indeed, reducing these types to a minimal subset would be a worthy

                                                                                                                 44
QUANTUM SELF-MOVE AND OTHER-MOVE IN JAPANESE  45

endeavor for further research, that topic is beyond the present scope.","The types presented in Table 3 seem much more complicated than one might
expect given that Japanese words reduce primarily to nouns, adjectives, and verbs
(Dolly, 2019b).","Further, while
terms like nominative, genitive, dative, accusative, and locative refer to Western
grammatical ideas, they help when considering how to parse a Japanese sentence and
translate it into the English language.",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12189,"This assertion remains yet untested and is an avenue for
further research.","Thus, it is likely that parsing programs would benefit from inserting the missing copula
at the end of the sentence when it is omitted, which would provide a degree of
uniformity across sentences.","67
QUANTUM SELF-MOVE AND OTHER-MOVE IN JAPANESE  68

          Now, what about the Train Engine Principle?",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12190,"More tightly formulating the solution to this problem, working with
larger blocks of text, and testing out potential solutions is an area for further research.","For now, including the topic marker in this way is a potential solution that
would involve a minimal amount of special consideration when working with the
Japanese language.","Same Old Verb, Different Forms

         Obviously, verbs are another part of speech that can produce a self-move
sentence.",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12191,"Especially deft
speakers can say an incredible amount with a minimal number of words.29 An other-

29 Exploring this idea is itself an area for further research.","This is

possible due to the high-context nature of the Japanese language.","For a quantum computer to provide
consistent and fluent translation outputs, it may be necessary for the translation algorithm to
insert implicit subjects and particles.",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12192,"Covering this in more detail is
beyond the scope of the current discussion, but it is a potential area for further research.","The two sometimes overlap enough in
meaning that the choice does not particularly matter, but often one sounds more natural than the
other depending on the context and content of the sentence.",Capturing the subtlety in semantics is critical.,2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12193,"Quantum implementations of “godan” verbs is
another potential avenue for further research beyond the scope of this work.","The five different vowel sounds each represent a different basic meaning
alteration applied to the core idea of the word.","35 This is not the only potential difference between self-move and other-move verbs, but it is the
difference that applies in this example.",2022-10-10 06:26:59+00:00,Self-move and Other-move: Quantum Categorical Foundations of Japanese,cs.CL,['cs.CL'],[arxiv.Result.Author('Ryder Dale Walton')],"The purpose of this work is to contribute toward the larger goal of creating
a Quantum Natural Language Processing (QNLP) translator program. This work
contributes original diagrammatic representations of the Japanese language
based on prior work that accomplished on the English language based on category
theory. The germane differences between the English and Japanese languages are
emphasized to help address English language bias in the current body of
research. Additionally, topological principles of these diagrams and many
potential avenues for further research are proposed. Why is this endeavor
important? Hundreds of languages have developed over the course of millennia
coinciding with the evolution of human interaction across time and geographic
location. These languages are foundational to human survival, experience,
flourishing, and living the good life. They are also, however, the strongest
barrier between people groups. Over the last several decades, advancements in
Natural Language Processing (NLP) have made it easier to bridge the gap between
individuals who do not share a common language or culture. Tools like Google
Translate and DeepL make it easier than ever before to share our experiences
with people globally. Nevertheless, these tools are still inadequate as they
fail to convey our ideas across the language barrier fluently, leaving people
feeling anxious and embarrassed. This is particularly true of languages born
out of substantially different cultures, such as English and Japanese. Quantum
computers offer the best chance to achieve translation fluency in that they are
better suited to simulating the natural world and natural phenomenon such as
natural speech.
  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English
grammar, translation, topology, Quantum Natural Language Processing, Natural
Language Processing"
12197,"Thus, we choose the
representative BERT for further research and ver-
Dataset / (λp, λr)                        Trec (0.2, 0.3)  AG-News (0.2, 0.3)                                 IMDB (0.1, 0.5)

Data Size                                 5,453 (All)      5,000         120,000 (All)                        5,000                                                         45,000 (All)

Noise Ratio (%)                           20     40        20 40 20 40                  20 40 20 40

BERT best                                 95.52  89.04     89.55  80.90  93.38  91.59   88.51                                                                        80.81  92.67  87.70
                                    last  93.48  69.88     84.40  62.33  90.32  74.04   81.20                                                                        63.55  87.40  61.82
                                          95.96  92.76     89.70  87.24  93.43  92.03   88.81                                                                        84.39  92.94  88.45
BERT+Co-Teaching best                     95.32  90.08     88.77  82.53  93.01  85.03   88.24                                                                        82.68  91.68  84.43
                                    last  96.37  91.14     89.45  85.81  92.93  90.96   88.57                                                                        81.75  92.71  87.94
                                          95.98  87.24     89.12  79.82  92.87  90.41   88.33                                                                        81.23  92.69  87.07
BERT+Co-Teaching+ best                    94.72  91.28     89.62  86.72  93.13  90.78   88.76                                                                        82.65  92.82  87.32
                                    last  94.04  82.44     89.43  74.37  93.03  87.34   87.74                                                                        74.38  92.77  82.52
                                          96.08  92.16     89.88  85.43  93.63  92.00   88.70                                                                        82.45  93.13  87.62
BERT+SCE best                             95.40  88.28     89.47  81.24  93.30  90.67   87.76                                                                        72.71  92.50  79.54
                                    last  95.92  91.80     89.83  84.77  93.57  91.96   89.05                                                                        81.65  92.66  87.13
                                          95.36  88.64     89.27  78.48  93.38  89.97   88.62                                                                        77.93  92.52  83.39
BERT+ELR best                             96.00  90.92     89.35  81.35  93.54  92.09   88.70                                                                        81.21  92.93  88.47
                                    last  94.84  79.76     85.41  63.26  93.47  84.55   88.41                                                                        74.62  92.28  86.60
                                          96.32  94.12     89.90  88.80  93.39  92.79   89.20                                                                        86.38  93.30  90.19
BERT+Conﬁdent-Learning best               96.04  93.80     89.79  88.63  93.04  92.40   88.84                                                                        86.38  92.86  90.12
                                    last

BERT+NM-Net best
                                    last

BERT+SelfMix best
                                    last

Table 3: Average test accuracy (%) of ﬁve runs on the Trec, AG-News, and IMDB datasets with different data
sizes under different ratios of asymmetric noise.","Table 2
shows that the pre-trained model is more robust
than traditional networks when dealing with label
noise in text classiﬁcation.","The results with outstanding improvement over the base model
are bolded, and underline values indicate the statistically signiﬁcantly better (by paired bootstrap test, p < 0.05)
performances than BERT.",2022-10-10 09:46:40+00:00,SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training,cs.CL,['cs.CL'],"[arxiv.Result.Author('Dan Qiao'), arxiv.Result.Author('Chenchen Dai'), arxiv.Result.Author('Yuyang Ding'), arxiv.Result.Author('Juntao Li'), arxiv.Result.Author('Qiang Chen'), arxiv.Result.Author('Wenliang Chen'), arxiv.Result.Author('Min Zhang')]","The conventional success of textual classification relies on annotated data,
and the new paradigm of pre-trained language models (PLMs) still requires a few
labeled data for downstream tasks. However, in real-world applications, label
noise inevitably exists in training data, damaging the effectiveness,
robustness, and generalization of the models constructed on such data.
Recently, remarkable achievements have been made to mitigate this dilemma in
visual data, while only a few explore textual data. To fill this gap, we
present SelfMix, a simple yet effective method, to handle label noise in text
classification tasks. SelfMix uses the Gaussian Mixture Model to separate
samples and leverages semi-supervised learning. Unlike previous works requiring
multiple models, our method utilizes the dropout mechanism on a single model to
reduce the confirmation bias in self-training and introduces a textual-level
mixup training strategy. Experimental results on three text classification
benchmarks with different types of text show that the performance of our
proposed method outperforms these strong baselines designed for both textual
and visual data under different noise ratios and noise types. Our anonymous
code is available at \url{https://github.com/noise-learning/SelfMix}."
12198,"Thus, we choose the
representative BERT for further research and ver-
Dataset / (λp, λr)                        Trec (0.2, 0.3)  AG-News (0.2, 0.3)                                 IMDB (0.1, 0.5)

Data Size                                 5,453 (All)      5,000         120,000 (All)                        5,000                                                         45,000 (All)

Noise Ratio (%)                           20     40        20 40 20 40                  20 40 20 40

BERT best                                 95.52  89.04     89.55  80.90  93.38  91.59   88.51                                                                        80.81  92.67  87.70
                                    last  93.48  69.88     84.40  62.33  90.32  74.04   81.20                                                                        63.55  87.40  61.82
                                          95.96  92.76     89.70  87.24  93.43  92.03   88.81                                                                        84.39  92.94  88.45
BERT+Co-Teaching best                     95.32  90.08     88.77  82.53  93.01  85.03   88.24                                                                        82.68  91.68  84.43
                                    last  96.37  91.14     89.45  85.81  92.93  90.96   88.57                                                                        81.75  92.71  87.94
                                          95.98  87.24     89.12  79.82  92.87  90.41   88.33                                                                        81.23  92.69  87.07
BERT+Co-Teaching+ best                    94.72  91.28     89.62  86.72  93.13  90.78   88.76                                                                        82.65  92.82  87.32
                                    last  94.04  82.44     89.43  74.37  93.03  87.34   87.74                                                                        74.38  92.77  82.52
                                          96.08  92.16     89.88  85.43  93.63  92.00   88.70                                                                        82.45  93.13  87.62
BERT+SCE best                             95.40  88.28     89.47  81.24  93.30  90.67   87.76                                                                        72.71  92.50  79.54
                                    last  95.92  91.80     89.83  84.77  93.57  91.96   89.05                                                                        81.65  92.66  87.13
                                          95.36  88.64     89.27  78.48  93.38  89.97   88.62                                                                        77.93  92.52  83.39
BERT+ELR best                             96.00  90.92     89.35  81.35  93.54  92.09   88.70                                                                        81.21  92.93  88.47
                                    last  94.84  79.76     85.41  63.26  93.47  84.55   88.41                                                                        74.62  92.28  86.60
                                          96.32  94.12     89.90  88.80  93.39  92.79   89.20                                                                        86.38  93.30  90.19
BERT+Conﬁdent-Learning best               96.04  93.80     89.79  88.63  93.04  92.40   88.84                                                                        86.38  92.86  90.12
                                    last

BERT+NM-Net best
                                    last

BERT+SelfMix best
                                    last

Table 3: Average test accuracy (%) of ﬁve runs on the Trec, AG-News, and IMDB datasets with different data
sizes under different ratios of asymmetric noise.","Table 2
shows that the pre-trained model is more robust
than traditional networks when dealing with label
noise in text classiﬁcation.","The results with outstanding improvement over the base model
are bolded, and underline values indicate the statistically signiﬁcantly better (by paired bootstrap test, p < 0.05)
performances than BERT.",2022-10-10 09:46:40+00:00,SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training,cs.CL,['cs.CL'],"[arxiv.Result.Author('Dan Qiao'), arxiv.Result.Author('Chenchen Dai'), arxiv.Result.Author('Yuyang Ding'), arxiv.Result.Author('Juntao Li'), arxiv.Result.Author('Qiang Chen'), arxiv.Result.Author('Wenliang Chen'), arxiv.Result.Author('Min Zhang')]","The conventional success of textual classification relies on annotated data,
and the new paradigm of pre-trained language models (PLMs) still requires a few
labeled data for downstream tasks. However, in real-world applications, label
noise inevitably exists in training data, damaging the effectiveness,
robustness, and generalization of the models constructed on such data.
Recently, remarkable achievements have been made to mitigate this dilemma in
visual data, while only a few explore textual data. To fill this gap, we
present SelfMix, a simple yet effective method, to handle label noise in text
classification tasks. SelfMix uses the Gaussian Mixture Model to separate
samples and leverages semi-supervised learning. Unlike previous works requiring
multiple models, our method utilizes the dropout mechanism on a single model to
reduce the confirmation bias in self-training and introduces a textual-level
mixup training strategy. Experimental results on three text classification
benchmarks with different types of text show that the performance of our
proposed method outperforms these strong baselines designed for both textual
and visual data under different noise ratios and noise types. Our code is
available at https://github.com/noise-learning/SelfMix."
12227,"And we will make the source             whole article dataset from the PLOS corporation
public for further research.","We draw the full text, abstracts, and plain          Ethics Statement
language summaries from papers published in peer-
reviewed biomedical journals to build the ﬁrst cor-       This paper presents a corpus built upon part of the
pus for the task.",We propose an effec-         which is freely open to the public.,2022-10-10 14:03:20+00:00,Readability Controllable Biomedical Document Summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zheheng Luo'), arxiv.Result.Author('Qianqian Xie'), arxiv.Result.Author('Sophia Ananiadou')]","Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task."
12228,"And we will make the source             vancement of large pre-trained language models
public for further research.","The ad-
pus for the task.","We propose an effec-         has greatly boosted the improvement of summa-
tive MLM-based readability metric and its variant,        rization models in various domains including the
which outperform traditional and previous MLM-            biomedical area.",2022-10-10 14:03:20+00:00,Readability Controllable Biomedical Document Summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zheheng Luo'), arxiv.Result.Author('Qianqian Xie'), arxiv.Result.Author('Sophia Ananiadou')]","Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task."
12231,We perceive and released them for further research.,"Lastly, we also

ever, DABERTa fails to understand the user’s in- developed an extensive set of annotation guidelines

tention and yields the wrong span.","similar behavior from the best-performing base-

line, RoBERTa, as well.",2022-10-10 14:08:46+00:00,Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter,cs.CL,['cs.CL'],"[arxiv.Result.Author('Megha Sundriyal'), arxiv.Result.Author('Atharva Kulkarni'), arxiv.Result.Author('Vaibhav Pulastya'), arxiv.Result.Author('Md Shad Akhtar'), arxiv.Result.Author('Tanmoy Chakraborty')]","The widespread diffusion of medical and political claims in the wake of
COVID-19 has led to a voluminous rise in misinformation and fake news. The
current vogue is to employ manual fact-checkers to efficiently classify and
verify such data to combat this avalanche of claim-ridden misinformation.
However, the rate of information dissemination is such that it vastly outpaces
the fact-checkers' strength. Therefore, to aid manual fact-checkers in
eliminating the superfluous content, it becomes imperative to automatically
identify and extract the snippets of claim-worthy (mis)information present in a
post. In this work, we introduce the novel task of Claim Span Identification
(CSI). We propose CURT, a large-scale Twitter corpus with token-level claim
spans on more than 7.5k tweets. Furthermore, along with the standard token
classification baselines, we benchmark our dataset with DABERTa, an
adapter-based variation of RoBERTa. The experimental results attest that
DABERTa outperforms the baseline systems across several evaluation metrics,
improving by about 1.5 points. We also report detailed error analysis to
validate the model's performance along with the ablation studies. Lastly, we
release our comprehensive span annotation guidelines for public use."
12232,We perceive and released them for further research.,"Lastly, we also

ever, DABERTa fails to understand the user’s in- developed an extensive set of annotation guidelines

tention and yields the wrong span.","similar behavior from the best-performing base-

line, RoBERTa, as well.",2022-10-10 14:08:46+00:00,Empowering the Fact-checkers! Automatic Identification of Claim Spans on Twitter,cs.CL,['cs.CL'],"[arxiv.Result.Author('Megha Sundriyal'), arxiv.Result.Author('Atharva Kulkarni'), arxiv.Result.Author('Vaibhav Pulastya'), arxiv.Result.Author('Md Shad Akhtar'), arxiv.Result.Author('Tanmoy Chakraborty')]","The widespread diffusion of medical and political claims in the wake of
COVID-19 has led to a voluminous rise in misinformation and fake news. The
current vogue is to employ manual fact-checkers to efficiently classify and
verify such data to combat this avalanche of claim-ridden misinformation.
However, the rate of information dissemination is such that it vastly outpaces
the fact-checkers' strength. Therefore, to aid manual fact-checkers in
eliminating the superfluous content, it becomes imperative to automatically
identify and extract the snippets of claim-worthy (mis)information present in a
post. In this work, we introduce the novel task of Claim Span Identification
(CSI). We propose CURT, a large-scale Twitter corpus with token-level claim
spans on more than 7.5k tweets. Furthermore, along with the standard token
classification baselines, we benchmark our dataset with DABERTa, an
adapter-based variation of RoBERTa. The experimental results attest that
DABERTa outperforms the baseline systems across several evaluation metrics,
improving by about 1.5 points. We also report detailed error analysis to
validate the model's performance along with the ablation studies. Lastly, we
release our comprehensive span annotation guidelines for public use."
12258,"Finally, we
   In the case of Goethe, the maximum correla-          were able to demonstrate how to use W2VPred to
tion at the (solely spatio-temporal) resulting point    gain insight into the relation between 19th century
is relatively low and interestingly, the highest        authors from the German Text Archive and also
disagreement between W2VPred and the prior              how to raise further research questions for high lit-
knowledge is between Schiller and Goethe.","of our methods, regardless of whether (reliable)
                                                        structure information is given or not.",The           erature.,2022-10-06 12:45:48+00:00,Domain-Specific Word Embeddings with Structure Prediction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Stephanie Brandl'), arxiv.Result.Author('David Lassner'), arxiv.Result.Author('Anne Baillot'), arxiv.Result.Author('Shinichi Nakajima')]","Complementary to finding good general word embeddings, an important question
for representation learning is to find dynamic word embeddings, e.g., across
time or domain. Current methods do not offer a way to use or predict
information on structure between sub-corpora, time or domain and dynamic
embeddings can only be compared after post-alignment. We propose novel word
embedding methods that provide general word representations for the whole
corpus, domain-specific representations for each sub-corpus, sub-corpus
structure, and embedding alignment simultaneously. We present an empirical
evaluation on New York Times articles and two English Wikipedia datasets with
articles on science and philosophy. Our method, called Word2Vec with Structure
Prediction (W2VPred), provides better performance than baselines in terms of
the general analogy tests, domain-specific analogy tests, and multiple specific
word embedding evaluations as well as structure prediction performance when no
structure is given a priori. As a use case in the field of Digital Humanities
we demonstrate how to raise novel research questions for high literature from
the German Text Archive."
12270,"(2020) have classiﬁed    further study the effects of dataset augmentation
                                        Bangla in the language group that has substantial       on a synthetic dataset using masked language mod-
                                        lackings of efforts for labeled data collection and     eling.",Joshi et al.,"Finally, we demonstrate the quality of our
                                        preparation.",2022-10-11 02:52:31+00:00,BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ajwad Akil'), arxiv.Result.Author('Najrin Sultana'), arxiv.Result.Author('Abhik Bhattacharjee'), arxiv.Result.Author('Rifat Shahriyar')]","In this work, we present BanglaParaphrase, a high-quality synthetic Bangla
Paraphrase dataset curated by a novel filtering pipeline. We aim to take a step
towards alleviating the low resource status of the Bangla language in the NLP
domain through the introduction of BanglaParaphrase, which ensures quality by
preserving both semantics and diversity, making it particularly useful to
enhance other Bangla datasets. We show a detailed comparative analysis between
our dataset and models trained on it with other existing works to establish the
viability of our synthetic paraphrase data generation pipeline. We are making
the dataset and models publicly available at
https://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla
NLP."
12285,"Finally, we extensively address the limitations of the study to provide
                                                                                      directions for further research.","We examine the quality of the dataset
                                                                                      via representing summary statistics while setting strong baseline results using attention-based models
                                                                                      like BERT and DistilBERT.",1.,2022-10-10 08:23:57+00:00,DEPTWEET: A Typology for Social Media Texts to Detect Depression Severities,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mohsinul Kabir'), arxiv.Result.Author('Tasnim Ahmed'), arxiv.Result.Author('Md. Bakhtiar Hasan'), arxiv.Result.Author('Md Tahmid Rahman Laskar'), arxiv.Result.Author('Tarun Kumar Joarder'), arxiv.Result.Author('Hasan Mahmud'), arxiv.Result.Author('Kamrul Hasan')]","Mental health research through data-driven methods has been hindered by a
lack of standard typology and scarcity of adequate data. In this study, we
leverage the clinical articulation of depression to build a typology for social
media texts for detecting the severity of depression. It emulates the standard
clinical assessment procedure Diagnostic and Statistical Manual of Mental
Disorders (DSM-5) and Patient Health Questionnaire (PHQ-9) to encompass subtle
indications of depressive disorders from tweets. Along with the typology, we
present a new dataset of 40191 tweets labeled by expert annotators. Each tweet
is labeled as 'non-depressed' or 'depressed'. Moreover, three severity levels
are considered for 'depressed' tweets: (1) mild, (2) moderate, and (3) severe.
An associated confidence score is provided with each label to validate the
quality of annotation. We examine the quality of the dataset via representing
summary statistics while setting strong baseline results using attention-based
models like BERT and DistilBERT. Finally, we extensively address the
limitations of the study to provide directions for further research."
12290,"Thus, a further study can fulﬁl this        and Eros Zanchetta.","However, it suffers from some
In this work, we introduced an unsupervised           limitations that are worth highlighting:
method for the WSI task based on the tuning of        (1) This method is training a MIM model from
scratch for each target word proving a lack of gen-     Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
eralizability.",2009.,2022-10-11 13:04:06+00:00,Word Sense Induction with Hierarchical Clustering and Mutual Information Maximization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hadi Abdine'), arxiv.Result.Author('Moussa Kamal Eddine'), arxiv.Result.Author('Michalis Vazirgiannis'), arxiv.Result.Author('Davide Buscaldi')]","Word sense induction (WSI) is a difficult problem in natural language
processing that involves the unsupervised automatic detection of a word's
senses (i.e. meanings). Recent work achieves significant results on the WSI
task by pre-training a language model that can exclusively disambiguate word
senses, whereas others employ previously pre-trained language models in
conjunction with additional strategies to induce senses. In this paper, we
propose a novel unsupervised method based on hierarchical clustering and
invariant information clustering (IIC). The IIC is used to train a small model
to optimize the mutual information between two vector representations of a
target word occurring in a pair of synthetic paraphrases. This model is later
used in inference mode to extract a higher quality vector representation to be
used in the hierarchical clustering. We evaluate our method on two WSI tasks
and in two distinct clustering configurations (fixed and dynamic number of
clusters). We empirically demonstrate that, in certain cases, our approach
outperforms prior WSI state-of-the-art methods, while in others, it achieves a
competitive performance."
12313,"results for six out of seven evaluation settings,  Thus, further research is required to fully under-
                                             outperforming strong baselines based on large      stand the potentiality of interaction data to perform
                                             pre-trained language models.","Our experiments on seven pub-          sidering also interaction data, most of the them em-
                                             licly available datasets and four different lan-   ploy manually engineered features tailored to each
                                             guages show that combining our relational em-      speciﬁc data type (Espinosa et al., 2020; Lai et al.,
                                             beddings with textual methods helps to sub-        2021; Alkhalifa and Zubiaga, 2020) making it difﬁ-
                                             stantially improve performance, obtaining best     cult to generalize over other languages and targets.","stance detection and its relation with concepts such
                                                                                                as political homophily, political polarization, echo
                                        1 Introduction                                          chambers or demographic analysis (Conover et al.,
                                                                                                2011; Colleoni et al., 2014; Zubiaga et al., 2019).",2022-10-11 18:13:43+00:00,Relational Embeddings for Language Independent Stance Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Joseba Fernandez de Landa'), arxiv.Result.Author('Rodrigo Agerri')]","The large majority of the research performed on stance detection has been
focused on developing more or less sophisticated text classification systems,
even when many benchmarks are based on social network data such as Twitter.
This paper aims to take on the stance detection task by placing the emphasis
not so much on the text itself but on the interaction data available on social
networks. More specifically, we propose a new method to leverage social
information such as friends and retweets by generating relational embeddings,
namely, dense vector representations of interaction pairs. Our method can be
applied to any language and target without any manual tuning. Our experiments
on seven publicly available datasets and four different languages show that
combining our relational embeddings with textual methods helps to substantially
improve performance, obtaining best results for six out of seven evaluation
settings, outperforming strong baselines based on large pre-trained language
models."
12314,"we need to pay more attention to social network
                                                         data, aiming to address the shortcomings discussed
   If we combine the graphical visualizations with       by further researching different strategies to lever-
the confusion matrices of textual and ensemble           age such interaction data.","the small size of the community of Basque Twitter
users, may add difﬁculties to catch those orienta-          The results and analysis performed shows that
tion in such a small dataset.","We are aware that our
classiﬁers in Figure 7, we can conﬁrm some of            system is conditioned to the availability of the rela-
the points raised above.",2022-10-11 18:13:43+00:00,Relational Embeddings for Language Independent Stance Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Joseba Fernandez de Landa'), arxiv.Result.Author('Rodrigo Agerri')]","The large majority of the research performed on stance detection has been
focused on developing more or less sophisticated text classification systems,
even when many benchmarks are based on social network data such as Twitter.
This paper aims to take on the stance detection task by placing the emphasis
not so much on the text itself but on the interaction data available on social
networks. More specifically, we propose a new method to leverage social
information such as friends and retweets by generating relational embeddings,
namely, dense vector representations of interaction pairs. Our method can be
applied to any language and target without any manual tuning. Our experiments
on seven publicly available datasets and four different languages show that
combining our relational embeddings with textual methods helps to substantially
improve performance, obtaining best results for six out of seven evaluation
settings, outperforming strong baselines based on large pre-trained language
models."
12323,"Although originally proposed
and evaluated based on self-attention attribution, AD-DROP can be potentially extended to other
neural network units as vanilla dropout, which deserves further research efforts.","Extensive experiments and analysis
on the GLUE benchmark demonstrate the effectiveness of AD-DROP.","Acknowledgments

We appreciate the anonymous reviewers for their valuable comments.",2022-10-12 02:54:41+00:00,AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tao Yang'), arxiv.Result.Author('Jinghao Deng'), arxiv.Result.Author('Xiaojun Quan'), arxiv.Result.Author('Qifan Wang'), arxiv.Result.Author('Shaoliang Nie')]","Fine-tuning large pre-trained language models on downstream tasks is apt to
suffer from overfitting when limited training data is available. While dropout
proves to be an effective antidote by randomly dropping a proportion of units,
existing research has not examined its effect on the self-attention mechanism.
In this paper, we investigate this problem through self-attention attribution
and find that dropping attention positions with low attribution scores can
accelerate training and increase the risk of overfitting. Motivated by this
observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly
discards some high-attribution positions to encourage the model to make
predictions by relying more on low-attribution positions to reduce overfitting.
We also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to
avoid dropping high-attribution positions excessively. Extensive experiments on
various benchmarks show that AD-DROP yields consistent improvements over
baselines. Analysis further confirms that AD-DROP serves as a strategic
regularizer to prevent overfitting during fine-tuning."
12328,"In this regard, their early
documents on predeﬁned topics from an unlabeled           work had a major impact on further research, which
document dataset without the need to consider             subsequently heavily focused on adding a lot of
documents belonging to different topics of no interest.","Our approach differs          neighbor classiﬁcation to assign the most likely
slightly from these, as we primarily attempt to retrieve  label to each document.",world knowledge for dataless classiﬁcation.,2022-10-12 08:57:01+00:00,Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Tim Schopf'), arxiv.Result.Author('Daniel Braun'), arxiv.Result.Author('Florian Matthes')]","In this paper, we consider the task of retrieving documents with predefined
topics from an unlabeled document dataset using an unsupervised approach. The
proposed unsupervised approach requires only a small number of keywords
describing the respective topics and no labeled document. Existing approaches
either heavily relied on a large amount of additionally encoded world knowledge
or on term-document frequencies. Contrariwise, we introduce a method that
learns jointly embedded document and word vectors solely from the unlabeled
document dataset in order to find documents that are semantically similar to
the topics described by the keywords. The proposed method requires almost no
text preprocessing but is simultaneously effective at retrieving relevant
documents with high probability. When successively retrieving documents on
different predefined topics from publicly available and commonly used datasets,
we achieved an average area under the receiver operating characteristic curve
value of 0.95 on one dataset and 0.92 on another. Further, our method can be
used for multiclass document classification, without the need to assign labels
to the dataset in advance. Compared with an unsupervised classification
baseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the
respective datasets. For easy replication of our approach, we make the
developed Lbl2Vec code publicly available as a ready-to-use tool under the
3-Clause BSD license."
12329,"and enables ﬁnancially disadvantaged users to conduct
further research.","Document
our approach is comparably environmentally friendly            embedding with paragraph vectors.","Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer,
                                                               T. K., and Harshman, R. (1990).",2022-10-12 08:57:01+00:00,Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Tim Schopf'), arxiv.Result.Author('Daniel Braun'), arxiv.Result.Author('Florian Matthes')]","In this paper, we consider the task of retrieving documents with predefined
topics from an unlabeled document dataset using an unsupervised approach. The
proposed unsupervised approach requires only a small number of keywords
describing the respective topics and no labeled document. Existing approaches
either heavily relied on a large amount of additionally encoded world knowledge
or on term-document frequencies. Contrariwise, we introduce a method that
learns jointly embedded document and word vectors solely from the unlabeled
document dataset in order to find documents that are semantically similar to
the topics described by the keywords. The proposed method requires almost no
text preprocessing but is simultaneously effective at retrieving relevant
documents with high probability. When successively retrieving documents on
different predefined topics from publicly available and commonly used datasets,
we achieved an average area under the receiver operating characteristic curve
value of 0.95 on one dataset and 0.92 on another. Further, our method can be
used for multiclass document classification, without the need to assign labels
to the dataset in advance. Compared with an unsupervised classification
baseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the
respective datasets. For easy replication of our approach, we make the
developed Lbl2Vec code publicly available as a ready-to-use tool under the
3-Clause BSD license."
12334,"The dataset
                                        and baselines will be released to support further research in
                                        question generation.1

                                           Index Terms—Question generation, multiple choice question,
                                        natural language processing, online learning.","We
                                        believe this new dataset can serve as a valuable resource for
                                        research and evaluation in the educational domain.",I.,2022-10-12 11:28:34+00:00,EduQG: A Multi-format Multiple Choice Dataset for the Educational Domain,cs.CL,['cs.CL'],"[arxiv.Result.Author('Amir Hadifar'), arxiv.Result.Author('Semere Kiros Bitew'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Chris Develder'), arxiv.Result.Author('Thomas Demeester')]","We introduce a high-quality dataset that contains 3,397 samples comprising
(i) multiple choice questions, (ii) answers (including distractors), and (iii)
their source documents, from the educational domain. Each question is phrased
in two forms, normal and close. Correct answers are linked to source documents
with sentence-level annotations. Thus, our versatile dataset can be used for
both question and distractor generation, as well as to explore new challenges
such as question format conversion. Furthermore, 903 questions are accompanied
by their cognitive complexity level as per Bloom's taxonomy. All questions have
been generated by educational experts rather than crowd workers to ensure they
are maintaining educational and learning standards. Our analysis and
experiments suggest distinguishable differences between our dataset and
commonly used ones for question generation for educational purposes. We believe
this new dataset can serve as a valuable resource for research and evaluation
in the educational domain. The dataset and baselines will be released to
support further research in question generation."
12344,"These ﬁndings call for
                                        that the existing methods are still far from achiev-    further research in the ﬁeld of ED, with the goal
                                        ing human-level performance: in particular, the         of building a method that outperforms existing sys-
                                        case of entity overshadowing remains a big chal-        tems on overshadowed entities while still achieving
                                        lenge.","Despite ED        equally good results at disambiguating entities sam-
                                        being a well-known task, recent work has shown          pled from Top and Shadow.",An entity e1 overshadows e2 if the two enti-     competitive results on standard datasets.,2022-10-12 13:05:37+00:00,Focusing on Context is NICE: Improving Overshadowed Entity Disambiguation,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Vera Provatorova'), arxiv.Result.Author('Simone Tedeschi'), arxiv.Result.Author('Svitlana Vakulenko'), arxiv.Result.Author('Roberto Navigli'), arxiv.Result.Author('Evangelos Kanoulas')]","Entity disambiguation (ED) is the task of mapping an ambiguous entity mention
to the corresponding entry in a structured knowledge base. Previous research
showed that entity overshadowing is a significant challenge for existing ED
models: when presented with an ambiguous entity mention, the models are much
more likely to rank a more frequent yet less contextually relevant entity at
the top. Here, we present NICE, an iterative approach that uses entity type
information to leverage context and avoid over-relying on the frequency-based
prior. Our experiments show that NICE achieves the best performance results on
the overshadowed entities while still performing competitively on the frequent
entities."
12351,"We believe        [7] Keqi Deng, Songjun Cao, Yike Zhang, Long Ma,
such a lightweight and simple method could be a potential             Gaofeng Cheng, Ji Xu, and Pengyuan Zhang, “Improv-
basis for further research.","Extensive experiments and analysis demonstrate the effec-
tiveness and efﬁciency of the proposed method.","In the future, we plan to evaluate        ing ctc-based speech recognition via knowledge trans-
the CAKT on other benchmark corpora and train it based on             ferring from pre-trained language models,” in ICASSP
other pre-trained language models and speech representation           2022 - 2022 IEEE International Conference on Acous-
learning methods.",2022-10-12 14:31:38+00:00,A context-aware knowledge transferring strategy for CTC-based ASR,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Ke-Han Lu'), arxiv.Result.Author('Kuan-Yu Chen')]","Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method."
12366,• Show future work for further research.,• Show recent innovations since previous surveys.,"2.3 Searching Strategy To Retrieve Studies

Given most to all studies were queried on Google Scholar with keywords including text generation, detection and
synthetic text, the engine includes searches into various other databases such as IEEE, ArXiv, Semantic Scholar,
Springer, ACM journals, Elsevier and more, even including schools inside one search page.",2022-10-01 04:43:41+00:00,Synthetic Text Detection: Systemic Literature Review,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jesus Guerrero'), arxiv.Result.Author('Izzat Alsmadi')]","Within the text analysis and processing fields, generated text attacks have
been made easier to create than ever before. To combat these attacks open
sourcing models and datasets have become a major trend to create automated
detection algorithms in defense of authenticity. For this purpose, synthetic
text detection has become an increasingly viable topic of research. This review
is written for the purpose of creating a snapshot of the state of current
literature and easing the barrier to entry for future authors. Towards that
goal, we identified few research trends and challenges in this field."
12374,"We hope our work will                        3For example, if we add the talk token into the statement
inspire further research on probing and improving                  “Birds can [MASK].” (i.e.","perturbations (e.g., simple synonym substitution,                  Although the generated ‘reasoning’ statements are
paraphrasing or negation insertion), advocating a                  potentially beneﬁcial to the effectiveness of some
careful rethinking of the details behind the seem-                 downstream tasks, no evidence has been shown that
ingly ﬂawless empirical performance of deductive
reasoning using the PLMs.",“Talk.,2022-10-12 17:44:15+00:00,Can Pretrained Language Models (Yet) Reason Deductively?,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zhangdie Yuan'), arxiv.Result.Author('Songbo Hu'), arxiv.Result.Author('Ivan Vulić'), arxiv.Result.Author('Anna Korhonen'), arxiv.Result.Author('Zaiqiao Meng')]","Acquiring factual knowledge with Pretrained Language Models (PLMs) has
attracted increasing attention, showing promising performance in many
knowledge-intensive tasks. Their good performance has led the community to
believe that the models do possess a modicum of reasoning competence rather
than merely memorising the knowledge. In this paper, we conduct a comprehensive
evaluation of the learnable deductive (also known as explicit) reasoning
capability of PLMs. Through a series of controlled experiments, we posit two
main findings. (i) PLMs inadequately generalise learned logic rules and perform
inconsistently against simple adversarial surface form edits. (ii) While the
deductive reasoning fine-tuning of PLMs does improve their performance on
reasoning over unseen knowledge facts, it results in catastrophically
forgetting the previously learnt knowledge. Our main results suggest that PLMs
cannot yet perform reliable deductive reasoning, demonstrating the importance
of controlled examinations and probing of PLMs' reasoning abilities; we reach
beyond (misleading) task performance, revealing that PLMs are still far from
human-level reasoning capabilities, even for simple deductive tasks."
12387,"Although training the
UCSF-BERT model on a large clinical corpus reduces these errors, further research is needed to provide additional
context to these terminologies, particularly in the setups that require cross-institution transfer.","-nt, NGTD, “started on sips”, “advanced to clears”.","Furthermore, we find that our concept detection model is highly contextual.",2022-10-12 20:08:45+00:00,Developing a general-purpose clinical language inference model from a large corpus of clinical notes,cs.CL,['cs.CL'],"[arxiv.Result.Author('Madhumita Sushil'), arxiv.Result.Author('Dana Ludwig'), arxiv.Result.Author('Atul J. Butte'), arxiv.Result.Author('Vivek A. Rudrapatna')]","Several biomedical language models have already been developed for clinical
language inference. However, these models typically utilize general
vocabularies and are trained on relatively small clinical corpora. We sought to
evaluate the impact of using a domain-specific vocabulary and a large clinical
training corpus on the performance of these language models in clinical
language inference. We trained a Bidirectional Encoder Decoder from
Transformers (BERT) model using a diverse, deidentified corpus of 75 million
deidentified clinical notes authored at the University of California, San
Francisco (UCSF). We evaluated this model on several clinical language
inference benchmark tasks: clinical and temporal concept recognition, relation
extraction and medical language inference. We also evaluated our model on two
tasks using discharge summaries from UCSF: diagnostic code assignment and
therapeutic class inference. Our model performs at par with the best publicly
available biomedical language models of comparable sizes on the public
benchmark tasks, and is significantly better than these models in a
within-system evaluation on the two tasks using UCSF data. The use of in-domain
vocabulary appears to improve the encoding of longer documents. The use of
large clinical corpora appears to enhance document encoding and inferential
accuracy. However, further research is needed to improve abbreviation
resolution, and numerical, temporal, and implicitly causal inference."
12388,"However, more complex inferences requiring an
          understanding of causation, for example, adverse drug events, need further research for reliable inference.","We found that the UCSF-BERT model performs surprisingly well in these cases, potentially due
          to the correlations learned from the clinical notes.","3) Temporal inference: We analyzed whether the UCSF-BERT model can make inferences over a sequence of
          events to infer the temporal ordering of different events from the clinical rhetoric.",2022-10-12 20:08:45+00:00,Developing a general-purpose clinical language inference model from a large corpus of clinical notes,cs.CL,['cs.CL'],"[arxiv.Result.Author('Madhumita Sushil'), arxiv.Result.Author('Dana Ludwig'), arxiv.Result.Author('Atul J. Butte'), arxiv.Result.Author('Vivek A. Rudrapatna')]","Several biomedical language models have already been developed for clinical
language inference. However, these models typically utilize general
vocabularies and are trained on relatively small clinical corpora. We sought to
evaluate the impact of using a domain-specific vocabulary and a large clinical
training corpus on the performance of these language models in clinical
language inference. We trained a Bidirectional Encoder Decoder from
Transformers (BERT) model using a diverse, deidentified corpus of 75 million
deidentified clinical notes authored at the University of California, San
Francisco (UCSF). We evaluated this model on several clinical language
inference benchmark tasks: clinical and temporal concept recognition, relation
extraction and medical language inference. We also evaluated our model on two
tasks using discharge summaries from UCSF: diagnostic code assignment and
therapeutic class inference. Our model performs at par with the best publicly
available biomedical language models of comparable sizes on the public
benchmark tasks, and is significantly better than these models in a
within-system evaluation on the two tasks using UCSF data. The use of in-domain
vocabulary appears to improve the encoding of longer documents. The use of
large clinical corpora appears to enhance document encoding and inferential
accuracy. However, further research is needed to improve abbreviation
resolution, and numerical, temporal, and implicitly causal inference."
12389,"Future directions include further research on long sequence encoding,
temporal and numerical inference bolstering, and out-of-domain generalization.","The UCSF-BERT model
is excellent at making contextual inferences from explicit mentions in the text However, clinical transformers
models still struggle with abbreviation and acronym resolution, temporal sequence inference, implicit causal
inference, and infrequent numeric inference.","Incorporating diverse corpora
during model training — biomedical abstracts and articles, the MIMIC-III corpus, as well as clinical notes from
multiple health systems — would potentially improve the generalizability of these models.",2022-10-12 20:08:45+00:00,Developing a general-purpose clinical language inference model from a large corpus of clinical notes,cs.CL,['cs.CL'],"[arxiv.Result.Author('Madhumita Sushil'), arxiv.Result.Author('Dana Ludwig'), arxiv.Result.Author('Atul J. Butte'), arxiv.Result.Author('Vivek A. Rudrapatna')]","Several biomedical language models have already been developed for clinical
language inference. However, these models typically utilize general
vocabularies and are trained on relatively small clinical corpora. We sought to
evaluate the impact of using a domain-specific vocabulary and a large clinical
training corpus on the performance of these language models in clinical
language inference. We trained a Bidirectional Encoder Decoder from
Transformers (BERT) model using a diverse, deidentified corpus of 75 million
deidentified clinical notes authored at the University of California, San
Francisco (UCSF). We evaluated this model on several clinical language
inference benchmark tasks: clinical and temporal concept recognition, relation
extraction and medical language inference. We also evaluated our model on two
tasks using discharge summaries from UCSF: diagnostic code assignment and
therapeutic class inference. Our model performs at par with the best publicly
available biomedical language models of comparable sizes on the public
benchmark tasks, and is significantly better than these models in a
within-system evaluation on the two tasks using UCSF data. The use of in-domain
vocabulary appears to improve the encoding of longer documents. The use of
large clinical corpora appears to enhance document encoding and inferential
accuracy. However, further research is needed to improve abbreviation
resolution, and numerical, temporal, and implicitly causal inference."
12405,"Improved evaluation would also enable
lines) can inspire further research on metrics for      us to evaluate stories much longer than the current
which we currently rely solely on human annota-         2000-2500 words: while Re3 is capable of generat-
tion.","Speciﬁcally,
proposed some possible measures (Barzilay and           we are limited in the sample sizes of all our exper-
Lapata, 2008; Castricato et al., 2021), we hope that    iments as well as our ability to run more detailed
analyzing our generated stories (both RE3 and base-     ablations.","For example, while there exist reasonable         ing such stories (Appendix M), we do not formally
metrics for text similarity on a sentence or para-      evaluate them in this work.",2022-10-13 06:29:57+00:00,Re3: Generating Longer Stories With Recursive Reprompting and Revision,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kevin Yang'), arxiv.Result.Author('Nanyun Peng'), arxiv.Result.Author('Yuandong Tian'), arxiv.Result.Author('Dan Klein')]","We consider the problem of automatically generating longer stories of over
two thousand words. Compared to prior work on shorter stories, long-range plot
coherence and relevance are more central challenges here. We propose the
Recursive Reprompting and Revision framework (Re3) to address these challenges
by (a) prompting a general-purpose language model to construct a structured
overarching plan, and (b) generating story passages by repeatedly injecting
contextual information from both the plan and current story state into a
language model prompt. We then revise by (c) reranking different continuations
for plot coherence and premise relevance, and finally (d) editing the best
continuation for factual consistency. Compared to similar-length stories
generated directly from the same base model, human evaluators judged
substantially more of Re3's stories as having a coherent overarching plot (by
14% absolute increase), and relevant to the given initial premise (by 20%)."
12406,"Improved evaluation would also enable
lines) can inspire further research on metrics for      us to evaluate stories much longer than the current
which we currently rely solely on human annota-         2000-2500 words: while Re3 is capable of generat-
tion.","Speciﬁcally,
proposed some possible measures (Barzilay and           we are limited in the sample sizes of all our exper-
Lapata, 2008; Castricato et al., 2021), we hope that    iments as well as our ability to run more detailed
analyzing our generated stories (both RE3 and base-     ablations.","For example, while there exist reasonable         ing such stories (Appendix M), we do not formally
metrics for text similarity on a sentence or para-      evaluate them in this work.",2022-10-13 06:29:57+00:00,Re3: Generating Longer Stories With Recursive Reprompting and Revision,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kevin Yang'), arxiv.Result.Author('Yuandong Tian'), arxiv.Result.Author('Nanyun Peng'), arxiv.Result.Author('Dan Klein')]","We consider the problem of automatically generating longer stories of over
two thousand words. Compared to prior work on shorter stories, long-range plot
coherence and relevance are more central challenges here. We propose the
Recursive Reprompting and Revision framework (Re3) to address these challenges
by (a) prompting a general-purpose language model to construct a structured
overarching plan, and (b) generating story passages by repeatedly injecting
contextual information from both the plan and current story state into a
language model prompt. We then revise by (c) reranking different continuations
for plot coherence and premise relevance, and finally (d) editing the best
continuation for factual consistency. Compared to similar-length stories
generated directly from the same base model, human evaluators judged
substantially more of Re3's stories as having a coherent overarching plot (by
14% absolute increase), and relevant to the given initial premise (by 20%)."
12407,"While prior works have
                                                         proposed some possible measures (Barzilay and
   Additionally, while we did not formally analyze       Lapata, 2008; Castricato et al., 2021), we hope that
the GPT3 Edit API’s ability to correct inconsisten-      analyzing our generated stories (both RE3 and base-
cies after they are detected (as this system is largely  lines) can inspire further research on metrics for
not our contribution), we generally observed that it     which we currently rely solely on human annota-
can ﬁx isolated details but may struggle with larger     tion.",non-expert annotators.,"For example, while there exist reasonable
changes.",2022-10-13 06:29:57+00:00,Re3: Generating Longer Stories With Recursive Reprompting and Revision,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kevin Yang'), arxiv.Result.Author('Yuandong Tian'), arxiv.Result.Author('Nanyun Peng'), arxiv.Result.Author('Dan Klein')]","We consider the problem of automatically generating longer stories of over
two thousand words. Compared to prior work on shorter stories, long-range plot
coherence and relevance are more central challenges here. We propose the
Recursive Reprompting and Revision framework (Re3) to address these challenges
by (a) prompting a general-purpose language model to construct a structured
overarching plan, and (b) generating story passages by repeatedly injecting
contextual information from both the plan and current story state into a
language model prompt. We then revise by (c) reranking different continuations
for plot coherence and premise relevance, and finally (d) editing the best
continuation for factual consistency. Compared to similar-length stories
generated directly from the same base model, human evaluators judged
substantially more of Re3's stories as having a coherent overarching plot (by
14% absolute increase), and relevant to the given initial premise (by 20%)."
12415,"We further study a different optimization
of BERT, ALBERT [16], that factorizes the embedding matrix into smaller matrices,
separating thus the size of vocabulary from the size of the hidden layers.","Then, we study an optimized version of BERT, RoBERTa [17], which is
trained longer and on prolonged sequences.","Moreover, we
consider DistilBERT [33], a distilled version of BERT trained on very large batches
while leveraging the computation of the gradients through dynamic masking.",2022-10-13 11:29:17+00:00,On the Evaluation of the Plausibility and Faithfulness of Sentiment Analysis Explanations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Julia El Zini'), arxiv.Result.Author('Mohamad Mansour'), arxiv.Result.Author('Basel Mousi'), arxiv.Result.Author('Mariette Awad')]","Current Explainable AI (ExAI) methods, especially in the NLP field, are
conducted on various datasets by employing different metrics to evaluate
several aspects. The lack of a common evaluation framework is hindering the
progress tracking of such methods and their wider adoption. In this work,
inspired by offline information retrieval, we propose different metrics and
techniques to evaluate the explainability of SA models from two angles. First,
we evaluate the strength of the extracted ""rationales"" in faithfully explaining
the predicted outcome. Second, we measure the agreement between ExAI methods
and human judgment on a homegrown dataset1 to reflect on the rationales
plausibility. Our conducted experiments comprise four dimensions: (1) the
underlying architectures of SA models, (2) the approach followed by the ExAI
method, (3) the reasoning difficulty, and (4) the homogeneity of the
ground-truth rationales. We empirically demonstrate that anchors explanations
are more aligned with the human judgment and can be more confident in
extracting supporting rationales. As can be foreseen, the reasoning complexity
of sentiment is shown to thwart ExAI methods from extracting supporting
evidence. Moreover, a remarkable discrepancy is discerned between the results
of different explainability methods on the various architectures suggesting the
need for consolidation to observe enhanced performance. Predominantly,
transformers are shown to exhibit better explainability than convolutional and
recurrent architectures. Our work paves the way towards designing more
interpretable NLP models and enabling a common evaluation ground for their
relative strengths and robustness."
12420,"As           OOV To further study the reason behind
shown Table 5, for the source side, we either apply      MTBP E and MTAT B+BP E top performance, we
MDMRAT B/MDMRD3 or BPEjoint on top of seg-               observe if the top-performing MT systems’ rank-
mentations provided by MDMRAT B/MDMRD3.","(2019), we explore combinations between BPE
and supervised morphology-based segmenters.","ing is linked with the percentage of OOV in the
For the target side, we either leave it in the raw for-  different translation systems.",2022-10-11 23:20:12+00:00,Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text,cs.CL,['cs.CL'],"[arxiv.Result.Author('Marwa Gaser'), arxiv.Result.Author('Manuel Mager'), arxiv.Result.Author('Injy Hamed'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Slim Abdennadher'), arxiv.Result.Author('Ngoc Thang Vu')]","Data sparsity is one of the main challenges posed by Code-switching (CS),
which is further exacerbated in the case of morphologically rich languages. For
the task of Machine Translation (MT), morphological segmentation has proven
successful in alleviating data sparsity in monolingual contexts; however, it
has not been investigated for CS settings. In this paper, we study the
effectiveness of different segmentation approaches on MT performance, covering
morphology-based and frequency-based segmentation techniques. We experiment on
MT from code-switched Arabic-English to English. We provide detailed analysis,
examining a variety of conditions, such as data size and sentences with
different degrees in CS. Empirical results show that morphology-aware
segmenters perform the best in segmentation tasks but under-perform in MT.
Nevertheless, we find that the choice of the segmentation setup to use for MT
is highly dependent on the data size. For extreme low-resource scenarios, a
combination of frequency and morphology-based segmentations is shown to perform
the best. For more resourced settings, such a combination does not bring
significant improvements over the use of frequency-based segmentation."
12448,"Given the importance of ﬁne granularity do-
mains in language modeling, we hope that M2D2
will encourage the community to further study do-
main transfer: how do we identify hierarchical ﬁne-
grained domains in naturally occurring text, and
how do we leverage this ﬁne-grained domain hier-
L1 (Abbrv)                              Size        #L2     #Tokens    Examples of L2 domains

Health and ﬁtness (HEAL)                761.2MB       7     116M       Exercise, Health Science
History and events (HIST)               1.4GB         4     226M       Regions, Periods
Society and social sciences (SOCI)      2.3GB         3     379M       Society, social sciences
Technology and applied sciences (TECH)  1.9GB         5     297M       Agriculture, Computing
Culture and the arts (CULT)             2.0GB         8     289M       Games and Toys, The arts and entertainment
Natural and physical sciences (NATU)    1.2GB         5     189M       Physical sciences, Earth sciences
Human activites (HUMA)                  2.1GB         3     343M       Impact of human activity
Mathematics and logic (MATH)            332.3MB       4     52M        Mathematics, Logic
General reference (GENE)                385.3MB       3     60M        Research tools and topics, Reference works
Religion and belief systems (RELI)      428.0MB       4     64M        Major beliefs of the world, Belief systems
Philosophy and thinking (PHIL)          1.0GB         3     165M       Philosophy, Thinking

Mathematics (math)                      4.5GB        26     1.4B       Topology, Number Theory
Quantitative Biology(q-bio)             1.9GB         3     336M       Biomolecules, Cell Behavior
Physics                                 4.1GB        12     737M       General Physics, Biological Physics
Nonlinear Sciences (nlin)               730MB         5     134M       Self-Organizing Systems, Chaotic Dynamics
Condensed Matter (cm)                   3.8GB        10     688M       Materials Science, Quantum Gases
Economics (econ)                        67MB          3     11M        Econometrics, General Econometrics, Theory
Computer Science (cs)                   4.5GB        23     1.1B       Machine Learning, Databases, Graphics
Statistics (stat)                       2.4GB         4     450M       Applications, Methodology
Astrophysics (astro-ph)                 4.0GB         7     728M       Earth/Planetary, Cosmology
Art†                                    575MB         1     98M        —
Philosophy†(phil)                       919MB         1     156M       —

Average±s.d.","We
ﬁnd that (1) more speciﬁc data is often more im-
portant for performance than larger, less-speciﬁc
data, shown by our comparison of coarse-grained,
ﬁne-grained and coarse-to-ﬁne adaptation compari-
son (in which coarse-to-ﬁne performed best) , (2)
vocabulary overlap is a surprising good indicator
for transfer, and (3) data source provenance infor-
mation is often a better predictor than ontology
when predicting transferability, perhaps indicating
a more multi-faceted deﬁnition of domain could be
developed in future work.","1.9G±1.7G  6.6±6.2  373M±347M  —
Total                                   41GB        145     8.5B       —

Table 1: Dataset statistics for M2D2.",2022-10-13 21:34:52+00:00,M2D2: A Massively Multi-domain Language Modeling Dataset,cs.CL,['cs.CL'],"[arxiv.Result.Author('Machel Reid'), arxiv.Result.Author('Victor Zhong'), arxiv.Result.Author('Suchin Gururangan'), arxiv.Result.Author('Luke Zettlemoyer')]","We present M2D2, a fine-grained, massively multi-domain corpus for studying
domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and
spans 145 domains extracted from Wikipedia and Semantic Scholar. Using
ontologies derived from Wikipedia and ArXiv categories, we organize the domains
in each data source into 22 groups. This two-level hierarchy enables the study
of relationships between domains and their effects on in- and out-of-domain
performance after adaptation. We also present a number of insights into the
nature of effective domain adaptation in LMs, as examples of the new types of
studies M2D2 enables. To improve in-domain performance, we show the benefits of
adapting the LM along a domain hierarchy; adapting to smaller amounts of
fine-grained domain-specific data can lead to larger in-domain performance
gains than larger amounts of weakly relevant data. We further demonstrate a
trade-off between in-domain specialization and out-of-domain generalization
within and across ontologies, as well as a strong correlation between
out-of-domain performance and lexical overlap between domains."
12449,"We
                                                           release M2D2 publicly to spur further research
Domain Adaptation Techniques (Gururangan                   on building effective language models on highly
et al., 2020) show that pretrained language mod-           heterogeneous data.","is more important than data quantity to improve
                                                           in-domain performance, a tradeoff between spe-
4 Related Work                                             cialization and out-of-domain generalization.","els can be adapted to new domains by con-
tinued pre-training on domain-speciﬁc corpora.",2022-10-13 21:34:52+00:00,M2D2: A Massively Multi-domain Language Modeling Dataset,cs.CL,['cs.CL'],"[arxiv.Result.Author('Machel Reid'), arxiv.Result.Author('Victor Zhong'), arxiv.Result.Author('Suchin Gururangan'), arxiv.Result.Author('Luke Zettlemoyer')]","We present M2D2, a fine-grained, massively multi-domain corpus for studying
domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and
spans 145 domains extracted from Wikipedia and Semantic Scholar. Using
ontologies derived from Wikipedia and ArXiv categories, we organize the domains
in each data source into 22 groups. This two-level hierarchy enables the study
of relationships between domains and their effects on in- and out-of-domain
performance after adaptation. We also present a number of insights into the
nature of effective domain adaptation in LMs, as examples of the new types of
studies M2D2 enables. To improve in-domain performance, we show the benefits of
adapting the LM along a domain hierarchy; adapting to smaller amounts of
fine-grained domain-specific data can lead to larger in-domain performance
gains than larger amounts of weakly relevant data. We further demonstrate a
trade-off between in-domain specialization and out-of-domain generalization
within and across ontologies, as well as a strong correlation between
out-of-domain performance and lexical overlap between domains."
12455,"Our                             only contains the minimal user requirements in the
collected data, code, and models will be released                              current turn (i.e., ﬁnd an east entertainment attrac-
at GitHub1, hoping to facilitate further research in                           tion) and discards the outdated attraction type of
task-oriented dialogue systems.","comparable performance with the previous state-                                As shown in the example of Figure 1, the query
of-the-art using only 5% of the training data.",concert hall.,2022-10-14 06:38:19+00:00,Q-TOD: A Query-driven Task-oriented Dialogue System,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xin Tian'), arxiv.Result.Author('Yingzhan Lin'), arxiv.Result.Author('Mengfei Song'), arxiv.Result.Author('Siqi Bao'), arxiv.Result.Author('Fan Wang'), arxiv.Result.Author('Huang He'), arxiv.Result.Author('Shuqi Sun'), arxiv.Result.Author('Hua Wu')]","Existing pipelined task-oriented dialogue systems usually have difficulties
adapting to unseen domains, whereas end-to-end systems are plagued by
large-scale knowledge bases in practice. In this paper, we introduce a novel
query-driven task-oriented dialogue system, namely Q-TOD. The essential
information from the dialogue context is extracted into a query, which is
further employed to retrieve relevant knowledge records for response
generation. Firstly, as the query is in the form of natural language and not
confined to the schema of the knowledge base, the issue of domain adaption is
alleviated remarkably in Q-TOD. Secondly, as the query enables the decoupling
of knowledge retrieval from the generation, Q-TOD gets rid of the issue of
knowledge base scalability. To evaluate the effectiveness of the proposed
Q-TOD, we collect query annotations for three publicly available task-oriented
dialogue datasets. Comprehensive experiments verify that Q-TOD outperforms
strong baselines and establishes a new state-of-the-art performance on these
datasets."
12456,"further research should be undertaken to explore
more efﬁcient inference strategy and high-ﬁdelity      Yanjie Gou, Yinjie Lei, Lingqiao Liu, Yong Dai,
knowledge-grounded response generation.",These ﬁndings suggest that         Association for Computational Linguistics.,and Chunxu Shen.,2022-10-14 06:38:19+00:00,Q-TOD: A Query-driven Task-oriented Dialogue System,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xin Tian'), arxiv.Result.Author('Yingzhan Lin'), arxiv.Result.Author('Mengfei Song'), arxiv.Result.Author('Siqi Bao'), arxiv.Result.Author('Fan Wang'), arxiv.Result.Author('Huang He'), arxiv.Result.Author('Shuqi Sun'), arxiv.Result.Author('Hua Wu')]","Existing pipelined task-oriented dialogue systems usually have difficulties
adapting to unseen domains, whereas end-to-end systems are plagued by
large-scale knowledge bases in practice. In this paper, we introduce a novel
query-driven task-oriented dialogue system, namely Q-TOD. The essential
information from the dialogue context is extracted into a query, which is
further employed to retrieve relevant knowledge records for response
generation. Firstly, as the query is in the form of natural language and not
confined to the schema of the knowledge base, the issue of domain adaption is
alleviated remarkably in Q-TOD. Secondly, as the query enables the decoupling
of knowledge retrieval from the generation, Q-TOD gets rid of the issue of
knowledge base scalability. To evaluate the effectiveness of the proposed
Q-TOD, we collect query annotations for three publicly available task-oriented
dialogue datasets. Comprehensive experiments verify that Q-TOD outperforms
strong baselines and establishes a new state-of-the-art performance on these
datasets."
12457,"Als, this observation opens up oppor-
                                                         tunity for further research as MaxProb has its own
                                                         limitations and is not the ideal indicator of model
                                                         conﬁdence (Kamath et al., 2020; Varshney et al.,
                                                         2022).","This may further indi-
                                                         cate the effectiveness of transformers in utilizing
                                                         training data.","Figure 7: Average MaxProb of correct classiﬁcations
using BERT-Base model across test sample-chunks of
SST-2 and IMDB in decreasing order of train (IMDB)-
test similarity.",2022-10-14 08:26:32+00:00,Hardness of Samples Need to be Quantified for a Reliable Evaluation System: Exploring Potential Opportunities with a New Task,cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Author('Swaroop Mishra'), arxiv.Result.Author('Anjana Arunkumar'), arxiv.Result.Author('Chris Bryan'), arxiv.Result.Author('Chitta Baral')]","Evaluation of models on benchmarks is unreliable without knowing the degree
of sample hardness; this subsequently overestimates the capability of AI
systems and limits their adoption in real world applications. We propose a Data
Scoring task that requires assignment of each unannotated sample in a benchmark
a score between 0 to 1, where 0 signifies easy and 1 signifies hard. Use of
unannotated samples in our task design is inspired from humans who can
determine a question difficulty without knowing its correct answer. This also
rules out the use of methods involving model based supervision (since they
require sample annotations to get trained), eliminating potential biases
associated with models in deciding sample difficulty. We propose a method based
on Semantic Textual Similarity (STS) for this task; we validate our method by
showing that existing models are more accurate with respect to the easier
sample-chunks than with respect to the harder sample-chunks. Finally we
demonstrate five novel applications."
12470,"Motivated by their im-     needs to track token indices implicitly (Maynez
pressive results, we further study how to bridge the  et al., 2020; Raman et al., 2022).","(2021)   too many hallucinated tokens and thus makes the
explored replacing entities between domains with      learning problem signiﬁcantly harder as the model
cross-domain anchor pairs.","In the sce-
gap of data difference between domains and ex-        nario where the number of annotated data is lim-
plore a better to leverage the data in high-resource  ited, the model may be confused with the label-
domains for data augmentation.",2022-10-14 16:02:03+00:00,Style Transfer as Data Augmentation: A Case Study on Named Entity Recognition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shuguang Chen'), arxiv.Result.Author('Leonardo Neves'), arxiv.Result.Author('Thamar Solorio')]","In this work, we take the named entity recognition task in the English
language as a case study and explore style transfer as a data augmentation
method to increase the size and diversity of training data in low-resource
scenarios. We propose a new method to effectively transform the text from a
high-resource domain to a low-resource domain by changing its style-related
attributes to generate synthetic data for training. Moreover, we design a
constrained decoding algorithm along with a set of key ingredients for data
selection to guarantee the generation of valid and coherent data. Experiments
and analysis on five different domain pairs under different data regimes
demonstrate that our approach can significantly improve results compared to
current state-of-the-art data augmentation methods. Our approach is a practical
solution to data scarcity, and we expect it to be applicable to other NLP
tasks."
12471,"The dataset is made
available to foster further research, especially on automated detection of antisemitic
and conspiracy-theory content.","We chose
Telegram because of its popularity among opponents of the government’s measures

    1The assaults in Christchurch, Halle, or Hanau were impelled by racist and antisemitic conspiracy narra-
tives disseminated via different online platforms, and the killers used online platforms to stage their killing
in live streams [Musyal and Stegemann, 2020]

                                                   2
to combat the coronavirus and the frequent spread of conspiracy theories and anti-
semitic statements [Comerford et al., 2021, Winter et al., 2021].","Related Work

Our literature review focuses on studies in which large amounts of data have been
collected and analyzed, typically in conjunction with annotation efforts, in order to
provide an overview of existing datasets and associated deﬁnitions and categorization
schemes.",2022-10-13 10:32:39+00:00,"Codes, Patterns and Shapes of Contemporary Online Antisemitism and Conspiracy Narratives -- an Annotation Guide and Labeled German-Language Dataset in the Context of COVID-19",cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Elisabeth Steffen'), arxiv.Result.Author('Helena Mihaljević'), arxiv.Result.Author('Milena Pustet'), arxiv.Result.Author('Nyco Bischoff'), arxiv.Result.Author('María do Mar Castro Varela'), arxiv.Result.Author('Yener Bayramoğlu'), arxiv.Result.Author('Bahar Oghalai')]","Over the course of the COVID-19 pandemic, existing conspiracy theories were
refreshed and new ones were created, often interwoven with antisemitic
narratives, stereotypes and codes. The sheer volume of antisemitic and
conspiracy theory content on the Internet makes data-driven algorithmic
approaches essential for anti-discrimination organizations and researchers
alike. However, the manifestation and dissemination of these two interrelated
phenomena is still quite under-researched in scholarly empirical research of
large text corpora. Algorithmic approaches for the detection and classification
of specific contents usually require labeled datasets, annotated based on
conceptually sound guidelines. While there is a growing number of datasets for
the more general phenomenon of hate speech, the development of corpora and
annotation guidelines for antisemitic and conspiracy content is still in its
infancy, especially for languages other than English. We contribute to closing
this gap by developing an annotation guide for antisemitic and conspiracy
theory online content in the context of the COVID-19 pandemic. We provide
working definitions, including specific forms of antisemitism such as encoded
and post-Holocaust antisemitism. We use these to annotate a German-language
dataset consisting of ~3,700 Telegram messages sent between 03/2020 and
12/2021."
12499,"We hope that the release of this large-scale dataset enables research commu-
nity to revisit and conduct further research into the problem of entity linking on social me-
dia.","6 https://spacy.io/api/entityrecognizer
    7https://huggingface.co/socialmediaie/bertweet-base_wnut17_ner
    8https://github.com/informagi/REL
    9https://github.com/gammaliu/tagme
   10https://cloud.google.com/natural-language

                                                          10
7 Conclusion

We described the largest dataset for NERD tasks on Tweets called TweetNERD and performed
benchmarking on popular NERD systems on its two subsets TweetNERD-OOD and TweetNERD-
Academic.","TweetNERD should foster research and development of robust NERD models for social
media which exhibit generalization across domains and time periods.",2022-10-14 21:55:07+00:00,TweetNERD -- End to End Entity Linking Benchmark for Tweets,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG', '68T50, 68T07', 'I.2.7']","[arxiv.Result.Author('Shubhanshu Mishra'), arxiv.Result.Author('Aman Saini'), arxiv.Result.Author('Raheleh Makki'), arxiv.Result.Author('Sneha Mehta'), arxiv.Result.Author('Aria Haghighi'), arxiv.Result.Author('Ali Mollahosseini')]","Named Entity Recognition and Disambiguation (NERD) systems are foundational
for information retrieval, question answering, event detection, and other
natural language processing (NLP) applications. We introduce TweetNERD, a
dataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on
Tweets. This is the largest and most temporally diverse open sourced dataset
benchmark for NERD on Tweets and can be used to facilitate research in this
area. We describe evaluation setup with TweetNERD for three NERD tasks: Named
Entity Recognition (NER), Entity Linking with True Spans (EL), and End to End
Entity Linking (End2End); and provide performance of existing publicly
available methods on specific TweetNERD splits. TweetNERD is available at:
https://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0
International (CC BY 4.0) license. Check out more details at
https://github.com/twitter-research/TweetNERD."
12513,"mance on the expert knowledge prediction task and      We further study the inﬂuence of loss weights λ1,
follow the same dataset splits as in Table 3.          λ2, λ3 and λ4.",We report model perfor-       ing the effectiveness of our loss function design.,"We ﬁx λ1 = 1 and λ4 = 10−5,
                                                       present model performance under different settings
                                                       of loss weights for auxiliary tasks λ2 and λ3 in
                                                       Figure 8.",2022-10-15 19:28:06+00:00,PAR: Political Actor Representation Learning with Social Context and Expert Knowledge,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shangbin Feng'), arxiv.Result.Author('Zhaoxuan Tan'), arxiv.Result.Author('Zilong Chen'), arxiv.Result.Author('Ningnan Wang'), arxiv.Result.Author('Peisheng Yu'), arxiv.Result.Author('Qinghua Zheng'), arxiv.Result.Author('Xiaojun Chang'), arxiv.Result.Author('Minnan Luo')]","Modeling the ideological perspectives of political actors is an essential
task in computational political science with applications in many downstream
tasks. Existing approaches are generally limited to textual data and voting
records, while they neglect the rich social context and valuable expert
knowledge for holistic ideological analysis. In this paper, we propose
\textbf{PAR}, a \textbf{P}olitical \textbf{A}ctor \textbf{R}epresentation
learning framework that jointly leverages social context and expert knowledge.
Specifically, we retrieve and extract factual statements about legislators to
leverage social context information. We then construct a heterogeneous
information network to incorporate social context and use relational graph
neural networks to learn legislator representations. Finally, we train PAR with
three objectives to align representation learning with expert knowledge, model
ideological stance consistency, and simulate the echo chamber phenomenon.
Extensive experiments demonstrate that PAR is better at augmenting political
text understanding and successfully advances the state-of-the-art in political
perspective detection and roll call vote prediction. Further analysis proves
that PAR learns representations that reflect the political reality and provide
new insights into political behavior."
12524,"In Proceedings of the 2nd
We therefore propose further research towards in-                Clinical Natural Language Processing Workshop,
cluding multiple prototypes into the system.",cal BERT embeddings.,"pages 72–78, Minneapolis, Minnesota, USA.",2022-10-16 10:12:07+00:00,This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text,cs.CL,['cs.CL'],"[arxiv.Result.Author('Betty van Aken'), arxiv.Result.Author('Jens-Michalis Papaioannou'), arxiv.Result.Author('Marcel G. Naik'), arxiv.Result.Author('Georgios Eleftheriadis'), arxiv.Result.Author('Wolfgang Nejdl'), arxiv.Result.Author('Felix A. Gers'), arxiv.Result.Author('Alexander Löser')]","The use of deep neural models for diagnosis prediction from clinical text has
shown promising results. However, in clinical practice such models must not
only be accurate, but provide doctors with interpretable and helpful results.
We introduce ProtoPatient, a novel method based on prototypical networks and
label-wise attention with both of these abilities. ProtoPatient makes
predictions based on parts of the text that are similar to prototypical
patients - providing justifications that doctors understand. We evaluate the
model on two publicly available clinical datasets and show that it outperforms
existing baselines. Quantitative and qualitative evaluations with medical
doctors further demonstrate that the model provides valuable explanations for
clinical decision support."
12527,"2021), we             logue contradiction detection, while this task is
automatically construct the CDCONV conversa-               still far from solved and requires further study.","Results show the importance of
tradiction in natural human-bot conversations are          properly modeling contextual information in dia-
extremely unbalanced (§3, Nie et al.","tions combined with elaborate manual inspection
(§4.1).",2022-10-16 11:37:09+00:00,CDConv: A Benchmark for Contradiction Detection in Chinese Conversations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Jinfeng Zhou'), arxiv.Result.Author('Yinhe Zheng'), arxiv.Result.Author('Libiao Peng'), arxiv.Result.Author('Zhen Guo'), arxiv.Result.Author('Wenquan Wu'), arxiv.Result.Author('Zhengyu Niu'), arxiv.Result.Author('Hua Wu'), arxiv.Result.Author('Minlie Huang')]","Dialogue contradiction is a critical issue in open-domain dialogue systems.
The contextualization nature of conversations makes dialogue contradiction
detection rather challenging. In this work, we propose a benchmark for
Contradiction Detection in Chinese Conversations, namely CDConv. It contains
12K multi-turn conversations annotated with three typical contradiction
categories: Intra-sentence Contradiction, Role Confusion, and History
Contradiction. To efficiently construct the CDConv conversations, we devise a
series of methods for automatic conversation generation, which simulate common
user behaviors that trigger chatbots to make contradictions. We conduct careful
manual quality screening of the constructed conversations and show that
state-of-the-art Chinese chatbots can be easily goaded into making
contradictions. Experiments on CDConv show that properly modeling contextual
information is critical for dialogue contradiction detection, but there are
still unresolved challenges that require future research."
12533,"ACKNOWLEDGMENTS
on some downstream tasks, but how to leverage visual information
to improve all downstream tasks requires further study9.","We see
the beneﬁt of incorporating visual information during pre-training                               6.","We thank National Center for High-performance Computing (NCHC)
                                                                           of National Applied Research Laboratories (NARLabs) in Taiwan
     On the public-set track, DistilHuBERT reduces HuBERT-                 and Taiwan Web Service (TWS) for providing computational and
based’s size by 75% while retaining the performance on some                storage resources used in this challenge.",2022-10-16 20:50:04+00:00,SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Tzu-hsun Feng'), arxiv.Result.Author('Annie Dong'), arxiv.Result.Author('Ching-Feng Yeh'), arxiv.Result.Author('Shu-wen Yang'), arxiv.Result.Author('Tzu-Quan Lin'), arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Zili Huang'), arxiv.Result.Author('Haibin Wu'), arxiv.Result.Author('Xuankai Chang'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Hung-yi Lee')]","We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research."
12534,"[24] Dongwei Jiang, Wubo Li, Ruixiong Zhang, Miao Cao, Ne Luo,
 [9] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,                      Yang Han, Wei Zou, et al., “A further study of unsuper-
      Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman                  vised pretraining for transformer based speech recognition,” in
      Mohamed, “HuBERT: Self-supervised speech representation                 ICASSP, 2021.
      learning by masked prediction of hidden units,” IEEE/ACM
      Transactions on Audio, Speech, and Language Processing, vol.","[23] Dongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yuxuan Hu,
 [8] Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen,                       Wei Zou, and Xiangang Li, “Improving transformer-based
      Zhuo Chen, Shujie Liu, Jian Wu, et al., “UniSpeech-SAT:                 speech recognition using unsupervised pre-training,” arXiv
      Universal speech representation learning with speaker aware             preprint arXiv:1910.09932, 2019.
      pre-training,” in ICASSP, 2022.","[25] Xianghu Yue and Haizhou Li, “Phonetically motivated self-
      29, pp.",2022-10-16 20:50:04+00:00,SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Tzu-hsun Feng'), arxiv.Result.Author('Annie Dong'), arxiv.Result.Author('Ching-Feng Yeh'), arxiv.Result.Author('Shu-wen Yang'), arxiv.Result.Author('Tzu-Quan Lin'), arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Zili Huang'), arxiv.Result.Author('Haibin Wu'), arxiv.Result.Author('Xuankai Chang'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Hung-yi Lee')]","We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research."
12535,"ACKNOWLEDGMENTS
on some downstream tasks, but how to leverage visual information
to improve all downstream tasks requires further study9.","We see
the beneﬁt of incorporating visual information during pre-training                               6.","We thank National Center for High-performance Computing (NCHC)
                                                                           of National Applied Research Laboratories (NARLabs) in Taiwan
     On the public-set track, DistilHuBERT reduces HuBERT-                 and Taiwan Web Service (TWS) for providing computational and
based’s size by 75% while retaining the performance on some                storage resources used in this challenge.",2022-10-16 20:50:04+00:00,SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Tzu-hsun Feng'), arxiv.Result.Author('Annie Dong'), arxiv.Result.Author('Ching-Feng Yeh'), arxiv.Result.Author('Shu-wen Yang'), arxiv.Result.Author('Tzu-Quan Lin'), arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Zili Huang'), arxiv.Result.Author('Haibin Wu'), arxiv.Result.Author('Xuankai Chang'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Hung-yi Lee')]","We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research."
12536,"[24] Dongwei Jiang, Wubo Li, Ruixiong Zhang, Miao Cao, Ne Luo,
 [9] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,                      Yang Han, Wei Zou, et al., “A further study of unsuper-
      Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman                  vised pretraining for transformer based speech recognition,” in
      Mohamed, “HuBERT: Self-supervised speech representation                 ICASSP, 2021.
      learning by masked prediction of hidden units,” IEEE/ACM
      Transactions on Audio, Speech, and Language Processing, vol.","[23] Dongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yuxuan Hu,
 [8] Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen,                       Wei Zou, and Xiangang Li, “Improving transformer-based
      Zhuo Chen, Shujie Liu, Jian Wu, et al., “UniSpeech-SAT:                 speech recognition using unsupervised pre-training,” arXiv
      Universal speech representation learning with speaker aware             preprint arXiv:1910.09932, 2019.
      pre-training,” in ICASSP, 2022.","[25] Xianghu Yue and Haizhou Li, “Phonetically motivated self-
      29, pp.",2022-10-16 20:50:04+00:00,SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Tzu-hsun Feng'), arxiv.Result.Author('Annie Dong'), arxiv.Result.Author('Ching-Feng Yeh'), arxiv.Result.Author('Shu-wen Yang'), arxiv.Result.Author('Tzu-Quan Lin'), arxiv.Result.Author('Jiatong Shi'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Zili Huang'), arxiv.Result.Author('Haibin Wu'), arxiv.Result.Author('Xuankai Chang'), arxiv.Result.Author('Shinji Watanabe'), arxiv.Result.Author('Abdelrahman Mohamed'), arxiv.Result.Author('Shang-Wen Li'), arxiv.Result.Author('Hung-yi Lee')]","We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research."
12545,"This      set distribution is too large, which can be a direc-
indicates a large amount of prior shift in common      tion for further research.","The SSR-PU     method, and there is still much left to be improved
method, on the other hand, can alleviate this prob-    for extremely unlabeled scenarios and scenarios
lem well, contributing to a better balance among       where the gap between the test set and the training
precision and recall and better performance.","However, for now, we
relations, which is consistent with (Huang et al.,     believe that our task is a valuable contribution to
2022) observation that common relations are more       advancing the application of document-level RE
likely to be labeled in the dataset.",2022-10-17 02:54:49+00:00,A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ye Wang'), arxiv.Result.Author('Xinxin Liu'), arxiv.Result.Author('Wenxin Hu'), arxiv.Result.Author('Tao Zhang')]","Document-level relation extraction (RE) aims to identify relations between
entities across multiple sentences. Most previous methods focused on
document-level RE under full supervision. However, in real-world scenario, it
is expensive and difficult to completely label all relations in a document
because the number of entity pairs in document-level RE grows quadratically
with the number of entities. To solve the common incomplete labeling problem,
we propose a unified positive-unlabeled learning framework - shift and squared
ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled
(PU) learning on document-level RE for the first time. Considering that labeled
data of a dataset may lead to prior shift of unlabeled data, we introduce a PU
learning under prior shift of training data. Also, using none-class score as an
adaptive threshold, we propose squared ranking loss and prove its Bayesian
consistency with multi-label ranking metrics. Extensive experiments demonstrate
that our method achieves an improvement of about 14 F1 points relative to the
previous baseline with incomplete labeling. In addition, it outperforms
previous state-of-the-art results under both fully supervised and extremely
unlabeled settings as well."
12546,"This      set distribution is too large, which can be a direc-
indicates a large amount of prior shift in common      tion for further research.","The SSR-PU     method, and there is still much left to be improved
method, on the other hand, can alleviate this prob-    for extremely unlabeled scenarios and scenarios
lem well, contributing to a better balance among       where the gap between the test set and the training
precision and recall and better performance.","However, for now, we
relations, which is consistent with (Huang et al.,     believe that our task is a valuable contribution to
2022) observation that common relations are more       advancing the application of document-level RE
likely to be labeled in the dataset.",2022-10-17 02:54:49+00:00,A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ye Wang'), arxiv.Result.Author('Xinxin Liu'), arxiv.Result.Author('Wenxin Hu'), arxiv.Result.Author('Tao Zhang')]","Document-level relation extraction (RE) aims to identify relations between
entities across multiple sentences. Most previous methods focused on
document-level RE under full supervision. However, in real-world scenario, it
is expensive and difficult to completely label all relations in a document
because the number of entity pairs in document-level RE grows quadratically
with the number of entities. To solve the common incomplete labeling problem,
we propose a unified positive-unlabeled learning framework - shift and squared
ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled
(PU) learning on document-level RE for the first time. Considering that labeled
data of a dataset may lead to prior shift of unlabeled data, we introduce a PU
learning under prior shift of training data. Also, using none-class score as an
adaptive threshold, we propose squared ranking loss and prove its Bayesian
consistency with multi-label ranking metrics. Extensive experiments demonstrate
that our method achieves an improvement of about 14 F1 points relative to the
previous baseline with incomplete labeling. In addition, it outperforms
previous state-of-the-art results under both fully supervised and extremely
unlabeled settings as well."
12550,"We release the ﬁrst Korean long-term dialogue
task of memory management in long-term conver-               dataset for further research on memory man-
sations and construct its corresponding dataset1,            agement in dialogues.",We formulate a new         3.,"by extending an existing Korean open-domain dia-
logue dataset (Bae et al., 2022) to multiple sessions  2 Related Work
with changing user information.",2022-10-17 05:06:38+00:00,Keep Me Updated! Memory Management in Long-term Conversations,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sanghwan Bae'), arxiv.Result.Author('Donghyun Kwak'), arxiv.Result.Author('Soyoung Kang'), arxiv.Result.Author('Min Young Lee'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Yuin Jeong'), arxiv.Result.Author('Hyeri Kim'), arxiv.Result.Author('Sang-Woo Lee'), arxiv.Result.Author('Woomyoung Park'), arxiv.Result.Author('Nako Sung')]","Remembering important information from the past and continuing to talk about
it in the present are crucial in long-term conversations. However, previous
literature does not deal with cases where the memorized information is
outdated, which may cause confusion in later conversations. To address this
issue, we present a novel task and a corresponding dataset of memory management
in long-term conversations, in which bots keep track of and bring up the latest
information about users while conversing through multiple sessions. In order to
support more precise and interpretable memory, we represent memory as
unstructured text descriptions of key information and propose a new mechanism
of memory management that selectively eliminates invalidated or redundant
information. Experimental results show that our approach outperforms the
baselines that leave the stored memory unchanged in terms of engagingness and
humanness, with larger performance gap especially in the later sessions."
12551,"We release the newly collected dataset,       sues, we carefully constructed criteria for harmful
looking forward to further research on this promis-      texts based on legal and ethical considerations of-
ing direction.","To address these is-
dialogues.",fered by our group’s specialists.,2022-10-17 05:06:38+00:00,Keep Me Updated! Memory Management in Long-term Conversations,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Sanghwan Bae'), arxiv.Result.Author('Donghyun Kwak'), arxiv.Result.Author('Soyoung Kang'), arxiv.Result.Author('Min Young Lee'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Yuin Jeong'), arxiv.Result.Author('Hyeri Kim'), arxiv.Result.Author('Sang-Woo Lee'), arxiv.Result.Author('Woomyoung Park'), arxiv.Result.Author('Nako Sung')]","Remembering important information from the past and continuing to talk about
it in the present are crucial in long-term conversations. However, previous
literature does not deal with cases where the memorized information is
outdated, which may cause confusion in later conversations. To address this
issue, we present a novel task and a corresponding dataset of memory management
in long-term conversations, in which bots keep track of and bring up the latest
information about users while conversing through multiple sessions. In order to
support more precise and interpretable memory, we represent memory as
unstructured text descriptions of key information and propose a new mechanism
of memory management that selectively eliminates invalidated or redundant
information. Experimental results show that our approach outperforms the
baselines that leave the stored memory unchanged in terms of engagingness and
humanness, with larger performance gap especially in the later sessions."
12552,"We further study the effectiveness of our pro-
                                                                         posed three pre-training tasks, and the results are
                                                                         shown in the lower side of Table 3.","It has relatively less inﬂuence when valuable infor-
                                                                         mation has already been reﬁned into the utterance
                                                                         representation.","We denote the
                                                                         model without pre-training on utterance-/history-
                                                                         /user-level as w/o Pre.",2022-10-17 05:16:23+00:00,MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zhaoheng Huang'), arxiv.Result.Author('Zhicheng Dou'), arxiv.Result.Author('Yutao Zhu'), arxiv.Result.Author('Zhengyi Ma')]","Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods."
12553,"We further study the effectiveness of our pro-
                                                                         posed three pre-training tasks, and the results are
                                                                         shown in the lower side of Table 3.","It has relatively less inﬂuence when valuable infor-
                                                                         mation has already been reﬁned into the utterance
                                                                         representation.","We denote the
                                                                         model without pre-training on utterance-/history-
                                                                         /user-level as w/o Pre.",2022-10-17 05:16:23+00:00,MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zhaoheng Huang'), arxiv.Result.Author('Zhicheng Dou'), arxiv.Result.Author('Yutao Zhu'), arxiv.Result.Author('Zhengyi Ma')]","Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods."
12554,"We further study the effectiveness of our pro-
                                                                         posed three pre-training tasks, and the results are
                                                                         shown in the lower side of Table 3.","It has relatively less inﬂuence when valuable infor-
                                                                         mation has already been reﬁned into the utterance
                                                                         representation.","We denote the
                                                                         model without pre-training on utterance-/history-
                                                                         /user-level as w/o Pre.",2022-10-17 05:16:23+00:00,MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zhaoheng Huang'), arxiv.Result.Author('Zhicheng Dou'), arxiv.Result.Author('Yutao Zhu'), arxiv.Result.Author('Zhengyi Ma')]","Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods."
12555,"We further study the effectiveness of our pro-
                                                                         posed three pre-training tasks, and the results are
                                                                         shown in the lower side of Table 3.","(2) MCP performs                             It has relatively less inﬂuence when valuable infor-
                                                                         mation has already been reﬁned into the utterance
                                                                         representation.","We denote the
                                                                         model without pre-training on utterance-/history-
                                                                         /user-level as w/o Pre.",2022-10-17 05:16:23+00:00,MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zhaoheng Huang'), arxiv.Result.Author('Zhicheng Dou'), arxiv.Result.Author('Yutao Zhu'), arxiv.Result.Author('Zhengyi Ma')]","Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods."
12556,"To facilitate further research, we    challenging than the latter, mainly due to the char-
                                             construct the ﬁrst synthetic training datasets,   acteristics of speech.","The former is more
                                             (SpeechRE).","(i) Speech carries much richer
                                             as well as the ﬁrst human-spoken test set         information beyond linguistic content (unlike text),
                                             with native English speakers.",2022-10-17 05:53:49+00:00,Towards Relation Extraction From Speech,cs.CL,"['cs.CL', 'cs.MM']","[arxiv.Result.Author('Tongtong Wu'), arxiv.Result.Author('Guitao Wang'), arxiv.Result.Author('Jinming Zhao'), arxiv.Result.Author('Zhaoran Liu'), arxiv.Result.Author('Guilin Qi'), arxiv.Result.Author('Yuan-Fang Li'), arxiv.Result.Author('Gholamreza Haffari')]","Relation extraction typically aims to extract semantic relationships between
entities from the unstructured text. One of the most essential data sources for
relation extraction is the spoken language, such as interviews and dialogues.
However, the error propagation introduced in automatic speech recognition (ASR)
has been ignored in relation extraction, and the end-to-end speech-based
relation extraction method has been rarely explored. In this paper, we propose
a new listening information extraction task, i.e., speech relation extraction.
We construct the training dataset for speech relation extraction via
text-to-speech systems, and we construct the testing dataset via crowd-sourcing
with native English speakers. We explore speech relation extraction via two
approaches: the pipeline approach conducting text-based extraction with a
pretrained ASR module, and the end2end approach via a new proposed
encoder-decoder model, or what we called SpeechRE. We conduct comprehensive
experiments to distinguish the challenges in speech relation extraction, which
may shed light on future explorations. We share the code and data on
https://github.com/wutong8023/SpeechRE."
12557,"(i) The ASR model is
   gap between the pipeline approach and the e2e      optimized to minimized Word Error Rate (WER)1,
   approach, motivating further research.","Our          A major drawback of this approach is that
   extensive experiments identify a performance       each module is trained and optimized indepen-
   gap between TextRE and SpeechRE, and the           dently (Serdyuk et al., 2018).","often equally weighting every word, whereas
                                                      not every word has the same impact on SLU.",2022-10-17 05:53:49+00:00,Towards Relation Extraction From Speech,cs.CL,"['cs.CL', 'cs.MM']","[arxiv.Result.Author('Tongtong Wu'), arxiv.Result.Author('Guitao Wang'), arxiv.Result.Author('Jinming Zhao'), arxiv.Result.Author('Zhaoran Liu'), arxiv.Result.Author('Guilin Qi'), arxiv.Result.Author('Yuan-Fang Li'), arxiv.Result.Author('Gholamreza Haffari')]","Relation extraction typically aims to extract semantic relationships between
entities from the unstructured text. One of the most essential data sources for
relation extraction is the spoken language, such as interviews and dialogues.
However, the error propagation introduced in automatic speech recognition (ASR)
has been ignored in relation extraction, and the end-to-end speech-based
relation extraction method has been rarely explored. In this paper, we propose
a new listening information extraction task, i.e., speech relation extraction.
We construct the training dataset for speech relation extraction via
text-to-speech systems, and we construct the testing dataset via crowd-sourcing
with native English speakers. We explore speech relation extraction via two
approaches: the pipeline approach conducting text-based extraction with a
pretrained ASR module, and the end2end approach via a new proposed
encoder-decoder model, or what we called SpeechRE. We conduct comprehensive
experiments to distinguish the challenges in speech relation extraction, which
may shed light on future explorations. We share the code and data on
https://github.com/wutong8023/SpeechRE."
12600,"Additionally, we                 To alleviate this ﬂaw and to facilitate reproducibility as well
                                        propose a new way of measuring the success of said extraction             as further research, we introduce KPI-EDGAR, a dataset based
                                        process by incorporating a word-level weighting scheme into               on publicly available data and manually annotated by us, with
                                        the conventional F1 score to better model the inherently fuzzy            the main difference being that the language changed from
                                        borders of the entity pairs of a relation in this domain.","We further provide four accompanying baselines
                                        for benchmarking potential future research.",German to English.,2022-10-17 15:06:20+00:00,KPI-EDGAR: A Novel Dataset and Accompanying Metric for Relation Extraction from Financial Documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tobias Deußer'), arxiv.Result.Author('Syed Musharraf Ali'), arxiv.Result.Author('Lars Hillebrand'), arxiv.Result.Author('Desiana Nurchalifah'), arxiv.Result.Author('Basil Jacob'), arxiv.Result.Author('Christian Bauckhage'), arxiv.Result.Author('Rafet Sifa')]","We introduce KPI-EDGAR, a novel dataset for Joint Named Entity Recognition
and Relation Extraction building on financial reports uploaded to the
Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system, where the
main objective is to extract Key Performance Indicators (KPIs) from financial
documents and link them to their numerical values and other attributes. We
further provide four accompanying baselines for benchmarking potential future
research. Additionally, we propose a new way of measuring the success of said
extraction process by incorporating a word-level weighting scheme into the
conventional F1 score to better model the inherently fuzzy borders of the
entity pairs of a relation in this domain."
12601,"The results should be seen as a baseline on which
and $80 million, respectively.”                                     further research can be benchmarked.","Herein, we brieﬂy evaluate the unmodiﬁed KPI-BERT [2]
   Given the following example sentence and ground truth            (see subsection IV-A), the same structure but with word
relations from a ﬁnancial document,                                 embeddings from EDGAR–W2V [5] and GloVe [43] instead
                                                                    of embeddings from BERT [27] (see subsection IV-C for these
“In 2021 and 2020 the total net revenue was $100 million            two), and a competing span-based approach (see subsection
                                 kpi              cy                IV-B), as introduced by [16] and named SpERT, on our novel
                                                                    dataset.","We also look into a few

          py                                                           4Please note that no extensive grid search and ﬁnetuning was performed in
                                                                    this context, as the aim of this work is to introduce a new dataset and a better
total net revenue − 100, total net revenue − 80,                    suited metric and not to ﬁnd the model best suited for this task.",2022-10-17 15:06:20+00:00,KPI-EDGAR: A Novel Dataset and Accompanying Metric for Relation Extraction from Financial Documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tobias Deußer'), arxiv.Result.Author('Syed Musharraf Ali'), arxiv.Result.Author('Lars Hillebrand'), arxiv.Result.Author('Desiana Nurchalifah'), arxiv.Result.Author('Basil Jacob'), arxiv.Result.Author('Christian Bauckhage'), arxiv.Result.Author('Rafet Sifa')]","We introduce KPI-EDGAR, a novel dataset for Joint Named Entity Recognition
and Relation Extraction building on financial reports uploaded to the
Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system, where the
main objective is to extract Key Performance Indicators (KPIs) from financial
documents and link them to their numerical values and other attributes. We
further provide four accompanying baselines for benchmarking potential future
research. Additionally, we propose a new way of measuring the success of said
extraction process by incorporating a word-level weighting scheme into the
conventional F1 score to better model the inherently fuzzy borders of the
entity pairs of a relation in this domain."
12602,"Accordingly,
kpi  cy                          kpi  py                            these results should be seen as baselines for further research.","We also look into a few

          py                                                           4Please note that no extensive grid search and ﬁnetuning was performed in
                                                                    this context, as the aim of this work is to introduce a new dataset and a better
total net revenue − 100, total net revenue − 80,                    suited metric and not to ﬁnd the model best suited for this task.","we found that predictions often omit a part of non-numerical
entities like the KPI entity.",2022-10-17 15:06:20+00:00,KPI-EDGAR: A Novel Dataset and Accompanying Metric for Relation Extraction from Financial Documents,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tobias Deußer'), arxiv.Result.Author('Syed Musharraf Ali'), arxiv.Result.Author('Lars Hillebrand'), arxiv.Result.Author('Desiana Nurchalifah'), arxiv.Result.Author('Basil Jacob'), arxiv.Result.Author('Christian Bauckhage'), arxiv.Result.Author('Rafet Sifa')]","We introduce KPI-EDGAR, a novel dataset for Joint Named Entity Recognition
and Relation Extraction building on financial reports uploaded to the
Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system, where the
main objective is to extract Key Performance Indicators (KPIs) from financial
documents and link them to their numerical values and other attributes. We
further provide four accompanying baselines for benchmarking potential future
research. Additionally, we propose a new way of measuring the success of said
extraction process by incorporating a word-level weighting scheme into the
conventional F1 score to better model the inherently fuzzy borders of the
entity pairs of a relation in this domain."
12610,"We release the data
and prompts used in the work, as well as the outputs from the Codex models, to facilitate further research.","Experiments demonstrate that
answer-only prompting underestimates model capabilities and that CoT prompting enables the most capable
Codex model to outperform the average human-rater baseline on 17 out of 23 tasks in BBH.","10
Acknowledgements

We would like to thank Ekin Akyürek, Michael Collins, Andrew Drozdov, Dan Jurafsky, Yoon Kim, Andrew
Lampinen, Shayne Longpre, Amil Merchant, and Freda Shi for helpful discussions and feedback.",2022-10-17 17:08:26+00:00,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Mirac Suzgun'), arxiv.Result.Author('Nathan Scales'), arxiv.Result.Author('Nathanael Schärli'), arxiv.Result.Author('Sebastian Gehrmann'), arxiv.Result.Author('Yi Tay'), arxiv.Result.Author('Hyung Won Chung'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Quoc V. Le'), arxiv.Result.Author('Ed H. Chi'), arxiv.Result.Author('Denny Zhou'), arxiv.Result.Author('Jason Wei')]","BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that
focuses on tasks believed to be beyond the capabilities of current language
models. Language models have already made good progress on this benchmark, with
the best model in the BIG-Bench paper outperforming average reported
human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But
on what tasks do language models fall short of average human-rater performance,
and are those tasks actually unsolvable by current language models?
  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we
call BIG-Bench Hard (BBH). These are the task for which prior language model
evaluations did not outperform the average human-rater. We find that applying
chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the
average human-rater performance on 10 of the 23 tasks, and Codex
(code-davinci-002) to surpass the average human-rater performance on 17 of the
23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot
prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,
2022), substantially underestimates the best performance and capabilities of
language models, which is better captured via CoT prompting. As further
analysis, we explore the interaction between CoT and model scale on BBH,
finding that CoT enables emergent task performance on several BBH tasks with
otherwise flat scaling curves."
12642,"Extensive analy-
and 11) correspond to the four models in Table 3.           ses also point out the drawbacks of larger models
We list our observations as below:                          and DRO in addressing the curse of multilinguality,
                                                            which warrants further research in the future.","Our submissions won the 1st
where the lines marked in blue (i.e., 2 , 5 , 10            place in the constrained track.","• 3 vs. 2 : Directly ﬁnetuning the TRANSF-DEEP
  model on the Large-234 dataset induces the
  performance drop.",2022-10-18 07:22:29+00:00,Tencent's Multilingual Machine Translation System for WMT22 Large-Scale African Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wenxiang Jiao'), arxiv.Result.Author('Zhaopeng Tu'), arxiv.Result.Author('Jiarui Li'), arxiv.Result.Author('Wenxuan Wang'), arxiv.Result.Author('Jen-tse Huang'), arxiv.Result.Author('Shuming Shi')]","This paper describes Tencent's multilingual machine translation systems for
the WMT22 shared task on Large-Scale Machine Translation Evaluation for African
Languages. We participated in the $\mathbf{constrained}$ translation track in
which only the data and pretrained models provided by the organizer are
allowed. The task is challenging due to three problems, including the absence
of training data for some to-be-evaluated language pairs, the uneven
optimization of language pairs caused by data imbalance, and the curse of
multilinguality. To address these problems, we adopt data augmentation,
distributionally robust optimization, and language family grouping,
respectively, to develop our multilingual neural machine translation (MNMT)
models. Our submissions won the $\mathbf{1st\ place}$ on the blind test sets in
terms of the automatic evaluation metrics. Codes, models, and detailed
competition results are available at
https://github.com/wxjiao/WMT2022-Large-Scale-African."
12702,"Are LMs still highly linearly dependent after    well, and further research is needed.","layer-wise and token-level (including the [CLS]
                                                      and [MASK] token) “correlation” is of interest as
  4.",ﬁne-tuning on downstream tasks?,2022-10-19 04:28:19+00:00,Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', '68T50 (Primary) 68T30, 68T07 (Secondary)', 'I.2.7']",[arxiv.Result.Author('Hao Zhang')],"Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its
variants, have led to significant improvements on various NLP tasks in past
years. However, a theoretical framework for studying their relationships is
still missing. In this paper, we fill this gap by investigating the linear
dependency between pre-trained LMs. The linear dependency of LMs is defined
analogously to the linear dependency of vectors. We propose Language Model
Decomposition (LMD) to represent a LM using a linear combination of other LMs
as basis, and derive the closed-form solution. A goodness-of-fit metric for LMD
similar to the coefficient of determination is defined and used to measure the
linear dependency of a set of LMs. In experiments, we find that BERT and eleven
(11) BERT-like LMs are 91% linearly dependent. This observation suggests that
current state-of-the-art (SOTA) LMs are highly ""correlated"". To further advance
SOTA we need more diverse and novel LMs that are less dependent on existing
LMs."
12703,"Are LMs still highly linearly dependent after    well, and further research is needed.","layer-wise and token-level (including the [CLS]
                                                      and [MASK] token) “correlation” is of interest as
  4.",ﬁne-tuning on downstream tasks?,2022-10-19 04:28:19+00:00,Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', '68T50 (Primary) 68T30, 68T07 (Secondary)', 'I.2.7']",[arxiv.Result.Author('Hao Zhang')],"Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its
variants, have led to significant improvements on various NLP tasks in past
years. However, a theoretical framework for studying their relationships is
still missing. In this paper, we fill this gap by investigating the linear
dependency between pre-trained LMs. The linear dependency of LMs is defined
analogously to the linear dependency of vectors. We propose Language Model
Decomposition (LMD) to represent a LM using a linear combination of other LMs
as basis, and derive the closed-form solution. A goodness-of-fit metric for LMD
similar to the coefficient of determination is defined and used to measure the
linear dependency of a set of LMs. In experiments, we find that BERT and eleven
(11) BERT-like LMs are 91% linearly dependent. This observation suggests that
current state-of-the-art (SOTA) LMs are highly ""correlated"". To further advance
SOTA we need more diverse and novel LMs that are less dependent on existing
LMs."
12706,"applicable to a wide range of current transformer-
based large pretrained language models, more ex-        James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
periments and theoretical explanations are needed          Joel Veness, Guillaume Desjardins, Andrei A Rusu,
for further research.","In ICLR
empirical investigation and conclusion is widely           (Poster).","Also, due to computational           Kieran Milan, John Quan, Tiago Ramalho, Ag-
resources limitation, we cannot investigate whether        nieszka Grabska-Barwinska, et al.",2022-10-19 06:44:20+00:00,Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chenghao Yang'), arxiv.Result.Author('Xuezhe Ma')]","Fine-tuning over large pretrained language models (PLMs) has established many
state-of-the-art results. Despite its superior performance, such fine-tuning
can be unstable, resulting in significant variance in performance and potential
risks for practical applications. Previous works have attributed such
instability to the catastrophic forgetting problem in the top layers of PLMs,
which indicates iteratively that fine-tuning layers in a top-down manner is a
promising solution. In this paper, we first point out that this method does not
always work out due to the different convergence speeds of different
layers/modules. Inspired by this observation, we propose a simple
component-wise gradient norm clipping method to adjust the convergence speed
for different components. Experiment results demonstrate that our method
achieves consistent improvements in terms of generalization performance,
convergence speed, and training stability. The codebase can be found at
https://github.com/yangalan123/FineTuningStability."
12721,"Although NRE brings ideas to
                                        relation extraction plays an important role in several ﬁelds       the further research about enterprise relations, deep learning
                                        such as economics and ﬁnance, information security and sup-        is data-consuming and there is still a lack of uniﬁed gold-
                                        ply chain security, etc.","In the real world, enterprise      tion of extraction methods.","On one hand, invest institutions can get  standard supervised dataset for enterprise relation extraction
                                        the relations of equity distribution and debtor-creditor among     which hinders the future research in this area.",2022-10-19 14:22:10+00:00,CEntRE: A paragraph-level Chinese dataset for Relation Extraction among Enterprises,cs.CL,"['cs.CL', 'cs.CR']","[arxiv.Result.Author('Peipei Liu'), arxiv.Result.Author('Hong Li'), arxiv.Result.Author('Zhiyu Wang'), arxiv.Result.Author('Yimo Ren'), arxiv.Result.Author('Jie Liu'), arxiv.Result.Author('Fei Lyu'), arxiv.Result.Author('Hongsong Zhu'), arxiv.Result.Author('Limin Sun')]","Enterprise relation extraction aims to detect pairs of enterprise entities
and identify the business relations between them from unstructured or
semi-structured text data, and it is crucial for several real-world
applications such as risk analysis, rating research and supply chain security.
However, previous work mainly focuses on getting attribute information about
enterprises like personnel and corporate business, and pays little attention to
enterprise relation extraction. To encourage further progress in the research,
we introduce the CEntRE, a new dataset constructed from publicly available
business news data with careful human annotation and intelligent data
processing. Extensive experiments on CEntRE with six excellent models
demonstrate the challenges of our proposed dataset."
12722,"In-depth Analysis To further study how the pro-
3.2 Evaluation Metrics                                         posed framework works during event planning, we
                                                               conduct a case study as illustrated in Figure 4.","et al., 2020), in line with previous work in the area.","We adopt a range of automatic metrics includ-                  Given the leading context, we can extract the con-
ing ROUGE-n (R-n) (Lin, 2004) and BLEU-n                       tained event had test.",2022-10-19 14:49:27+00:00,NGEP: A Graph-based Event Planning Framework for Story Generation,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Chen Tang'), arxiv.Result.Author('Zhihao Zhang'), arxiv.Result.Author('Tyler Loakman'), arxiv.Result.Author('Chenghua Lin'), arxiv.Result.Author('Frank Guerin')]","To improve the performance of long text generation, recent studies have
leveraged automatically planned event structures (i.e. storylines) to guide
story generation. Such prior works mostly employ end-to-end neural generation
models to predict event sequences for a story. However, such generation models
struggle to guarantee the narrative coherence of separate events due to the
hallucination problem, and additionally the generated event sequences are often
hard to control due to the end-to-end nature of the models. To address these
challenges, we propose NGEP, an novel event planning framework which generates
an event sequence by performing inference on an automatically constructed event
graph and enhances generalisation ability through a neural event advisor. We
conduct a range of experiments on multiple criteria, and the results
demonstrate that our graph-based neural framework outperforms the
state-of-the-art (SOTA) event planning approaches, considering both the
performance of event sequence generation and the effectiveness on the
downstream task of story generation."
12723,"We leave further study
   The performance improvements seen on BLEU-                     of incorporating both features to future work.","may also lead to the inadequacy of incorporating
                                                                  both features for decoding.","n and Coverage indicate that our generated sto-
ries have a higher degree of overlap with the ref-                Case Study Several generated Chinese stories
erence stories.",2022-10-19 15:01:52+00:00,Improving Chinese Story Generation via Awareness of Syntactic Dependencies and Semantics,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Henglin Huang'), arxiv.Result.Author('Chen Tang'), arxiv.Result.Author('Tyler Loakman'), arxiv.Result.Author('Frank Guerin'), arxiv.Result.Author('Chenghua Lin')]","Story generation aims to generate a long narrative conditioned on a given
input. In spite of the success of prior works with the application of
pre-trained models, current neural models for Chinese stories still struggle to
generate high-quality long text narratives. We hypothesise that this stems from
ambiguity in syntactically parsing the Chinese language, which does not have
explicit delimiters for word segmentation. Consequently, neural models suffer
from the inefficient capturing of features in Chinese narratives. In this
paper, we present a new generation framework that enhances the feature
capturing mechanism by informing the generation model of dependencies between
words and additionally augmenting the semantic representation learning through
synonym denoising training. We conduct a range of experiments, and the results
demonstrate that our framework outperforms the state-of-the-art Chinese
generation models on all evaluation metrics, demonstrating the benefits of
enhanced dependency and semantic representation learning."
12727,Since SQL queries can sometimes be       rics and annotation need further research attention on Spider.,"Our studies show that met-
Query rewrite errors.","rewritten in multiple ways, EM can yield false negatives.",2022-10-19 15:35:06+00:00,N-Best Hypotheses Reranking for Text-To-SQL Systems,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Lu Zeng'), arxiv.Result.Author('Sree Hari Krishnan Parthasarathi'), arxiv.Result.Author('Dilek Hakkani-Tur')]","Text-to-SQL task maps natural language utterances to structured queries that
can be issued to a database. State-of-the-art (SOTA) systems rely on finetuning
large, pre-trained language models in conjunction with constrained decoding
applying a SQL parser. On the well established Spider dataset, we begin with
Oracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's
10-best list, yields a $7.7\%$ absolute improvement in both exact match (EM)
and execution (EX) accuracy, showing significant potential improvements with
reranking. Identifying coherence and correctness as reranking approaches, we
design a model generating a query plan and propose a heuristic schema linking
algorithm. Combining both approaches, with T5-Large, we obtain a consistent
$1\% $ improvement in EM accuracy, and a $~2.5\%$ improvement in EX,
establishing a new SOTA for this task. Our comprehensive error studies on DEV
data show the underlying difficulty in making progress on this task."
12733,"This
                                                               suggests that Heart and Diabetes datasets could be good
6 DISCUSSION                                                   proxies for the community to further study medical-domain
                                                               tabular classiﬁcation with language models without need-
For all datasets except Heart, the List Template and Text      ing access to large private datasets.",lines tend to outperform TabLLM on these datasets.,"Template serializations showed nontrivial zero-shot perfor-
mance, indicating TabLLM is able to effectively utilize the    7 LIMITATION AND CONCLUSIONS
prior knowledge in the LLM for classiﬁcation.",2022-10-19 17:08:13+00:00,TabLLM: Few-shot Classification of Tabular Data with Large Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Stefan Hegselmann'), arxiv.Result.Author('Alejandro Buendia'), arxiv.Result.Author('Hunter Lang'), arxiv.Result.Author('Monica Agrawal'), arxiv.Result.Author('Xiaoyi Jiang'), arxiv.Result.Author('David Sontag')]","We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting."
12760,"The experiments
that are natural, and coherent while being diverse              show that our tasks are still challenging, which
                                                                suggests room for further research.","Second,
like human-human conversations, it is desirable              • We evaluate our proposed tasks with the latest
to have samples of human-system conversations                   machine learning methods.","2 Related Works                                       generated by labeling and linking paragraphs, 2)
                                                      crowdsourcers then write conversations based on
Our work is most closely related to the document-     the suggested ﬂows.",2022-10-20 07:33:05+00:00,Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots,cs.CL,['cs.CL'],"[arxiv.Result.Author('Haomin Fu'), arxiv.Result.Author('Yeqin Zhang'), arxiv.Result.Author('Haiyang Yu'), arxiv.Result.Author('Jian Sun'), arxiv.Result.Author('Fei Huang'), arxiv.Result.Author('Luo Si'), arxiv.Result.Author('Yongbin Li'), arxiv.Result.Author('Cam-Tu Nguyen')]","This paper introduces Doc2Bot, a novel dataset for building machines that
help users seek information via conversations. This is of particular interest
for companies and organizations that own a large number of manuals or
instruction books. Despite its potential, the nature of our task poses several
challenges: (1) documents contain various structures that hinder the ability of
machines to comprehend, and (2) user information needs are often
underspecified. Compared to prior datasets that either focus on a single
structural type or overlook the role of questioning to uncover user needs, the
Doc2Bot dataset is developed to target such challenges systematically. Our
dataset contains over 100,000 turns based on Chinese documents from five
domains, larger than any prior document-grounded dialog dataset for information
seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track
user intentions, (2) dialog policy learning to plan system actions and
contents, and (3) response generation which generates responses based on the
outputs of the dialog policy. Baseline methods based on the latest deep
learning models are presented, indicating that our proposed tasks are
challenging and worthy of further research."
12761,"The experiments
that are natural, and coherent while being diverse              show that our tasks are still challenging, which
                                                                suggests room for further research.","Second,
like human-human conversations, it is desirable              • We evaluate our proposed tasks with the latest
to have samples of human-system conversations                   machine learning methods.","2 Related Works                                       generated by labeling and linking paragraphs, 2)
                                                      crowdsourcers then write conversations based on
Our work is most closely related to the document-     the suggested ﬂows.",2022-10-20 07:33:05+00:00,Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots,cs.CL,['cs.CL'],"[arxiv.Result.Author('Haomin Fu'), arxiv.Result.Author('Yeqin Zhang'), arxiv.Result.Author('Haiyang Yu'), arxiv.Result.Author('Jian Sun'), arxiv.Result.Author('Fei Huang'), arxiv.Result.Author('Luo Si'), arxiv.Result.Author('Yongbin Li'), arxiv.Result.Author('Cam-Tu Nguyen')]","This paper introduces Doc2Bot, a novel dataset for building machines that
help users seek information via conversations. This is of particular interest
for companies and organizations that own a large number of manuals or
instruction books. Despite its potential, the nature of our task poses several
challenges: (1) documents contain various structures that hinder the ability of
machines to comprehend, and (2) user information needs are often
underspecified. Compared to prior datasets that either focus on a single
structural type or overlook the role of questioning to uncover user needs, the
Doc2Bot dataset is developed to target such challenges systematically. Our
dataset contains over 100,000 turns based on Chinese documents from five
domains, larger than any prior document-grounded dialog dataset for information
seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track
user intentions, (2) dialog policy learning to plan system actions and
contents, and (3) response generation which generates responses based on the
outputs of the dialog policy. Baseline methods based on the latest deep
learning models are presented, indicating that our proposed tasks are
challenging and worthy of further research."
12762,"The experiments
that are natural, and coherent while being diverse              show that our tasks are still challenging, which
                                                                suggests room for further research.","Second,
like human-human conversations, it is desirable              • We evaluate our proposed tasks with the latest
to have samples of human-system conversations                   machine learning methods.","2 Related Works                                       generated by labeling and linking paragraphs, 2)
                                                      crowdsourcers then write conversations based on
Our work is most closely related to the document-     the suggested ﬂows.",2022-10-20 07:33:05+00:00,Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots,cs.CL,['cs.CL'],"[arxiv.Result.Author('Haomin Fu'), arxiv.Result.Author('Yeqin Zhang'), arxiv.Result.Author('Haiyang Yu'), arxiv.Result.Author('Jian Sun'), arxiv.Result.Author('Fei Huang'), arxiv.Result.Author('Luo Si'), arxiv.Result.Author('Yongbin Li'), arxiv.Result.Author('Cam-Tu Nguyen')]","This paper introduces Doc2Bot, a novel dataset for building machines that
help users seek information via conversations. This is of particular interest
for companies and organizations that own a large number of manuals or
instruction books. Despite its potential, the nature of our task poses several
challenges: (1) documents contain various structures that hinder the ability of
machines to comprehend, and (2) user information needs are often
underspecified. Compared to prior datasets that either focus on a single
structural type or overlook the role of questioning to uncover user needs, the
Doc2Bot dataset is developed to target such challenges systematically. Our
dataset contains over 100,000 turns based on Chinese documents from five
domains, larger than any prior document-grounded dialog dataset for information
seeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track
user intentions, (2) dialog policy learning to plan system actions and
contents, and (3) response generation which generates responses based on the
outputs of the dialog policy. Baseline methods based on the latest deep
learning models are presented, indicating that our proposed tasks are
challenging and worthy of further research."
12786,"While efforts to handle noise in so-
an exceedingly high number of test submissions        cial media text (such as AA-based pretraining) can
and would encourage future shared tasks to put in     also help, further research is required to establish
place measures to avoid this.","approximation, can be fruitful and pivotal to high
Furthermore, we note that some participants have      performance.",the most optimal ways to do the same.,2022-10-20 14:40:10+00:00,The University of Edinburgh's Submission to the WMT22 Code-Mixing Shared Task (MixMT),cs.CL,['cs.CL'],"[arxiv.Result.Author('Faheem Kirefu'), arxiv.Result.Author('Vivek Iyer'), arxiv.Result.Author('Pinzhen Chen'), arxiv.Result.Author('Laurie Burchell')]","The University of Edinburgh participated in the WMT22 shared task on
code-mixed translation. This consists of two subtasks: i) generating code-mixed
Hindi/English (Hinglish) text generation from parallel Hindi and English
sentences and ii) machine translation from Hinglish to English. As both
subtasks are considered low-resource, we focused our efforts on careful data
generation and curation, especially the use of backtranslation from monolingual
resources. For subtask 1 we explored the effects of constrained decoding on
English and transliterated subwords in order to produce Hinglish. For subtask
2, we investigated different pretraining techniques, namely comparing simple
initialisation from existing machine translation models and aligned
augmentation. For both subtasks, we found that our baseline systems worked
best. Our systems for both subtasks were one of the overall top-performing
submissions."
12814,will be open sourced for further research.,"Our adapta-
ing training, second set tests a stricter level of zero-          tions and complete training and evaluation scripts
shot generalizability to unseen tasks.","Instruction components: We use four conﬁg-                     5.2 Summary of Results
urations of the instruction components appended
to the input of train instances: 1) None uses the                 We ﬁrst provide a short summary of conclusions
short task description, 2) Desc uses the long task                from our study before diving into the details.",2022-10-20 22:23:23+00:00,Boosting Natural Language Generation from Instructions with Meta-Learning,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Budhaditya Deb'), arxiv.Result.Author('Guoqing Zheng'), arxiv.Result.Author('Ahmed Hassan Awadallah')]","Recent work has shown that language models (LMs) trained with multi-task
\textit{instructional learning} (MTIL) can solve diverse NLP tasks in zero- and
few-shot settings with improved performance compared to prompt tuning. MTIL
illustrates that LMs can extract and use information about the task from
instructions beyond the surface patterns of the inputs and outputs. This
suggests that meta-learning may further enhance the utilization of instructions
for effective task transfer. In this paper we investigate whether meta-learning
applied to MTIL can further improve generalization to unseen tasks in a
zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in
three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network
(HNet) based adaptation to generate task specific parameters conditioned on
instructions, and 3) an approach combining HNet and MAML. Through extensive
experiments on the large scale Natural Instructions V2 dataset, we show that
our proposed approaches significantly improve over strong baselines in
zero-shot settings. In particular, meta-learning improves the effectiveness of
instructions and is most impactful when the test tasks are strictly zero-shot
(i.e. no similar tasks in the training set) and are ""hard"" for LMs,
illustrating the potential of meta-learning for MTIL for out-of-distribution
tasks."
12829,"We
As well, several datasets (Liu et al., 2021; Welivita
et al., 2021) of empathetic dialogue generation have     take the representation of [CLS] to represent the
been published for further research.","ance, and d is the hidden size of the encoder.","However, most
of the current approaches do not pay enough atten-       utterance:
tion to the emotion ﬂow of the conversations.",2022-10-21 03:51:18+00:00,Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Lanrui Wang'), arxiv.Result.Author('Jiangnan Li'), arxiv.Result.Author('Zheng Lin'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Chenxu Yang'), arxiv.Result.Author('Weiping Wang'), arxiv.Result.Author('Jie Zhou')]","Empathy, which is widely used in psychological counselling, is a key trait of
everyday human conversations. Equipped with commonsense knowledge, current
approaches to empathetic response generation focus on capturing implicit
emotion within dialogue context, where the emotions are treated as a static
variable throughout the conversations. However, emotions change dynamically
between utterances, which makes previous works difficult to perceive the
emotion flow and predict the correct emotion of the target response, leading to
inappropriate response. Furthermore, simply importing commonsense knowledge
without harmonization may trigger the conflicts between knowledge and emotion,
which confuse the model to choose incorrect information to guide the generation
process. To address the above problems, we propose a Serial Encoding and
Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.
We use a fine-grained encoding strategy which is more sensitive to the emotion
dynamics (emotion flow) in the conversations to predict the emotion-intent
characteristic of response. Besides, we design a novel framework to model the
interaction between knowledge and emotion to generate more sensible response.
Extensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms
the strong baselines in both automatic and manual evaluations."
12830,"We
As well, several datasets (Liu et al., 2021; Welivita
et al., 2021) of empathetic dialogue generation have     take the representation of [CLS] to represent the
been published for further research.","ance, and d is the hidden size of the encoder.","However, most
of the current approaches do not pay enough atten-       utterance:
tion to the emotion ﬂow of the conversations.",2022-10-21 03:51:18+00:00,Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Lanrui Wang'), arxiv.Result.Author('Jiangnan Li'), arxiv.Result.Author('Zheng Lin'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Chenxu Yang'), arxiv.Result.Author('Weiping Wang'), arxiv.Result.Author('Jie Zhou')]","Empathy, which is widely used in psychological counselling, is a key trait of
everyday human conversations. Equipped with commonsense knowledge, current
approaches to empathetic response generation focus on capturing implicit
emotion within dialogue context, where the emotions are treated as a static
variable throughout the conversations. However, emotions change dynamically
between utterances, which makes previous works difficult to perceive the
emotion flow and predict the correct emotion of the target response, leading to
inappropriate response. Furthermore, simply importing commonsense knowledge
without harmonization may trigger the conflicts between knowledge and emotion,
which confuse the model to choose incorrect information to guide the generation
process. To address the above problems, we propose a Serial Encoding and
Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.
We use a fine-grained encoding strategy which is more sensitive to the emotion
dynamics (emotion flow) in the conversations to predict the emotion-intent
characteristic of response. Besides, we design a novel framework to model the
interaction between knowledge and emotion to generate more sensible response.
Extensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms
the strong baselines in both automatic and manual evaluations."
12831,"We
As well, several datasets (Liu et al., 2021; Welivita
et al., 2021) of empathetic dialogue generation have     take the representation of [CLS] to represent the
been published for further research.","ance, and d is the hidden size of the encoder.","However, most
of the current approaches do not pay enough atten-       utterance:
tion to the emotion ﬂow of the conversations.",2022-10-21 03:51:18+00:00,Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Lanrui Wang'), arxiv.Result.Author('Jiangnan Li'), arxiv.Result.Author('Zheng Lin'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Chenxu Yang'), arxiv.Result.Author('Weiping Wang'), arxiv.Result.Author('Jie Zhou')]","Empathy, which is widely used in psychological counselling, is a key trait of
everyday human conversations. Equipped with commonsense knowledge, current
approaches to empathetic response generation focus on capturing implicit
emotion within dialogue context, where the emotions are treated as a static
variable throughout the conversations. However, emotions change dynamically
between utterances, which makes previous works difficult to perceive the
emotion flow and predict the correct emotion of the target response, leading to
inappropriate response. Furthermore, simply importing commonsense knowledge
without harmonization may trigger the conflicts between knowledge and emotion,
which confuse the model to choose incorrect information to guide the generation
process. To address the above problems, we propose a Serial Encoding and
Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.
We use a fine-grained encoding strategy which is more sensitive to the emotion
dynamics (emotion flow) in the conversations to predict the emotion-intent
characteristic of response. Besides, we design a novel framework to model the
interaction between knowledge and emotion to generate more sensible response.
Extensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms
the strong baselines in both automatic and manual evaluations."
12838,"The difference between two kinds of syntax          52nd Annual Meeting of the Association for Com-
structure on different source languages are left for      putational Linguistics, ACL 2014, June 22-27, 2014,
further study.","In Proceedings of the
tion.","Baltimore, MD, USA, Volume 2: Short Papers, pages
                                                          302–308.",2022-10-21 06:37:25+00:00,Syntax-guided Localized Self-attention by Constituency Syntactic Distance,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shengyuan Hou'), arxiv.Result.Author('Jushi Kai'), arxiv.Result.Author('Haotian Xue'), arxiv.Result.Author('Bingyu Zhu'), arxiv.Result.Author('Bo Yuan'), arxiv.Result.Author('Longtao Huang'), arxiv.Result.Author('Xinbing Wang'), arxiv.Result.Author('Zhouhan Lin')]","Recent works have revealed that Transformers are implicitly learning the
syntactic information in its lower layers from data, albeit is highly dependent
on the quality and scale of the training data. However, learning syntactic
information from data is not necessary if we can leverage an external syntactic
parser, which provides better parsing quality with well-defined syntactic
structures. This could potentially improve Transformer's performance and sample
efficiency. In this work, we propose a syntax-guided localized self-attention
for Transformer that allows directly incorporating grammar structures from an
external constituency parser. It prohibits the attention mechanism to
overweight the grammatically distant tokens over close ones. Experimental
results show that our model could consistently improve translation performance
on a variety of machine translation datasets, ranging from small to large
dataset sizes, and with different source languages."
12839,"2019.
tivate further research into more compact models           Findings of the 2019 conference on machine transla-
and more general neural network interfaces.","We further discuss            Shervin Malmasi, Christof Monz, Mathias Müller,
implications and subtleties of such models to mo-          Santanu Pal, Matt Post, and Marcos Zampieri.",tion (WMT19).,2022-10-21 08:33:55+00:00,Is Encoder-Decoder Redundant for Neural Machine Translation?,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Yingbo Gao'), arxiv.Result.Author('Christian Herold'), arxiv.Result.Author('Zijian Yang'), arxiv.Result.Author('Hermann Ney')]","Encoder-decoder architecture is widely adopted for sequence-to-sequence
modeling tasks. For machine translation, despite the evolution from long
short-term memory networks to Transformer networks, plus the introduction and
development of attention mechanism, encoder-decoder is still the de facto
neural network architecture for state-of-the-art models. While the motivation
for decoding information from some hidden space is straightforward, the strict
separation of the encoding and decoding steps into an encoder and a decoder in
the model architecture is not necessarily a must. Compared to the task of
autoregressive language modeling in the target language, machine translation
simply has an additional source sentence as context. Given the fact that neural
language models nowadays can already handle rather long contexts in the target
language, it is natural to ask whether simply concatenating the source and
target sentences and training a language model to do translation would work. In
this work, we investigate the aforementioned concept for machine translation.
Specifically, we experiment with bilingual translation, translation with
additional target monolingual data, and multilingual translation. In all cases,
this alternative approach performs on par with the baseline encoder-decoder
Transformer, suggesting that an encoder-decoder architecture might be redundant
for neural machine translation."
12843,"If s and b are      good results in the following experiments by simply
sufﬁciently smaller than l, it can be considered to      allocating the same epochs to steps 2 and 3, but
have linear time complexity to input length l. We        better balancing is worth further study.","We obtained
attention is done in O (l(4b + 2s)).",choose s = b = 64.,2022-10-21 10:46:41+00:00,LittleBird: Efficient Faster & Longer Transformer for Question Answering,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Minchul Lee'), arxiv.Result.Author('Kijong Han'), arxiv.Result.Author('Myeong Cheol Shin')]","BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a
limitation dealing with long inputs due to its attention mechanism. Longformer,
ETC and BigBird addressed this issue and effectively solved the quadratic
dependency problem. However we find that these models are not sufficient, and
propose LittleBird, a novel model based on BigBird with improved speed and
memory footprint while maintaining accuracy. In particular, we devise a more
flexible and efficient position representation method based on Attention with
Linear Biases (ALiBi). We also show that replacing the method of global
information represented in the BigBird with pack and unpack attention is more
effective. The proposed model can work on long inputs even after being
pre-trained on short inputs, and can be trained efficiently reusing existing
pre-trained language model for short inputs. This is a significant benefit for
low-resource languages where large amounts of long text data are difficult to
obtain. As a result, our experiments show that LittleBird works very well in a
variety of languages, achieving high performance in question answering tasks,
particularly in KorQuAD2.0, Korean Question Answering Dataset for long
paragraphs."
12844,"datasets released with this work: the collection of
posts by authors with blue-yellow color blindness              A common way to study perceptual aspects re-
(tritanopia) and monochrome vision (achromatop-             lated to language in psycholinguistic literature dis-
sia), to facilitate further research in this ﬁeld.","We              image in the mind of a person observing it (Cortese
also report the statistics of two complementary CB          and Fugett, 2004; Scott et al., 2019).","tinguishes between nine major psycholinguistic
                                                            dimensions, including imageability.",2022-10-21 12:11:10+00:00,Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ella Rabinovich'), arxiv.Result.Author('Boaz Carmeli')]","Prominent questions about the role of sensory vs. linguistic input in the way
we acquire and use language have been extensively studied in the
psycholinguistic literature. However, the relative effect of various factors in
a person's overall experience on their linguistic system remains unclear. We
study this question by making a step forward towards a better understanding of
the conceptual perception of colors by color-blind individuals, as reflected in
their spontaneous linguistic productions. Using a novel and carefully curated
dataset, we show that red-green color-blind speakers use the ""red"" and ""green""
color terms in less predictable contexts, and in linguistic environments
evoking mental image to a lower extent, when compared to their normal-sighted
counterparts. These findings shed some new and interesting light on the role of
sensory experience on our linguistic system."
12845,"datasets released with this work: the collection of
posts by authors with blue-yellow color blindness              A common way to study perceptual aspects re-
(tritanopia) and monochrome vision (achromatop-             lated to language in psycholinguistic literature dis-
sia), to facilitate further research in this ﬁeld.","We              image in the mind of a person observing it (Cortese
also report the statistics of two complementary CB          and Fugett, 2004; Scott et al., 2019).","tinguishes between nine major psycholinguistic
                                                            dimensions, including imageability.",2022-10-21 12:11:10+00:00,Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ella Rabinovich'), arxiv.Result.Author('Boaz Carmeli')]","Prominent questions about the role of sensory vs. linguistic input in the way
we acquire and use language have been extensively studied in the
psycholinguistic literature. However, the relative effect of various factors in
a person's overall experience on their linguistic system remains unclear. We
study this question by making a step forward towards a better understanding of
the conceptual perception of colors by color-blind individuals, as reflected in
their spontaneous linguistic productions. Using a novel and carefully curated
dataset, we show that red-green color-blind speakers use the ""red"" and ""green""
color terms in less predictable contexts, and in linguistic environments
evoking mental image to a lower extent, when compared to their normal-sighted
counterparts. These findings shed some new and interesting light on the role of
sensory experience on our linguistic system."
12852,"bootstrapping potential as well, with both fer-
                                             tile ground for further research.","three of the six categories: the “left behinds”, the
                                             Cladistic approaches in comparative linguis-      “scraping-bys”, and the “hopefuls”, with isiZulu
                                             tics may inform bootstrapping strategies and      (spoken in South Africa) and Swahili (spoken pri-
                                             similarity measures might serve as proxy for      marily in Tanzania and Kenya) in the latter group.","A selection of the usual NLP tasks have been
                                                                                               taken up for a few languages of the Niger-Congo
                                        1 Introduction                                         family, notably indeed Swahili and isiZulu, and
                                                                                               to a lesser extent Yoruba, Igbo, isiXhosa, and
                                        Nearly 1.5 billion people live in Africa, of which     Runyankore.",2022-10-21 15:16:45+00:00,Bootstrapping NLP tools across low-resourced African languages: an overview and prospects,cs.CL,"['cs.CL', 'I.2.7']",[arxiv.Result.Author('C. Maria Keet')],"Computing and Internet access are substantially growing markets in Southern
Africa, which brings with it increasing demands for local content and tools in
indigenous African languages. Since most of those languages are low-resourced,
efforts have gone into the notion of bootstrapping tools for one African
language from another. This paper provides an overview of these efforts for
Niger-Congo B (`Bantu') languages. Bootstrapping grammars for geographically
distant languages has been shown to still have positive outcomes for morphology
and rules or grammar-based natural language generation. Bootstrapping with
data-driven approaches to NLP tasks is difficult to use meaningfully regardless
geographic proximity, which is largely due to lexical diversity due to both
orthography and vocabulary. Cladistic approaches in comparative linguistics may
inform bootstrapping strategies and similarity measures might serve as proxy
for bootstrapping potential as well, with both fertile ground for further
research."
12860,"Sample a database, d ∼ UNIFORM(D)                   Question Match (QM) 56.2 60.5 62.4 61.8
   Z =∅
   for z ∈ Z do                                        Table 7: The QM score on SParC validation set with
                                                       T5-base trained with different ﬁlter value w.
         if z can be ﬁlled with d then
               Z .ADD(z)                                  We further study if conditioning on a user goal
                                                       G when generating interactions is necessary.","Replace columns and values in Xtr with typed slots
                                                       w  0 0.3 0.5 0.7
     to form coarse templates Z.","When
         end                                           we ablate the user goal, the QM score on SParC
   end                                                 drops from 62.4 to 59.8.",2022-10-21 16:40:07+00:00,Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play,cs.CL,['cs.CL'],"[arxiv.Result.Author('Qi Liu'), arxiv.Result.Author('Zihuiwen Ye'), arxiv.Result.Author('Tao Yu'), arxiv.Result.Author('Phil Blunsom'), arxiv.Result.Author('Linfeng Song')]","The task of context-dependent text-to-SQL aims to convert multi-turn user
utterances to formal SQL queries. This is a challenging task due to both the
scarcity of training data from which to learn complex contextual dependencies
and to generalize to unseen databases. In this paper we explore augmenting the
training datasets using self-play, which leverages contextual information to
synthesize new interactions to adapt the model to new databases. We first
design a SQL-to-text model conditioned on a sampled goal query, which
represents a user's intent, that then converses with a text-to-SQL semantic
parser to generate new interactions. We then filter the synthesized
interactions and retrain the models with the augmented data. We find that
self-play improves the accuracy of a strong baseline on SParC and CoSQL, two
widely used cross-domain text-to-SQL datasets. Our analysis shows that
self-play simulates various conversational thematic relations, enhances
cross-domain generalization and improves beam-search."
12868,"Optimizing the
A Code release                                                   "" Support Devices "": 36 ,
                                                                 "" Atelectasis "": 26 ,
To help with further research, we make our code                  "" Cardiomegaly "": 19 ,
publicly available using the ViLMedic library (Del-              ""Lung Lesion "" : 19 ,
brouck et al., 2022).",2020.,"More speciﬁcally, we re-                   "" Pleural Other "": 16 ,
lease the code of all the factually-oriented met-                "" Enlarged Cardiomediastinum "": 15 ,
rics presented in Section 5.2 in one package.",2022-10-21 18:27:45+00:00,Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Jean-Benoit Delbrouck'), arxiv.Result.Author('Pierre Chambon'), arxiv.Result.Author('Christian Bluethgen'), arxiv.Result.Author('Emily Tsai'), arxiv.Result.Author('Omar Almusa'), arxiv.Result.Author('Curtis P. Langlotz')]","Neural image-to-text radiology report generation systems offer the potential
to improve radiology reporting by reducing the repetitive process of report
drafting and identifying possible medical errors. These systems have achieved
promising performance as measured by widely used NLG metrics such as BLEU and
CIDEr. However, the current systems face important limitations. First, they
present an increased complexity in architecture that offers only marginal
improvements on NLG metrics. Secondly, these systems that achieve high
performance on these metrics are not always factually complete or consistent
due to both inadequate training and evaluation. Recent studies have shown the
systems can be substantially improved by using new methods encouraging 1) the
generation of domain entities consistent with the reference and 2) describing
these entities in inferentially consistent ways. So far, these methods rely on
weakly-supervised approaches (rule-based) and named entity recognition systems
that are not specific to the chest X-ray domain. To overcome this limitation,
we propose a new method, the RadGraph reward, to further improve the factual
completeness and correctness of generated radiology reports. More precisely, we
leverage the RadGraph dataset containing annotated chest X-ray reports with
entities and relations between entities. On two open radiology report datasets,
our system substantially improves the scores up to 14.2% and 25.3% on metrics
evaluating the factual correctness and completeness of reports."
12871,"Indeed, we have considered
it is at the word or sentence level, which allows for              only the most fundamental geometric properties of
systematic further study.","Fortunately, the method is robust enough to                  and the dimension container—we have just barely
be applied to any encoder and any dataset, whether                 scratched the surface.","vectors, yet vectors have other (distributed) prop-
                                                                   erties that could potentially be considered distinct
8 Limitations                                                      information containers in their own right, such as
                                                                   the vector’s minimum and maximum value, their
While our insights into how linguistic information                 ratio, the entropy in the vector etc.",2022-10-21 19:33:33+00:00,Probing with Noise: Unpicking the Warp and Weft of Embeddings,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', '68Uxx']","[arxiv.Result.Author('Filip Klubička'), arxiv.Result.Author('John D. Kelleher')]","Improving our understanding of how information is encoded in vector space can
yield valuable interpretability insights. Alongside vector dimensions, we argue
that it is possible for the vector norm to also carry linguistic information.
We develop a method to test this: an extension of the probing framework which
allows for relative intrinsic interpretations of probing results. It relies on
introducing noise that ablates information encoded in embeddings, grounded in
random baselines and confidence intervals. We apply the method to
well-established probing tasks and find evidence that confirms the existence of
separate information containers in English GloVe and BERT embeddings. Our
correlation analysis aligns with the experimental findings that different
encoders use the norm to encode different kinds of information: GloVe stores
syntactic and sentence length information in the vector norm, while BERT uses
it to encode contextual incongruity."
12873,"To this end, we fuse
string match and does not need further study.",then it is naturally entailed in the premise through          label of Hi is not available.,"the sentence-level results to judge the entailment
                                                              relationship between the documents, and indirectly
2.3 Reading & Credibility Estimation                          train the model through document-level entailment
                                                              label.",2022-10-22 02:02:35+00:00,"R$^2$F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference",cs.CL,['cs.CL'],"[arxiv.Result.Author('Hao Wang'), arxiv.Result.Author('Yixin Cao'), arxiv.Result.Author('Yangguang Li'), arxiv.Result.Author('Zhen Huang'), arxiv.Result.Author('Kun Wang'), arxiv.Result.Author('Jing Shao')]","Document-level natural language inference (DOCNLI) is a new challenging task
in natural language processing, aiming at judging the entailment relationship
between a pair of hypothesis and premise documents. Current datasets and
baselines largely follow sentence-level settings, but fail to address the
issues raised by longer documents. In this paper, we establish a general
solution, named Retrieval, Reading and Fusion (R2F) framework, and a new
setting, by analyzing the main challenges of DOCNLI: interpretability,
long-range dependency, and cross-sentence inference. The basic idea of the
framework is to simplify document-level task into a set of sentence-level
tasks, and improve both performance and interpretability with the power of
evidence. For each hypothesis sentence, the framework retrieves evidence
sentences from the premise, and reads to estimate its credibility. Then the
sentence-level results are fused to judge the relationship between the
documents. For the setting, we contribute complementary evidence and entailment
label annotation on hypothesis sentences, for interpretability study. Our
experimental results show that R2F framework can obtain state-of-the-art
performance and is robust for diverse evidence retrieval methods. Moreover, it
can give more interpretable prediction results. Our model and code are released
at https://github.com/phoenixsecularbird/R2F."
12874,"UniLM (Dong et al.,
and will be released for further research on gener-     2019) pre-train a transformer encoder/decoder with
                                                        both MLM task and sequence-to-sequence task,
                                                        considering two unidirectional orders, i.e., L2R and
                                                        R2L, while our model leverages permuted orders.","GPT-3 (Brown et al., 2020) pre-train
P3LM obtains new state-of-the-art results on GLGE       a transformer decoder with extremely large corpus
benchmark compared with published methods; (III)        and parameters, which is not ﬁnetuned on down-
Three P3LM models, which cost about 100,000 dol-        stream tasks, while our model follows the pre-train
lars, are pre-trained based on large scale datasets     then ﬁnetune framework.","Additional strong generative pre-trained models in-
                                                        cluding MASS(Song et al., 2019), BART(Lewis
                                                        et al., 2020), T5 (Raffel et al., 2019), and PE-
                                                        GASUS (Zhang et al., 2019) utilize a transformer
                                                        encoder-decoder framework to pre-train generative
                                                        models, all of which are limited to train a L2R
                                                        decoder, while our model learns to decode tokens
                                                        in permuted order.",2022-10-22 03:50:59+00:00,P$^3$LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training,cs.CL,['cs.CL'],"[arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yifan Wang'), arxiv.Result.Author('Jiangyong Ying'), arxiv.Result.Author('Yeyun Gong'), arxiv.Result.Author('Jing Zhao'), arxiv.Result.Author('Youzheng Wu'), arxiv.Result.Author('Xiaodong He')]","Conventional autoregressive left-to-right (L2R) sequence generation faces two
issues during decoding: limited to unidirectional target sequence modeling, and
constrained on strong local dependencies. To address the aforementioned
problem, we propose P$^3$LM, a probabilistically permuted prophet language
model, which strengthens the modeling of bidirectional information and long
token dependencies for sequence generation. Specifically, P$^3$LM learns to
generate tokens in permuted order upon an order-aware transformer decoder, as
well as to generate the corresponding future $N$ tokens with a multi-stream
attention mechanism. Extensive experiments are conducted on the GLGE benchmark,
which includes four datasets for summarization, two for question generation,
one for conversational question answering, and one for dialog response
generation, where P$^3$LM achieves state-of-the-art results compared with
strong publicly available generative pre-training methods."
12876,"asked to judge whether these generated texts are
better than, tie with, or worse than the baseline             Perturbed Layer We further study how perturb-
model from the informativeness and faithfulness               ing representations from different layers affect gen-
aspects, separately.",The annotators are              performance with BART (w/o KL vs BART).,Evaluation results of 100 ran-           eration performance.,2022-10-22 06:38:28+00:00,Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wenhao Wu'), arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Jiachen Liu'), arxiv.Result.Author('Xinyan Xiao'), arxiv.Result.Author('Sujian Li'), arxiv.Result.Author('Yajuan Lyu')]","Though model robustness has been extensively studied in language
understanding, the robustness of Seq2Seq generation remains understudied. In
this paper, we conduct the first quantitative analysis on the robustness of
pre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq
model (BART) is still vulnerable, which leads to significant degeneration in
faithfulness and informativeness for text generation tasks. This motivated us
to further propose a novel adversarial augmentation framework, namely AdvSeq,
for generally improving faithfulness and informativeness of Seq2Seq models via
enhancing their robustness. AdvSeq automatically constructs two types of
adversarial augmentations during training, including implicit adversarial
samples by perturbing word representations and explicit adversarial samples by
word swapping, both of which effectively improve Seq2Seq robustness. Extensive
experiments on three popular text generation tasks demonstrate that AdvSeq
significantly improves both the faithfulness and informativeness of Seq2Seq
generation under both automatic and human evaluation settings."
12877,"(2020);
To further study whether the factuality corrections   Goyal and Durrett (2021) model factuality as an
performed by our model align with human expec-        entailment verifying whether the summary is en-
tations of automated summaries, we conduct a hu-      tailed by the source.",(2020); Maynez et al.,Lee et al.,2022-10-22 07:16:19+00:00,Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling,cs.CL,['cs.CL'],"[arxiv.Result.Author('Vidhisha Balachandran'), arxiv.Result.Author('Hannaneh Hajishirzi'), arxiv.Result.Author('William Cohen'), arxiv.Result.Author('Yulia Tsvetkov')]","Abstractive summarization models often generate inconsistent summaries
containing factual errors or hallucinated content. Recent works focus on
correcting factual errors in generated summaries via post-editing. Such
correction models are trained using adversarial non-factual summaries
constructed using heuristic rules for injecting errors. However, generating
non-factual summaries using heuristics often does not generalize well to actual
model errors. In this work, we propose to generate hard, representative
synthetic examples of non-factual summaries through infilling language models.
With this data, we train a more robust fact-correction model to post-edit the
summaries to improve factual consistency. Through quantitative and qualitative
experiments on two popular summarization datasets -- CNN/DM and XSum -- we show
that our approach vastly outperforms prior methods in correcting erroneous
summaries. Our model -- FactEdit -- improves factuality scores by over ~11
points on CNN/DM and over ~31 points on XSum on average across multiple
summarization models, producing more factual summaries while maintaining
competitive summarization quality."
12878,"(2020);
To further study whether the factuality corrections   Goyal and Durrett (2021) model factuality as an
performed by our model align with human expec-        entailment verifying whether the summary is en-
tations of automated summaries, we conduct a hu-      tailed by the source.",(2020); Maynez et al.,Lee et al.,2022-10-22 07:16:19+00:00,Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling,cs.CL,['cs.CL'],"[arxiv.Result.Author('Vidhisha Balachandran'), arxiv.Result.Author('Hannaneh Hajishirzi'), arxiv.Result.Author('William W. Cohen'), arxiv.Result.Author('Yulia Tsvetkov')]","Abstractive summarization models often generate inconsistent summaries
containing factual errors or hallucinated content. Recent works focus on
correcting factual errors in generated summaries via post-editing. Such
correction models are trained using adversarial non-factual summaries
constructed using heuristic rules for injecting errors. However, generating
non-factual summaries using heuristics often does not generalize well to actual
model errors. In this work, we propose to generate hard, representative
synthetic examples of non-factual summaries through infilling language models.
With this data, we train a more robust fact-correction model to post-edit the
summaries to improve factual consistency. Through quantitative and qualitative
experiments on two popular summarization datasets -- CNN/DM and XSum -- we show
that our approach vastly outperforms prior methods in correcting erroneous
summaries. Our model -- FactEdit -- improves factuality scores by over ~11
points on CNN/DM and over ~31 points on XSum on average across multiple
summarization models, producing more factual summaries while maintaining
competitive summarization quality."
12879,"The main cause of this problem is usu-           publicly available to support further research on
                                        ally attributed to inherently biased data that is used          gender rewriting.","All of the
                                        grade users’ experiences (Sun et al., 2019; Blodgett            datasets created for this shared task will be made
                                        et al., 2020).","to build these systems and which mirrors the in-
                                        equalities of the world we live in.",2022-10-22 10:27:53+00:00,The Shared Task on Gender Rewriting,cs.CL,['cs.CL'],"[arxiv.Result.Author('Bashar Alhafni'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Houda Bouamor'), arxiv.Result.Author('Ossama Obeid'), arxiv.Result.Author('Sultan Alrowili'), arxiv.Result.Author('Daliyah Alzeer'), arxiv.Result.Author('Khawlah M. Alshanqiti'), arxiv.Result.Author('Ahmed ElBakry'), arxiv.Result.Author('Muhammad ElNokrashy'), arxiv.Result.Author('Mohamed Gabr'), arxiv.Result.Author('Abderrahmane Issam'), arxiv.Result.Author('Abdelrahim Qaddoumi'), arxiv.Result.Author('K. Vijay-Shanker'), arxiv.Result.Author('Mahmoud Zyate')]","In this paper, we present the results and findings of the Shared Task on
Gender Rewriting, which was organized as part of the Seventh Arabic Natural
Language Processing Workshop. The task of gender rewriting refers to generating
alternatives of a given sentence to match different target user gender contexts
(e.g., female speaker with a male listener, a male speaker with a male
listener, etc.). This requires changing the grammatical gender (masculine or
feminine) of certain words referring to the users. In this task, we focus on
Arabic, a gender-marking morphologically rich language. A total of five teams
from four countries participated in the shared task."
12880,"AMRGS
                                                                                                                      0.2  StructBART

                                                                                                                           Spring
                                                                                                                           AMRBART
5 Key Challenges to OOD AMR Parsing                                                                                   0.0 Unigram Bigram Trigram Concept Triplet Relation

Based on the results above, we further study                                                                           (b)
two important questions: which AMR components
are the most challenging for cross-domain AMR                                        Figure 4: Pearson correlation coefﬁcient between per-
parsing (in Section 5.1); and what contributes most                                  formance (SMATCH) degradation rate and difference
to the performance degradation on OOD test sets                                      of feature distribution measured by (a) OOV rate; (b)
(in Section 5.2)?","We also give the full                                                                                 JAMR
evaluation results, please refer Appendix A.2.",Jensen-Shannon divergence.,2022-10-22 13:24:13+00:00,Cross-domain Generalization for AMR Parsing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xuefeng Bai'), arxiv.Result.Author('Seng Yang'), arxiv.Result.Author('Leyang Cui'), arxiv.Result.Author('Linfeng Song'), arxiv.Result.Author('Yue Zhang')]","Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph
from textual input. Recently, there has been notable growth in AMR parsing
performance. However, most existing work focuses on improving the performance
in the specific domain, ignoring the potential domain dependence of AMR parsing
systems. To address this, we extensively evaluate five representative AMR
parsers on five domains and analyze challenges to cross-domain AMR parsing. We
observe that challenges to cross-domain AMR parsing mainly arise from the
distribution shift of words and AMR concepts. Based on our observation, we
investigate two approaches to reduce the domain distribution divergence of text
and AMR features, respectively. Experimental results on two out-of-domain test
sets show the superiority of our method."
12882,"For instance, we did not further study the
                                                           performance of difference event representations
                                                           because it is not the focus of this study.","• Experiments: For the limitation of resources, we
                                                           did not conduct some further experiments in this
                                                           study.","References                                                   Sarik Ghazarian, Zixi Liu, Akash S M, Ralph Weischedel,
                                                                Aram Galstyan, and Nanyun Peng.",2022-10-22 14:51:12+00:00,EtriCA: Event-Triggered Context-Aware Story Generation Augmented by Cross Attention,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Chen Tang'), arxiv.Result.Author('Chenghua Lin'), arxiv.Result.Author('Henglin Huang'), arxiv.Result.Author('Frank Guerin'), arxiv.Result.Author('Zhihao Zhang')]","One of the key challenges of automatic story generation is how to generate a
long narrative that can maintain fluency, relevance, and coherence. Despite
recent progress, current story generation systems still face the challenge of
how to effectively capture contextual and event features, which has a profound
impact on a model's generation performance. To address these challenges, we
present EtriCA, a novel neural generation model, which improves the relevance
and coherence of the generated stories through residually mapping context
features to event sequences with a cross-attention mechanism. Such a feature
capturing mechanism allows our model to better exploit the logical relatedness
between events when generating stories. Extensive experiments based on both
automatic and human evaluations show that our model significantly outperforms
state-of-the-art baselines, demonstrating the effectiveness of our model in
leveraging context and event features."
12883,"These nu-
cases respectively, the scores for correctness and    ances necessitates further research on the ECTSum
relevance were the same for both models.",For 16% and 11% of the           is different from earnings per share.,"Also,        corpus, and ﬁnancial summarization in general.",2022-10-22 15:02:41+00:00,ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Rajdeep Mukherjee'), arxiv.Result.Author('Abhinav Bohra'), arxiv.Result.Author('Akash Banerjee'), arxiv.Result.Author('Soumya Sharma'), arxiv.Result.Author('Manjunath Hegde'), arxiv.Result.Author('Afreen Shaikh'), arxiv.Result.Author('Shivani Shrivastava'), arxiv.Result.Author('Koustuv Dasgupta'), arxiv.Result.Author('Niloy Ganguly'), arxiv.Result.Author('Saptarshi Ghosh'), arxiv.Result.Author('Pawan Goyal')]","Despite tremendous progress in automatic summarization, state-of-the-art
methods are predominantly trained to excel in summarizing short newswire
articles, or documents with strong layout biases such as scientific articles or
government reports. Efficient techniques to summarize financial documents,
including facts and figures, have largely been unexplored, majorly due to the
unavailability of suitable datasets. In this work, we present ECTSum, a new
dataset with transcripts of earnings calls (ECTs), hosted by publicly traded
companies, as documents, and short experts-written telegram-style bullet point
summaries derived from corresponding Reuters articles. ECTs are long
unstructured documents without any prescribed length limit or format. We
benchmark our dataset with state-of-the-art summarizers across various metrics
evaluating the content quality and factual consistency of the generated
summaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to
generate a set of bullet points that precisely capture the important facts
discussed in the calls."
12884,"These nu-
cases respectively, the scores for correctness and    ances necessitates further research on the ECTSum
relevance were the same for both models.",For 16% and 11% of the           is different from earnings per share.,"Also,        corpus, and ﬁnancial summarization in general.",2022-10-22 15:02:41+00:00,ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts,cs.CL,"['cs.CL', 'I.2.7']","[arxiv.Result.Author('Rajdeep Mukherjee'), arxiv.Result.Author('Abhinav Bohra'), arxiv.Result.Author('Akash Banerjee'), arxiv.Result.Author('Soumya Sharma'), arxiv.Result.Author('Manjunath Hegde'), arxiv.Result.Author('Afreen Shaikh'), arxiv.Result.Author('Shivani Shrivastava'), arxiv.Result.Author('Koustuv Dasgupta'), arxiv.Result.Author('Niloy Ganguly'), arxiv.Result.Author('Saptarshi Ghosh'), arxiv.Result.Author('Pawan Goyal')]","Despite tremendous progress in automatic summarization, state-of-the-art
methods are predominantly trained to excel in summarizing short newswire
articles, or documents with strong layout biases such as scientific articles or
government reports. Efficient techniques to summarize financial documents,
including facts and figures, have largely been unexplored, majorly due to the
unavailability of suitable datasets. In this work, we present ECTSum, a new
dataset with transcripts of earnings calls (ECTs), hosted by publicly traded
companies, as documents, and short experts-written telegram-style bullet point
summaries derived from corresponding Reuters articles. ECTs are long
unstructured documents without any prescribed length limit or format. We
benchmark our dataset with state-of-the-art summarizers across various metrics
evaluating the content quality and factual consistency of the generated
summaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to
generate a set of bullet points that precisely capture the important facts
discussed in the calls."
12887,"In Proceed-
We will further study how to appropriately utilize           ings of ACL, pages 793–805.",types for grammatical error correction.,such target-side syntax in our future work.,2022-10-22 15:54:29+00:00,SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored GEC-Oriented Parser,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yue Zhang'), arxiv.Result.Author('Bo Zhang'), arxiv.Result.Author('Zhenghua Li'), arxiv.Result.Author('Zuyi Bao'), arxiv.Result.Author('Chen Li'), arxiv.Result.Author('Min Zhang')]","This work proposes a syntax-enhanced grammatical error correction (GEC)
approach named SynGEC that effectively incorporates dependency syntactic
information into the encoder part of GEC models. The key challenge for this
idea is that off-the-shelf parsers are unreliable when processing ungrammatical
sentences. To confront this challenge, we propose to build a tailored
GEC-oriented parser (GOPar) using parallel GEC training data as a pivot. First,
we design an extended syntax representation scheme that allows us to represent
both grammatical errors and syntax in a unified tree structure. Then, we obtain
parse trees of the source incorrect sentences by projecting trees of the target
correct sentences. Finally, we train GOPar with such projected trees. For GEC,
we employ the graph convolution network to encode source-side syntactic
information produced by GOPar, and fuse them with the outputs of the
Transformer encoder. Experiments on mainstream English and Chinese GEC datasets
show that our proposed SynGEC approach consistently and substantially
outperforms strong baselines and achieves competitive performance. Our code and
data are all publicly available at https://github.com/HillZhang1999/SynGEC."
12891,"As ex-
ﬁndings, we believe that the conﬁrmation of our           plained in §5.4, negative preference interpretations
hypothesis helps to motivate further research in          are a small component task that is useful when de-
automating this step.",In line with their          tual comparisons and decision templates.,cision templates are partially seen during training.,2022-10-23 03:22:34+00:00,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Victor S. Bursztyn'), arxiv.Result.Author('David Demeter'), arxiv.Result.Author('Doug Downey'), arxiv.Result.Author('Larry Birnbaum')]","How to usefully encode compositional task structure has long been a core
challenge in AI. Recent work in chain of thought prompting has shown that for
very large neural language models (LMs), explicitly demonstrating the
inferential steps involved in a target task may improve performance over
end-to-end learning that focuses on the target task alone. However, chain of
thought prompting has significant limitations due to its dependency on huge
pretrained LMs. In this work, we present compositional fine-tuning (CFT): an
approach based on explicitly decomposing a target task into component tasks,
and then fine-tuning smaller LMs on a curriculum of such component tasks. We
apply CFT to recommendation tasks in two domains, world travel and local
dining, as well as a previously studied inferential task (sports
understanding). We show that CFT outperforms end-to-end learning even with
equal amounts of data, and gets consistently better as more component tasks are
modeled via fine-tuning. Compared with chain of thought prompting, CFT performs
at least as well using LMs only 7.4% of the size, and is moreover applicable to
task domains for which data are not available during pretraining."
12892,"We hope to encourage further research in          erences or items (i.e., |P | > 1 or |I| > 2) in a
other principled, task-agnostic methods for lever-      decision template without losing performance, then
aging compositional structure in LM ﬁne-tuning.","For example,
positional structure or focus only on factual knowl-    if a model is not able to handle larger sets of pref-
edge.","one potential solution is to use an upstream agent to

   Compared to chain of thought prompting, meth-            9Two requests required, with 625 prompt tokens each.",2022-10-23 03:22:34+00:00,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Victor S. Bursztyn'), arxiv.Result.Author('David Demeter'), arxiv.Result.Author('Doug Downey'), arxiv.Result.Author('Larry Birnbaum')]","How to usefully encode compositional task structure has long been a core
challenge in AI. Recent work in chain of thought prompting has shown that for
very large neural language models (LMs), explicitly demonstrating the
inferential steps involved in a target task may improve performance over
end-to-end learning that focuses on the target task alone. However, chain of
thought prompting has significant limitations due to its dependency on huge
pretrained LMs. In this work, we present compositional fine-tuning (CFT): an
approach based on explicitly decomposing a target task into component tasks,
and then fine-tuning smaller LMs on a curriculum of such component tasks. We
apply CFT to recommendation tasks in two domains, world travel and local
dining, as well as a previously studied inferential task (sports
understanding). We show that CFT outperforms end-to-end learning even with
equal amounts of data, and gets consistently better as more component tasks are
modeled via fine-tuning. Compared with chain of thought prompting, CFT performs
at least as well using LMs only 7.4% of the size, and is moreover applicable to
task domains for which data are not available during pretraining."
12894,"Ablation Study: Inﬂuence of the Target Number

                  P  R        F0.5   P  R         F0.5                  In order to further study the impact of the target number on the model
                                                                        performance, we train models using three artiﬁcial datasets with a
Zhang et al.","Al-
                                                                             though different ONETARGET strategies lead to the same num-
                                                                             ber of samples, both Seq2Seq-based and Seq2Edit-based models
                     Seq2Seq            Seq2Edit                        4.4.","[8]  43.81 28.56 39.58  44.11 27.18 39.22                  different number of target sentences but the same number of source
Zhang et al.",2022-10-23 10:44:50+00:00,Focus Is All You Need For Chinese Grammatical Error Correction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jingheng Ye'), arxiv.Result.Author('Yinghui Li'), arxiv.Result.Author('Shirong Ma'), arxiv.Result.Author('Rui Xie'), arxiv.Result.Author('Wei Wu'), arxiv.Result.Author('Hai-Tao Zheng')]","Chinese Grammatical Error Correction (CGEC) aims to automatically detect and
correct grammatical errors contained in Chinese text. In the long term,
researchers regard CGEC as a task with a certain degree of uncertainty, that
is, an ungrammatical sentence may often have multiple references. However, we
argue that even though this is a very reasonable hypothesis, it is too harsh
for the intelligence of the mainstream models in this era. In this paper, we
first discover that multiple references do not actually bring positive gains to
model training. On the contrary, it is beneficial to the CGEC model if the
model can pay attention to small but essential data during the training
process. Furthermore, we propose a simple yet effective training strategy
called OneTarget to improve the focus ability of the CGEC models and thus
improve the CGEC performance. Extensive experiments and detailed analyses
demonstrate the correctness of our discovery and the effectiveness of our
proposed method."
12895,"Ablation Study: Inﬂuence of the Target Number

                  P  R        F0.5   P  R         F0.5                  In order to further study the impact of the target number on the model
                                                                        performance, we train models using three artiﬁcial datasets with a
Zhang et al.",Seq2Seq            Seq2Edit                        4.4.,"[8]  43.81 28.56 39.58  44.11 27.18 39.22                  different number of target sentences but the same number of source
Zhang et al.",2022-10-23 10:44:50+00:00,Focus Is What You Need For Chinese Grammatical Error Correction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jingheng Ye'), arxiv.Result.Author('Yinghui Li'), arxiv.Result.Author('Shirong Ma'), arxiv.Result.Author('Rui Xie'), arxiv.Result.Author('Wei Wu'), arxiv.Result.Author('Hai-Tao Zheng')]","Chinese Grammatical Error Correction (CGEC) aims to automatically detect and
correct grammatical errors contained in Chinese text. In the long term,
researchers regard CGEC as a task with a certain degree of uncertainty, that
is, an ungrammatical sentence may often have multiple references. However, we
argue that even though this is a very reasonable hypothesis, it is too harsh
for the intelligence of the mainstream models in this era. In this paper, we
first discover that multiple references do not actually bring positive gains to
model training. On the contrary, it is beneficial to the CGEC model if the
model can pay attention to small but essential data during the training
process. Furthermore, we propose a simple yet effective training strategy
called OneTarget to improve the focus ability of the CGEC models and thus
improve the CGEC performance. Extensive experiments and detailed analyses
demonstrate the correctness of our discovery and the effectiveness of our
proposed method."
12896,"Ablation Study: Inﬂuence of the Target Number

                  P  R        F0.5   P  R         F0.5                  In order to further study the impact of the target number on the model
                                                                        performance, we train models using three artiﬁcial datasets with a
Zhang et al.",Seq2Seq            Seq2Edit                        4.4.,"[8]  43.81 28.56 39.58  44.11 27.18 39.22                  different number of target sentences but the same number of source
Zhang et al.",2022-10-23 10:44:50+00:00,Focus Is What You Need For Chinese Grammatical Error Correction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jingheng Ye'), arxiv.Result.Author('Yinghui Li'), arxiv.Result.Author('Shirong Ma'), arxiv.Result.Author('Rui Xie'), arxiv.Result.Author('Wei Wu'), arxiv.Result.Author('Hai-Tao Zheng')]","Chinese Grammatical Error Correction (CGEC) aims to automatically detect and
correct grammatical errors contained in Chinese text. In the long term,
researchers regard CGEC as a task with a certain degree of uncertainty, that
is, an ungrammatical sentence may often have multiple references. However, we
argue that even though this is a very reasonable hypothesis, it is too harsh
for the intelligence of the mainstream models in this era. In this paper, we
first discover that multiple references do not actually bring positive gains to
model training. On the contrary, it is beneficial to the CGEC model if the
model can pay attention to small but essential data during the training
process. Furthermore, we propose a simple yet effective training strategy
called OneTarget to improve the focus ability of the CGEC models and thus
improve the CGEC performance. Extensive experiments and detailed analyses
demonstrate the correctness of our discovery and the effectiveness of our
proposed method."
12912,"his eye is generous]’ to mean ‘one-eyed’; and
as ADJ/NOUN in our lexicon, as it is a class that
deserves further study.",We mark the POS of such nominals                  ‘[lit.,the phrase        /b ee t # kh aa l t i/ ‘[lit.,2022-10-24 07:19:03+00:00,Maknuune: A Large Open Palestinian Arabic Lexicon,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shahd Dibas'), arxiv.Result.Author('Christian Khairallah'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Omar Fayez Sadi'), arxiv.Result.Author('Tariq Sairafy'), arxiv.Result.Author('Karmel Sarabta'), arxiv.Result.Author('Abrar Ardah')]","We present Maknuune, a large open lexicon for the Palestinian Arabic dialect.
Maknuune has over 36K entries from 17K lemmas, and 3.7K roots. All entries
include diacritized Arabic orthography, phonological transcription and English
glosses. Some entries are enriched with additional information such as broken
plurals and templatic feminine forms, associated phrases and collocations,
Standard Arabic glosses, and examples or notes on grammar, usage, or location
of collected entry."
12913,"his eye is generous]’ to mean ‘one-eyed’; and
as ADJ/NOUN in our lexicon, as it is a class that
deserves further study.",We mark the POS of such nominals                  ‘[lit.,the phrase        /b ee t # kh aa l t i/ ‘[lit.,2022-10-24 07:19:03+00:00,Maknuune: A Large Open Palestinian Arabic Lexicon,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shahd Dibas'), arxiv.Result.Author('Christian Khairallah'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Omar Fayez Sadi'), arxiv.Result.Author('Tariq Sairafy'), arxiv.Result.Author('Karmel Sarabta'), arxiv.Result.Author('Abrar Ardah')]","We present Maknuune, a large open lexicon for the Palestinian Arabic dialect.
Maknuune has over 36K entries from 17K lemmas, and 3.7K roots. All entries
include diacritized Arabic orthography, phonological transcription and English
glosses. Some entries are enriched with additional information such as broken
plurals and templatic feminine forms, associated phrases and collocations,
Standard Arabic glosses, and examples or notes on grammar, usage, or location
of collected entry."
12935,We encourage further research in devel-     refute our ﬁndings.,"main an elusive goal for some time, developing         Nevertheless, we hope developers of larger LLMs
measures of those abilities in systems can be done     will investigate these TOM abilities to conﬁrm or
in tandem.","oping benchmarks that measure speciﬁc social abil-
ities in LLMs (e.g., Sap et al., 2019b; Zadeh et al.,     We measure the ability to answer questions about
2019), especially those that minimize annotation       people’s mental states using TOMI, which is an au-
artifacts and spurious correlations (Schwartz et al.,  tomatically constructed corpus of stories involving
2017; Gururangan et al., 2018; Le et al., 2019).",2022-10-24 14:58:58+00:00,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Maarten Sap'), arxiv.Result.Author('Ronan LeBras'), arxiv.Result.Author('Daniel Fried'), arxiv.Result.Author('Yejin Choi')]","Social intelligence and Theory of Mind (ToM), i.e., the ability to reason
about the different mental states, intents, and reactions of all people
involved, allow humans to effectively navigate and understand everyday social
interactions. As NLP systems are used in increasingly complex social
situations, their ability to grasp social dynamics becomes crucial.
  In this work, we examine the open question of social intelligence and Theory
of Mind in modern NLP systems from an empirical and theory-based perspective.
We show that one of today's largest language models (GPT-3; Brown et al., 2020)
lacks this kind of social intelligence out-of-the box, using two tasks:
SocialIQa (Sap et al., 2019), which measures models' ability to understand
intents and reactions of participants of social interactions, and ToMi (Le et
al., 2019), which measures whether models can infer mental states and realities
of participants of situations.
  Our results show that models struggle substantially at these Theory of Mind
tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi,
respectively. To conclude, we draw on theories from pragmatics to contextualize
this shortcoming of large language models, by examining the limitations
stemming from their data, neural architecture, and training paradigms.
Challenging the prevalent narrative that only scale is needed, we posit that
person-centric NLP approaches might be more effective towards neural Theory of
Mind."
12953,"A further study with 15
added beneﬁt that they can spell out these using        qualiﬁed crowd-workers shows that users success-
natural language instructions.","In our work, we utilize     licly available LLMs trained on instructions (In-
large language models to generate text that satisﬁes    structGPT), but also capable of satisfying unseen
the various constraints speciﬁed by users, with the     compositional instructions.","Concurrent work has      fully write poems with CoPoet on diverse topics,
also shown that large language models can help          which are also preferred by third-party evaluators
users write scripts and screenplays (Mirowski et al.,   over poems written by solo-writers.",2022-10-25 00:07:10+00:00,Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tuhin Chakrabarty'), arxiv.Result.Author('Vishakh Padmakumar'), arxiv.Result.Author('He He')]","Recent work in training large language models (LLMs) to follow natural
language instructions has opened up exciting opportunities for natural language
interface design. Building on the prior success of LLMs in the realm of
computer-assisted creativity, we aim to study if LLMs can improve the quality
of user-generated content through collaboration. We present CoPoet, a
collaborative poetry writing system. In contrast to auto-completing a user's
text, CoPoet is controlled by user instructions that specify the attributes of
the desired text, such as Write a sentence about `love' or Write a sentence
ending in `fly'. The core component of our system is a language model
fine-tuned on a diverse collection of instructions for poetry writing. Our
model is not only competitive with publicly available LLMs trained on
instructions (InstructGPT), but is also capable of satisfying unseen
compositional instructions. A study with 15 qualified crowdworkers shows that
users successfully write poems with CoPoet on diverse topics ranging from
Monarchy to Climate change. Further, the collaboratively written poems are
preferred by third-party evaluators over those written without the system."
12957,"This is a particular interesting
phenomenon that requires further study.","Similarly, in
Appendix A.14, we have also observed that mean-
ing preservation label (Table 3) has little indication
of whether the augmentation performs well as a
single positives.","While a
sentence can represent semantically exactly oppo-
site meaning, it is still discussing similar topics,
and due to the symmetric nature of cosine simi-
larity, it is difﬁcult to use negation in deep learn-
ing.",2022-10-20 03:52:07+00:00,AugCSE: Contrastive Sentence Embedding with Diverse Augmentations,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zilu Tang'), arxiv.Result.Author('Muhammed Yusuf Kocyigit'), arxiv.Result.Author('Derry Wijaya')]","Data augmentation techniques have been proven useful in many applications in
NLP fields. Most augmentations are task-specific, and cannot be used as a
general-purpose tool. In our work, we present AugCSE, a unified framework to
utilize diverse sets of data augmentations to achieve a better, general
purpose, sentence embedding model. Building upon the latest sentence embedding
models, our approach uses a simple antagonistic discriminator that
differentiates the augmentation types. With the finetuning objective borrowed
from domain adaptation, we show that diverse augmentations, which often lead to
conflicting contrastive signals, can be tamed to produce a better and more
robust sentence representation. Our methods achieve state-of-the-art results on
downstream transfer tasks and perform competitively on semantic textual
similarity tasks, using only unsupervised data."
12959,"allow for further study of claim-evidence phenom-
ena and model generalizability as encountered in          As we show in §4 and §5, a key determinant
open-domain scientiﬁc claim veriﬁcation.",This dataset will     respect to the claim.,"of system generalization is the negative sampling
                                                       ratio.",2022-10-25 05:45:00+00:00,SciFact-Open: Towards open-domain scientific claim verification,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('David Wadden'), arxiv.Result.Author('Kyle Lo'), arxiv.Result.Author('Bailey Kuehl'), arxiv.Result.Author('Arman Cohan'), arxiv.Result.Author('Iz Beltagy'), arxiv.Result.Author('Lucy Lu Wang'), arxiv.Result.Author('Hannaneh Hajishirzi')]","While research on scientific claim verification has led to the development of
powerful systems that appear to approach human performance, these approaches
have yet to be tested in a realistic setting against large corpora of
scientific literature. Moving to this open-domain evaluation setting, however,
poses unique challenges; in particular, it is infeasible to exhaustively
annotate all evidence documents. In this work, we present SciFact-Open, a new
test collection designed to evaluate the performance of scientific claim
verification systems on a corpus of 500K research abstracts. Drawing upon
pooling techniques from information retrieval, we collect evidence for
scientific claims by pooling and annotating the top predictions of four
state-of-the-art scientific claim verification models. We find that systems
developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting
performance drops of at least 15 F1. In addition, analysis of the evidence in
SciFact-Open reveals interesting phenomena likely to appear when claim
verification systems are deployed in practice, e.g., cases where the evidence
supports only a special case of the claim. Our dataset is available at
https://github.com/dwadden/scifact-open."
12966,"The premise of this
that further research on effective deconfounding of    work is that the behavior of legal outcome predic-
legal NLP systems is needed if these systems are to    tion systems is to be scrutinized with great care.",While we are convinced       of technology in legal systems.,"become robust and trustworthy, the time-intensive      This paper does not advocate for the practical use
nature of collaboratively developing and qualita-      of such systems, but rather empirically explores
tively evaluating such models with legal experts       difﬁculties that arise in their development and rec-
poses a considerable resource challenge.",2022-10-25 08:37:25+00:00,Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts,cs.CL,['cs.CL'],"[arxiv.Result.Author('T. Y. S. S Santosh'), arxiv.Result.Author('Shanshan Xu'), arxiv.Result.Author('Oana Ichim'), arxiv.Result.Author('Matthias Grabmair')]","This work demonstrates that Legal Judgement Prediction systems without
expert-informed adjustments can be vulnerable to shallow, distracting surface
signals that arise from corpus construction, case distribution, and confounding
factors. To mitigate this, we use domain expertise to strategically identify
statistically predictive but legally irrelevant information. We adopt
adversarial training to prevent the system from relying on it. We evaluate our
deconfounded models by employing interpretability techniques and comparing to
expert annotations. Quantitative experiments and qualitative analysis show that
our deconfounded model consistently aligns better with expert rationales than
baselines trained for prediction only. We further contribute a set of reference
expert annotations to the validation and testing partitions of an existing
benchmark dataset of European Court of Human Rights cases."
12974,"Finally, we released a test
consisting of 298 educational MCQs with annotated distractors covering six subjects and a 77K
distractor vocabulary to promote further research.","Additionally, they found two more distractors to be within topic, albeit of lower quality, and useful
as inspiration for teachers to come up with their own good distractors.","In future work, we foresee three directions.",2022-10-25 12:48:56+00:00,Learning to Reuse Distractors to support Multiple Choice Question Generation in Education,cs.CL,['cs.CL'],"[arxiv.Result.Author('Semere Kiros Bitew'), arxiv.Result.Author('Amir Hadifar'), arxiv.Result.Author('Lucas Sterckx'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Chris Develder'), arxiv.Result.Author('Thomas Demeester')]","Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects & languages and a 77k multilingual pool of
distractor vocabulary for future research."
12975,"Finally, we released a test
consisting of 298 educational MCQs with annotated distractors covering six subjects and a 77K
distractor vocabulary to promote further research.","Additionally, they found two more distractors to be within topic, albeit of lower quality, and useful
as inspiration for teachers to come up with their own good distractors.","In future work, we foresee three directions.",2022-10-25 12:48:56+00:00,Learning to Reuse Distractors to support Multiple Choice Question Generation in Education,cs.CL,['cs.CL'],"[arxiv.Result.Author('Semere Kiros Bitew'), arxiv.Result.Author('Amir Hadifar'), arxiv.Result.Author('Lucas Sterckx'), arxiv.Result.Author('Johannes Deleu'), arxiv.Result.Author('Chris Develder'), arxiv.Result.Author('Thomas Demeester')]","Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects & languages and a 77k multilingual pool of
distractor vocabulary for future research."
12998,"standing of their latent mechanisms, but also further
We hope our results will foster further research in    reduce the energy consumption and environmental
the LM OIE benchmark direction.","Although the         ing proper zero-shot learning strategies for large
results are promising, we argue that our results just  language models can not only deepen our under-
indicate a lower bound about what the LMs have.","impacts that language models with ever-growing
                                                       size may cause.",2022-10-25 16:25:00+00:00,IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Chenguang Wang'), arxiv.Result.Author('Xiao Liu'), arxiv.Result.Author('Dawn Song')]","We introduce a new open information extraction (OIE) benchmark for
pre-trained language models (LM). Recent studies have demonstrated that
pre-trained LMs, such as BERT and GPT, may store linguistic and relational
knowledge. In particular, LMs are able to answer ``fill-in-the-blank''
questions when given a pre-defined relation category. Instead of focusing on
pre-defined relations, we create an OIE benchmark aiming to fully examine the
open relational information present in the pre-trained LMs. We accomplish this
by turning pre-trained LMs into zero-shot OIE systems. Surprisingly,
pre-trained LMs are able to obtain competitive performance on both standard OIE
datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets
(TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For
instance, the zero-shot pre-trained LMs outperform the F1 score of the
state-of-the-art supervised OIE methods on our factual OIE datasets without
needing to use any training sets. Our code and datasets are available at
https://github.com/cgraywang/IELM"
13001,"Given the only two datasets for the hope speech detection task mentioned above, our dataset provides opportunities to
explore further research in this domain.","Hence,
text belonging to the “Hope"" class can be further classiﬁed as classes like “rational and irrational or realistic and wishful
hopes"" [9, 10, 11].","In a comprehensive and general categorization, hope has been identiﬁed either as “Particularized"" or as “Generalized""
hope [10, 12, 13, 14, 15].",2022-10-25 16:34:03+00:00,PolyHope: Dataset Creation for a Two-Level Hope Speech Detection Task from Tweets,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Fazlourrahman Balouchzahi'), arxiv.Result.Author('Grigori Sidorov'), arxiv.Result.Author('Alexander Gelbukh')]","Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into ""Hope"" and ""Not Hope"",
then into three fine-grained hope categories: ""Generalized Hope"", ""Realistic
Hope"", and ""Unrealistic Hope"" (along with ""Not Hope""). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset."
13002,"Given the only two datasets for the hope speech detection task mentioned above, our dataset provides opportunities to
explore further research in this domain.","Hence,
text belonging to the “Hope"" class can be further classiﬁed as classes like “rational and irrational or realistic and wishful
hopes"" [9, 10, 11].","In a comprehensive and general categorization, hope has been identiﬁed either as “Particularized"" or as “Generalized""
hope [10, 12, 13, 14, 15].",2022-10-25 16:34:03+00:00,PolyHope: Two-Level Hope Speech Detection from Tweets,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Fazlourrahman Balouchzahi'), arxiv.Result.Author('Grigori Sidorov'), arxiv.Result.Author('Alexander Gelbukh')]","Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into ""Hope"" and ""Not Hope"",
then into three fine-grained hope categories: ""Generalized Hope"", ""Realistic
Hope"", and ""Unrealistic Hope"" (along with ""Not Hope""). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset."
13025,"Why,
further research to better represent and understand       when, and what: analyzing stack overﬂow questions
source code for code-based question answering.",2013.,"by topic, type, and code.",2022-10-26 05:40:34+00:00,CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course,cs.CL,['cs.CL'],"[arxiv.Result.Author('Changyoon Lee'), arxiv.Result.Author('Yeon Seonwoo'), arxiv.Result.Author('Alice Oh')]","We introduce CS1QA, a dataset for code-based question answering in the
programming education domain. CS1QA consists of 9,237 question-answer pairs
gathered from chat logs in an introductory programming class using Python, and
17,698 unannotated chat data with code. Each question is accompanied with the
student's code, and the portion of the code relevant to answering the question.
We carefully design the annotation process to construct CS1QA, and analyze the
collected dataset in detail. The tasks for CS1QA are to predict the question
type, the relevant code snippet given the question and the code and retrieving
an answer from the annotated corpus. Results for the experiments on several
baseline models are reported and thoroughly analyzed. The tasks for CS1QA
challenge models to understand both the code and natural language. This unique
dataset can be used as a benchmark for source code comprehension and question
answering in the educational setting."
13042,"How-
ever, performance remains low, which underscores                                   100%
the need for further research on handling these dif-                                80%
ﬁcult adversarial examples.","when we train the model under the balanced distri-      Likelihood of Occurrence              Perturbations Classified Correctly in Dataset
bution the performance improves, somewhat.","60%
                                                                                    40%
4.4 Model Consistency on Adversarial                                                20%
      Instances                                                                       0% 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

In addition to the per-perturbation evaluation, we                                                 Percentage of Correctly Classified Perturbations
also merged all available positive and negative
instance for each entry of the dataset, and com-                                Figure 2: Percentage of correctly classiﬁed perturba-
puted what percentage of them are classiﬁed cor-                                tions in the dev set.",2022-10-26 16:02:49+00:00,BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Mohaddeseh Bastan'), arxiv.Result.Author('Mihai Surdeanu'), arxiv.Result.Author('Niranjan Balasubramanian')]","Natural language inference (NLI) is critical for complex decision-making in
biomedical domain. One key question, for example, is whether a given biomedical
mechanism is supported by experimental evidence. This can be seen as an NLI
problem but there are no directly usable datasets to address this. The main
challenge is that manually creating informative negative examples for this task
is difficult and expensive. We introduce a novel semi-supervised procedure that
bootstraps an NLI dataset from existing biomedical dataset that pairs
mechanisms with experimental evidence in abstracts. We generate a range of
negative examples using nine strategies that manipulate the structure of the
underlying mechanisms both with rules, e.g., flip the roles of the entities in
the interaction, and, more importantly, as perturbations via logical
constraints in a neuro-logical decoding system. We use this procedure to create
a novel dataset for NLI in the biomedical domain, called BioNLI and benchmark
two state-of-the-art biomedical classifiers. The best result we obtain is
around mid 70s in F1, suggesting the difficulty of the task. Critically, the
performance on the different classes of negative examples varies widely, from
97% F1 on the simple role change negative examples, to barely better than
chance on the negative examples generated using neuro-logic decoding."
13043,While models are able to get reasonable                                  gesting avenues for further research.,"The cumulative results are shown in Fig-
ure 2.","accuracy overall, they are not consistent in their de-
cisions.",2022-10-26 16:02:49+00:00,BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Mohaddeseh Bastan'), arxiv.Result.Author('Mihai Surdeanu'), arxiv.Result.Author('Niranjan Balasubramanian')]","Natural language inference (NLI) is critical for complex decision-making in
biomedical domain. One key question, for example, is whether a given biomedical
mechanism is supported by experimental evidence. This can be seen as an NLI
problem but there are no directly usable datasets to address this. The main
challenge is that manually creating informative negative examples for this task
is difficult and expensive. We introduce a novel semi-supervised procedure that
bootstraps an NLI dataset from existing biomedical dataset that pairs
mechanisms with experimental evidence in abstracts. We generate a range of
negative examples using nine strategies that manipulate the structure of the
underlying mechanisms both with rules, e.g., flip the roles of the entities in
the interaction, and, more importantly, as perturbations via logical
constraints in a neuro-logical decoding system. We use this procedure to create
a novel dataset for NLI in the biomedical domain, called BioNLI and benchmark
two state-of-the-art biomedical classifiers. The best result we obtain is
around mid 70s in F1, suggesting the difficulty of the task. Critically, the
performance on the different classes of negative examples varies widely, from
97% F1 on the simple role change negative examples, to barely better than
chance on the negative examples generated using neuro-logic decoding."
13044,"Abbrevia-         access dataset4 will enable further research both on
tions can cause difﬁculties, when one of the main          biomedical NLI, and on language understanding in
entities is mentioned in its full form in one sentence     general.","We hope that this open-
makes it difﬁcult to classify correctly.",but abbreviated in the rest of the sentences.,2022-10-26 16:02:49+00:00,BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples,cs.CL,"['cs.CL', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Mohaddeseh Bastan'), arxiv.Result.Author('Mihai Surdeanu'), arxiv.Result.Author('Niranjan Balasubramanian')]","Natural language inference (NLI) is critical for complex decision-making in
biomedical domain. One key question, for example, is whether a given biomedical
mechanism is supported by experimental evidence. This can be seen as an NLI
problem but there are no directly usable datasets to address this. The main
challenge is that manually creating informative negative examples for this task
is difficult and expensive. We introduce a novel semi-supervised procedure that
bootstraps an NLI dataset from existing biomedical dataset that pairs
mechanisms with experimental evidence in abstracts. We generate a range of
negative examples using nine strategies that manipulate the structure of the
underlying mechanisms both with rules, e.g., flip the roles of the entities in
the interaction, and, more importantly, as perturbations via logical
constraints in a neuro-logical decoding system. We use this procedure to create
a novel dataset for NLI in the biomedical domain, called BioNLI and benchmark
two state-of-the-art biomedical classifiers. The best result we obtain is
around mid 70s in F1, suggesting the difficulty of the task. Critically, the
performance on the different classes of negative examples varies widely, from
97% F1 on the simple role change negative examples, to barely better than
chance on the negative examples generated using neuro-logic decoding."
13051,"We present our ﬁndings as the starting point for further research into
                                                  evaluating how LLMs interpret language in context and to drive the development
                                                  of more pragmatic and useful models of human discourse.","Models adapted to be “aligned with
                                                  human intent” perform much better, but still show a signiﬁcant gap with human
                                                  performance.","1 INTRODUCTION

                                                  User: “Have you seen my phone?”
                                                  InstructGPT: “Yes, I have seen your phone.”

                                        InstructGPT’s response1 is a perfectly ﬁne answer to the question, but a human might answer dif-
                                        ferently.",2022-10-26 19:04:23+00:00,Large language models are not zero-shot communicators,cs.CL,['cs.CL'],"[arxiv.Result.Author('Laura Ruis'), arxiv.Result.Author('Akbir Khan'), arxiv.Result.Author('Stella Biderman'), arxiv.Result.Author('Sara Hooker'), arxiv.Result.Author('Tim Rocktäschel'), arxiv.Result.Author('Edward Grefenstette')]","Despite widespread use of LLMs as conversational agents, evaluations of
performance fail to capture a crucial aspect of communication: interpreting
language in context. Humans interpret language using beliefs and prior
knowledge about the world. For example, we intuitively understand the response
""I wore gloves"" to the question ""Did you leave fingerprints?"" as meaning ""No"".
To investigate whether LLMs have the ability to make this type of inference,
known as an implicature, we design a simple task and evaluate widely used
state-of-the-art models. We find that, despite only evaluating on utterances
that require a binary inference (yes or no), most perform close to random.
Models adapted to be ""aligned with human intent"" perform much better, but still
show a significant gap with human performance. We present our findings as the
starting point for further research into evaluating how LLMs interpret language
in context and to drive the development of more pragmatic and useful models of
human discourse."
13067,"recreate the true distribution, like stochastic beam   We aim for these insights, and the evaluations we
search (Kool et al., 2019) and conditional pois-       use, to drive further research in understanding and
son stochastic beam search (Meister et al., 2021a).","We ﬁnd the
Some stochastic decoding methods attempt to ﬁnd        tendency of top-p decoding to over-truncate low-
high-likelihood sequences instead of attempting to     entropy distributions to be particularly surprising.","improving how we generate from neural language
Truncation sampling algorithms, like top-k (Fan        models.",2022-10-27 05:52:35+00:00,Truncation Sampling as Language Model Desmoothing,cs.CL,['cs.CL'],"[arxiv.Result.Author('John Hewitt'), arxiv.Result.Author('Christopher D. Manning'), arxiv.Result.Author('Percy Liang')]","Long samples of text from neural language models can be of poor quality.
Truncation sampling algorithms--like top-$p$ or top-$k$ -- address this by
setting some words' probabilities to zero at each step. This work provides
framing for the aim of truncation, and an improved algorithm for that aim. We
propose thinking of a neural language model as a mixture of a true distribution
and a smoothing distribution that avoids infinite perplexity. In this light,
truncation algorithms aim to perform desmoothing, estimating a subset of the
support of the true distribution. Finding a good subset is crucial: we show
that top-$p$ unnecessarily truncates high-probability words, for example
causing it to truncate all words but Trump for a document that starts with
Donald. We introduce $\eta$-sampling, which truncates words below an
entropy-dependent probability threshold. Compared to previous algorithms,
$\eta$-sampling generates more plausible long English documents according to
humans, is better at breaking out of repetition, and behaves more reasonably on
a battery of test distributions."
13084,"It remains a question        that LSI can also work with relational domain data
for further research to establish ways of evaluat-       thus opening up a broader range of data sources.","We demonstrate
our evaluation methodology.","ing embedding quality on a larger variety of OOV
The metric structure induced by node embeddings          Zhang of the National Institute of Health for shar-
trained on a domain knowledge graph provides an          ing their word embedding models.",2022-10-27 12:15:26+00:00,Leveraging knowledge graphs to update scientific word embeddings using latent semantic imputation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jason Hoelscher-Obermaier'), arxiv.Result.Author('Edward Stevinson'), arxiv.Result.Author('Valentin Stauber'), arxiv.Result.Author('Ivaylo Zhelev'), arxiv.Result.Author('Victor Botev'), arxiv.Result.Author('Ronin Wu'), arxiv.Result.Author('Jeremy Minton')]","The most interesting words in scientific texts will often be novel or rare.
This presents a challenge for scientific word embedding models to determine
quality embedding vectors for useful terms that are infrequent or newly
emerging. We demonstrate how \gls{lsi} can address this problem by imputing
embeddings for domain-specific words from up-to-date knowledge graphs while
otherwise preserving the original word embedding model. We use the MeSH
knowledge graph to impute embedding vectors for biomedical terminology without
retraining and evaluate the resulting embedding model on a domain-specific
word-pair similarity task. We show that LSI can produce reliable embedding
vectors for rare and OOV terms in the biomedical domain."
13093,"(2020) also provides
   • We establish two Multilingual MMT bench-           a comprehensive study of the contribution of dif-
      mark datasets to nourish the further research     ferent components in M-BERT to its cross-lingual
      on Multilingual MMT, and extensive experi-        ability.","to evaluate the cross-lingual generalization capa-
                                                        bilities, Karthikeyan et al.",Rust et al.,2022-10-19 12:21:39+00:00,LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Hongcheng Guo'), arxiv.Result.Author('Jiaheng Liu'), arxiv.Result.Author('Haoyang Huang'), arxiv.Result.Author('Jian Yang'), arxiv.Result.Author('Zhoujun Li'), arxiv.Result.Author('Dongdong Zhang'), arxiv.Result.Author('Furu Wei')]","Multimodal Machine Translation (MMT) focuses on enhancing text-only
translation with visual features, which has attracted considerable attention
from both natural language processing and computer vision communities. Recent
advances still struggle to train a separate model for each language pair, which
is costly and unaffordable when the number of languages increases in the real
world. In other words, the multilingual multimodal machine translation
(Multilingual MMT) task has not been investigated, which aims to handle the
aforementioned issues by providing a shared semantic space for multiple
languages. Besides, the image modality has no language boundaries, which is
superior to bridging the semantic gap between languages. To this end, we first
propose the Multilingual MMT task by establishing two new Multilingual MMT
benchmark datasets covering seven languages. Then, an effective baseline LVP-M3
using visual prompts is proposed to support translations between different
languages, which includes three stages (token encoding, language-aware visual
prompt generation, and language translation). Extensive experimental results on
our constructed benchmark datasets demonstrate the effectiveness of LVP-M3
method for Multilingual MMT."
13094,"(2020) also provides
   • We establish two Multilingual MMT bench-           a comprehensive study of the contribution of dif-
      mark datasets to nourish the further research     ferent components in M-BERT to its cross-lingual
      on Multilingual MMT, and extensive experi-        ability.","to evaluate the cross-lingual generalization capa-
                                                        bilities, Karthikeyan et al.",Rust et al.,2022-10-19 12:21:39+00:00,LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Hongcheng Guo'), arxiv.Result.Author('Jiaheng Liu'), arxiv.Result.Author('Haoyang Huang'), arxiv.Result.Author('Jian Yang'), arxiv.Result.Author('Zhoujun Li'), arxiv.Result.Author('Dongdong Zhang'), arxiv.Result.Author('Zheng Cui'), arxiv.Result.Author('Furu Wei')]","Multimodal Machine Translation (MMT) focuses on enhancing text-only
translation with visual features, which has attracted considerable attention
from both natural language processing and computer vision communities. Recent
advances still struggle to train a separate model for each language pair, which
is costly and unaffordable when the number of languages increases in the real
world. In other words, the multilingual multimodal machine translation
(Multilingual MMT) task has not been investigated, which aims to handle the
aforementioned issues by providing a shared semantic space for multiple
languages. Besides, the image modality has no language boundaries, which is
superior to bridging the semantic gap between languages. To this end, we first
propose the Multilingual MMT task by establishing two new Multilingual MMT
benchmark datasets covering seven languages. Then, an effective baseline LVP-M3
using visual prompts is proposed to support translations between different
languages, which includes three stages (token encoding, language-aware visual
prompt generation, and language translation). Extensive experimental results on
our constructed benchmark datasets demonstrate the effectiveness of LVP-M3
method for Multilingual MMT."
13099,"Take X as an example:
automatic terminology annotation framework, as well as a
corresponding dataset for the purpose of further study (see         Xγ = Flatten(Identify(X))                                           (1)
our repository).","Due to the lack of available large-scale medical         contained in dialogues (of both X and Y ) via inserting a spe-
dialogue corpora with annotated terminology, we develop an          cial token [TERM].","The experimental results show superior per-
formance on a range of metrics compared to SOTA language            xi = xT , xi, xi is term                                            (2)
models, demonstrating the effectiveness of our proposed frame-      xi,                               otherwise
work and the importance of medical terminologies to medical
dialogue generation.",2022-10-27 15:41:46+00:00,Terminology-aware Medical Dialogue Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chen Tang'), arxiv.Result.Author('Hongbo Zhang'), arxiv.Result.Author('Tyler Loakman'), arxiv.Result.Author('Chenghua Lin'), arxiv.Result.Author('Frank Guerin')]","Medical dialogue generation aims to generate responses according to a history
of dialogue turns between doctors and patients. Unlike open-domain dialogue
generation, this requires background knowledge specific to the medical domain.
Existing generative frameworks for medical dialogue generation fall short of
incorporating domain-specific knowledge, especially with regard to medical
terminology. In this paper, we propose a novel framework to improve medical
dialogue generation by considering features centered on domain-specific
terminology. We leverage an attention mechanism to incorporate terminologically
centred features, and fill in the semantic gap between medical background
knowledge and common utterances by enforcing language models to learn
terminology representations with an auxiliary terminology recognition task.
Experimental results demonstrate the effectiveness of our approach, in which
our proposed framework outperforms SOTA language models. Additionally, we
provide a new dataset with medical terminology annotations to support the
research on medical dialogue generation. Our dataset and code are available at
https://github.com/tangg555/meddialog."
13100,"Existing language       ogy annotation framework, and the corresponding large-scale
models struggle to learn domain-speciﬁc knowledge without          dataset for further study.","We also provide the terminol-
which diminishing returns are experienced.","the explicit modeling of medical concepts, and this is also how
we manage to ﬁll in the gap in terminological representations.",2022-10-27 15:41:46+00:00,Terminology-aware Medical Dialogue Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chen Tang'), arxiv.Result.Author('Hongbo Zhang'), arxiv.Result.Author('Tyler Loakman'), arxiv.Result.Author('Chenghua Lin'), arxiv.Result.Author('Frank Guerin')]","Medical dialogue generation aims to generate responses according to a history
of dialogue turns between doctors and patients. Unlike open-domain dialogue
generation, this requires background knowledge specific to the medical domain.
Existing generative frameworks for medical dialogue generation fall short of
incorporating domain-specific knowledge, especially with regard to medical
terminology. In this paper, we propose a novel framework to improve medical
dialogue generation by considering features centered on domain-specific
terminology. We leverage an attention mechanism to incorporate terminologically
centred features, and fill in the semantic gap between medical background
knowledge and common utterances by enforcing language models to learn
terminology representations with an auxiliary terminology recognition task.
Experimental results demonstrate the effectiveness of our approach, in which
our proposed framework outperforms SOTA language models. Additionally, we
provide a new dataset with medical terminology annotations to support the
research on medical dialogue generation. Our dataset and code are available at
https://github.com/tangg555/meddialog."
13163,"4.3 TC14 Datasets

4.2.3 Clustering Algorithm                               To further study the potential applications and limi-
                                                         tations of SimPTC, we collect 14 publicly available
In this section, we aim to show what makes a good        text classiﬁcation datasets with various topics, text
choice of clustering algorithm for SimPTC by com-        lengths, and numbers of classes (Table 5).","a similar performance after clustering, showing the
robustness of SimPTC to the initialization method.","For sim-
paring BGMM with K-Means and GMM.",2022-10-29 16:01:51+00:00,Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Yu Fei'), arxiv.Result.Author('Ping Nie'), arxiv.Result.Author('Zhao Meng'), arxiv.Result.Author('Roger Wattenhofer'), arxiv.Result.Author('Mrinmaya Sachan')]","Recent work has demonstrated that pre-trained language models (PLMs) are
zero-shot learners. However, most existing zero-shot methods involve heavy
human engineering or complicated self-training pipelines, hindering their
application to new situations. In this work, we show that zero-shot text
classification can be improved simply by clustering texts in the embedding
spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian
Gaussian Mixture Model after initializing cluster positions and shapes using
class names. Despite its simplicity, this approach achieves superior or
comparable performance on both topic and sentiment classification datasets and
outperforms prior works significantly on unbalanced datasets. We further
explore the applicability of our clustering approach by evaluating it on 14
datasets with more diverse topics, text lengths, and numbers of classes. Our
approach achieves an average of 20% absolute improvement over prompt-based
zero-shot learning. Finally, we compare different PLM embedding spaces and find
that texts are well-clustered by topics even if the PLM is not explicitly
pre-trained to generate meaningful sentence embeddings. This work indicates
that PLM embeddings can categorize texts without task-specific fine-tuning,
thus providing a new way to analyze and utilize their knowledge and zero-shot
learning ability."
13164,"To further study the potential applications and limi-
4.2.3 Clustering Algorithm                               tations of SimPTC, we collect 14 publicly available
                                                         text classiﬁcation datasets with various topics, text
In this section, we aim to show what makes a good        lengths, and numbers of classes (Table 5).","worse initialization performance, SimPTC achieves
a similar performance after clustering, showing the      4.3 TC14 Datasets
robustness of SimPTC to the initialization method.","For sim-
choice of clustering algorithm for SimPTC by com-        plicity, we refer to these datasets as TC14.",2022-10-29 16:01:51+00:00,Beyond Prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Yu Fei'), arxiv.Result.Author('Ping Nie'), arxiv.Result.Author('Zhao Meng'), arxiv.Result.Author('Roger Wattenhofer'), arxiv.Result.Author('Mrinmaya Sachan')]","Recent work has demonstrated that pre-trained language models (PLMs) are
zero-shot learners. However, most existing zero-shot methods involve heavy
human engineering or complicated self-training pipelines, hindering their
application to new situations. In this work, we show that zero-shot text
classification can be improved simply by clustering texts in the embedding
spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian
Gaussian Mixture Model after initializing cluster positions and shapes using
class names. Despite its simplicity, this approach achieves superior or
comparable performance on both topic and sentiment classification datasets and
outperforms prior works significantly on unbalanced datasets. We further
explore the applicability of our clustering approach by evaluating it on 14
datasets with more diverse topics, text lengths, and numbers of classes. Our
approach achieves an average of 20% absolute improvement over prompt-based
zero-shot learning. Finally, we compare different PLM embedding spaces and find
that texts are well-clustered by topics even if the PLM is not explicitly
pre-trained to generate meaningful sentence embeddings. This work indicates
that PLM embeddings can categorize texts without task-specific fine-tuning,
thus providing a new way to analyze and utilize their knowledge and zero-shot
learning ability."
13180,In       can be used for further research in the future.,"This data
ready very high, using synthetic supervision shows     includes rules of spatial reasoning and the chain of
no improvements compared to the model that only        logical reasoning for answering the questions that
trained with MSPRL training set for the SRol.","contrast, on SPARTQA-HUMAN, using synthetic
supervision helps the model perform better.",2022-10-30 21:23:34+00:00,Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Roshanak Mirzaee'), arxiv.Result.Author('Parisa Kordjamshidi')]","Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small."
13181,"However, since the accuracy of the SRol is al-   includes rules of spatial reasoning and the chain of
ready very high, using synthetic supervision shows     logical reasoning for answering the questions that
no improvements compared to the model that only        can be used for further research in the future.","This data
task.",trained with MSPRL training set for the SRol.,2022-10-30 21:23:34+00:00,Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Roshanak Mirzaee'), arxiv.Result.Author('Parisa Kordjamshidi')]","Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small."
13194,"All thunderstorms         son lays the groundwork for further study of
(c) what essential oil to  produce lightning and are very dangerous.","ning strike and static     Lightning is a bright ﬂash of electricity pro-     Static Electricity: Introducing Atoms This les-
electricity                duced by a thunderstorm.","If       static and current electricity by focusing on
stop the itching of bug    you hear the sound of thunder, then you are in     the idea of positive and negative charges at the
bites                      danger from lightning.",2022-10-31 09:25:42+00:00,Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives,cs.CL,['cs.CL'],"[arxiv.Result.Author('Si Sun'), arxiv.Result.Author('Chenyan Xiong'), arxiv.Result.Author('Yue Yu'), arxiv.Result.Author('Arnold Overwijk'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Jie Bao')]","In this paper, we investigate the instability in the standard dense retrieval
training, which iterates between model training and hard negative selection
using the being-trained model. We show the catastrophic forgetting phenomena
behind the training instability, where models learn and forget different
negative groups during training iterations. We then propose ANCE-Tele, which
accumulates momentum negatives from past iterations and approximates future
iterations using lookahead negatives, as ""teleportations"" along the time axis
to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms
previous state-of-the-art systems of similar size, eliminates the dependency on
sparse retrieval negatives, and is competitive among systems using
significantly more (50x) parameters. Our analysis demonstrates that
teleportation negatives reduce catastrophic forgetting and improve convergence
speed for dense retrieval training. Our code is available at
https://github.com/OpenMatch/ANCE-Tele."
13221,"L¯ila’s uniﬁed problem format decouples
synthesis from computation, while opening directions for further study on either
aspect.","We even see beneﬁts of program synthesis
in NLI, a classiﬁcation-based task.","10
Dimension        Neo-A         Neo-P
              IID OOD       IID OOD
Math ability
Language      0.191  0.129  0.445  0.188
Format        0.189  0.147  0.429  0.246
Knowledge     0.246  0.382  0.372  0.404
              0.206  0.143  0.331  0.213
Average
              0.208 0.200   0.394 0.263

Table 4: Multi-task models are able to generalize to unseen tasks in some
categories.",2022-10-31 17:41:26+00:00,Lila: A Unified Benchmark for Mathematical Reasoning,cs.CL,"['cs.CL', 'cs.AI', '68T50', 'I.2.7']","[arxiv.Result.Author('Swaroop Mishra'), arxiv.Result.Author('Matthew Finlayson'), arxiv.Result.Author('Pan Lu'), arxiv.Result.Author('Leonard Tang'), arxiv.Result.Author('Sean Welleck'), arxiv.Result.Author('Chitta Baral'), arxiv.Result.Author('Tanmay Rajpurohit'), arxiv.Result.Author('Oyvind Tafjord'), arxiv.Result.Author('Ashish Sabharwal'), arxiv.Result.Author('Peter Clark'), arxiv.Result.Author('Ashwin Kalyan')]","Mathematical reasoning skills are essential for general-purpose intelligent
systems to perform tasks from grocery shopping to climate modeling. Towards
evaluating and improving AI systems in this domain, we propose LILA, a unified
mathematical reasoning benchmark consisting of 23 diverse tasks along four
dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language
format e.g., question-answering, fill-in-the-blanks (iii) language diversity
e.g., no language, simple language (iv) external knowledge e.g., commonsense,
physics. We construct our benchmark by extending 20 datasets benchmark by
collecting task instructions and solutions in the form of Python programs,
thereby obtaining explainable solutions in addition to the correct answer. We
additionally introduce two evaluation datasets to measure out-of-distribution
performance and robustness to language perturbation. Finally, we introduce
BHASKARA, a general-purpose mathematical reasoning model trained on LILA.
Importantly, we find that multi-tasking leads to significant improvements
(average relative improvement of 21.83% F1 score vs. single-task models), while
the best performing model only obtains 60.40%, indicating the room for
improvement in general mathematical reasoning and understanding."
13222,"and an associated automatic evaluation framework,
aiming to facilitate further research along the lines          Exposure to the task and template/prompt.","We             even weakly labeled data is presumably essential
release our code1, including access to all datasets,        to learn such distinctions.",explored here.,2022-10-31 17:55:00+00:00,Zero-Shot Text Classification with Self-Training,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Ariel Gera'), arxiv.Result.Author('Alon Halfon'), arxiv.Result.Author('Eyal Shnarch'), arxiv.Result.Author('Yotam Perlitz'), arxiv.Result.Author('Liat Ein-Dor'), arxiv.Result.Author('Noam Slonim')]","Recent advances in large pretrained language models have increased attention
to zero-shot text classification. In particular, models finetuned on natural
language inference datasets have been widely adopted as zero-shot classifiers
due to their promising results and off-the-shelf availability. However, the
fact that such models are unfamiliar with the target task can lead to
instability and performance issues. We propose a plug-and-play method to bridge
this gap using a simple self-training approach, requiring only the class names
along with an unlabeled dataset, and without the need for domain expertise or
trial and error. We show that fine-tuning the zero-shot classifier on its most
confident predictions leads to significant performance gains across a wide
range of text classification tasks, presumably since self-training adapts the
zero-shot model to the task at hand."
13232,"If you are unsure about any particular       ample, “the result merits further study” is accept-
answer, please enter “-1” in the relevant cell.","For ex-
to the table.","able, but “Two men claimed that the results merit
                                                           further study” is not.",2022-10-31 21:05:42+00:00,TaTa: A Multilingual Table-to-Text Dataset for African Languages,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Sebastian Gehrmann'), arxiv.Result.Author('Sebastian Ruder'), arxiv.Result.Author('Vitaly Nikolaev'), arxiv.Result.Author('Jan A. Botha'), arxiv.Result.Author('Michael Chavinda'), arxiv.Result.Author('Ankur Parikh'), arxiv.Result.Author('Clara Rivera')]","Existing data-to-text generation datasets are mostly limited to English. To
address this lack of data, we create Table-to-Text in African languages (TaTa),
the first large multilingual table-to-text dataset with a focus on African
languages. We created TaTa by transcribing figures and accompanying text in
bilingual reports by the Demographic and Health Surveys Program, followed by
professional translation to make the dataset fully parallel. TaTa includes
8,700 examples in nine languages including four African languages (Hausa, Igbo,
Swahili, and Yor\`ub\'a) and a zero-shot test language (Russian). We
additionally release screenshots of the original figures for future research on
multilingual multi-modal approaches. Through an in-depth human evaluation, we
show that TaTa is challenging for current models and that less than half the
outputs from an mT5-XXL-based model are understandable and attributable to the
source data. We further demonstrate that existing metrics perform poorly for
TaTa and introduce learned metrics that achieve a high correlation with human
judgments. We release all data and annotations at
https://github.com/google-research/url-nlp."
13233,"able, but “Two men claimed that the results merit
                                                           further study” is not.","If you are unsure about any particular       ample, “the result merits further study” is accept-
answer, please enter “-1” in the relevant cell.",(Q1) The text is understandable.,2022-10-31 21:05:42+00:00,TaTa: A Multilingual Table-to-Text Dataset for African Languages,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Sebastian Gehrmann'), arxiv.Result.Author('Sebastian Ruder'), arxiv.Result.Author('Vitaly Nikolaev'), arxiv.Result.Author('Jan A. Botha'), arxiv.Result.Author('Michael Chavinda'), arxiv.Result.Author('Ankur Parikh'), arxiv.Result.Author('Clara Rivera')]","Existing data-to-text generation datasets are mostly limited to English. To
address this lack of data, we create Table-to-Text in African languages (TaTa),
the first large multilingual table-to-text dataset with a focus on African
languages. We created TaTa by transcribing figures and accompanying text in
bilingual reports by the Demographic and Health Surveys Program, followed by
professional translation to make the dataset fully parallel. TaTa includes
8,700 examples in nine languages including four African languages (Hausa, Igbo,
Swahili, and Yor\`ub\'a) and a zero-shot test language (Russian). We
additionally release screenshots of the original figures for future research on
multilingual multi-modal approaches. Through an in-depth human evaluation, we
show that TaTa is challenging for current models and that less than half the
outputs from an mT5-XXL-based model are understandable and attributable to the
source data. We further demonstrate that existing metrics perform poorly for
TaTa and introduce learned metrics that achieve a high correlation with human
judgments. We release all data and annotations at
https://github.com/google-research/url-nlp."
13234,We further study this scaling effect in Sec.,"We ﬁnd
that using larger capacity model as the extrinsic calibrator shows the same trend with shifting from
the vanilla ﬁne-tuning to learnable methods.","5.4; (2) For

                                                                           7
Preprint

              Table 2: Results of the calibration performance of T5 with increasing dataset sizes.",2022-10-31 21:31:07+00:00,A Close Look into the Calibration of Pre-trained Language Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Yangyi Chen'), arxiv.Result.Author('Lifan Yuan'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Heng Ji')]","Pre-trained language models (PLMs) achieve remarkable performance on many
downstream tasks, but may fail in giving reliable estimates of their predictive
uncertainty. Given the lack of a comprehensive understanding of PLMs
calibration, we take a close look into this new research problem, aiming to
answer two questions: (1) Do PLMs learn to become calibrated in the training
process? (2) How effective are existing calibration methods? For the first
question, we conduct fine-grained control experiments to study the dynamic
change in PLMs' calibration performance in training. We consider six factors as
control variables, including dataset difficulty, available training samples,
training steps, the number of tunable parameters, model scale, and pretraining.
In experiments, we observe a consistent change in calibration performance
across six factors. We find that PLMs don't learn to become calibrated in
training, evidenced by the continual increase in confidence, no matter the
predictions are correct or not. We highlight that our finding presents some
contradiction with two established conclusions: (a) Larger PLMs are more
calibrated; (b) Pretraining improves model calibration. Next, we study the
effectiveness of existing calibration methods in mitigating the overconfidence
issue, in both in-distribution and various out-of-distribution settings.
Besides unlearnable calibration methods, we adapt two recently proposed
learnable methods that directly collect data to train models to have reasonable
confidence estimations. Also, we propose extended learnable methods based on
existing ones to further improve or maintain PLMs calibration without
sacrificing the original task performance. Experimental results show that
learnable methods significantly reduce PLMs' confidence in wrong predictions,
and our methods exhibit superior performance compared with previous methods."
13235,We further study this scaling effect in Sec.,"We ﬁnd
that using larger capacity model as the extrinsic calibrator shows the same trend with shifting from
the vanilla ﬁne-tuning to learnable methods.","5.4; (2) For
intrinsic methods, the three different training paradigms don’t show substantial difference consid-
ering the calibration performance, and none of them consistently achieves the best performance on
all datasets.",2022-10-31 21:31:07+00:00,A Close Look into the Calibration of Pre-trained Language Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Yangyi Chen'), arxiv.Result.Author('Lifan Yuan'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Heng Ji')]","Pre-trained language models (PLMs) achieve remarkable performance on many
downstream tasks, but may fail in giving reliable estimates of their predictive
uncertainty. Given the lack of a comprehensive understanding of PLMs
calibration, we take a close look into this new research problem, aiming to
answer two questions: (1) Do PLMs learn to become calibrated in the training
process? (2) How effective are existing calibration methods? For the first
question, we conduct fine-grained control experiments to study the dynamic
change in PLMs' calibration performance in training. We consider six factors as
control variables, including dataset difficulty, available training samples,
training steps, the number of tunable parameters, model scale, and pretraining.
In experiments, we observe a consistent change in calibration performance
across six factors. We find that PLMs don't learn to become calibrated in
training, evidenced by the continual increase in confidence, no matter the
predictions are correct or not. We highlight that our finding presents some
contradiction with two established conclusions: (a) Larger PLMs are more
calibrated; (b) Pretraining improves model calibration. Next, we study the
effectiveness of existing calibration methods in mitigating the overconfidence
issue, in both in-distribution and various out-of-distribution settings.
Besides unlearnable calibration methods, we adapt two recently proposed
learnable methods that directly collect data to train models to have reasonable
confidence estimations. Also, we propose extended learnable methods based on
existing ones to further improve or maintain PLMs calibration without
sacrificing the original task performance. Experimental results show that
learnable methods significantly reduce PLMs' confidence in wrong predictions,
and our methods exhibit superior performance compared with previous methods."
13292,"We have to         former encoder·encoder model to share parameters between the hy-
mention that this does not imply the context is just ﬁve segments be-    pothesis encoder and query encoder and between different input lan-
cause several Transformer blocks are stacked; therefore, the context     guages opens further research questions, such as the possibility of
of the last layer is wider than these ﬁve segments.","The ability of the Trans-
current, two preceding, and two following time steps.","combining STD (graphemic query) and QbE (spoken query) in a sin-
                                                                         gle, multi-task trained model.",2022-11-02 13:03:15+00:00,Transformer-based encoder-encoder architecture for Spoken Term Detection,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Jan Švec'), arxiv.Result.Author('Luboš Šmídl'), arxiv.Result.Author('Jan Lehečka')]","The paper presents a method for spoken term detection based on the
Transformer architecture. We propose the encoder-encoder architecture employing
two BERT-like encoders with additional modifications, including convolutional
and upsampling layers, attention masking, and shared parameters. The encoders
project a recognized hypothesis and a searched term into a shared embedding
space, where the score of the putative hit is computed using the calibrated dot
product. In the experiments, we used the Wav2Vec 2.0 speech recognizer, and the
proposed system outperformed a baseline method based on deep LSTMs on the
English and Czech STD datasets based on USC Shoah Foundation Visual History
Archive (MALACH)."
13333,"Alter-
                                        1 Introduction                                          natively, generating synthetic but realistic EHRs
                                                                                                can circumvent data leakage while preserving the
                                        The prevalence of electronic patient healthcare         patterns of real EHRs for further research and de-
                                        records fuel the development of machine learn-          velopment (Biswal et al., 2020).","1                                           argued not immune to the hack for re-identiﬁcation
                                                                                                (El Emam et al., 2011; Choi et al., 2017).","ing models for many healthcare applications (Choi
                                        et al., 2016b,a; Wang et al., 2021a,b; Wang and            Deep generative models like GANs (Goodfellow
                                        Sun, 2022a).",2022-10-11 14:48:15+00:00,PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Zifeng Wang'), arxiv.Result.Author('Jimeng Sun')]","Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is
challenging due to privacy concerns, which hinders the use of ML for healthcare
applications. Synthetic EHRs generation bypasses the need to share sensitive
real patient records. However, existing methods generate single-modal EHRs by
unconditional generation or by longitudinal inference, which falls short of low
flexibility and makes unrealistic EHRs. In this work, we propose to formulate
EHRs generation as a text-to-text translation task by language models (LMs),
which suffices to highly flexible event imputation during generation. We also
design prompt learning to control the generation conditioned by numerical and
categorical demographic features. We evaluate synthetic EHRs quality by two
perplexity measures accounting for their longitudinal pattern (longitudinal
imputation perplexity, lpl) and the connections cross modalities
(cross-modality imputation perplexity, mpl). Moreover, we utilize two
adversaries: membership and attribute inference attacks for privacy-preserving
evaluation. Experiments on MIMIC-III data demonstrate the superiority of our
methods on realistic EHRs generation (53.1\% decrease of lpl and 45.3\%
decrease of mpl on average compared to the best baselines) with low privacy
risks. Software is available at https://github.com/RyanWangZf/PromptEHR."
13356,"We hope that highlighting this dis-
(Hendrycks et al., 2021; Gehrmann et al., 2021;           crepancy will encourage further research into the
Srivastava et al., 2022).","As the performance             language models on many complex tasks, their per-
of large language models rapidly improves, re-            formance on LMentry’s trivial tasks is rather un-
cent benchmarks become larger and more complex            derwhelming.","LMentry takes a different       underlining principles governing the behavior of
approach, avoiding this “arms race” between model         large language models, particularly making sure
and benchmark development by focusing on trivial          their performance on complex user-facing tasks
tasks.",2022-11-03 18:01:12+00:00,LMentry: A Language Model Benchmark of Elementary Language Tasks,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Avia Efrat'), arxiv.Result.Author('Or Honovich'), arxiv.Result.Author('Omer Levy')]","As the performance of large language models rapidly improves, benchmarks are
getting larger and more complex as well. We present LMentry, a benchmark that
avoids this ""arms race"" by focusing on a compact set of tasks that are trivial
to humans, e.g. writing a sentence containing a specific word, identifying
which words in a list belong to a specific category, or choosing which of two
words is longer. LMentry is specifically designed to provide quick and
interpretable insights into the capabilities and robustness of large language
models. Our experiments reveal a wide variety of failure cases that, while
immediately obvious to humans, pose a considerable challenge for large language
models, including OpenAI's latest 175B-parameter instruction-tuned model,
TextDavinci002. LMentry complements contemporary evaluation approaches of large
language models, providing a quick, automatic, and easy-to-run ""unit test"",
without resorting to large benchmark suites of complex tasks."
13357,"We hope that highlighting this dis-
(Hendrycks et al., 2021; Gehrmann et al., 2021;           crepancy will encourage further research into the
Srivastava et al., 2022).","As the performance             language models on many complex tasks, their per-
of large language models rapidly improves, re-            formance on LMentry’s trivial tasks is rather un-
cent benchmarks become larger and more complex            derwhelming.","LMentry takes a different       underlining principles governing the behavior of
approach, avoiding this “arms race” between model         large language models, particularly making sure
and benchmark development by focusing on trivial          their performance on complex user-facing tasks
tasks.",2022-11-03 18:01:12+00:00,LMentry: A Language Model Benchmark of Elementary Language Tasks,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Avia Efrat'), arxiv.Result.Author('Or Honovich'), arxiv.Result.Author('Omer Levy')]","As the performance of large language models rapidly improves, benchmarks are
getting larger and more complex as well. We present LMentry, a benchmark that
avoids this ""arms race"" by focusing on a compact set of tasks that are trivial
to humans, e.g. writing a sentence containing a specific word, identifying
which words in a list belong to a specific category, or choosing which of two
words is longer. LMentry is specifically designed to provide quick and
interpretable insights into the capabilities and robustness of large language
models. Our experiments reveal a wide variety of failure cases that, while
immediately obvious to humans, pose a considerable challenge for large language
models, including OpenAI's latest 175B-parameter instruction-tuned model,
TextDavinci002. LMentry complements contemporary evaluation approaches of large
language models, providing a quick, automatic, and easy-to-run ""unit test"",
without resorting to large benchmark suites of complex tasks."
13362,"In this work, we do not further study              rence, Italy.","In Proceedings
could generate incorrect temporal information due             of the 57th Annual Meeting of the Association for
to the lack of world knowledge (Figure 9 of Ap-               Computational Linguistics, pages 4884–4895, Flo-
pendix G).","Association for Computational Linguis-
methods that can incorporate extra world knowl-               tics.",2022-11-03 22:10:25+00:00,Time-aware Prompting for Text Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shuyang Cao'), arxiv.Result.Author('Lu Wang')]","In this paper, we study the effects of incorporating timestamps, such as
document creation dates, into generation systems. Two types of time-aware
prompts are investigated: (1) textual prompts that encode document timestamps
in natural language sentences; and (2) linear prompts that convert timestamps
into continuous vectors. To explore extrapolation to future data points, we
further introduce a new data-to-text generation dataset, TempWikiBio,
containing more than 4 millions of chronologically ordered revisions of
biographical articles from English Wikipedia, each paired with structured
personal profiles. Through data-to-text generation on TempWikiBio, text-to-text
generation on the content transfer dataset, and summarization on XSum, we show
that linear prompts on encoder and textual prompts improve the generation
quality on all datasets. Despite having less performance drop when testing on
data drawn from a later time, linear prompts focus more on non-temporal
information and are less sensitive to the given timestamps, according to human
evaluations and sensitivity analyses. Meanwhile, textual prompts establish the
association between the given timestamps and the output dates, yielding more
factual temporal information in the output."
13448,"It                     Q
also motivates further research on simpler or more
efficient Transformer models, either for pretraining     them with the input X results in: Qh, Kh, V h ∈
(Lee-Thorp et al., 2021; Liu et al., 2021; Hua et al.,   Rn×d′ (the queries, keys and values, respectively).",", H} has three learnable
                                                                                             Rd×d′ .3
   This work grants a better understanding of the        matrices:  W  h  ,  WKh  ,  WVh  ∈            Multiplying
attention mechanism in pretrained Transformers.","2022) or potentially as an adaptation of existing
pretrained models (Peng et al., 2020a, 2022; Kasai          The queries and the keys compute a n × n atten-
et al., 2021).",2022-11-07 12:37:54+00:00,How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Michael Hassid'), arxiv.Result.Author('Hao Peng'), arxiv.Result.Author('Daniel Rotem'), arxiv.Result.Author('Jungo Kasai'), arxiv.Result.Author('Ivan Montero'), arxiv.Result.Author('Noah A. Smith'), arxiv.Result.Author('Roy Schwartz')]","The attention mechanism is considered the backbone of the widely-used
Transformer architecture. It contextualizes the input by computing
input-specific attention matrices. We find that this mechanism, while powerful
and elegant, is not as important as typically thought for pretrained language
models. We introduce PAPA, a new probing method that replaces the
input-dependent attention matrices with constant ones -- the average attention
weights over multiple inputs. We use PAPA to analyze several established
pretrained Transformers on six downstream tasks. We find that without any
input-dependent attention, all models achieve competitive performance -- an
average relative drop of only 8% from the probing baseline. Further, little or
no performance drop is observed when replacing half of the input-dependent
attention matrices with constant (input-independent) ones. Interestingly, we
show that better-performing models lose more from applying our method than
weaker models, suggesting that the utilization of the input-dependent attention
mechanism might be a factor in their success. Our results motivate research on
simpler alternatives to input-dependent attention, as well as on methods for
better utilization of this mechanism in the Transformer architecture."
13489,"We
the context “Dolly is running on the grassland”,       hope our benchmark and ﬁndings could facili-
PLMs should conceptualize Dolly as an Animal           tate further research on concept-aware PLMs and
since there is no enough evidence for Mammal.","For instance, given     ceptual knowledge and analyze the reasons.",human-like language understandings.,2022-11-08 08:18:06+00:00,COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hao Peng'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Hailong Jin'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Qun Liu')]","Conceptual knowledge is fundamental to human cognition and knowledge bases.
However, existing knowledge probing works only focus on evaluating factual
knowledge of pre-trained language models (PLMs) and ignore conceptual
knowledge. Since conceptual knowledge often appears as implicit commonsense
behind texts, designing probes for conceptual knowledge is hard. Inspired by
knowledge representation schemata, we comprehensively evaluate conceptual
knowledge of PLMs by designing three tasks to probe whether PLMs organize
entities by conceptual similarities, learn conceptual properties, and
conceptualize entities in contexts, respectively. For the tasks, we collect and
annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual
knowledge Probing bENchmark. Extensive experiments on different sizes and types
of PLMs show that existing PLMs systematically lack conceptual knowledge and
suffer from various spurious correlations. We believe this is a critical
bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are
publicly released at https://github.com/THU-KEG/COPEN."
13490,"We hope our ﬁndings
could facilitate further research on more powerful    Acknowledgements
PLMs with fewer parameters.",comprehensive conclusions.,"This work is supported by the Key-Area Research
Ethical Considerations                                and Development Program of Guangdong Province
                                                      (2019B010153002), the Institute for Guo Qiang,
We discuss the ethical considerations and broader     Tsinghua University (2019GQB0003), and Huawei
impact of this work in this section: (1) Intellec-    Noah’s Ark Lab.",2022-11-08 08:18:06+00:00,COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hao Peng'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Hailong Jin'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Qun Liu')]","Conceptual knowledge is fundamental to human cognition and knowledge bases.
However, existing knowledge probing works only focus on evaluating factual
knowledge of pre-trained language models (PLMs) and ignore conceptual
knowledge. Since conceptual knowledge often appears as implicit commonsense
behind texts, designing probes for conceptual knowledge is hard. Inspired by
knowledge representation schemata, we comprehensively evaluate conceptual
knowledge of PLMs by designing three tasks to probe whether PLMs organize
entities by conceptual similarities, learn conceptual properties, and
conceptualize entities in contexts, respectively. For the tasks, we collect and
annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual
knowledge Probing bENchmark. Extensive experiments on different sizes and types
of PLMs show that existing PLMs systematically lack conceptual knowledge and
suffer from various spurious correlations. We believe this is a critical
bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are
publicly released at https://github.com/THU-KEG/COPEN."
13498,"We conclude
                                                       that the localization of sociodemographic knowl-

                                                           9No large conﬁguration of mDeBERTa was available
edge in multilingual models follows their mono-        ﬁndings will fuel further research towards human-
lingual counterparts.","the sociodemographics, the results of DeBERTa
                                                       v3 base show an opposite trend.",The layer-wise behavior of       like language understanding.,2022-11-08 14:37:45+00:00,"SocioProbe: What, When, and Where Language Models Learn about Sociodemographics",cs.CL,['cs.CL'],"[arxiv.Result.Author('Anne Lauscher'), arxiv.Result.Author('Federico Bianchi'), arxiv.Result.Author('Samuel Bowman'), arxiv.Result.Author('Dirk Hovy')]","Pre-trained language models (PLMs) have outperformed other NLP models on a
wide range of tasks. Opting for a more thorough understanding of their
capabilities and inner workings, researchers have established the extend to
which they capture lower-level knowledge like grammaticality, and mid-level
semantic knowledge like factual understanding. However, there is still little
understanding of their knowledge of higher-level aspects of language. In
particular, despite the importance of sociodemographic aspects in shaping our
language, the questions of whether, where, and how PLMs encode these aspects,
e.g., gender or age, is still unexplored. We address this research gap by
probing the sociodemographic knowledge of different single-GPU PLMs on multiple
English data sets via traditional classifier probing and information-theoretic
minimum description length probing. Our results show that PLMs do encode these
sociodemographics, and that this knowledge is sometimes spread across the
layers of some of the tested PLMs. We further conduct a multilingual analysis
and investigate the effect of supplementary training to further explore to what
extent, where, and with what amount of pre-training data the knowledge is
encoded. Our overall results indicate that sociodemographic knowledge is still
a major challenge for NLP. PLMs require large amounts of pre-training data to
acquire the knowledge and models that excel in general language understanding
do not seem to own more knowledge about these aspects."
13561,"In a further study [4], the dataset was augmented with
       Will adversarial training with unlabeled examples in tar-        Bulgarian and Dutch tweets, and initial experiments on multilin-
       get language improve over the zero-shot cross-lingual            gual classification were conducted.",pendently.,"In the proposed system, mBERT
       setup?",2022-11-09 18:18:53+00:00,Cross-lingual Transfer Learning for Check-worthy Claim Identification over Twitter,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR']","[arxiv.Result.Author('Maram Hasanain'), arxiv.Result.Author('Tamer Elsayed')]","Misinformation spread over social media has become an undeniable infodemic.
However, not all spreading claims are made equal. If propagated, some claims
can be destructive, not only on the individual level, but to organizations and
even countries. Detecting claims that should be prioritized for fact-checking
is considered the first step to fight against spread of fake news. With
training data limited to a handful of languages, developing supervised models
to tackle the problem over lower-resource languages is currently infeasible.
Therefore, our work aims to investigate whether we can use existing datasets to
train models for predicting worthiness of verification of claims in tweets in
other languages. We present a systematic comparative study of six approaches
for cross-lingual check-worthiness estimation across pairs of five diverse
languages with the help of Multilingual BERT (mBERT) model. We run our
experiments using a state-of-the-art multilingual Twitter dataset. Our results
show that for some language pairs, zero-shot cross-lingual transfer is possible
and can perform as good as monolingual models that are trained on the target
language. We also show that in some languages, this approach outperforms (or at
least is comparable to) state-of-the-art models."
13577,"We further study the models’ behavior with respect
Finally, we observe a drop in performance for both    to the type of attributes they see and their general-
T5 and RoBERTa single-attribute prompt, which         ization properties.","This model generates        6 Discussion
sentences describing entity state changes but, un-
like our models, does not verbalize the attributes.","This analysis serves to uncover
conﬁrms that attribute dependencies are important     advantages and disadvantages of each technique
in our task.",2022-11-10 07:48:01+00:00,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Evangelia Spiliopoulou'), arxiv.Result.Author('Artidoro Pagnoni'), arxiv.Result.Author('Yonatan Bisk'), arxiv.Result.Author('Eduard Hovy')]","This paper investigates models of event implications. Specifically, how well
models predict entity state-changes, by targeting their understanding of
physical attributes. Nominally, Large Language models (LLM) have been exposed
to procedural knowledge about how objects interact, yet our benchmarking shows
they fail to reason about the world. Conversely, we also demonstrate that
existing approaches often misrepresent the surprising abilities of LLMs via
improper task encodings and that proper model prompting can dramatically
improve performance of reported baseline results across multiple tasks. In
particular, our results indicate that our prompting technique is especially
useful for unseen attributes (out-of-domain) or when only limited data is
available."
13584,"motivate further research in making ToD systems
simpler and quicker to develop.","We hope that           scalable multi-domain conversational agents: The
this work on using sample efﬁcient LLMs serves to         schema-guided dialogue dataset.",Nils Reimers and Iryna Gurevych.,2022-11-10 14:16:00+00:00,Prompt Learning for Domain Adaptation in Task-Oriented Dialogue,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Makesh Narsimhan Sreedhar'), arxiv.Result.Author('Christopher Parisien')]","Conversation designers continue to face significant obstacles when creating
production quality task-oriented dialogue systems. The complexity and cost
involved in schema development and data collection is often a major barrier for
such designers, limiting their ability to create natural, user-friendly
experiences. We frame the classification of user intent as the generation of a
canonical form, a lightweight semantic representation using natural language.
We show that canonical forms offer a promising alternative to traditional
methods for intent classification. By tuning soft prompts for a frozen large
language model, we show that canonical forms generalize very well to new,
unseen domains in a zero- or few-shot setting. The method is also
sample-efficient, reducing the complexity and effort of developing new
task-oriented dialogue domains."
13585,"We model topics in      our knowledge, it is the ﬁrst work that proposes to
                                             citation sentences in order to understand what     use MDS for topic labeling on top of topic clusters
                                             further research needs to be done to fully op-     constructed with contextualized embeddings.","To
                                             an exploratory case study.",erationalize MDS for topic labeling.,2022-10-28 17:47:12+00:00,Moving beyond word lists: towards abstractive topic labels for human-like topics of scientific documents,cs.CL,['cs.CL'],[arxiv.Result.Author('Domenic Rosati')],"Topic models represent groups of documents as a list of words (the topic
labels). This work asks whether an alternative approach to topic labeling can
be developed that is closer to a natural language description of a topic than a
word list. To this end, we present an approach to generating human-like topic
labels using abstractive multi-document summarization (MDS). We investigate our
approach with an exploratory case study. We model topics in citation sentences
in order to understand what further research needs to be done to fully
operationalize MDS for topic labeling. Our case study shows that in addition to
more human-like topics there are additional advantages to evaluation by using
clustering and summarization measures instead of topic model measures. However,
we find that there are several developments needed before we can design a
well-powered study to evaluate MDS for topic modeling fully. Namely, improving
cluster cohesion, improving the factuality and faithfulness of MDS, and
increasing the number of documents that might be supported by MDS. We present a
number of ideas on how these can be tackled and conclude with some thoughts on
how topic modeling can also be used to improve MDS in general."
13605,"and encouraged participants to further research in
                                             We discuss the submissions and the baselines        this direction.","As part of the CRE-       of datasets recently released in the community, we
                                             ATIVESUMM workshop at COLING 2022, the              develop a shared task, composed of four sub-tasks,
                                             shared task attracted 18 submissions in total.","for each sub-task in this paper, along with di-
                                             rections for facilitating future work in the ﬁeld.",2022-11-10 21:31:03+00:00,CREATIVESUMM: Shared Task on Automatic Summarization for Creative Writing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Divyansh Agarwal'), arxiv.Result.Author('Alexander R. Fabbri'), arxiv.Result.Author('Simeng Han'), arxiv.Result.Author('Wojciech Kryscinski'), arxiv.Result.Author('Faisal Ladhak'), arxiv.Result.Author('Bryan Li'), arxiv.Result.Author('Kathleen McKeown'), arxiv.Result.Author('Dragomir Radev'), arxiv.Result.Author('Tianyi Zhang'), arxiv.Result.Author('Sam Wiseman')]","This paper introduces the shared task of summarizing documents in several
creative domains, namely literary texts, movie scripts, and television scripts.
Summarizing these creative documents requires making complex literary
interpretations, as well as understanding non-trivial temporal dependencies in
texts containing varied styles of plot development and narrative structure.
This poses unique challenges and is yet underexplored for text summarization
systems. In this shared task, we introduce four sub-tasks and their
corresponding datasets, focusing on summarizing books, movie scripts, primetime
television scripts, and daytime soap opera scripts. We detail the process of
curating these datasets for the task, as well as the metrics used for the
evaluation of the submissions. As part of the CREATIVESUMM workshop at COLING
2022, the shared task attracted 18 submissions in total. We discuss the
submissions and the baselines for each sub-task in this paper, along with
directions for facilitating future work in the field."
13606,"and encouraged participants to further research in
                                            We discuss the submissions and the baselines        this direction.","As part of the CRE-       of datasets recently released in the community, we
                                            ATIVESUMM workshop at COLING 2022, the              develop a shared task, composed of four sub-tasks,
                                            shared task attracted 18 submissions in total.","for each sub-task in this paper, along with di-
                                            rections for facilitating future work in the ﬁeld.",2022-11-10 21:31:03+00:00,CREATIVESUMM: Shared Task on Automatic Summarization for Creative Writing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Divyansh Agarwal'), arxiv.Result.Author('Alexander R. Fabbri'), arxiv.Result.Author('Simeng Han'), arxiv.Result.Author('Wojciech Kryściński'), arxiv.Result.Author('Faisal Ladhak'), arxiv.Result.Author('Bryan Li'), arxiv.Result.Author('Kathleen McKeown'), arxiv.Result.Author('Dragomir Radev'), arxiv.Result.Author('Tianyi Zhang'), arxiv.Result.Author('Sam Wiseman')]","This paper introduces the shared task of summarizing documents in several
creative domains, namely literary texts, movie scripts, and television scripts.
Summarizing these creative documents requires making complex literary
interpretations, as well as understanding non-trivial temporal dependencies in
texts containing varied styles of plot development and narrative structure.
This poses unique challenges and is yet underexplored for text summarization
systems. In this shared task, we introduce four sub-tasks and their
corresponding datasets, focusing on summarizing books, movie scripts, primetime
television scripts, and daytime soap opera scripts. We detail the process of
curating these datasets for the task, as well as the metrics used for the
evaluation of the submissions. As part of the CREATIVESUMM workshop at COLING
2022, the shared task attracted 18 submissions in total. We discuss the
submissions and the baselines for each sub-task in this paper, along with
directions for facilitating future work in the field."
13626,"We further study the ef-
tory HGSR, which even surpasses the BSR trained         fectiveness of the decoding features by removing
by 100% training data.","And
only 40% training data is enough to train a satisfac-      Decoding features.","This indicates that the pro-     modelt/modelv/modelp, respectively.",2022-11-11 03:22:45+00:00,Getting the Most out of Simile Recognition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xiaoyue Wang'), arxiv.Result.Author('Linfeng Song'), arxiv.Result.Author('Xin Liu'), arxiv.Result.Author('Chulun Zhou'), arxiv.Result.Author('Jinsong Su')]","Simile recognition involves two subtasks: simile sentence classification that
discriminates whether a sentence contains simile, and simile component
extraction that locates the corresponding objects (i.e., tenors and vehicles).
Recent work ignores features other than surface strings. In this paper, we
explore expressive features for this task to achieve more effective data
utilization. Particularly, we study two types of features: 1) input-side
features that include POS tags, dependency trees and word definitions, and 2)
decoding features that capture the interdependence among various decoding
decisions. We further construct a model named HGSR, which merges the input-side
features as a heterogeneous graph and leverages decoding features via
distillation. Experiments show that HGSR significantly outperforms the current
state-of-the-art systems and carefully designed baselines, verifying the
effectiveness of introduced features. Our code is available at
https://github.com/DeepLearnXMU/HGSR."
13629,"Pseudo label F1 (%) Performance with GIRL based on three IE      We further study cases where pseudo labels are improved
sub-tasks.",7.,"with GIRL on three IE sub-tasks, and present them in Tables
                                                                         5 and 6.",2022-11-11 05:37:19+00:00,Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xuming Hu'), arxiv.Result.Author('Shiao Meng'), arxiv.Result.Author('Chenwei Zhang'), arxiv.Result.Author('Xiangli Yang'), arxiv.Result.Author('Lijie Wen'), arxiv.Result.Author('Irwin King'), arxiv.Result.Author('Philip S. Yu')]","Information Extraction (IE) aims to extract structured information from
heterogeneous sources. IE from natural language texts include sub-tasks such as
Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction
(EE). Most IE systems require comprehensive understandings of sentence
structure, implied semantics, and domain knowledge to perform well; thus, IE
tasks always need adequate external resources and annotations. However, it
takes time and effort to obtain more human annotations. Low-Resource
Information Extraction (LRIE) strives to use unsupervised data, reducing the
required resources and human annotation. In practice, existing systems either
utilize self-training schemes to generate pseudo labels that will cause the
gradual drift problem, or leverage consistency regularization methods which
inevitably possess confirmation bias. To alleviate confirmation bias due to the
lack of feedback loops in existing LRIE learning paradigms, we develop a
Gradient Imitation Reinforcement Learning (GIRL) method to encourage
pseudo-labeled data to imitate the gradient descent direction on labeled data,
which can force pseudo-labeled data to achieve better optimization capabilities
similar to labeled data. Based on how well the pseudo-labeled data imitates the
instructive gradient descent direction obtained from labeled data, we design a
reward to quantify the imitation process and bootstrap the optimization
capability of pseudo-labeled data through trial and error. In addition to
learning paradigms, GIRL is not limited to specific sub-tasks, and we leverage
GIRL to solve all IE sub-tasks (named entity recognition, relation extraction,
and event extraction) in low-resource settings (semi-supervised IE and few-shot
IE)."
13630,"Pseudo label F1 (%) Performance with GIRL based on three IE      We further study cases where pseudo labels are improved
sub-tasks.",7.,"with GIRL on three IE sub-tasks, and present them in Tables
                                                                         5 and 6.",2022-11-11 05:37:19+00:00,Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xuming Hu'), arxiv.Result.Author('Shiao Meng'), arxiv.Result.Author('Chenwei Zhang'), arxiv.Result.Author('Xiangli Yang'), arxiv.Result.Author('Lijie Wen'), arxiv.Result.Author('Irwin King'), arxiv.Result.Author('Philip S. Yu')]","Information Extraction (IE) aims to extract structured information from
heterogeneous sources. IE from natural language texts include sub-tasks such as
Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction
(EE). Most IE systems require comprehensive understandings of sentence
structure, implied semantics, and domain knowledge to perform well; thus, IE
tasks always need adequate external resources and annotations. However, it
takes time and effort to obtain more human annotations. Low-Resource
Information Extraction (LRIE) strives to use unsupervised data, reducing the
required resources and human annotation. In practice, existing systems either
utilize self-training schemes to generate pseudo labels that will cause the
gradual drift problem, or leverage consistency regularization methods which
inevitably possess confirmation bias. To alleviate confirmation bias due to the
lack of feedback loops in existing LRIE learning paradigms, we develop a
Gradient Imitation Reinforcement Learning (GIRL) method to encourage
pseudo-labeled data to imitate the gradient descent direction on labeled data,
which can force pseudo-labeled data to achieve better optimization capabilities
similar to labeled data. Based on how well the pseudo-labeled data imitates the
instructive gradient descent direction obtained from labeled data, we design a
reward to quantify the imitation process and bootstrap the optimization
capability of pseudo-labeled data through trial and error. In addition to
learning paradigms, GIRL is not limited to specific sub-tasks, and we leverage
GIRL to solve all IE sub-tasks (named entity recognition, relation extraction,
and event extraction) in low-resource settings (semi-supervised IE and few-shot
IE)."
13653,"However, further study revealed
     9There were just too many plots for the paper to address.","However, in the event of a bal-                  According to preliminary descriptive statistics and
      anced dataset with an equal number of en-                 frequency analyses, the word choices appeared to
                                                                be rather similar.","signiﬁcant discrepancies in word choices between
Hence , we excluded them.",2022-11-11 17:30:28+00:00,Analysis of Male and Female Speakers' Word Choices in Public Speeches,cs.CL,"['cs.CL', 'stat.AP']","[arxiv.Result.Author('Md Zobaer Hossain'), arxiv.Result.Author('Ahnaf Mozib Samin')]","The extent to which men and women use language differently has been
questioned previously. Finding clear and consistent gender differences in
language is not conclusive in general, and the research is heavily influenced
by the context and method employed to identify the difference. In addition, the
majority of the research was conducted in written form, and the sample was
collected in writing. Therefore, we compared the word choices of male and
female presenters in public addresses such as TED lectures. The frequency of
numerous types of words, such as parts of speech (POS), linguistic,
psychological, and cognitive terms were analyzed statistically to determine how
male and female speakers use words differently. Based on our data, we
determined that male speakers use specific types of linguistic, psychological,
cognitive, and social words in considerably greater frequency than female
speakers."
13682,"These         tional Linguistics: Human Language Technologies,
concerns warrant further research and considera-           pages 110–119, San Diego, California.","Considering the              Proceedings of the 2016 Conference of the North
high annotation cost, our character relation anno-         American Chapter of the Association for Computa-
tation works are restricted to Harry Potter.","Association
tion when utilizing this work to build intelligent         for Computational Linguistics.",2022-11-13 10:16:39+00:00,What would Harry say? Building Dialogue Agents for Characters in a Story,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Nuo Chen'), arxiv.Result.Author('Yan Wang'), arxiv.Result.Author('Haiyun Jiang'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Ziyang Chen'), arxiv.Result.Author('Longyue Wang'), arxiv.Result.Author('Jia Li')]","We have a Christmas gift for Harry Potter fans all over the world. In this
paper, we present Harry Potter Dialogue (HPD), a dataset that helps train Harry
Potter-like dialogue agents. Such a task is typically viewed as a variant of
personalized dialogue agents, but they differ significantly in three respects:
1) Harry lived in a virtual world of wizards, thus, real-world commonsense may
not apply to Harry's conversations; 2) Harry's behavior is strongly linked to
background information in conversations: the scene, its attributes and its
relationship to other speakers; and 3) Such backgrounds are dynamically altered
as the storyline goes on. The HPD dataset, as the first dataset to facilitate
the study of dialogue agent construction for characters within a story,
provides rich contextual information about each dialogue session such as
scenes, character attributes, and relations. More importantly, all the
background information will change over the course of the story. In addition,
HPD could support both dialogue generation and retrieval tasks. We evaluate
baselines such as Dialog-GPT and BOB to determine the extent to which they can
generate Harry Potter-like responses. The experimental results disappoint us in
that although the generated responses are fluent, they still seem out of
character for Harry. Besides, we validate the current most robust dialogue
agent, ChatGPT, which also can't generate plausible Harry-Potter-like responses
in some cases, either. Our results suggest that there is much scope for future
research."
13694,"We will release the resources and baselines to en-
Language models like mBERT have shown strong           able further research on the new task.","and how to learn
4.2 Language Transfer Capability of mBERT              language-agnostic embeddings in KGE models?","cross lingual performance due to shared multilin-
6 Limitations                                              Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,
                                                              and Sebastian Riedel.",2022-11-13 17:10:49+00:00,mOKB6: A Multilingual Open Knowledge Base Completion Benchmark,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shubham Mittal'), arxiv.Result.Author('Keshav Kolluru'), arxiv.Result.Author('Soumen Chakrabarti'), arxiv.Result.Author('Mausam')]","Automated completion of open knowledge bases (KBs), which are constructed
from triples of the form (subject phrase, relation phrase, object phrase)
obtained via open information extraction (IE) from text, is useful for
discovering novel facts that may not directly be present in the text. However,
research in open knowledge base completion (KBC) has so far been limited to
resource-rich languages like English. Using the latest advances in multilingual
open IE, we construct the first multilingual open KBC dataset, called mOKB6,
that contains facts from Wikipedia in six languages (including English).
Improving the previous open KB construction pipeline by doing multilingual
coreference resolution and keeping only entity-linked triples, we create a
dense open KB. We experiment with several baseline models that have been
proposed for both open and closed KBs and observe a consistent benefit of using
knowledge gained from other languages. The dataset and accompanying code will
be made publicly available."
13700,"Therefore, it will be interesting further research to ﬁgure out this cause.","Although schema graph
expansions provide some performance gains but it was not consistent performance gains for all structure
and size of the language model.","As a future study, we will also develop better schema graph expansion because there is still room for
performance improvement.",2022-11-14 01:39:26+00:00,ALBERT with Knowledge Graph Encoder Utilizing Semantic Similarity for Commonsense Question Answering,cs.CL,"['cs.CL', 'cs.AI', '68T07 (Secondary), 68T30, 68T50 (Primary)', 'I.2.2; I.2.7']","[arxiv.Result.Author('Byeongmin Choi'), arxiv.Result.Author('YongHyun Lee'), arxiv.Result.Author('Yeunwoong Kyung'), arxiv.Result.Author('Eunchan Kim')]","Recently, pre-trained language representation models such as bidirectional
encoder representations from transformers (BERT) have been performing well in
commonsense question answering (CSQA). However, there is a problem that the
models do not directly use explicit information of knowledge sources existing
outside. To augment this, additional methods such as knowledge-aware graph
network (KagNet) and multi-hop graph relation network (MHGRN) have been
proposed. In this study, we propose to use the latest pre-trained language
model a lite bidirectional encoder representations from transformers (ALBERT)
with knowledge graph information extraction technique. We also propose to
applying the novel method, schema graph expansion to recent language models.
Then, we analyze the effect of applying knowledge graph-based knowledge
extraction techniques to recent pre-trained language models and confirm that
schema graph expansion is effective in some extent. Furthermore, we show that
our proposed model can achieve better performance than existing KagNet and
MHGRN models in CommonsenseQA dataset."
13717,"Previous methods use average (Dai et al., 2021)         4.3 Skill Neurons are Task-speciﬁc
and maximum (Suau et al., 2020) activations on in-
put tokens instead of activations on prompts to ﬁnd     We further study whether skill neurons are task-
selective neurons, which are shown as the “Avg.”        speciﬁc, i.e., do skill neurons encode task-speciﬁc
and “Max.” results in Figure 3, respectively.",skill neurons with high predictivities via prompts.,"The       high-level skills like distinguishing sentiments for
experimental results indicate that previous methods     sentiment analysis, or do they just encode some
hardly ﬁnd highly-predictive neurons, which sug-        task-general low-level skills like recognizing parts
                                                        of speech, which are also helpful for handling tasks.",2022-11-14 13:43:46+00:00,Finding Skill Neurons in Pre-trained Transformer-based Language Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Kaiyue Wen'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Juanzi Li')]","Transformer-based pre-trained language models have demonstrated superior
performance on various natural language processing tasks. However, it remains
unclear how the skills required to handle these tasks distribute among model
parameters. In this paper, we find that after prompt tuning for specific tasks,
the activations of some neurons within pre-trained Transformers are highly
predictive of the task labels. We dub these neurons skill neurons and confirm
they encode task-specific skills by finding that: (1) Skill neurons are crucial
for handling tasks. Performances of pre-trained Transformers on a task
significantly drop when corresponding skill neurons are perturbed. (2) Skill
neurons are task-specific. Similar tasks tend to have similar distributions of
skill neurons. Furthermore, we demonstrate the skill neurons are most likely
generated in pre-training rather than fine-tuning by showing that the skill
neurons found with prompt tuning are also crucial for other fine-tuning methods
freezing neuron weights, such as the adapter-based tuning and BitFit. We also
explore the applications of skill neurons, including accelerating Transformers
with network pruning and building better transferability indicators. These
findings may promote further research on understanding Transformers. The source
code can be obtained from https://github.com/THU-KEG/Skill-Neuron."
13737,"To further research in this
16.","It is interesting to note that the reminder    the data requirements by 20x for the code switched
domain had the highest number of unique intents,       semantic parsing task.","Also, reminder domain had the highest number       area, we will release the dataset of over 10k manu-
of average intents per utterance of 2.66.",2022-11-14 16:45:30+00:00,CST5: Data Augmentation for Code-Switched Semantic Parsing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Anmol Agarwal'), arxiv.Result.Author('Jigar Gupta'), arxiv.Result.Author('Rahul Goel'), arxiv.Result.Author('Shyam Upadhyay'), arxiv.Result.Author('Pankaj Joshi'), arxiv.Result.Author('Rengarajan Aravamudhan')]","Extending semantic parsers to code-switched input has been a challenging
problem, primarily due to a lack of supervised training data. In this work, we
introduce CST5, a new data augmentation technique that finetunes a T5 model
using a small seed set ($\approx$100 utterances) to generate code-switched
utterances from English utterances. We show that CST5 generates high quality
code-switched data, both intrinsically (per human evaluation) and extrinsically
by comparing baseline models which are trained without data augmentation to
models which are trained with augmented data. Empirically we observe that using
CST5, one can achieve the same semantic parsing performance by using up to 20x
less labeled data. To aid further research in this area, we are also releasing
(a) Hinglish-TOP, the largest human annotated code-switched semantic parsing
dataset to date, containing 10k human annotated Hindi-English (Hinglish)
code-switched utterances, and (b) Over 170K CST5 generated code-switched
utterances from the TOPv2 dataset. Human evaluation shows that both the human
annotated data as well as the CST5 generated data is of good quality."
13751,"• give an outlook into further research and integration and
Further training of the model to better encode and match                application of our methods.","A language model pre-trained on general               gorithm is a capable recommendation system for both
language data will assign latent vectors with the latent space          known and new requirements, and
similarity property described above to any text and can be
used to rank text paragraphs for relevance out of the box.","domain-speciﬁc texts can be done both in a supervised and
unsupervised manner.",2022-10-28 11:52:16+00:00,Zero-Shot Text Matching for Automated Auditing using Sentence Transformers,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('David Biesner'), arxiv.Result.Author('Maren Pielka'), arxiv.Result.Author('Rajkumar Ramamurthy'), arxiv.Result.Author('Tim Dilmaghani'), arxiv.Result.Author('Bernd Kliem'), arxiv.Result.Author('Rüdiger Loitz'), arxiv.Result.Author('Rafet Sifa')]","Natural language processing methods have several applications in automated
auditing, including document or passage classification, information retrieval,
and question answering. However, training such models requires a large amount
of annotated data which is scarce in industrial settings. At the same time,
techniques like zero-shot and unsupervised learning allow for application of
models pre-trained using general domain data to unseen domains.
  In this work, we study the efficiency of unsupervised text matching using
Sentence-Bert, a transformer-based model, by applying it to the semantic
similarity of financial passages. Experimental results show that this model is
robust to documents from in- and out-of-domain data."
13753,"Therefore, we advocate      questions could lead to multiple types of issues:
for further research on teaching cultural character-     The models may reproduce and reify questionable
istics to LMs.","Besides undue anthropomorphis-
ions is a requirement for models to work faithfully      ing of language models, using them to score moral
in a cross-cultural context.",moral beliefs.,2022-11-14 20:08:54+00:00,Speaking Multiple Languages Affects the Moral Bias of Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Katharina Hämmerl'), arxiv.Result.Author('Björn Deiseroth'), arxiv.Result.Author('Patrick Schramowski'), arxiv.Result.Author('Jindřich Libovický'), arxiv.Result.Author('Constantin A. Rothkopf'), arxiv.Result.Author('Alexander Fraser'), arxiv.Result.Author('Kristian Kersting')]","Pre-trained multilingual language models (PMLMs) are commonly used when
dealing with data from multiple languages and cross-lingual transfer. However,
PMLMs are trained on varying amounts of data for each language. In practice
this means their performance is often much better on English than many other
languages. We explore to what extent this also applies to moral norms. Do the
models capture moral norms from English and impose them on other languages? Do
the models exhibit random and thus potentially harmful beliefs in certain
languages? Both these issues could negatively impact cross-lingual transfer and
potentially lead to harmful outcomes. In this paper, we (1) apply the
MoralDirection framework to multilingual models, comparing results in German,
Czech, Arabic, Mandarin Chinese, and English, (2) analyse model behaviour on
filtered parallel subtitles corpora, and (3) apply the models to a Moral
Foundations Questionnaire, comparing with human responses from different
countries. Our experiments demonstrate that, indeed, PMLMs encode differing
moral biases, but these do not necessarily correspond to cultural differences
or commonalities in human opinions."
13772,"3.4 A Generalized Universal Discriminator               Besides, we also evaluate zero-shot performance
                                                        on several BigBench (Srivastava et al., 2022) tasks,
To further study how the discriminative approaches      which are also adopted by T0 (Sanh et al., 2021).†
work in combination with generative tasks, we also
propose to experiment with a generalized version        Baselines We primarily compare our method
of UD (denoted as generalized UD).","prompt-based baselines, we report the average ac-
                                                        curacy over multiple prompts for each test task.","with T0 (Sanh et al., 2021), which is a generative
                                                        approach.",2022-11-15 12:33:31+00:00,A Universal Discriminator for Zero-Shot Generalization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Haike Xu'), arxiv.Result.Author('Zongyu Lin'), arxiv.Result.Author('Jing Zhou'), arxiv.Result.Author('Yanan Zheng'), arxiv.Result.Author('Zhilin Yang')]","Generative modeling has been the dominant approach for large-scale
pretraining and zero-shot generalization. In this work, we challenge this
convention by showing that discriminative approaches perform substantially
better than generative ones on a large number of NLP tasks. Technically, we
train a single discriminator to predict whether a text sample comes from the
true data distribution, similar to GANs. Since many NLP tasks can be formulated
as selecting from a few options, we use this discriminator to predict the
option with the highest probability. This simple formulation achieves
state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by
16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning
setting, our approach also achieves new state-of-the-art results on a wide
range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,
our approach requires minimal prompting efforts, which largely improves
robustness and is essential for real-world applications. Furthermore, we also
jointly train a generalized UD in combination with generative tasks, which
maintains its advantage on discriminative tasks and simultaneously works on
generative tasks."
13794,"Code and data release
rization of radiology ﬁnding into natural
language impression statements is proposed      To help with further research, we also make
by Zhang et al.",The ﬁrst attempt at automatic summa-            Appendix B.,(2018).,2022-11-15 23:57:34+00:00,Toward expanding the scope of radiology report summarization to multiple anatomies and modalities,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jean-Benoit Delbrouck'), arxiv.Result.Author('Maya Varma'), arxiv.Result.Author('Curtis P. Langlotz')]","Radiology report summarization is a growing area of research. Given the
Findings and/or Background sections of a radiology report, the goal is to
generate a summary (called an Impression section) that highlights the key
observations and conclusions of the radiology study. Recent efforts have
released systems that achieve promising performance as measured by widely used
summarization metrics such as BLEU and ROUGE. However, the research area of
radiology report summarization currently faces important limitations. First,
most of the results are reported on private datasets. This limitation prevents
the ability to reproduce results and fairly compare different systems and
solutions. Secondly, to the best of our knowledge, most research is carried out
on chest X-rays. Sometimes, studies even omit to mention the concerned modality
and anatomy in the radiology reports used for their experiments. To palliate
these limitations, we propose a new dataset of six different modalities and
anatomies based on the MIMIC-III database. We further release our results and
the data splits used to carry out our experiments. Finally, we propose a simple
report summarization system that outperforms the previous replicable research
on the existing dataset."
13795,"Code and data release
rization of radiology ﬁnding into natural
language impression statements is proposed      To help with further research, we also make
by Zhang et al.",The ﬁrst attempt at automatic summa-            Appendix B.,(2018).,2022-11-15 23:57:34+00:00,Toward expanding the scope of radiology report summarization to multiple anatomies and modalities,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Jean-Benoit Delbrouck'), arxiv.Result.Author('Maya Varma'), arxiv.Result.Author('Curtis P. Langlotz')]","Radiology report summarization is a growing area of research. Given the
Findings and/or Background sections of a radiology report, the goal is to
generate a summary (called an Impression section) that highlights the key
observations and conclusions of the radiology study. Recent efforts have
released systems that achieve promising performance as measured by widely used
summarization metrics such as BLEU and ROUGE. However, the research area of
radiology report summarization currently faces important limitations. First,
most of the results are reported on private datasets. This limitation prevents
the ability to reproduce results and fairly compare different systems and
solutions. Secondly, to the best of our knowledge, most research is carried out
on chest X-rays. Sometimes, studies even omit to mention the concerned modality
and anatomy in the radiology reports used for their experiments. To palliate
these limitations, we propose a new dataset of six different modalities and
anatomies based on the MIMIC-III database. We further release our results and
the data splits used to carry out our experiments. Finally, we propose a simple
report summarization system that outperforms the previous replicable research
on the existing dataset."
13796,"By checking the classes
4.5 The Effect of Tree Structure                                                  sets of both datasets, we have found that sev-
                                                                                  eral classes in the two datasets are highly-related
We further study the effect of tree structure to the                              (e.g., Huffpost: “taste” and “word news”, Reuters:
performance.","indicates that the Huffpost and Reuters datasets
                                                                                  may have a large overlap.",We vary the tree structure and record                                “sugar” and “wholesale price Iindex”).,2022-11-16 00:19:53+00:00,Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Juan Zha'), arxiv.Result.Author('Zheng Li'), arxiv.Result.Author('Ying Wei'), arxiv.Result.Author('Yu Zhang')]","Few-Shot Text Classification (FSTC) imitates humans to learn a new text
classifier efficiently with only few examples, by leveraging prior knowledge
from historical tasks. However, most prior works assume that all the tasks are
sampled from a single data source, which cannot adapt to real-world scenarios
where tasks are heterogeneous and lie in different distributions. As such,
existing methods may suffer from their globally knowledge-shared mechanisms to
handle the task heterogeneity. On the other hand, inherent task relation are
not explicitly captured, making task knowledge unorganized and hard to transfer
to new tasks. Thus, we explore a new FSTC setting where tasks can come from a
diverse range of data sources. To address the task heterogeneity, we propose a
self-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only
customizes cluster-specific knowledge by dynamically organizing heterogeneous
tasks into different clusters in hierarchical levels but also disentangles
underlying relations between tasks to improve the interpretability. Extensive
experiments on five public FSTC benchmark datasets demonstrate the
effectiveness of SS-HTC."
13804,"Additionally, further research on anti-     2022.",ing behavior.,"Training a helpful and harmless assistant with
gaming approaches is needed to fulﬁll the potential       reinforcement learning from human feedback.",2022-11-16 07:10:02+00:00,Reward Gaming in Conditional Text Generation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Richard Yuanzhe Pang'), arxiv.Result.Author('Vishakh Padmakumar'), arxiv.Result.Author('Thibault Sellam'), arxiv.Result.Author('Ankur P. Parikh'), arxiv.Result.Author('He He')]","To align conditional text generation model outputs with desired behaviors,
there has been an increasing focus on training the model using reinforcement
learning (RL) with reward functions learned from human annotations. Under this
framework, we identify three common cases where high rewards are incorrectly
assigned to undesirable patterns: noise-induced spurious correlation, naturally
occurring spurious correlation, and covariate shift. We show that even though
learned metrics achieve high performance on the distribution of the data used
to train the reward function, the undesirable patterns may be amplified during
RL training of the text generation model. While there has been discussion about
reward gaming in the RL or safety community, in this short discussion piece, we
would like to highlight reward gaming in the NLG community using concrete
conditional text generation examples and discuss potential fixes and areas for
future work."
13808,"On the contrary, further research into CS      ology proposed in the paper to add simpliﬁcation
from an NLP context can only provide beneﬁts to         operations to SI uses simplistic rules to do so.","The method-
lead to.",people with cognitive disabilities.,2022-11-16 10:51:03+00:00,Cognitive Simplification Operations Improve Text Simplification,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Eytan Chamovitz'), arxiv.Result.Author('Omri Abend')]","Text Simplification (TS) is the task of converting a text into a form that is
easier to read while maintaining the meaning of the original text. A sub-task
of TS is Cognitive Simplification (CS), converting text to a form that is
readily understood by people with cognitive disabilities without rendering it
childish or simplistic. This sub-task has yet to be explored with neural
methods in NLP, and resources for it are scarcely available. In this paper, we
present a method for incorporating knowledge from the cognitive accessibility
domain into a TS model, by introducing an inductive bias regarding what
simplification operations to use. We show that by adding this inductive bias to
a TS-trained model, it is able to adapt better to CS without ever seeing CS
data, and outperform a baseline model on a traditional TS benchmark. In
addition, we provide a novel test dataset for CS, and analyze the differences
between CS corpora and existing TS corpora, in terms of how simplification
operations are applied."
13809,"The goal of
this paper however is to highlight the need and         Fernando Alva-Manchego, Louis Martin, Carolina
possibilities of further research into CS, and pro-        Scarton, and Lucia Specia.",and such a comparison is warranted.,2019.,2022-11-16 10:51:03+00:00,Cognitive Simplification Operations Improve Text Simplification,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Eytan Chamovitz'), arxiv.Result.Author('Omri Abend')]","Text Simplification (TS) is the task of converting a text into a form that is
easier to read while maintaining the meaning of the original text. A sub-task
of TS is Cognitive Simplification (CS), converting text to a form that is
readily understood by people with cognitive disabilities without rendering it
childish or simplistic. This sub-task has yet to be explored with neural
methods in NLP, and resources for it are scarcely available. In this paper, we
present a method for incorporating knowledge from the cognitive accessibility
domain into a TS model, by introducing an inductive bias regarding what
simplification operations to use. We show that by adding this inductive bias to
a TS-trained model, it is able to adapt better to CS without ever seeing CS
data, and outperform a baseline model on a traditional TS benchmark. In
addition, we provide a novel test dataset for CS, and analyze the differences
between CS corpora and existing TS corpora, in terms of how simplification
operations are applied."
13815,"We then motivate further research in EE by         nomenon for in-domain and OOD scenario;
                                       computing an optimal bound for performance versus speed trade-offs.","This paper ﬁrst shows that SSL models        • We show that ASR models do overthinking and analyze such phe-
                                       do overthinking in ASR.","To approach this bound we propose two new strategies for ASR: (1)        • We compute theoretical lower bounds of speed/quality trade-offs
                                       we adapt the recently proposed patience strategy to ASR; and (2) we        for EE strategies through dynamic programming;
                                       design a new EE strategy speciﬁc to ASR that performs better than
                                       all strategies previously introduced.",2022-11-01 15:26:46+00:00,Avoid Overthinking in Self-Supervised Models for Speech Recognition,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Dan Berrebbi'), arxiv.Result.Author('Brian Yan'), arxiv.Result.Author('Shinji Watanabe')]","Self-supervised learning (SSL) models reshaped our approach to speech,
language and vision. However their huge size and the opaque relations between
their layers and tasks result in slow inference and network overthinking, where
predictions made from the last layer of large models is worse than those made
from intermediate layers. Early exit (EE) strategies can solve both issues by
dynamically reducing computations at inference time for certain samples.
Although popular for classification tasks in vision and language, EE has seen
less use for sequence-to-sequence speech recognition (ASR) tasks where outputs
from early layers are often degenerate. This challenge is further compounded
when speech SSL models are applied on out-of-distribution (OOD) data. This
paper first shows that SSL models do overthinking in ASR. We then motivate
further research in EE by computing an optimal bound for performance versus
speed trade-offs. To approach this bound we propose two new strategies for ASR:
(1) we adapt the recently proposed patience strategy to ASR; and (2) we design
a new EE strategy specific to ASR that performs better than all strategies
previously introduced."
13818,"1https://huggingface.co/bert-base-cased
                                                             To further study the ability to handle the over-
                                                          lapping problem and extracting multiple triples,
Model Normal SEO EPO SOO L= 1 L= 2 L= 3 L= 4 L≥ 5

CasRel  87.3 91.4 92.0 77.0 88.2 90.3 91.9 94.2 83.7

TPlinker 90.1 93.4 94.0 90.1 90.0 92.8 93.1 96.1 90.0

PRGC    91.0 94.0 94.5 81.8 91.1 93.0 93.5 95.5 93.0

R-BPtrNet 90.4 94.4 95.2  -                              89.5 93.1 93.5 96.7 91.3

PFN     90.2 95.3 94.1    -                              90.5 92.9 93.7 96.3 92.6

TDEER 90.8 94.1 94.5      -                              90.8 92.8 94.1 95.9 92.8

GRTE    91.1 94.4 95.0    -                              90.8 93.7 94.4 96.2 93.4

OneRel 90.6 95.1 94.8 90.8 90.5 93.4 93.9 96.5 94.2

UniRel 91.6±0.3 95.3±0.2 95.2±0.1 89.8±3.6 91.5±0.3 94.3±0.2 94.5±0.3 96.6±0.2 94.2±0.8

Table 3: F1-score on sentences with different overlapping patterns and different triple numbers.","the baselines in terms of all the evaluation metrics,
                                                          which shows the superiority of our model.","L is the number
of triples in one sentence.",2022-11-16 16:53:13+00:00,UniRel: Unified Representation and Interaction for Joint Relational Triple Extraction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wei Tang'), arxiv.Result.Author('Benfeng Xu'), arxiv.Result.Author('Yuyue Zhao'), arxiv.Result.Author('Zhendong Mao'), arxiv.Result.Author('Yifeng Liu'), arxiv.Result.Author('Yong Liao'), arxiv.Result.Author('Haiyong Xie')]","Relational triple extraction is challenging for its difficulty in capturing
rich correlations between entities and relations. Existing works suffer from 1)
heterogeneous representations of entities and relations, and 2) heterogeneous
modeling of entity-entity interactions and entity-relation interactions.
Therefore, the rich correlations are not fully exploited by existing works. In
this paper, we propose UniRel to address these challenges. Specifically, we
unify the representations of entities and relations by jointly encoding them
within a concatenated natural language sequence, and unify the modeling of
interactions with a proposed Interaction Map, which is built upon the
off-the-shelf self-attention mechanism within any Transformer block. With
comprehensive experiments on two popular relational triple extraction datasets,
we demonstrate that UniRel is more effective and computationally efficient. The
source code is available at https://github.com/wtangdev/UniRel."
13821,"This is an open question, and further research is needed to ﬁnd the best procedure for
incorporating new knowledge into the model.","Continual Learning Should we re-train from scratch to incorporate new scientiﬁc knowledge or train from
older checkpoints?","Retrieval Augmentation While we have shown how large language models can absorb large bodies of
scientiﬁc knowledge, retrieval has a place for ﬁne-grained types of knowledge, and we believe this is a strong
direction to pursue to complement the ﬂexible weight memory of the Transformer.",2022-11-16 18:06:33+00:00,Galactica: A Large Language Model for Science,cs.CL,"['cs.CL', 'stat.ML']","[arxiv.Result.Author('Ross Taylor'), arxiv.Result.Author('Marcin Kardas'), arxiv.Result.Author('Guillem Cucurull'), arxiv.Result.Author('Thomas Scialom'), arxiv.Result.Author('Anthony Hartshorn'), arxiv.Result.Author('Elvis Saravia'), arxiv.Result.Author('Andrew Poulton'), arxiv.Result.Author('Viktor Kerkez'), arxiv.Result.Author('Robert Stojnic')]","Information overload is a major obstacle to scientific progress. The
explosive growth in scientific literature and data has made it ever harder to
discover useful insights in a large mass of information. Today scientific
knowledge is accessed through search engines, but they are unable to organize
scientific knowledge alone. In this paper we introduce Galactica: a large
language model that can store, combine and reason about scientific knowledge.
We train on a large scientific corpus of papers, reference material, knowledge
bases and many other sources. We outperform existing models on a range of
scientific tasks. On technical knowledge probes such as LaTeX equations,
Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also
performs well on reasoning, outperforming Chinchilla on mathematical MMLU by
41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It
also sets a new state-of-the-art on downstream tasks such as PubMedQA and
MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general
corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these
results demonstrate the potential for language models as a new interface for
science. We open source the model for the benefit of the scientific community."
13835,"tion answering (TriviaQA (Joshi et al., 2017, TQA),
HotpotQA (Yang et al., 2018, HPQA) and Natu-            Nicola De Cao, Ledell Wu, Kashyap Popat, Mikel
ralQuestions (Kwiatkowski et al., 2019, NQ)) re-           Artetxe, Naman Goyal, Mikhail Plekhanov, Luke
quires further study.","However, application to ques-       tional Linguistics.","Table 4 shows pre-training           Zettlemoyer, Nicola Cancedda, Sebastian Riedel,
does offer a beneﬁt, but with fewer data for ﬁne-          and Fabio Petroni.",2022-11-17 07:27:50+00:00,Data-Efficient Autoregressive Document Retrieval for Fact Verification,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']",[arxiv.Result.Author('James Thorne')],"Document retrieval is a core component of many knowledge-intensive natural
language processing task formulations such as fact verification and question
answering. Sources of textual knowledge, such as Wikipedia articles, condition
the generation of answers from the models. Recent advances in retrieval use
sequence-to-sequence models to incrementally predict the title of the
appropriate Wikipedia page given a query. However, this method requires
supervision in the form of human annotation to label which Wikipedia pages
contain appropriate context. This paper introduces a distant-supervision method
that does not require any annotation to train autoregressive retrievers that
attain competitive R-Precision and Recall in a zero-shot setting. Furthermore
we show that with task-specific supervised fine-tuning, autoregressive
retrieval performance for two Wikipedia-based fact verification tasks can
approach or even exceed full supervision using less than $1/4$ of the annotated
data indicating possible directions for data-efficient autoregressive
retrieval."
13853,"While a solution to these attacks remains open, our ﬁndings demonstrate the difﬁculty of defending
against them and highlight the need for further research and discussion on the subject.","Another possible approach could be to modify
LLMs to accept two parameters – instruction (safe) and data (unsafe) – and avoid following any
instructions from the unsafe data parameters [27].","We hope that
our framework support researchers answer these questions, and ultimately reduce AI risks as we
discuss in Appendix A.",2022-11-17 13:43:20+00:00,Ignore Previous Prompt: Attack Techniques For Language Models,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Fábio Perez'), arxiv.Result.Author('Ian Ribeiro')]","Transformer-based large language models (LLMs) provide a powerful foundation
for natural language tasks in large-scale customer-facing applications.
However, studies that explore their vulnerabilities emerging from malicious
user interaction are scarce. By proposing PromptInject, a prosaic alignment
framework for mask-based iterative adversarial prompt composition, we examine
how GPT-3, the most widely deployed language model in production, can be easily
misaligned by simple handcrafted inputs. In particular, we investigate two
types of attacks -- goal hijacking and prompt leaking -- and demonstrate that
even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit
GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject
is available at https://github.com/agencyenterprise/PromptInject."
13870,"Therefore, it is necessary to conduct further research to generate and share a
complete dataset for research purposes.","With pandemic diseases like COVID-19, which require many resources and
equipment types, deep learning models perform better with a more extensive and comprehensive
dataset [71, 72].",This paper determined ways to improve future research.,2022-11-04 14:35:56+00:00,BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets,cs.CL,"['cs.CL', 'cs.SI']","[arxiv.Result.Author('Javad Hassannataj Joloudari'), arxiv.Result.Author('Sadiq Hussain'), arxiv.Result.Author('Mohammad Ali Nematollahi'), arxiv.Result.Author('Rouhollah Bagheri'), arxiv.Result.Author('Fatemeh Fazl'), arxiv.Result.Author('Roohallah Alizadehsani'), arxiv.Result.Author('Reza Lashgari')]","The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article."
13872,"sitive to verb transitivity and use it to guide next-
word predictions, resulting in incremental parser-                  In a further study, we present each of the probes
like behavior.",(2019)’s result showing that ALMs are sen-                the part of the model.,"Speciﬁcally, we show that changing                with the two NP/Z parses in two conditions: 1)
the verb in NP/Z-ambiguous preﬁxes from intran-                  where the sentence sufﬁx was consistent with the
sitive (unambiguously Z-complement favoring) to                  parse being probed, and 2) where the sentence suf-
transitive (ambiguous), e.g.,:                                   ﬁx was consistent with the other parse.",2022-11-17 18:15:31+00:00,Probing for Incremental Parse States in Autoregressive Language Models,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Tiwalayo Eisape'), arxiv.Result.Author('Vineet Gangireddy'), arxiv.Result.Author('Roger P. Levy'), arxiv.Result.Author('Yoon Kim')]","Next-word predictions from autoregressive neural language models show
remarkable sensitivity to syntax. This work evaluates the extent to which this
behavior arises as a result of a learned ability to maintain implicit
representations of incremental syntactic structures. We extend work in
syntactic probing to the incremental setting and present several probes for
extracting incomplete syntactic structure (operationalized through parse states
from a stack-based parser) from autoregressive language models. We find that
our probes can be used to predict model preferences on ambiguous sentence
prefixes and causally intervene on model representations and steer model
behavior. This suggests implicit incremental syntactic inferences underlie
next-word predictions in autoregressive neural language models."
13879,"To enable further study of the CQA summariza-             • We conduct comprehensive experiments on
tion task, we create a corpus COQASUM by col-                  a collection of extractative and abstractive
lecting reference summaries on QA pairs from the               summarization methods and develop a strong
Amazon QA dataset (Wan and McAuley, 2016;                      baseline DedupLED, which implements key
McAuley and Yang, 2016).",(Section 3).,"Reference summary                     characteristics of sentence-type transfer and
annotation is challenging for CQA summarization,               duplication removal functions.",2022-11-17 21:09:41+00:00,Summarizing Community-based Question-Answer Pairs,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ting-Yao Hsu'), arxiv.Result.Author('Yoshi Suhara'), arxiv.Result.Author('Xiaolan Wang')]","Community-based Question Answering (CQA), which allows users to acquire their
desired information, has increasingly become an essential component of online
services in various domains such as E-commerce, travel, and dining. However, an
overwhelming number of CQA pairs makes it difficult for users without
particular intent to find useful information spread over CQA pairs. To help
users quickly digest the key information, we propose the novel CQA
summarization task that aims to create a concise summary from CQA pairs. To
this end, we first design a multi-stage data annotation process and create a
benchmark dataset, CoQASUM, based on the Amazon QA corpus. We then compare a
collection of extractive and abstractive summarization methods and establish a
strong baseline approach DedupLED for the CQA summarization task. Our
experiment further confirms two key challenges, sentence-type transfer and
deduplication removal, towards the CQA summarization task. Our data and code
are publicly available."
13901,"18Unfortunately, we did not observe similar consistent eﬀects for the decade variable, this
will require further research.","We cal-
culated the mean absolute error between the predicted and true year of pub-

  17We also ran a regression model on the diﬀerences between HMDistilBERT and each
ERWT model, which suggested that these changes in the parameter values are statistically
signiﬁcant.","11
model           random    no year           bad ocr
random           19.458            6.678*/32.343
majority         13.929     8.625  4.488*/33.452
ERWT               8.235    9.744
ERWT st            9.420    7.756              7.487
ERWT masked 25     7.353   6.317               7.523
ERWT masked 75    6.066                        7.311
                                              7.182

Table 4: Mean absolute error between predicted and actual year using the most
probable ﬁller for the masked year token as the predicted year.",2022-11-18 08:29:00+00:00,Metadata Might Make Language Models Better,cs.CL,"['cs.CL', 'cs.DL']","[arxiv.Result.Author('Kaspar Beelen'), arxiv.Result.Author('Daniel van Strien')]","This paper discusses the benefits of including metadata when training
language models on historical collections. Using 19th-century newspapers as a
case study, we extend the time-masking approach proposed by Rosin et al., 2022
and compare different strategies for inserting temporal, political and
geographical information into a Masked Language Model. After fine-tuning
several DistilBERT on enhanced input data, we provide a systematic evaluation
of these models on a set of evaluation tasks: pseudo-perplexity, metadata
mask-filling and supervised classification. We find that showing relevant
metadata to a language model has a beneficial impact and may even produce more
robust and fairer models."
13902,"The
supervised models are moreover very sensitive to hyperparameters and require further study
to become more robust.","24As a disclaimer, we need to add that in contrast to the previous evaluation tasks, super-
vised results are harder to reproduce and can vary just by changing the random seed.","16
8 Appendix

References

Alessio Palmero Aprosio, Stefano Menini, and Sara Tonelli.",2022-11-18 08:29:00+00:00,Metadata Might Make Language Models Better,cs.CL,"['cs.CL', 'cs.DL']","[arxiv.Result.Author('Kaspar Beelen'), arxiv.Result.Author('Daniel van Strien')]","This paper discusses the benefits of including metadata when training
language models on historical collections. Using 19th-century newspapers as a
case study, we extend the time-masking approach proposed by Rosin et al., 2022
and compare different strategies for inserting temporal, political and
geographical information into a Masked Language Model. After fine-tuning
several DistilBERT on enhanced input data, we provide a systematic evaluation
of these models on a set of evaluation tasks: pseudo-perplexity, metadata
mask-filling and supervised classification. We find that showing relevant
metadata to a language model has a beneficial impact and may even produce more
robust and fairer models."
13984,"To further study the feasibility and robustness of
CI-BERT continued training concerning the model
property.","This is non-trivial since these two biases are
in pair of trade-off and hard to be decreased at the
same time (de Vassimon Manela et al., 2021).","We experiment on both “bert-tiny” and
“bert-base-uncased” models and plot the average
SEAT curve along with training steps (Figure 4).",2022-11-20 21:24:48+00:00,Conceptor-Aided Debiasing of Contextualized Embeddings,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yifei Li'), arxiv.Result.Author('Lyle Ungar'), arxiv.Result.Author('João Sedoc')]","Pre-trained language models reflect the inherent social biases of their
training corpus. Many methods have been proposed to mitigate this issue, but
they often fail to debias or they sacrifice model accuracy. We use
conceptors--a soft projection method--to identify and remove the bias subspace
in contextual embeddings in BERT and GPT. We propose two methods of applying
conceptors (1) bias subspace projection by post-processing; and (2) a new
architecture, conceptor-intervened BERT (CI-BERT), which explicitly
incorporates the conceptor projection into all layers during training. We find
that conceptor post-processing achieves state-of-the-art debiasing results
while maintaining or improving BERT's performance on the GLUE benchmark.
Although CI-BERT's training takes all layers' bias into account and can
outperform its post-processing counterpart in bias mitigation, CI-BERT reduces
the language model accuracy. We also show the importance of carefully
constructing the bias subspace. The best results are obtained by removing
outliers from the list of biased words, intersecting them (using the conceptor
AND operation), and computing their embeddings using the sentences from a
cleaner corpus."
13993,"Finally, we hope that our work facilitates further study and trials in the Hindi
and Marathi NLP domains.","We conclude that the method of two-step training
proves to be eﬃcient for developing MahaSBERT-STS and HindSBERT-STS.","Acknowledgements This work was done under the L3Cube Pune mentorship
program.",2022-11-21 05:15:48+00:00,L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Ananya Joshi'), arxiv.Result.Author('Aditi Kajale'), arxiv.Result.Author('Janhavi Gadre'), arxiv.Result.Author('Samruddhi Deode'), arxiv.Result.Author('Raviraj Joshi')]","Sentence representation from vanilla BERT models does not work well on
sentence similarity tasks. Sentence-BERT models specifically trained on STS or
NLI datasets are shown to provide state-of-the-art performance. However,
building these models for low-resource languages is not straightforward due to
the lack of these specialized datasets. This work focuses on two low-resource
Indian languages, Hindi and Marathi. We train sentence-BERT models for these
languages using synthetic NLI and STS datasets prepared using machine
translation. We show that the strategy of NLI pre-training followed by STSb
fine-tuning is effective in generating high-performance sentence-similarity
models for Hindi and Marathi. The vanilla BERT models trained using this simple
strategy outperform the multilingual LaBSE trained using a complex training
strategy. These models are evaluated on downstream text classification and
similarity tasks. We evaluate these models on real text classification datasets
to show embeddings obtained from synthetic data training are generalizable to
real datasets as well and thus represent an effective training strategy for
low-resource languages. We also provide a comparative analysis of sentence
embeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,
xlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and
monolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release
L3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for
Marathi and Hindi respectively. Our work also serves as a guide to building
low-resource sentence embedding models."
13994,"Finally, we hope that our work facilitates further study and trials in the Hindi
and Marathi NLP domains.","We conclude that the method of two-step training
proves to be eﬃcient for developing MahaSBERT-STS and HindSBERT-STS.","Acknowledgements This work was done under the L3Cube Pune mentorship
program.",2022-11-21 05:15:48+00:00,L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Ananya Joshi'), arxiv.Result.Author('Aditi Kajale'), arxiv.Result.Author('Janhavi Gadre'), arxiv.Result.Author('Samruddhi Deode'), arxiv.Result.Author('Raviraj Joshi')]","Sentence representation from vanilla BERT models does not work well on
sentence similarity tasks. Sentence-BERT models specifically trained on STS or
NLI datasets are shown to provide state-of-the-art performance. However,
building these models for low-resource languages is not straightforward due to
the lack of these specialized datasets. This work focuses on two low-resource
Indian languages, Hindi and Marathi. We train sentence-BERT models for these
languages using synthetic NLI and STS datasets prepared using machine
translation. We show that the strategy of NLI pre-training followed by STSb
fine-tuning is effective in generating high-performance sentence-similarity
models for Hindi and Marathi. The vanilla BERT models trained using this simple
strategy outperform the multilingual LaBSE trained using a complex training
strategy. These models are evaluated on downstream text classification and
similarity tasks. We evaluate these models on real text classification datasets
to show embeddings obtained from synthetic data training are generalizable to
real datasets as well and thus represent an effective training strategy for
low-resource languages. We also provide a comparative analysis of sentence
embeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,
xlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and
monolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release
L3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for
Marathi and Hindi respectively. Our work also serves as a guide to building
low-resource sentence embedding models."
14018,"Let us further study the potential regularization eﬀect of random-LTD on
GPT-2350M ﬁnetuning tasks.",GPT-2350M Finetuning.,"We train two sets of models, one without dropout and one with dropout (the
default rate is 0.1).",2022-11-17 23:14:58+00:00,Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Zhewei Yao'), arxiv.Result.Author('Xiaoxia Wu'), arxiv.Result.Author('Conglong Li'), arxiv.Result.Author('Connor Holmes'), arxiv.Result.Author('Minjia Zhang'), arxiv.Result.Author('Cheng Li'), arxiv.Result.Author('Yuxiong He')]","Large-scale transformer models have become the de-facto architectures for
various machine learning applications, e.g., CV and NLP. However, those large
models also introduce prohibitive training costs. To mitigate this issue, we
propose a novel random and layerwise token dropping method (random-LTD), which
skips the computation of a subset of the input tokens at all middle layers.
Particularly, random-LTD achieves considerable speedups and comparable accuracy
as the standard training baseline. Compared to other token dropping methods,
random-LTD does not require (1) any importance score-based metrics, (2) any
special token treatment (e.g., [CLS]), and (3) many layers in full sequence
length training except the first and the last layers. Besides, a new LayerToken
learning rate schedule is proposed for pretraining problems that resolve the
heavy tuning requirement for our proposed training mechanism. Finally, we
demonstrate that random-LTD can be applied to broader applications, including
GPT and BERT pretraining as well as ViT and GPT finetuning tasks. Our results
show that random-LTD can save about 33.3% theoretical compute cost and 25.6%
wall-clock training time while achieving similar zero-shot evaluations on
GPT-31.3B as compared to baseline."
14048,"We provide
                                             lieve this is a valuable resource that can moti-   benchmark baseline results for the tasks of auto-
                                             vate and facilitate further research studying the  matic speech recognition (ASR), machine transla-
                                             code-switching phenomenon from a linguistic        tion (MT), and ST. We make the translation guide-
                                             perspective and can be used to train and evalu-    lines and full corpus available, as well as the exper-
                                             ate NLP systems.",We be-        building and evaluating NLP systems.,"iments’ scripts and data splits.2

                                        1 Introduction                                             The paper is organized as follows.",2022-11-22 04:37:14+00:00,ArzEn-ST: A Three-way Speech Translation Corpus for Code-Switched Egyptian Arabic - English,cs.CL,['cs.CL'],"[arxiv.Result.Author('Injy Hamed'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Slim Abdennadher'), arxiv.Result.Author('Ngoc Thang Vu')]","We present our work on collecting ArzEn-ST, a code-switched Egyptian Arabic -
English Speech Translation Corpus. This corpus is an extension of the ArzEn
speech corpus, which was collected through informal interviews with bilingual
speakers. In this work, we collect translations in both directions, monolingual
Egyptian Arabic and monolingual English, forming a three-way speech translation
corpus. We make the translation guidelines and corpus publicly available. We
also report results for baseline systems for machine translation and speech
translation tasks. We believe this is a valuable resource that can motivate and
facilitate further research studying the code-switching phenomenon from a
linguistic perspective and can be used to train and evaluate NLP systems."
14049,"We make this corpus available to
Table 4 presents the results for the MT and ST base-                motivate and facilitate further research in this area.","We
5.2 Results                                                         reported benchmark results for baseline ASR, MT,
                                                                    and ST systems.",line systems.,2022-11-22 04:37:14+00:00,ArzEn-ST: A Three-way Speech Translation Corpus for Code-Switched Egyptian Arabic - English,cs.CL,['cs.CL'],"[arxiv.Result.Author('Injy Hamed'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Slim Abdennadher'), arxiv.Result.Author('Ngoc Thang Vu')]","We present our work on collecting ArzEn-ST, a code-switched Egyptian Arabic -
English Speech Translation Corpus. This corpus is an extension of the ArzEn
speech corpus, which was collected through informal interviews with bilingual
speakers. In this work, we collect translations in both directions, monolingual
Egyptian Arabic and monolingual English, forming a three-way speech translation
corpus. We make the translation guidelines and corpus publicly available. We
also report results for baseline systems for machine translation and speech
translation tasks. We believe this is a valuable resource that can motivate and
facilitate further research studying the code-switching phenomenon from a
linguistic perspective and can be used to train and evaluate NLP systems."
14050,"Given that                    Acknowledgements
CSW ST has only been slightly tackled by other
researchers, we hope that this corpus will motivate                 This project has beneﬁted from ﬁnancial support
further research on this task.","This
highlights the difﬁculty of the task.",by DAAD (German Academic Exchange Service).,2022-11-22 04:37:14+00:00,ArzEn-ST: A Three-way Speech Translation Corpus for Code-Switched Egyptian Arabic - English,cs.CL,['cs.CL'],"[arxiv.Result.Author('Injy Hamed'), arxiv.Result.Author('Nizar Habash'), arxiv.Result.Author('Slim Abdennadher'), arxiv.Result.Author('Ngoc Thang Vu')]","We present our work on collecting ArzEn-ST, a code-switched Egyptian Arabic -
English Speech Translation Corpus. This corpus is an extension of the ArzEn
speech corpus, which was collected through informal interviews with bilingual
speakers. In this work, we collect translations in both directions, monolingual
Egyptian Arabic and monolingual English, forming a three-way speech translation
corpus. We make the translation guidelines and corpus publicly available. We
also report results for baseline systems for machine translation and speech
translation tasks. We believe this is a valuable resource that can motivate and
facilitate further research studying the code-switching phenomenon from a
linguistic perspective and can be used to train and evaluate NLP systems."
14074,"We give an overview of each step in our procedure and point out the
                                                   potential pitfalls to motivate further research in this ﬁeld.","In this paper, we elaborate
                                                   on the challenges we faced in starting our collection of therapist-patient dialogues
                                                   in a psychiatry clinic under the General Data Privacy Regulation of the European
                                                   Union with the goal to use the data for Natural Language Processing (NLP) re-
                                                   search.","1 Introduction

                                        Mental health problems are highly prevalent conditions that are rising constantly worldwide
                                        WHO et al.",2022-11-22 15:51:10+00:00,GDPR Compliant Collection of Therapist-Patient-Dialogues,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tobias Mayer'), arxiv.Result.Author('Neha Warikoo'), arxiv.Result.Author('Oliver Grimm'), arxiv.Result.Author('Andreas Reif'), arxiv.Result.Author('Iryna Gurevych')]","According to the Global Burden of Disease list provided by the World Health
Organization (WHO), mental disorders are among the most debilitating
disorders.To improve the diagnosis and the therapy effectiveness in recent
years, researchers have tried to identify individual biomarkers. Gathering
neurobiological data however, is costly and time-consuming. Another potential
source of information, which is already part of the clinical routine, are
therapist-patient dialogues. While there are some pioneering works
investigating the role of language as predictors for various therapeutic
parameters, for example patient-therapist alliance, there are no large-scale
studies. A major obstacle to conduct these studies is the availability of
sizeable datasets, which are needed to train machine learning models. While
these conversations are part of the daily routine of clinicians, gathering them
is usually hindered by various ethical (purpose of data usage), legal (data
privacy) and technical (data formatting) limitations. Some of these limitations
are particular to the domain of therapy dialogues, like the increased
difficulty in anonymisation, or the transcription of the recordings. In this
paper, we elaborate on the challenges we faced in starting our collection of
therapist-patient dialogues in a psychiatry clinic under the General Data
Privacy Regulation of the European Union with the goal to use the data for
Natural Language Processing (NLP) research. We give an overview of each step in
our procedure and point out the potential pitfalls to motivate further research
in this field."
14075,"Any further research on this data
requires another updated informed consent.","By signing the informed consent form the participants gives allowance for the data to be used for
the purpose stated in the information sheet and only this purpose.","However, asking initially for general consent allows
future data usage for other research projects conducted within the hospital.",2022-11-22 15:51:10+00:00,GDPR Compliant Collection of Therapist-Patient-Dialogues,cs.CL,['cs.CL'],"[arxiv.Result.Author('Tobias Mayer'), arxiv.Result.Author('Neha Warikoo'), arxiv.Result.Author('Oliver Grimm'), arxiv.Result.Author('Andreas Reif'), arxiv.Result.Author('Iryna Gurevych')]","According to the Global Burden of Disease list provided by the World Health
Organization (WHO), mental disorders are among the most debilitating
disorders.To improve the diagnosis and the therapy effectiveness in recent
years, researchers have tried to identify individual biomarkers. Gathering
neurobiological data however, is costly and time-consuming. Another potential
source of information, which is already part of the clinical routine, are
therapist-patient dialogues. While there are some pioneering works
investigating the role of language as predictors for various therapeutic
parameters, for example patient-therapist alliance, there are no large-scale
studies. A major obstacle to conduct these studies is the availability of
sizeable datasets, which are needed to train machine learning models. While
these conversations are part of the daily routine of clinicians, gathering them
is usually hindered by various ethical (purpose of data usage), legal (data
privacy) and technical (data formatting) limitations. Some of these limitations
are particular to the domain of therapy dialogues, like the increased
difficulty in anonymisation, or the transcription of the recordings. In this
paper, we elaborate on the challenges we faced in starting our collection of
therapist-patient dialogues in a psychiatry clinic under the General Data
Privacy Regulation of the European Union with the goal to use the data for
Natural Language Processing (NLP) research. We give an overview of each step in
our procedure and point out the potential pitfalls to motivate further research
in this field."
14093,"These
      signed a higher score (0.745) to D3R containing the correct          “long-distance reasoning” questions are very challenging
      argument “412”, successfully switching the generator’s at-           for existing methods and await further study.","We also found that both models per-
      When generating the second argument, our reranker as-                formed poorly when expression length exceeded two.","tention to the correct sentence, while FinQANet still stayed
      in D2R and selects “210”, thus failing to answer this question.",2022-11-23 02:41:50+00:00,DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR']","[arxiv.Result.Author('Xiao Li'), arxiv.Result.Author('Yin Zhu'), arxiv.Result.Author('Sichen Liu'), arxiv.Result.Author('Jiangzhou Ju'), arxiv.Result.Author('Yuzhong Qu'), arxiv.Result.Author('Gong Cheng')]","Numerical reasoning over hybrid data containing tables and long texts has
recently received research attention from the AI community. To generate an
executable reasoning program consisting of math and table operations to answer
a question, state-of-the-art methods use a retriever-generator pipeline.
However, their retrieval results are static, while different generation steps
may rely on different sentences. To attend to the retrieved information that is
relevant to each generation step, in this paper, we propose DyRRen, an extended
retriever-reranker-generator framework where each generation step is enhanced
by a dynamic reranking of retrieved sentences. It outperforms existing
baselines on the FinQA dataset."
14118,"4.1.6 Ablation Study
    1http://nlp.ee.ncu.edu.tw/resource/csc.html
                                                        In this section, we further study the impact
                                                        of the masking ratio (i.e., the hyper-parameter
p).",better performance.,"Without loss of generality, we choose               4.2.2 Baseline Methods and Evaluation
the best baseline BERT + MaskCorrect as the                      Metric
framework and conduct experiments on both
SIGHAN14 and SIGHAN15 with p varying in                 FELIX FELIX (Mallinson et al., 2020) is a NAR
{0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5}.",2022-11-23 19:05:48+00:00,Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Kai Shen'), arxiv.Result.Author('Yichong Leng'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Siliang Tang'), arxiv.Result.Author('Yuan Zhang'), arxiv.Result.Author('Wenjie Liu'), arxiv.Result.Author('Edward Lin')]","Text error correction aims to correct the errors in text sequences such as
those typed by humans or generated by speech recognition models. Previous error
correction methods usually take the source (incorrect) sentence as encoder
input and generate the target (correct) sentence through the decoder. Since the
error rate of the incorrect sentence is usually low (e.g., 10\%), the
correction model can only learn to correct on limited error tokens but
trivially copy on most tokens (correct tokens), which harms the effective
training of error correction. In this paper, we argue that the correct tokens
should be better utilized to facilitate effective training and then propose a
simple yet effective masking strategy to achieve this goal. Specifically, we
randomly mask out a part of the correct tokens in the source sentence and let
the model learn to not only correct the original error tokens but also predict
the masked tokens based on their context information. Our method enjoys several
advantages: 1) it alleviates trivial copy; 2) it leverages effective training
signals from correct tokens; 3) it is a plug-and-play module and can be applied
to different models and tasks. Experiments on spelling error correction and
speech recognition error correction on Mandarin datasets and grammar error
correction on English datasets with both autoregressive and non-autoregressive
generation models show that our method improves the correction accuracy
consistently."
14149,"All models per-          set a fair baseline for further research in Finnish
form better with shorter passages and struggle at            QA and QG.","They
different from the target question.","All data used in the experiments is
inﬂecting rare words.",2022-11-24 20:40:00+00:00,Question Answering and Question Generation for Finnish,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ilmari Kylliäinen'), arxiv.Result.Author('Roman Yangarber')]","Recent advances in the field of language modeling have improved the
state-of-the-art in question answering (QA) and question generation (QG).
However, the development of modern neural models, their benchmarks, and
datasets for training them has mainly focused on English. Finnish, like many
other languages, faces a shortage of large QA/QG model training resources,
which has prevented experimenting with state-of-the-art QA/QG fine-tuning
methods. We present the first neural QA and QG models that work with Finnish.
To train the models, we automatically translate the SQuAD dataset and then use
normalization methods to reduce the amount of problematic data created during
the translation. Using the synthetic data, together with the Finnish partition
of the TyDi-QA dataset, we fine-tune several transformer-based models to both
QA and QG and evaluate their performance. To the best of our knowledge, the
resulting dataset is the first large-scale QA/QG resource for Finnish. This
paper also sets the initial benchmarks for Finnish-language QA and QG."
14154,"has been shown regarding what particular features
                                             We further study how to form maximally effec-       of explanations make them effective and how they
                                             tive sets of explanations for solving a given test  function in ICL.","However, while including explanations in
                                             explanations, indicating that LLMs do faith-        prompts has been demonstrated to be useful, little
                                             fully follow the explanations to some extent.",query.,2022-11-25 04:40:47+00:00,Complementary Explanations for Effective In-Context Learning,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xi Ye'), arxiv.Result.Author('Srinivasan Iyer'), arxiv.Result.Author('Asli Celikyilmaz'), arxiv.Result.Author('Ves Stoyanov'), arxiv.Result.Author('Greg Durrett'), arxiv.Result.Author('Ramakanth Pasunuru')]","Large language models (LLMs) have exhibited remarkable capabilities in
learning from explanations in prompts. Yet, there has been limited
understanding of what makes explanations effective for in-context learning.
This work aims to better understand the mechanisms by which explanations are
used for in-context learning. We first study the impact of two different
factors on prompting performance when using explanations: the computation trace
(the way the solution is decomposed) and the natural language of the prompt. By
perturbing explanations on three controlled tasks, we show that both factors
contribute to the effectiveness of explanations, indicating that LLMs do
faithfully follow the explanations to some extent. We further study how to form
maximally effective sets of explanations for solving a given test query. We
find that LLMs can benefit from the complementarity of the explanation set as
they are able to fuse different reasoning specified by individual exemplars in
prompts. Additionally, having relevant exemplars also contributes to more
effective prompts. Therefore, we propose a maximal-marginal-relevance-based
exemplar selection approach for constructing exemplar sets that are both
relevant as well as complementary, which successfully improves the in-context
learning performance across three real-world tasks on multiple LLMs."
14186,"On      of the North American Chapter of the Association
this front, we encourage further research from the        for Computational Linguistics: Human Language
NLP & CSS community to tackle the challenge of            Technologies, Volume 1 (Long and Short Papers),
interpreting humor subtype and offensive humor            pages 4171–4186, Minneapolis, Minnesota.","In Proceedings of the 2019 Conference
to belong to a certain class, or to be offensive.","Associ-
detection.",2022-11-25 20:37:58+00:00,The Naughtyformer: A Transformer Understands Offensive Humor,cs.CL,['cs.CL'],"[arxiv.Result.Author('Leonard Tang'), arxiv.Result.Author('Alexander Cai'), arxiv.Result.Author('Steve Li'), arxiv.Result.Author('Jason Wang')]","Jokes are intentionally written to be funny, but not all jokes are created
the same. Some jokes may be fit for a classroom of kindergarteners, but others
are best reserved for a more mature audience. While recent work has shown
impressive results on humor detection in text, here we instead investigate the
more nuanced task of detecting humor subtypes, especially of the less innocent
variety. To that end, we introduce a novel jokes dataset filtered from Reddit
and solve the subtype classification task using a finetuned Transformer dubbed
the Naughtyformer. Moreover, we show that our model is significantly better at
detecting offensiveness in jokes compared to state-of-the-art methods."
14194,"To ad-          and encourage further research on training models
                                             dress the above issues, we propose a novel        that can generalize to unseen relations.","(2022) propose a chal-
                                             generate data as additional training samples,     lenging task called zero-shot relation triplet extrac-
                                             which increases the training cost and severely    tion (ZeroRTE) to overcome the above limitations
                                             constrains the model performance.","ZeroRTE
                                             method named PCRED for ZeroRTE with               aims to extract relational triplets under the zero-
                                             Potential Candidate Relation Selection and        shot setting.",2022-11-26 04:27:31+00:00,PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yuquan Lan'), arxiv.Result.Author('Dongxu Li'), arxiv.Result.Author('Yunqi Zhang'), arxiv.Result.Author('Hui Zhao'), arxiv.Result.Author('Gang Zhao')]","Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation
triplets from unstructured texts under the zero-shot setting, where the
relation sets at the training and testing stages are disjoint. Previous
state-of-the-art method handles this challenging task by leveraging pretrained
language models to generate data as additional training samples, which
increases the training cost and severely constrains the model performance. To
address the above issues, we propose a novel method named PCRED for ZeroRTE
with Potential Candidate Relation Selection and Entity Boundary Detection. The
remarkable characteristic of PCRED is that it does not rely on additional data
and still achieves promising performance. The model adopts a relation-first
paradigm, recognizing unseen relations through candidate relation selection.
With this approach, the semantics of relations are naturally infused in the
context. Entities are extracted based on the context and the semantics of
relations subsequently. We evaluate our model on two ZeroRTE datasets. The
experiment results show that our method consistently outperforms previous
works. Our code will be available at https://anonymous.4open.science/r/PCRED."
14252,"We released the dataset1       Twitter, YouTube, and Instagram for offensive lan-
                                             for further research.","(2021) created a corpus of comments from
                                             99.83% weighted F1.",guage detection in Spanish.,2022-11-28 12:30:11+00:00,HERDPhobia: A Dataset for Hate Speech against Fulani in Nigeria,cs.CL,['cs.CL'],"[arxiv.Result.Author('Saminu Mohammad Aliyu'), arxiv.Result.Author('Gregory Maksha Wajiga'), arxiv.Result.Author('Muhammad Murtala'), arxiv.Result.Author('Shamsuddeen Hassan Muhammad'), arxiv.Result.Author('Idris Abdulmumin'), arxiv.Result.Author('Ibrahim Said Ahmad')]","Social media platforms allow users to freely share their opinions about
issues or anything they feel like. However, they also make it easier to spread
hate and abusive content. The Fulani ethnic group has been the victim of this
unfortunate phenomenon. This paper introduces the HERDPhobia - the first
annotated hate speech dataset on Fulani herders in Nigeria - in three
languages: English, Nigerian-Pidgin, and Hausa. We present a benchmark
experiment using pre-trained languages models to classify the tweets as either
hateful or non-hateful. Our experiment shows that the XML-T model provides
better performance with 99.83% weighted F1. We released the dataset at
https://github.com/hausanlp/HERDPhobia for further research."
14276,"We hope the results presented in this paper will motivate
                                                                                                                                     further research along these lines and better understand the model
                                        Permission to make digital or hard copies of all or part of this work for personal or        behavior under different task settings.",given task.,"classroom use is granted without fee provided that copies are not made or distributed
                                        for profit or commercial advantage and that copies bear this notice and the full citation
                                        on the first page.",2022-11-28 17:49:38+00:00,GPT-Neo for commonsense reasoning-a theoretical and practical lens,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Rohan Kashyap'), arxiv.Result.Author('Vivek Kashyap'), arxiv.Result.Author('Narendra C. P')]","Recent work has demonstrated substantial gains in pre-training large-scale
unidirectional language models such as the GPT-2, GPT-3, and GPT-neo, followed
by fine-tuning on a downstream task. In this paper, we evaluate the performance
of the GPT-neo 1.3 billion model for commonsense reasoning tasks. We assess the
model performance on six commonsense reasoning benchmark tasks and report the
accuracy scores for these tasks. When fine-tuned using the right set of
hyperparameters, we obtain competitive scores on three of these tasks but
struggle when the dataset size is significantly smaller. The low model
performance on a few of these tasks suggests the inherent difficulty in these
datasets and since it fails to establish coherent patterns given their limited
training samples. We also investigate and substantiate our results using
visualization and conduct numerous inference tests to understand the model
performance better. Finally, we conduct thorough robustness tests using various
methods to gauge the model performance under numerous settings. These findings
suggest a promising path for exploring smaller language models than the GPT-3
175 billion model to perform tasks requiring natural language understanding."
14349,We further study mapping the           is challenging.,"show a fairly robust zero-shot transfer abil-
                                             ity, yet fall short of estimated human accuracy         Crowdsourcing labels in low-resource languages
                                             signiﬁcantly.","Thus, we carefully design an an-
                                             English politeness strategy lexicon into nine        notation process that includes a translation task
                                             languages via automatic translation and lexi-        to evaluate annotator’s language proﬁciency and a
                                             con induction, analyzing whether each strat-         model-in-the-loop qualiﬁcation task which ﬁlters
                                             egy’s impact stays consistent across languages.",2022-11-29 18:58:15+00:00,TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Anirudh Srinivasan'), arxiv.Result.Author('Eunsol Choi')]","We study politeness phenomena in nine typologically diverse languages.
Politeness is an important facet of communication and is sometimes argued to be
cultural-specific, yet existing computational linguistic study is limited to
English. We create TyDiP, a dataset containing three-way politeness annotations
for 500 examples in each language, totaling 4.5K examples. We evaluate how well
multilingual models can identify politeness levels -- they show a fairly robust
zero-shot transfer ability, yet fall short of estimated human accuracy
significantly. We further study mapping the English politeness strategy lexicon
into nine languages via automatic translation and lexicon induction, analyzing
whether each strategy's impact stays consistent across languages. Lastly, we
empirically study the complicated relationship between formality and politeness
through transfer experiments. We hope our dataset will support various research
questions and applications, from evaluating multilingual models to constructing
polite multilingual agents."
14351,"We also highlight a     billion-scale COVID-19 tweets dataset at IEEE [11], curated
                                        major concern with the currently regarded gold standard test        since the inception of the pandemic, reports 480k tweets being
                                        set (ground truth) methodology, introduce a new data set, and       geotagged with point coordinates [10] out of 2 billion tweets
                                        identify further research avenues for advancing the area.","For instance, the
                                        mentioning origin and non-origin locations.","collected (the daily distribution of tweets for both the datasets
                                                                                                            is shown in Figure 1).",2022-11-18 01:33:01+00:00,Where did you tweet from? Inferring the origin locations of tweets based on contextual information,cs.CL,"['cs.CL', 'cs.SI']","[arxiv.Result.Author('Rabindra Lamsal'), arxiv.Result.Author('Aaron Harwood'), arxiv.Result.Author('Maria Rodriguez Read')]","Public conversations on Twitter comprise many pertinent topics including
disasters, protests, politics, propaganda, sports, climate change,
epidemics/pandemic outbreaks, etc., that can have both regional and global
aspects. Spatial discourse analysis rely on geographical data. However, today
less than 1% of tweets are geotagged; in both cases--point location or bounding
place information. A major issue with tweets is that Twitter users can be at
location A and exchange conversations specific to location B, which we call the
Location A/B problem. The problem is considered solved if location entities can
be classified as either origin locations (Location As) or non-origin locations
(Location Bs). In this work, we propose a simple yet effective framework--the
True Origin Model--to address the problem that uses machine-level natural
language understanding to identify tweets that conceivably contain their origin
location information. The model achieves promising accuracy at country (80%),
state (67%), city (58%), county (56%) and district (64%) levels with support
from a Location Extraction Model as basic as the CoNLL-2003-based RoBERTa. We
employ a tweet contexualizer (locBERT) which is one of the core components of
the proposed model, to investigate multiple tweets' distributions for
understanding Twitter users' tweeting behavior in terms of mentioning origin
and non-origin locations. We also highlight a major concern with the currently
regarded gold standard test set (ground truth) methodology, introduce a new
data set, and identify further research avenues for advancing the area."
14364,"The ﬁrst goal is to analyze the challenges
                                        ing the high efﬁciency of machine translation (MT)     in the area of TS, which can provide some new
                                        (Cho et al., 2014; Bahdanau et al., 2015; Vaswani      directions for the further researches and applica-
                                        et al., 2017) and high accuracy of human transla-      tions in this area.","et al., 2009; Green et al., 2014; Knowles and Koehn,
                                        2016; Santy et al., 2019) has attained more and           The main motivation of this shared task is two-
                                        more attention for its promising ability in combin-    fold.","Secondly, we want to make the
                                        tion (HT).",2022-11-30 03:48:36+00:00,Findings of the WMT 2022 Shared Task on Translation Suggestion,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zhen Yang'), arxiv.Result.Author('Fandong Meng'), arxiv.Result.Author('Yingxue Zhang'), arxiv.Result.Author('Ernan Li'), arxiv.Result.Author('Jie Zhou')]","We report the result of the first edition of the WMT shared task on
Translation Suggestion (TS). The task aims to provide alternatives for specific
words or phrases given the entire documents generated by machine translation
(MT). It consists two sub-tasks, namely, the naive translation suggestion and
translation suggestion with hints. The main difference is that some hints are
provided in sub-task two, therefore, it is easier for the model to generate
more accurate suggestions. For sub-task one, we provide the corpus for the
language pairs English-German and English-Chinese. And only English-Chinese
corpus is provided for the sub-task two.
  We received 92 submissions from 5 participating teams in sub-task one and 6
submissions for the sub-task 2, most of them covering all of the translation
directions. We used the automatic metric BLEU for evaluating the performance of
each submission."
14393,"CREPE provides a bench-           Reaction (Physics)
                                             mark to study question answering in the wild,
                                             and our analyses provide avenues for future                From Wikipedia, the free encyclopedia
                                             work in better modeling and further studying
                                             the task.","in retrieving relevant evidence passages from
                                             a large text corpus.","… One problem frequently observed by physics educators is that
                                                                                                     students tend to apply Newton’s Third Law to pairs of ‘equal and
                                        1 Introduction                                               opposite’ forces acting on the same object.",2022-11-30 18:54:49+00:00,CREPE: Open-Domain Question Answering with False Presuppositions,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Xinyan Velocity Yu'), arxiv.Result.Author('Sewon Min'), arxiv.Result.Author('Luke Zettlemoyer'), arxiv.Result.Author('Hannaneh Hajishirzi')]","Information seeking users often pose questions with false presuppositions,
especially when asking about unfamiliar topics. Most existing question
answering (QA) datasets, in contrast, assume all questions have well defined
answers. We introduce CREPE, a QA dataset containing a natural distribution of
presupposition failures from online information-seeking forums. We find that
25% of questions contain false presuppositions, and provide annotations for
these presuppositions and their corrections. Through extensive baseline
experiments, we show that adaptations of existing open-domain QA models can
find presuppositions moderately well, but struggle when predicting whether a
presupposition is factually correct. This is in large part due to difficulty in
retrieving relevant evidence passages from a large text corpus. CREPE provides
a benchmark to study question answering in the wild, and our analyses provide
avenues for future work in better modeling and further studying the task."
14410,"In-             a top'Lic',for further research.","Abs'eCnacreasosfiussta't,istics for manually-labeled dataset

   Transformer based language models, such as                     'auratus',
BERT and BioBERT, use trained tokenizers, such
as BPE-tokenizers (Sennrich et al., 2016), and do           rem'a(in',s as limitation of this auxiliary study and is
not remove stopwords or special characters.","stead those tokenizers make stopwords or special
characters an independent token.",2022-12-01 02:07:55+00:00,Biomedical NER for the Enterprise with Distillated BERN2 and the Kazu Framework,cs.CL,['cs.CL'],"[arxiv.Result.Author('Wonjin Yoon'), arxiv.Result.Author('Richard Jackson'), arxiv.Result.Author('Elliot Ford'), arxiv.Result.Author('Vladimir Poroshin'), arxiv.Result.Author('Jaewoo Kang')]","In order to assist the drug discovery/development process, pharmaceutical
companies often apply biomedical NER and linking techniques over internal and
public corpora. Decades of study of the field of BioNLP has produced a plethora
of algorithms, systems and datasets. However, our experience has been that no
single open source system meets all the requirements of a modern pharmaceutical
company. In this work, we describe these requirements according to our
experience of the industry, and present Kazu, a highly extensible, scalable
open source framework designed to support BioNLP for the pharmaceutical sector.
Kazu is a built around a computationally efficient version of the BERN2 NER
model (TinyBERN2), and subsequently wraps several other BioNLP technologies
into one coherent system. KAZU framework is open-sourced:
https://github.com/AstraZeneca/KAZU"
14411,"To further study the effectiveness of CDM, we con-
duct experiments over these mappings.","We design Suitability to
   We also collect the generated responses and            validate whether the model can learn the diversity
show them in Appendix B.                                  and relevance from CDM samples, and we use Eru-
                                                          dition to assess whether the semantic information
5.5 Effectiveness Analysis                                of multiple ground-truths is involved in multiple
                                                          responses generated by the model.",Results and Analysis Table 6 reports the result.,2022-12-01 02:31:10+00:00,Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Bin Sun'), arxiv.Result.Author('Shaoxiong Feng'), arxiv.Result.Author('Yiwei Li'), arxiv.Result.Author('Weichao Wang'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Yitong Li'), arxiv.Result.Author('Kan Li')]","Complex dialogue mappings (CDM), including one-to-many and many-to-one
mappings, tend to make dialogue models generate incoherent or dull responses,
and modeling these mappings remains a huge challenge for neural dialogue
systems. To alleviate these problems, methods like introducing external
information, reconstructing the optimization function, and manipulating data
samples are proposed, while they primarily focus on avoiding training with CDM,
inevitably weakening the model's ability of understanding CDM in human
conversations and limiting further improvements in model performance. This
paper proposes a Sentence Semantic \textbf{Seg}mentation guided
\textbf{C}onditional \textbf{V}ariational \textbf{A}uto-\textbf{E}ncoder
(SegCVAE) method which can model and take advantages of the CDM data.
Specifically, to tackle the incoherent problem caused by one-to-many, SegCVAE
uses response-related prominent semantics to constrained the latent variable.
To mitigate the non-diverse problem brought by many-to-one, SegCVAE segments
multiple prominent semantics to enrich the latent variables. Three novel
components, Internal Separation, External Guidance, and Semantic Norms, are
proposed to achieve SegCVAE. On dialogue generation tasks, both the automatic
and human evaluation results show that SegCVAE achieves new state-of-the-art
performance."
14413,"We show that systems that generate EXR notation signiﬁcantly outperform their TOP-
           generating counterparts, which calls for further research in generating executable semantic
           representations with resolved entities instead of a blend of utterance tokens and semantic
           constructors.",3.,4.,2022-12-01 04:20:07+00:00,PIZZA: A new benchmark for complex end-to-end task-oriented parsing,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Konstantine Arkoudas'), arxiv.Result.Author('Nicolas Guenon des Mesnards'), arxiv.Result.Author('Melanie Rubino'), arxiv.Result.Author('Sandesh Swamy'), arxiv.Result.Author('Saarthak Khanna'), arxiv.Result.Author('Weiqi Sun'), arxiv.Result.Author('Khan Haidar')]","Much recent work in task-oriented parsing has focused on finding a middle
ground between flat slots and intents, which are inexpressive but easy to
annotate, and powerful representations such as the lambda calculus, which are
expressive but costly to annotate. This paper continues the exploration of
task-oriented parsing by introducing a new dataset for parsing pizza and drink
orders, whose semantics cannot be captured by flat slots and intents. We
perform an extensive evaluation of deep-learning techniques for task-oriented
parsing on this dataset, including different flavors of seq2seq systems and
RNNGs. The dataset comes in two main versions, one in a recently introduced
utterance-level hierarchical notation that we call TOP, and one whose targets
are executable representations (EXR). We demonstrate empirically that training
the parser to directly generate EXR notation not only solves the problem of
entity resolution in one fell swoop and overcomes a number of expressive
limitations of TOP notation, but also results in significantly greater parsing
accuracy."
14414,"While the gains in performance in generating EXR call
for further research on end-to-end systems, it must be noted that the integration of such models
in production has some caveats.",Generating EXR comes with challenges.,"Notably, every time we want to expand the catalogs with new
entities, the whole system needs to be retrained to augment its output space.",2022-12-01 04:20:07+00:00,PIZZA: A new benchmark for complex end-to-end task-oriented parsing,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Konstantine Arkoudas'), arxiv.Result.Author('Nicolas Guenon des Mesnards'), arxiv.Result.Author('Melanie Rubino'), arxiv.Result.Author('Sandesh Swamy'), arxiv.Result.Author('Saarthak Khanna'), arxiv.Result.Author('Weiqi Sun'), arxiv.Result.Author('Khan Haidar')]","Much recent work in task-oriented parsing has focused on finding a middle
ground between flat slots and intents, which are inexpressive but easy to
annotate, and powerful representations such as the lambda calculus, which are
expressive but costly to annotate. This paper continues the exploration of
task-oriented parsing by introducing a new dataset for parsing pizza and drink
orders, whose semantics cannot be captured by flat slots and intents. We
perform an extensive evaluation of deep-learning techniques for task-oriented
parsing on this dataset, including different flavors of seq2seq systems and
RNNGs. The dataset comes in two main versions, one in a recently introduced
utterance-level hierarchical notation that we call TOP, and one whose targets
are executable representations (EXR). We demonstrate empirically that training
the parser to directly generate EXR notation not only solves the problem of
entity resolution in one fell swoop and overcomes a number of expressive
limitations of TOP notation, but also results in significantly greater parsing
accuracy."
14474,"To further study the effectiveness of our approach, we add an extra variant of our UniKGQA, namely
UniKGQA+NSM, which relies on UniKGQA for retrieval while NSM for performing reasoning.","Then, we compare UniKGQA with
SR+NSM (Zhang et al., 2022) and PPR+NSM (He et al., 2021) based on their ﬁnal QA performance.",Figure 3(a) and Figure 3(b) show the comparison results of the above methods.,2022-12-02 04:08:09+00:00,UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jinhao Jiang'), arxiv.Result.Author('Kun Zhou'), arxiv.Result.Author('Wayne Xin Zhao'), arxiv.Result.Author('Ji-Rong Wen')]","Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the
answer entities that are multiple hops away from the topic entities mentioned
in a natural language question on a large-scale Knowledge Graph (KG). To cope
with the vast search space, existing work usually adopts a two-stage approach:
it firstly retrieves a relatively small subgraph related to the question and
then performs the reasoning on the subgraph to accurately find the answer
entities. Although these two stages are highly related, previous work employs
very different technical solutions for developing the retrieval and reasoning
models, neglecting their relatedness in task essence. In this paper, we propose
UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and
reasoning in both model architecture and parameter learning. For model
architecture, UniKGQA consists of a semantic matching module based on a
pre-trained language model~(PLM) for question-relation semantic matching, and a
matching information propagation module to propagate the matching information
along the edges on KGs. For parameter learning, we design a shared pre-training
task based on question-relation matching for both retrieval and reasoning
models, and then propose retrieval- and reasoning-oriented fine-tuning
strategies. Compared with previous studies, our approach is more unified,
tightly relating the retrieval and reasoning stages. Extensive experiments on
three benchmark datasets have demonstrated the effectiveness of our method on
the multi-hop KGQA task. Our codes and data are publicly available at
https://github.com/RUCAIBox/UniKGQA."
14475,"To further study our UniKGQA model, we conduct parameter
sensitivity analysis w.r.t.","It veriﬁes that our model can be ﬁne-tuned in an
efﬁcient way with very few epochs.","pre-training steps, hidden dimensions, and the number of retrieved nodes
K, shown in Appendix H.

5 RELATED WORK

Multi-hop Knowledge Graph Question Answering.",2022-12-02 04:08:09+00:00,UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jinhao Jiang'), arxiv.Result.Author('Kun Zhou'), arxiv.Result.Author('Wayne Xin Zhao'), arxiv.Result.Author('Ji-Rong Wen')]","Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the
answer entities that are multiple hops away from the topic entities mentioned
in a natural language question on a large-scale Knowledge Graph (KG). To cope
with the vast search space, existing work usually adopts a two-stage approach:
it firstly retrieves a relatively small subgraph related to the question and
then performs the reasoning on the subgraph to accurately find the answer
entities. Although these two stages are highly related, previous work employs
very different technical solutions for developing the retrieval and reasoning
models, neglecting their relatedness in task essence. In this paper, we propose
UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and
reasoning in both model architecture and parameter learning. For model
architecture, UniKGQA consists of a semantic matching module based on a
pre-trained language model~(PLM) for question-relation semantic matching, and a
matching information propagation module to propagate the matching information
along the edges on KGs. For parameter learning, we design a shared pre-training
task based on question-relation matching for both retrieval and reasoning
models, and then propose retrieval- and reasoning-oriented fine-tuning
strategies. Compared with previous studies, our approach is more unified,
tightly relating the retrieval and reasoning stages. Extensive experiments on
three benchmark datasets have demonstrated the effectiveness of our method on
the multi-hop KGQA task. Our codes and data are publicly available at
https://github.com/RUCAIBox/UniKGQA."
14482,"points(Meta_P_P & Meta_A_A & Multi_P_P &
The actual reason may need further research to        Multi_A_A).",6 (b)(d)).,"We can easily get to the conclusion
fully unveil.",2022-12-02 08:56:53+00:00,General Framework for Self-Supervised Model Priming for Parameter-Efficient Fine-tuning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Shih-Cheng Huang'), arxiv.Result.Author('Shih-Heng Wang'), arxiv.Result.Author('Min-Han Shih'), arxiv.Result.Author('Saurav Sahay'), arxiv.Result.Author('Hung-yi Lee')]","Parameter-efficient methods (like Prompt or Adapters) for adapting
pre-trained language models to downstream tasks have been popular recently.
However, hindrances still prevent these methods from reaching their full
potential. For example, two significant challenges are few-shot adaptation and
cross-task generalization ability. To tackle these issues, we propose a general
framework to enhance the few-shot adaptation and cross-domain generalization
ability of parameter-efficient methods. In our framework, we prime the
self-supervised model for parameter-efficient methods to rapidly adapt to
various downstream few-shot tasks. To evaluate the authentic generalization
ability of these parameter-efficient methods, we conduct experiments on a
few-shot cross-domain benchmark containing 160 diverse NLP tasks. The
experiment result reveals that priming by tuning PLM only with extra training
tasks leads to the best performance. Also, we perform a comprehensive analysis
of various parameter-efficient methods under few-shot cross-domain scenarios."
14485,"However, this could also be a mat-    biases in this work; hence, we advise against using
ter for further research on the topic; for example,    our model in a production environment without a
non-autoregressive generative models are steadily      careful analysis beforehand.","However, we did not investigate such
criminative one.","Finally, we remark
narrowing the performance gap (Gu and Tan, 2022)       that the test sets of CoNLL-2009, CoNLL-2012,
while mitigating the weaknesses of current autore-     and FrameNet 1.7 also contain relatively old doc-
gressive approaches.",2022-12-02 11:19:16+00:00,Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Simone Conia'), arxiv.Result.Author('Edoardo Barba'), arxiv.Result.Author('Alessandro Scirè'), arxiv.Result.Author('Roberto Navigli')]","One of the common traits of past and present approaches for Semantic Role
Labeling (SRL) is that they rely upon discrete labels drawn from a predefined
linguistic inventory to classify predicate senses and their arguments. However,
we argue this need not be the case. In this paper, we present an approach that
leverages Definition Modeling to introduce a generalized formulation of SRL as
the task of describing predicate-argument structures using natural language
definitions instead of discrete labels. Our novel formulation takes a first
step towards placing interpretability and flexibility foremost, and yet our
experiments and analyses on PropBank-style and FrameNet-style, dependency-based
and span-based SRL also demonstrate that a flexible model with an interpretable
output does not necessarily come at the expense of performance. We release our
software for research purposes at https://github.com/SapienzaNLP/dsrl."
14486,instances will require further study.,"726487 un-
believe that such a benchmark could be a great con-             der the European Union’s Horizon
tribution to the area of SRL, but the endeavor of               2020 research and innovation pro-
annotating a signiﬁcant number of out-of-inventory              gramme.","This work was supported in part by the MIUR
Multilinguality.",2022-12-02 11:19:16+00:00,Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Simone Conia'), arxiv.Result.Author('Edoardo Barba'), arxiv.Result.Author('Alessandro Scirè'), arxiv.Result.Author('Roberto Navigli')]","One of the common traits of past and present approaches for Semantic Role
Labeling (SRL) is that they rely upon discrete labels drawn from a predefined
linguistic inventory to classify predicate senses and their arguments. However,
we argue this need not be the case. In this paper, we present an approach that
leverages Definition Modeling to introduce a generalized formulation of SRL as
the task of describing predicate-argument structures using natural language
definitions instead of discrete labels. Our novel formulation takes a first
step towards placing interpretability and flexibility foremost, and yet our
experiments and analyses on PropBank-style and FrameNet-style, dependency-based
and span-based SRL also demonstrate that a flexible model with an interpretable
output does not necessarily come at the expense of performance. We release our
software for research purposes at https://github.com/SapienzaNLP/dsrl."
14506,"that further research of those prompting approaches
is needed.","The zero-shot with legal reasoning approach shows                   Entails(S1, S2, ..., Sn, Q) or
the best result for one year only and may be more
prone to overﬁtting to a speciﬁc test set indicating                Entails(S1, S2, ..., Sn, ¬Q).","The answer of this task is binary: “YES” (“Q”)
                                                        or “NO” (“¬Q”).",2022-12-02 17:41:22+00:00,Legal Prompting: Teaching a Language Model to Think Like a Lawyer,cs.CL,"['cs.CL', 'cs.AI', 'I.2.7']","[arxiv.Result.Author('Fangyi Yu'), arxiv.Result.Author('Lee Quartey'), arxiv.Result.Author('Frank Schilder')]","Large language models that are capable of zero or few-shot prompting
approaches have given rise to the new research area of prompt engineering.
Recent advances showed that for example Chain-of-Thought (CoT) prompts can
improve arithmetic or common sense tasks significantly. We explore how such
approaches fair with legal reasoning tasks and take the COLIEE entailment task
based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning
approaches. Our findings show that while CoT prompting and fine-tuning with
explanations approaches show improvements, the best results are produced by
prompts that are derived from specific legal reasoning techniques such as IRAC
(Issue, Rule, Application, Conclusion). Based on our experiments we improve the
2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best
system of 0.6789 accuracy with an accuracy of 0.7431."
14507,"that further research of those prompting approaches
is needed.","The zero-shot with legal reasoning approach shows                   Entails(S1, S2, ..., Sn, Q) or
the best result for one year only and may be more
prone to overﬁtting to a speciﬁc test set indicating                Entails(S1, S2, ..., Sn, ¬Q).","The answer of this task is binary: “YES” (“Q”)
                                                        or “NO” (“¬Q”).",2022-12-02 17:41:22+00:00,Legal Prompting: Teaching a Language Model to Think Like a Lawyer,cs.CL,"['cs.CL', 'cs.AI', 'I.2.7']","[arxiv.Result.Author('Fangyi Yu'), arxiv.Result.Author('Lee Quartey'), arxiv.Result.Author('Frank Schilder')]","Large language models that are capable of zero or few-shot prompting
approaches have given rise to the new research area of prompt engineering.
Recent advances showed that for example Chain-of-Thought (CoT) prompts can
improve arithmetic or common sense tasks significantly. We explore how such
approaches fare with legal reasoning tasks and take the COLIEE entailment task
based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning
approaches. Our findings show that while CoT prompting and fine-tuning with
explanations approaches show improvements, the best results are produced by
prompts that are derived from specific legal reasoning techniques such as IRAC
(Issue, Rule, Application, Conclusion). Based on our experiments we improve the
2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best
system of 0.6789 accuracy with an accuracy of 0.7431."
14527,"It further highlights the
rich source of SDoH information clinical social worker notes represent and develops methods
which can make further research within the field of SDoH more tractable.","This
work demonstrates how unsupervised topic modeling approaches can elucidate and categorize
the information relating to SDoH within unstructured clinical notes.","MATERIALS AND METHODS

Study design and cohort selection
This study uses the deidentified clinical notes at UCSF recorded between 2012 and 2021[24].",2022-12-02 21:54:55+00:00,Topic Modeling on Clinical Social Work Notes for Exploring Social Determinants of Health Factors,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shenghuan Sun'), arxiv.Result.Author('Travis Zack'), arxiv.Result.Author('Madhumita Sushil'), arxiv.Result.Author('Atul J. Butte')]","Most research studying social determinants of health (SDoH) has focused on
physician notes or structured elements of the electronic medical record (EMR).
We hypothesize that clinical notes from social workers, whose role is to
ameliorate social and economic factors, might provide a richer source of data
on SDoH. We sought to perform topic modeling to identify robust topics of
discussion within a large cohort of social work notes. We retrieved a diverse,
deidentified corpus of 0.95 million clinical social work notes from 181,644
patients at the University of California, San Francisco. We used word frequency
analysis and Latent Dirichlet Allocation (LDA) topic modeling analysis to
characterize this corpus and identify potential topics of discussion. Word
frequency analysis identified both medical and non-medical terms associated
with specific ICD10 chapters. The LDA topic modeling analysis extracted 11
topics related to social determinants of health risk factors including
financial status, abuse history, social support, risk of death, and mental
health. In addition, the topic modeling approach captured the variation between
different types of social work notes and across patients with different types
of diseases or conditions. We demonstrated that social work notes contain rich,
unique, and otherwise unobtainable information on an individual's SDoH."
14532,"We conduct a
in both faithfulness and informativeness, and can                              simple hyper-parameter search for the learning rate
foster further research on narrative summarization.","It                         RASUM with AdamW optimizer (Loshchilov and
demonstrates that NARRASUM is of high quality                                  Hutter, 2019) and batch size of 64.","from {3e−4, 1e−4, 3e−5} based on the validation
                                                                               loss.",2022-12-02 22:51:51+00:00,NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Chao Zhao'), arxiv.Result.Author('Faeze Brahman'), arxiv.Result.Author('Kaiqiang Song'), arxiv.Result.Author('Wenlin Yao'), arxiv.Result.Author('Dian Yu'), arxiv.Result.Author('Snigdha Chaturvedi')]","Narrative summarization aims to produce a distilled version of a narrative to
describe its most salient events and characters. Summarizing a narrative is
challenging as it requires an understanding of event causality and character
behaviors. To encourage research in this direction, we propose NarraSum, a
large-scale narrative summarization dataset. It contains 122K narrative
documents, which are collected from plot descriptions of movies and TV episodes
with diverse genres, and their corresponding abstractive summaries. Experiments
show that there is a large performance gap between humans and the
state-of-the-art summarization models on NarraSum. We hope that this dataset
will promote future research in summarization, as well as broader studies of
natural language understanding and generation. The dataset is available at
https://github.com/zhaochaocs/narrasum."
14543,"We believe that further research on the
a slight sacriﬁce of reference-dependent metrics.","As       tion where the output does not cover the full answer
shown in the last four rows of Table 1, RHO per-      as expressed by all triples mentioned in the given
forms best in automatic hallucination metrics with    sub-graph.","reasoning ability and interpretability of the model
Speciﬁcally, compared to models equipped with         can help address this issue.",2022-12-03 10:36:34+00:00,RHO ($ρ$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Ziwei Ji'), arxiv.Result.Author('Zihan Liu'), arxiv.Result.Author('Nayeon Lee'), arxiv.Result.Author('Tiezheng Yu'), arxiv.Result.Author('Bryan Wilie'), arxiv.Result.Author('Min Zeng'), arxiv.Result.Author('Pascale Fung')]","Dialogue systems can leverage large pre-trained language models and knowledge
to generate fluent and informative responses. However, these models are still
prone to produce hallucinated responses not supported by the input source,
which greatly hinders their application. The heterogeneity between external
knowledge and dialogue context challenges representation learning and source
integration, and further contributes to unfaithfulness. To handle this
challenge and generate more faithful responses, this paper presents RHO
($\rho$) utilizing the representations of linked entities and relation
predicates from a knowledge graph (KG). We propose (1) local knowledge
grounding to combine textual embeddings with the corresponding KG embeddings;
and (2) global knowledge grounding to equip RHO with multi-hop reasoning
abilities via the attention mechanism. In addition, we devise a response
re-ranking technique based on walks over KG sub-graphs for better
conversational reasoning. Experimental results on OpenDialKG show that our
approach significantly outperforms state-of-the-art methods on both automatic
and human evaluation by a large margin, especially in hallucination reduction
(17.54% in FeQA)."
14545,"To finish,
we suggest some potential future directions for further research.","We cover
the formal definition of the Medical Summarization task, a detailed analysis of different medical
tasks based on the type of medical documents and specific datasets and challenges associated with
them, a detailed categorization of existing works based on input, output, and technique, and an
in-depth look at the evaluation metrics utilized to measure the quality of the summaries.","We are confident that this survey
will encourage more work in medical document summarization.",2022-12-03 18:46:44+00:00,A Survey on Medical Document Summarization,cs.CL,['cs.CL'],"[arxiv.Result.Author('Raghav Jain'), arxiv.Result.Author('Anubhav Jangra'), arxiv.Result.Author('Sriparna Saha'), arxiv.Result.Author('Adam Jatowt')]","The internet has had a dramatic effect on the healthcare industry, allowing
documents to be saved, shared, and managed digitally. This has made it easier
to locate and share important data, improving patient care and providing more
opportunities for medical studies. As there is so much data accessible to
doctors and patients alike, summarizing it has become increasingly necessary -
this has been supported through the introduction of deep learning and
transformer-based networks, which have boosted the sector significantly in
recent years. This paper gives a comprehensive survey of the current techniques
and trends in medical summarization"
14594,"contradicted the hypothesis statement; these may
be due to the model’s external knowledge from                  8 Conclusions
pre-training, but this requires further study.","In T5 models, we noticed some errors                  scores for factuality and salience.","We have argued that a deeper understanding of
7 Effect of Discourse Text on Models                           complex events can be achieved by examining their
                                                               cumulative outcomes, grounded as changes of state.",2022-12-05 22:23:27+00:00,POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sai Vallurupalli'), arxiv.Result.Author('Sayontan Ghosh'), arxiv.Result.Author('Katrin Erk'), arxiv.Result.Author('Niranjan Balasubramanian'), arxiv.Result.Author('Francis Ferraro')]","Knowledge about outcomes is critical for complex event understanding but is
hard to acquire. We show that by pre-identifying a participant in a complex
event, crowd workers are able to (1) infer the collective impact of salient
events that make up the situation, (2) annotate the volitional engagement of
participants in causing the situation, and (3) ground the outcome of the
situation in state changes of the participants. By creating a multi-step
interface and a careful quality control strategy, we collect a high quality
annotated dataset of 8K short newswire narratives and ROCStories with high
inter-annotator agreement (0.74-0.96 weighted Fleiss Kappa). Our dataset, POQue
(Participant Outcome Questions), enables the exploration and development of
models that address multiple aspects of semantic understanding. Experimentally,
we show that current language models lag behind human performance in subtle
ways through our task formulations that target abstract and specific
comprehension of a complex event, its outcome, and a participant's influence
over the event culmination."
14613,"For future work, an in-depth analysis of the extent of
                                                                              memorization taking place in our proposed main model could
                                                                              be performed along with a further research study of methods
                                                                              and techniques which can be incorporated to minimize this
                                                                              effect maybe in a privacy preserving perspective.","model’s predictions behave and learn an interpretable new
model based on those perturbations on top of our ﬁnetuned                        Also, the results obtained via the interpretability evaluation
                                                                              of the classiﬁer trained with generated data along with the
                                                                              outcomes of the extrinsic evaluation of the generated data,
                                                                              suggests that these generated texts may be used in AI tasks
                                                                              such as model training, as this model trained with generated
                                                                              data produced classiﬁcation results comparable to the results
                                                                              of a classiﬁer trained with real labelled data where this
                                                                              classiﬁer trained with artiﬁcial data also seemed to make the
                                                                              classiﬁcation decisions based on features of the inputs which
                                                                              seemed to be interpretable in general context.","8
                        ACKNOWLEDGMENT                                   conversation models, in ‘Proceedings of the 2016 Confer-
                                                                         ence of the North American Chapter of the Association
   We’d like to express our heartiest gratitude to the School            for Computational Linguistics: Human Language Technolo-
of Electronic Engineering and Computer Science of Queen                  gies’, pp.",2022-12-06 12:31:53+00:00,Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Author('Damith Chamalke Senadeera'), arxiv.Result.Author('Julia Ive')]","Controlled text generation is a very important task in the arena of natural
language processing due to its promising applications. In order to achieve this
task we mainly introduce the novel soft prompt tuning method of using soft
prompts at both encoder and decoder levels together in a T5 model and
investigate the performance as the behaviour of an additional soft prompt
related to the decoder of a T5 model in controlled text generation remained
unexplored. Then we also investigate the feasibility of steering the output of
this extended soft prompted T5 model at decoder level and finally analyse the
utility of generated text to be used in AI related tasks such as training AI
models with an interpretability analysis of the classifier trained with
synthetic text, as there is a lack of proper analysis of methodologies in
generating properly labelled data to be utilized in AI tasks. Through the
performed in-depth intrinsic and extrinsic evaluations of this generation model
along with the artificially generated data, we found that this model produced
better results compared to the T5 model with a single soft prompt at encoder
level and the sentiment classifier trained using this artificially generated
data can produce comparable classification results to the results of a
classifier trained with real labelled data and also the classifier decision is
interpretable with respect to the input text content."
14661,"For long-tail domains such
as Trec-Covid [55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily
on exact lexical match (Fever) [54], further research efforts are still necessary to improve current
dense retrievers.","BM25 still holds
obvious advantages in terms of simplicity, efﬁciency, and interpretability.","6 Conclusion

In this work, we train a general-purpose text embedding model E5 from weak supervision signals.",2022-12-07 09:25:54+00:00,Text Embeddings by Weakly-Supervised Contrastive Pre-training,cs.CL,"['cs.CL', 'cs.IR']","[arxiv.Result.Author('Liang Wang'), arxiv.Result.Author('Nan Yang'), arxiv.Result.Author('Xiaolong Huang'), arxiv.Result.Author('Binxing Jiao'), arxiv.Result.Author('Linjun Yang'), arxiv.Result.Author('Daxin Jiang'), arxiv.Result.Author('Rangan Majumder'), arxiv.Result.Author('Furu Wei')]","This paper presents E5, a family of state-of-the-art text embeddings that
transfer well to a wide range of tasks. The model is trained in a contrastive
manner with weak supervision signals from our curated large-scale text pair
dataset (called CCPairs). E5 can be readily used as a general-purpose embedding
model for any tasks requiring a single-vector representation of texts such as
retrieval, clustering, and classification, achieving strong performance in both
zero-shot and fine-tuned settings. We conduct extensive evaluations on 56
datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the
first model that outperforms the strong BM25 baseline on the BEIR retrieval
benchmark without using any labeled data. When fine-tuned, E5 obtains the best
results on the MTEB benchmark, beating existing embedding models with 40x more
parameters."
14673,"Therefore, further research is
   Although the ConvAI2 dataset provided a method for as-           needed to address these limitations, in addition to a strong
sociating a conversation with a persona, there is still a large     collaboration with different domain experts, as well as Turkers
gap between this data and more realistic data that might be         and volunteers to guide the process of generating rich persona-
available in practical use cases.","(e.g., Twitter dataset), and may require guidance from highly
                                                                    skilled behavioral scientists.",The ConvAI2 dataset relies        based conversational data.,2022-12-04 18:16:57+00:00,Persona-Based Conversational AI: State of the Art and Challenges,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Junfeng Liu'), arxiv.Result.Author('Christopher Symons'), arxiv.Result.Author('Ranga Raju Vatsavai')]","Conversational AI has become an increasingly prominent and practical
application of machine learning. However, existing conversational AI techniques
still suffer from various limitations. One such limitation is a lack of
well-developed methods for incorporating auxiliary information that could help
a model understand conversational context better. In this paper, we explore how
persona-based information could help improve the quality of response generation
in conversations. First, we provide a literature review focusing on the current
state-of-the-art methods that utilize persona information. We evaluate two
strong baseline methods, the Ranking Profile Memory Network and the
Poly-Encoder, on the NeurIPS ConvAI2 benchmark dataset. Our analysis elucidates
the importance of incorporating persona information into conversational
systems. Additionally, our study highlights several limitations with current
state-of-the-art methods and outlines challenges and future research directions
for advancing personalized conversational AI technology."
14674,"To facilitate further research, we make our experimental setups                                          [21] Markus Freitag and Yaser Al-Onaizan.",17–32.,2017.,2022-12-07 16:20:50+00:00,A Study on Extracting Named Entities from Fine-tuned vs. Differentially Private Fine-tuned BERT Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Andor Diera'), arxiv.Result.Author('Nicolas Lell'), arxiv.Result.Author('Aygul Garifullina'), arxiv.Result.Author('Ansgar Scherp')]","Privacy preserving deep learning is an emerging field in machine learning
that aims to mitigate the privacy risks in the use of deep neural networks. One
such risk is training data extraction from language models that have been
trained on datasets , which contain personal and privacy sensitive information.
In our study, we investigate the extent of named entity memorization in
fine-tuned BERT models. We use single-label text classification as
representative downstream task and employ three different fine-tuning setups in
our experiments, including one with Differentially Privacy (DP). We create a
large number of text samples from the fine-tuned BERT models utilizing a custom
sequential sampling strategy with two prompting strategies. We search in these
samples for named entities and check if they are also present in the
fine-tuning datasets. We experiment with two benchmark datasets in the domains
of emails and blogs. We show that the application of DP has a huge effect on
the text generation capabilities of BERT. Furthermore, we show that a
fine-tuned BERT does not generate more named entities entities specific to the
fine-tuning dataset than a BERT model that is pre-trained only. This suggests
that BERT is unlikely to emit personal or privacy sensitive named entities.
Overall, our results are important to understand to what extent BERT-based
services are prone to training data extraction attacks."
14700,"We release our code for
   Based on this observation, we ﬁrst propose           further research.","and supervised performance on a public scientiﬁc
                                                        summarization dataset.","a Multi-granularity Unsupervised Summarization
(MUS) as a light solution to the task without the       2 Related Work
requirement of rich supervision information.",2022-12-08 11:53:12+00:00,Scientific Paper Extractive Summarization Enhanced by Citation Graphs,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xiuying Chen'), arxiv.Result.Author('Mingzhe Li'), arxiv.Result.Author('Shen Gao'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Xin Gao'), arxiv.Result.Author('Xiangliang Zhang')]","In a citation graph, adjacent paper nodes share related scientific terms and
topics. The graph thus conveys unique structure information of document-level
relatedness that can be utilized in the paper summarization task, for exploring
beyond the intra-document information. In this work, we focus on leveraging
citation graphs to improve scientific paper extractive summarization under
different settings. We first propose a Multi-granularity Unsupervised
Summarization model (MUS) as a simple and low-cost solution to the task. MUS
finetunes a pre-trained encoder model on the citation graph by link prediction
tasks. Then, the abstract sentences are extracted from the corresponding paper
considering multi-granularity information. Preliminary results demonstrate that
citation graph is helpful even in a simple unsupervised framework. Motivated by
this, we next propose a Graph-based Supervised Summarization model (GSS) to
achieve more accurate results on the task when large-scale labeled data are
available. Apart from employing the link prediction as an auxiliary task, GSS
introduces a gated sentence encoder and a graph information fusion module to
take advantage of the graph information to polish the sentence representation.
Experiments on a public benchmark dataset show that MUS and GSS bring
substantial improvements over the prior state-of-the-art model."
14745,"Thus, recruiting human        orations such as recognizing discourse-level causal relations
judges to rate stories is indispensable for fair evaluation,      and character emotions need further study.","More sophisticated collab-
teresting and good-quality stories.",despite the cost.,2022-12-09 02:19:07+00:00,Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yuxin Wang'), arxiv.Result.Author('Jieru Lin'), arxiv.Result.Author('Zhiwei Yu'), arxiv.Result.Author('Wei Hu'), arxiv.Result.Author('Börje F. Karlsson')]","Storytelling and narrative are fundamental to human experience, intertwined
with our social and cultural engagement. As such, researchers have long
attempted to create systems that can generate stories automatically. In recent
years, powered by deep learning and massive data resources, automatic story
generation has shown significant advances. However, considerable challenges,
like the need for global coherence in generated stories, still hamper
generative models from reaching the same storytelling ability as human
narrators. To tackle these challenges, many studies seek to inject structured
knowledge into the generation process, which is referred to as structure
knowledge-enhanced story generation. Incorporating external knowledge can
enhance the logical coherence among story events, achieve better knowledge
grounding, and alleviate over-generalization and repetition problems in
stories. This survey provides the latest and comprehensive review of this
research field: (i) we present a systematical taxonomy regarding how existing
methods integrate structured knowledge into story generation; (ii) we summarize
involved story corpora, structured knowledge datasets, and evaluation metrics;
(iii) we give multidimensional insights into the challenges of
knowledge-enhanced story generation and cast light on promising directions for
future study."
14803,We further study the precision of triplets grouped                                      draw the following conclusions.,"The average
precision is 0.71, and the Fleiss’ Kappa score (Fleiss 1971)                                     Results The results are shown in Table 6, from which we
is 0.75.","First, MAPS-KB with our
by frequency.",2022-12-10 10:06:05+00:00,MAPS-KB: A Million-scale Probabilistic Simile Knowledge Base,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Qianyu He'), arxiv.Result.Author('Xintao Wang'), arxiv.Result.Author('Jiaqing Liang'), arxiv.Result.Author('Yanghua Xiao')]","The ability to understand and generate similes is an imperative step to
realize human-level AI. However, there is still a considerable gap between
machine intelligence and human cognition in similes, since deep models based on
statistical distribution tend to favour high-frequency similes. Hence, a
large-scale symbolic knowledge base of similes is required, as it contributes
to the modeling of diverse yet unpopular similes while facilitating additional
evaluation and reasoning. To bridge the gap, we propose a novel framework for
large-scale simile knowledge base construction, as well as two probabilistic
metrics which enable an improved understanding of simile phenomena in natural
language. Overall, we construct MAPS-KB, a million-scale probabilistic simile
knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB
corpora. We conduct sufficient experiments to justify the effectiveness and
necessity of the methods of our framework. We also apply MAPS-KB on three
downstream tasks to achieve state-of-the-art performance, further demonstrating
the value of MAPS-KB."
14822,"com/sustainability
els from scratch, and to enable further research, we       cross-lingual sentence representations.","To limit the pretraining of such mod-

                                                                 15The TPUs reside in the Google Cloud Platform
                                                              which is carbon neutral: https://cloud.google.","In Proceed-
release all models trained as part of this work.",2022-12-11 04:45:50+00:00,IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sumanth Doddapaneni'), arxiv.Result.Author('Rahul Aralikatte'), arxiv.Result.Author('Gowtham Ramesh'), arxiv.Result.Author('Shreya Goyal'), arxiv.Result.Author('Mitesh M. Khapra'), arxiv.Result.Author('Anoop Kunchukuttan'), arxiv.Result.Author('Pratyush Kumar')]","In this work, we introduce IndicXTREME, a benchmark consisting of nine
diverse tasks covering 18 languages from the Indic sub-continent belonging to
four different families. Across languages and tasks, IndicXTREME contains a
total of 103 evaluation sets, of which 51 are new contributions to the
literature. To maintain high quality, we only use human annotators to curate or
translate\footnote{for IndicXParaphrase, where an automatic translation system
is used, a second human verification and correction step is done.} our
datasets. To the best of our knowledge, this is the first effort toward
creating a standard benchmark for Indic languages that aims to test the
zero-shot capabilities of pretrained language models. We also release IndicCorp
v2, an updated and much larger version of IndicCorp that contains 20.9 billion
tokens in 24 languages. We pretrain IndicBERT v2 on IndicCorp v2 and evaluate
it on IndicXTREME to show that it outperforms existing multilingual language
models such as XLM-R and MuRIL."
14823,"To limit the pretraining of such mod-
els from scratch, and to enable further research, we    Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-
release all models trained as part of this work.",kg of CO2e.,"ina Williams, Samuel R. Bowman, Holger Schwenk,
                                                           and Veselin Stoyanov.",2022-12-11 04:45:50+00:00,IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sumanth Doddapaneni'), arxiv.Result.Author('Rahul Aralikatte'), arxiv.Result.Author('Gowtham Ramesh'), arxiv.Result.Author('Shreya Goyal'), arxiv.Result.Author('Mitesh M. Khapra'), arxiv.Result.Author('Anoop Kunchukuttan'), arxiv.Result.Author('Pratyush Kumar')]","In this work, we introduce IndicXTREME, a benchmark consisting of nine
diverse tasks covering 18 languages from the Indic sub-continent belonging to
four different families. Across languages and tasks, IndicXTREME contains a
total of 103 evaluation sets, of which 51 are new contributions to the
literature. To maintain high quality, we only use human annotators to curate or
translate our datasets. To the best of our knowledge, this is the first effort
toward creating a standard benchmark for Indic languages that aims to test the
zero-shot capabilities of pretrained language models. We also release IndicCorp
v2, an updated and much larger version of IndicCorp that contains 20.9 billion
tokens in 24 languages. We pretrain IndicBERT v2 on IndicCorp v2 and evaluate
it on IndicXTREME to show that it outperforms existing multilingual language
models such as XLM-R and MuRIL."
14842,"However, further research should be undertaken to investigate the ability of Transformer-
based to deal with the extreme large label set on datasets which include a greater number of codes
than the MIMIC-III-full dataset.","The
segmentation method gives Transformer-based models the ability to tackle the long text challenge of
ICD coding.","DATA AVAILABILITY

The source code of this study is available at https://github.com/leiboliu/xr-lat.",2022-12-12 12:48:33+00:00,Automated ICD Coding using Extreme Multi-label Long Text Transformer-based Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Leibo Liu'), arxiv.Result.Author('Oscar Perez-Concha'), arxiv.Result.Author('Anthony Nguyen'), arxiv.Result.Author('Vicki Bennett'), arxiv.Result.Author('Louisa Jorm')]","Background: Encouraged by the success of pretrained Transformer models in
many natural language processing tasks, their use for International
Classification of Diseases (ICD) coding tasks is now actively being explored.
In this study, we investigate three types of Transformer-based models, aiming
to address the extreme label set and long text classification challenges that
are posed by automated ICD coding tasks. Methods: The Transformer-based model
PLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD
coding benchmark dataset MIMIC-III. It was chosen as our baseline model to be
further optimised. XR-Transformer, the new SOTA model in the general extreme
multi-label text classification domain, and XR-LAT, a novel adaptation of the
XR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a
recursively trained model chain on a predefined hierarchical code tree with
label-wise attention, knowledge transferring and dynamic negative sampling
mechanisms. Results: Our optimised PLM-ICD model, which was trained with longer
total and chunk sequence lengths, significantly outperformed the current SOTA
PLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The
XR-Transformer model, although SOTA in the general domain, did not perform well
across all metrics. The best XR-LAT based model obtained results that were
competitive with the current SOTA PLM-ICD model, including improving the
macro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA
model for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT
model performs competitively with the previous SOTA PLM-ICD model."
14843,"However, further research should be undertaken to investigate the ability of Transformer-
based to deal with the extreme large label set on datasets which include a greater number of codes
than the MIMIC-III-full dataset.","The
segmentation method gave Transformer-based models the ability to tackle the long text challenge of
ICD coding.","DATA AVAILABILITY

The source code of this study is available at https://github.com/leiboliu/xr-lat.",2022-12-12 12:48:33+00:00,Automated ICD Coding using Extreme Multi-label Long Text Transformer-based Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Leibo Liu'), arxiv.Result.Author('Oscar Perez-Concha'), arxiv.Result.Author('Anthony Nguyen'), arxiv.Result.Author('Vicki Bennett'), arxiv.Result.Author('Louisa Jorm')]","Background: Encouraged by the success of pretrained Transformer models in
many natural language processing tasks, their use for International
Classification of Diseases (ICD) coding tasks is now actively being explored.
In this study, we investigate three types of Transformer-based models, aiming
to address the extreme label set and long text classification challenges that
are posed by automated ICD coding tasks. Methods: The Transformer-based model
PLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD
coding benchmark dataset MIMIC-III. It was chosen as our baseline model to be
further optimised. XR-Transformer, the new SOTA model in the general extreme
multi-label text classification domain, and XR-LAT, a novel adaptation of the
XR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a
recursively trained model chain on a predefined hierarchical code tree with
label-wise attention, knowledge transferring and dynamic negative sampling
mechanisms. Results: Our optimised PLM-ICD model, which was trained with longer
total and chunk sequence lengths, significantly outperformed the current SOTA
PLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The
XR-Transformer model, although SOTA in the general domain, did not perform well
across all metrics. The best XR-LAT based model obtained results that were
competitive with the current SOTA PLM-ICD model, including improving the
macro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA
model for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT
model performs competitively with the previous SOTA PLM-ICD model."
14893,"It       further research in the ﬁeld of multilingual natural
                                             is common to have NLU systems limited to          language understanding is needed to enable nat-
                                             a subset of languages due to lack of avail-       ural language understanding for currently not- or
                                             able data.","To overcome these hurdles,
                                             lingual NLU systems remains a challenge.",They also often vary widely in         under-served languages.,2022-12-13 03:00:36+00:00,The Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition,cs.CL,['cs.CL'],"[arxiv.Result.Author('Christopher Hench'), arxiv.Result.Author('Charith Peris'), arxiv.Result.Author('Jack FitzGerald'), arxiv.Result.Author('Kay Rottmann')]","Despite recent progress in Natural Language Understanding (NLU), the creation
of multilingual NLU systems remains a challenge. It is common to have NLU
systems limited to a subset of languages due to lack of available data. They
also often vary widely in performance. We launch a three-phase approach to
address the limitations in NLU and help propel NLU technology to new heights.
We release a 52 language dataset called the Multilingual Amazon SLU resource
package (SLURP) for Slot-filling, Intent classification, and Virtual assistant
Evaluation, or MASSIVE, in an effort to address parallel data availability for
voice assistants. We organize the Massively Multilingual NLU 2022 Challenge to
provide a competitive environment and push the state-of-the art in the
transferability of models into other languages. Finally, we host the first
Massively Multilingual NLU workshop which brings these components together. The
MMNLU workshop seeks to advance the science behind multilingual NLU by
providing a platform for the presentation of new research in the field and
connecting teams working on this research direction. This paper summarizes the
dataset, workshop and the competition and the findings of each phase."
14894,"The experimental
results revealed that this strategy improved greatly on DBPedia, QQP and SST-2 datasets, but declined
signiﬁcantly on the SNLI and QNLI datasets, and the overall result was the same as the baseline.This

    3https://github.com/jasonwei20/eda_nlp
method was not used in the ﬁnal submission, but it is still worth further research to ﬁnd out why it performs
so differently on different datasets, which will be covered in the discussion.","We
attempted to train a GMM model based on the last hidden states of <mask> token.","Table 5: Ablation results of every modiﬁcations

                               Performance (%) ↑ Time Cost ↓ Comments

Multiple Label Words (3.2)           single +2.73       +0    Compared to “all-in-time”
Selecting Tokens as P0 (3.3)         single +4.22       +0
Rolling Strategy (3.4)               single +0.30      -8hr   Great time cost
                                  DFO single +1.10            No optimization of prompts
Multi-task Loss (3.5)             MLP single +0.25      +0    Bad at QNLI & SNLI
                                   ensemble +2.46
Ensemble (3.6)                                          +0
Curriculum Learning (3.7)            single +1.00      +5hr
Independent MLP (3.8)              ensemble +0.51     +0.7hr

Unsupervised Clustering (3.9)        single +0.01       +0
                               QNLI -2.91 SNLI -1.78

4 Experiments

We conclude the results of previous method in this section.",2022-12-13 04:57:04+00:00,Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiang-Long Song'), arxiv.Result.Author('Wu-He Zou'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Xiao-Lei Qin')]","Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition."
14895,"The experimental
results revealed that this strategy improved greatly on DBPedia, QQP and SST-2 datasets, but declined
signiﬁcantly on the SNLI and QNLI datasets, and the overall result was the same as the baseline.This

    3https://github.com/jasonwei20/eda_nlp
method was not used in the ﬁnal submission, but it is still worth further research to ﬁnd out why it performs
so differently on different datasets, which will be covered in the discussion.","We
attempted to train a GMM model based on the last hidden states of <mask> token.","Table 5: Ablation results of every modiﬁcations

                               Performance (%) ↑ Time Cost ↓ Comments

Multiple Label Words (3.2)           single +2.73       +0    Compared to “all-in-time”
Selecting Tokens as P0 (3.3)         single +4.22       +0
Rolling Strategy (3.4)               single +0.30      -8hr   Great time cost
                                  DFO single +1.10            No optimization of prompts
Multi-task Loss (3.5)             MLP single +0.25      +0    Bad at QNLI & SNLI
                                   ensemble +2.46
Ensemble (3.6)                                          +0
Curriculum Learning (3.7)            single +1.00      +5hr
Independent MLP (3.8)              ensemble +0.51     +0.7hr

Unsupervised Clustering (3.9)        single +0.01       +0
                               QNLI -2.91 SNLI -1.78

4 Experiments

We conclude the results of previous method in this section.",2022-12-13 04:57:04+00:00,Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiang-Long Song'), arxiv.Result.Author('Wu-He Zou'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Xiao-Lei Qin')]","Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition."
14896,"The experimental
results revealed that this strategy improved greatly on DBPedia, QQP and SST-2 datasets, but declined
signiﬁcantly on the SNLI and QNLI datasets, and the overall result was the same as the baseline.This

    3https://github.com/jasonwei20/eda_nlp
method was not used in the ﬁnal submission, but it is still worth further research to ﬁnd out why it performs
so differently on different datasets, which will be covered in the discussion.","We
attempted to train a GMM model based on the last hidden states of <mask> token.","Table 5: Ablation results of every modiﬁcations

                               Performance (%) ↑ Time Cost ↓ Comments

Multiple Label Words (3.2)           single +2.73       +0    Compared to “all-in-time”
Selecting Tokens as P0 (3.3)         single +4.22       +0
Rolling Strategy (3.4)               single +0.30      -8hr   Great time cost
                                  DFO single +1.10            No optimization of prompts
Multi-task Loss (3.5)             MLP single +0.25      +0    Bad at QNLI & SNLI
                                   ensemble +2.46
Ensemble (3.6)                                          +0
Curriculum Learning (3.7)            single +1.00      +5hr
Independent MLP (3.8)              ensemble +0.51     +0.7hr

Unsupervised Clustering (3.9)        single +0.01       +0
                               QNLI -2.91 SNLI -1.78

4 Experiments

We conclude the results of previous method in this section.",2022-12-13 04:57:04+00:00,Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiang-Long Song'), arxiv.Result.Author('Wu-He Zou'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Xiao-Lei Qin'), arxiv.Result.Author('Wei-Dong Zhang')]","Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition."
14962,"The model                       sically, the techniques is based on lexical resources
may serve as a baseline for further study on DL-based                  (e.g.","Ba-
tagging accuracy of 86.52% in F1 score.","WordNet), lexical patterns, and on statistics
Assamese POS tagging.",2022-12-14 05:36:18+00:00,AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG', 'I.2.7']","[arxiv.Result.Author('Dhrubajyoti Pathak'), arxiv.Result.Author('Sukumar Nandi'), arxiv.Result.Author('Priyankoo Sarmah')]","Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging."
14966,further research on A and B yourself.,"you could probably do
                                           datasets.","Introduction                                   Figure 1: The different styles of responses in the hybrid di-
                                                                                                             alogue system, where TOD responses are in grey, and ODD
                                        Previous research on dialogue systems has been distinctly            responses are in orange.",2022-12-14 12:13:34+00:00,Mitigating Negative Style Transfer in Hybrid Dialogue System,cs.CL,['cs.CL'],"[arxiv.Result.Author('Shimin Li'), arxiv.Result.Author('Qinyuan Cheng'), arxiv.Result.Author('Linyang Li'), arxiv.Result.Author('Xipeng Qiu')]","As the functionality of dialogue systems evolves, hybrid dialogue systems
that accomplish user-specific goals and participate in open-topic chitchat with
users are attracting growing attention. Existing research learns both tasks
concurrently utilizing a multi-task fusion technique but ignores the negative
transfer phenomenon induced by the unique textual style differences. Therefore,
contrastive learning based on the latent variable model is used to decouple the
various textual genres in the latent space. We devise supervised and
self-supervised positive and negative sample constructions for diverse
datasets. In addition, to capitalize on the style information contained in the
decoupled latent variables, we employ a style prefix that incorporates latent
variables further to control the generation of responses with varying styles.
We performed extensive experiments on three dialogue datasets, including a
hybrid dialogue dataset and two task-oriented dialogue datasets. The
experimental results demonstrate that our method can mitigate the negative
style transfer issue and achieves state-of-the-art performance on multiple
dialogue datasets."
14978,The proposed dataset could pave the path for further research endeavours.,iii.,"This dataset
         can also be used on platforms like YouTube, Facebook, Instagram, etc., for content
         moderation.",2022-11-14 10:58:22+00:00,Hope Speech Detection on Social Media Platforms,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Pranjal Aggarwal'), arxiv.Result.Author('Pasupuleti Chandana'), arxiv.Result.Author('Jagrut Nemade'), arxiv.Result.Author('Shubham Sharma'), arxiv.Result.Author('Sunil Saumya'), arxiv.Result.Author('Shankar Biradar')]","Since personal computers became widely available in the consumer market, the
amount of harmful content on the internet has significantly expanded. In simple
terms, harmful content is anything online which causes a person distress or
harm. It may include hate speech, violent content, threats, non-hope speech,
etc. The online content must be positive, uplifting and supportive. Over the
past few years, many studies have focused on solving this problem through hate
speech detection, but very few focused on identifying hope speech. This paper
discusses various machine learning approaches to identify a sentence as Hope
Speech, Non-Hope Speech, or a Neutral sentence. The dataset used in the study
contains English YouTube comments and is released as a part of the shared task
""EACL-2021: Hope Speech Detection for Equality, Diversity, and Inclusion"".
Initially, the dataset obtained from the shared task had three classes: Hope
Speech, non-Hope speech, and not in English; however, upon deeper inspection,
we discovered that dataset relabeling is required. A group of undergraduates
was hired to help perform the entire dataset's relabeling task. We experimented
with conventional machine learning models (such as Na\""ive Bayes, logistic
regression and support vector machine) and pre-trained models (such as BERT) on
relabeled data. According to the experimental results, the relabeled data has
achieved a better accuracy for Hope speech identification than the original
data set."
14987,"Our goal is to shed
                                        light on the role of regret in human emotions and decision-making, and provide a foundation for further research in
ReDDIT/ Balouchzahi et al.","In
                                        this paper, we aim to address this gap by proposing a computational approach to studying regret.",this area.,2022-12-14 23:41:57+00:00,ReDDIT: Regret Detection and Domain Identification from Text,cs.CL,"['cs.CL', 'cs.AI', 'cs.CY', 'cs.LG']","[arxiv.Result.Author('Fazlourrahman Balouchzahi'), arxiv.Result.Author('Sabur Butt'), arxiv.Result.Author('Grigori Sidorov'), arxiv.Result.Author('Alexander Gelbukh')]","In this paper, we present a study of regret and its expression on social
media platforms. Specifically, we present a novel dataset of Reddit texts that
have been classified into three classes: Regret by Action, Regret by Inaction,
and No Regret. We then use this dataset to investigate the language used to
express regret on Reddit and to identify the domains of text that are most
commonly associated with regret. Our findings show that Reddit users are most
likely to express regret for past actions, particularly in the domain of
relationships. We also found that deep learning models using GloVe embedding
outperformed other models in all experiments, indicating the effectiveness of
GloVe for representing the meaning and context of words in the domain of
regret. Overall, our study provides valuable insights into the nature and
prevalence of regret on social media, as well as the potential of deep learning
and word embeddings for analyzing and understanding emotional language in
online text. These findings have implications for the development of natural
language processing algorithms and the design of social media platforms that
support emotional expression and communication."
14998,"Further improving the model performance in generating
relevant clarifying questions is an interesting avenue for further research.","We ﬁnd that for 84% of the ambiguous questions, the model is able
to generate the correct clarifying question.",Table 2: Human evaluation of each of the conversational turns.,2022-12-15 12:47:18+00:00,CLAM: Selective Clarification for Ambiguous Questions with Large Language Models,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Lorenz Kuhn'), arxiv.Result.Author('Yarin Gal'), arxiv.Result.Author('Sebastian Farquhar')]","State-of-the-art language models are often accurate on many
question-answering benchmarks with well-defined questions. Yet, in real
settings questions are often unanswerable without asking the user for
clarifying information. We show that current SotA models often do not ask the
user for clarification when presented with imprecise questions and instead
provide incorrect answers or ""hallucinate"". To address this, we introduce CLAM,
a framework that first uses the model to detect ambiguous questions, and if an
ambiguous question is detected, prompts the model to ask the user for
clarification. Furthermore, we show how to construct a scalable and
cost-effective automatic evaluation protocol using an oracle language model
with privileged information to provide clarifying information. We show that our
method achieves a 20.15 percentage point accuracy improvement over SotA on a
novel ambiguous question-answering answering data set derived from TriviaQA."
15020,"Foresight allows A/B testing of historical events, helpful for purposes of further research into historical real-
world data through emulation of virtual trials, real-world risk estimation, analysis of bias, as well as synthetic
data generation.","Discussion

We present a novel deep-learning generative model of entire patient timelines within secondary care across
mental and physical health, incorporating interoperable disease, procedure, medication and symptom concepts.","Foresight allows simulation of a synthetic patient from single time-steps during a time-constrained inpatient
episode all the way to a multi-year timeline of chronic conditions.",2022-12-13 19:06:00+00:00,Foresight -- Deep Generative Modelling of Patient Timelines using Electronic Health Records,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Zeljko Kraljevic'), arxiv.Result.Author('Dan Bean'), arxiv.Result.Author('Anthony Shek'), arxiv.Result.Author('Rebecca Bendayan'), arxiv.Result.Author('Joshua Au Yeung'), arxiv.Result.Author('Alexander Deng'), arxiv.Result.Author('Alfie Baston'), arxiv.Result.Author('Jack Ross'), arxiv.Result.Author('Esther Idowu'), arxiv.Result.Author('James T Teo'), arxiv.Result.Author('Richard J Dobson')]","Electronic Health Records (EHRs) hold detailed longitudinal information about
each patient's health status and general clinical history, a large portion of
which is stored within the unstructured text. Temporal modelling of this
medical history, which considers the sequence of events, can be used to
forecast and simulate future events, estimate risk, suggest alternative
diagnoses or forecast complications. While most prediction approaches use
mainly structured data or a subset of single-domain forecasts and outcomes, we
processed the entire free-text portion of EHRs for longitudinal modelling. We
present Foresight, a novel GPT3-based pipeline that uses NER+L tools (i.e.
MedCAT) to convert document text into structured, coded concepts, followed by
providing probabilistic forecasts for future medical events such as disorders,
medications, symptoms and interventions. Since large portions of EHR data are
in text form, such an approach benefits from a granular and detailed view of a
patient while introducing modest additional noise. On tests in two large UK
hospitals (King's College Hospital, South London and Maudsley) and the US
MIMIC-III dataset precision@10 of 0.80, 0.81 and 0.91 was achieved for
forecasting the next biomedical concept. Foresight was also validated on 34
synthetic patient timelines by 5 clinicians and achieved relevancy of 97% for
the top forecasted candidate disorder. Foresight can be easily trained and
deployed locally as it only requires free-text data (as a minimum). As a
generative model, it can simulate follow-on disorders, medications and
interventions for as many steps as required. Foresight is a general-purpose
model for biomedical concept modelling that can be used for real-world risk
estimation, virtual trials and clinical research to study the progression of
diseases, simulate interventions and counterfactuals, and for educational
purposes."
15023,"Ethical
considerations must be taken into account, and further research is needed to fully understand
the advantages and limitations of AI in this area7.","However, the use
of AI in developing new bioactive compounds is not without challenges and limitations.","Despite these challenges, AI is expected to
significantly contribute to the development of new medications and therapies in the following
few years.",2022-12-08 23:23:39+00:00,"The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies",cs.CL,"['cs.CL', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Alexandre Blanco-Gonzalez'), arxiv.Result.Author('Alfonso Cabezon'), arxiv.Result.Author('Alejandro Seco-Gonzalez'), arxiv.Result.Author('Daniel Conde-Torres'), arxiv.Result.Author('Paula Antelo-Riveiro'), arxiv.Result.Author('Angel Pineiro'), arxiv.Result.Author('Rebeca Garcia-Fandino')]","Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section."
15031,"Here, “F” denotes the REP variant that applies the   Results on MCNC
fusion event encoder; “-S” means that the model use verb
lemmas instead of their senses; “-T” means that the types        To further study the ability of the proposed rich event
are not used; “-RT” means that both the extra semantic roles     encoder under the existing event description (denoted as
and the types are not used (i.e., only three kinds of semantic   REP*), we evaluate it on the MCNC dataset.","The experimental results on MCNC-rich dataset are shown
in Table 3.","The experi-
roles, namely, ARG0, ARG1, and ARG2, are considered).",2022-12-16 05:17:59+00:00,Rich Event Modeling for Script Event Prediction,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Long Bai'), arxiv.Result.Author('Saiping Guan'), arxiv.Result.Author('Zixuan Li'), arxiv.Result.Author('Jiafeng Guo'), arxiv.Result.Author('Xiaolong Jin'), arxiv.Result.Author('Xueqi Cheng')]","Script is a kind of structured knowledge extracted from texts, which contains
a sequence of events. Based on such knowledge, script event prediction aims to
predict the subsequent event. To do so, two aspects should be considered for
events, namely, event description (i.e., what the events should contain) and
event encoding (i.e., how they should be encoded). Most existing methods
describe an event by a verb together with only a few core arguments (i.e.,
subject, object, and indirect object), which are not precise. In addition,
existing event encoders are limited to a fixed number of arguments, which are
not flexible to deal with extra information. Thus, in this paper, we propose
the Rich Event Prediction (REP) framework for script event prediction.
Fundamentally, it is based on the proposed rich event description, which
enriches the existing ones with three kinds of important information, namely,
the senses of verbs, extra semantic roles, and types of participants. REP
contains an event extractor to extract such information from texts. Based on
the extracted rich information, a predictor then selects the most probable
subsequent event. The core component of the predictor is a transformer-based
event encoder to flexibly deal with an arbitrary number of arguments.
Experimental results on the widely used Gigaword Corpus show the effectiveness
of the proposed framework."
15032,"According to history, “authority” and
                                                                             “city” frequently participate in the same events, which im-
To further study the interactions among the predicate and                    plies that they are more likely to participate in the subse-
the arguments, we study the self-attention heatmaps of two                   quent event together.","models obtain the same information from the wrong answer,
                                                                             while REP obtains an additional participant “authority” from
Analysis on Attention Weights                                                the correct answer.","However, SCPredictor cannot handle
events in the development set of MCNC-rich, where REP                        multiple participants for the same semantic role.",2022-12-16 05:17:59+00:00,Rich Event Modeling for Script Event Prediction,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Long Bai'), arxiv.Result.Author('Saiping Guan'), arxiv.Result.Author('Zixuan Li'), arxiv.Result.Author('Jiafeng Guo'), arxiv.Result.Author('Xiaolong Jin'), arxiv.Result.Author('Xueqi Cheng')]","Script is a kind of structured knowledge extracted from texts, which contains
a sequence of events. Based on such knowledge, script event prediction aims to
predict the subsequent event. To do so, two aspects should be considered for
events, namely, event description (i.e., what the events should contain) and
event encoding (i.e., how they should be encoded). Most existing methods
describe an event by a verb together with only a few core arguments (i.e.,
subject, object, and indirect object), which are not precise. In addition,
existing event encoders are limited to a fixed number of arguments, which are
not flexible to deal with extra information. Thus, in this paper, we propose
the Rich Event Prediction (REP) framework for script event prediction.
Fundamentally, it is based on the proposed rich event description, which
enriches the existing ones with three kinds of important information, namely,
the senses of verbs, extra semantic roles, and types of participants. REP
contains an event extractor to extract such information from texts. Based on
the extracted rich information, a predictor then selects the most probable
subsequent event. The core component of the predictor is a transformer-based
event encoder to flexibly deal with an arbitrary number of arguments.
Experimental results on the widely used Gigaword Corpus show the effectiveness
of the proposed framework."
15066,"We use all available data    with all relational similarity scores to further study
for training.","We look into the detailed performance
sentence is a trivial task.","For validation and testing, we ran-        the effect.",2022-12-17 05:25:17+00:00,Relational Sentence Embedding for Flexible Semantic Matching,cs.CL,['cs.CL'],"[arxiv.Result.Author('Bin Wang'), arxiv.Result.Author('Haizhou Li')]","We present Relational Sentence Embedding (RSE), a new paradigm to further
discover the potential of sentence embeddings. Prior work mainly models the
similarity between sentences based on their embedding distance. Because of the
complex semantic meanings conveyed, sentence pairs can have various relation
types, including but not limited to entailment, paraphrasing, and
question-answer. It poses challenges to existing embedding methods to capture
such relational information. We handle the problem by learning associated
relational embeddings. Specifically, a relation-wise translation operation is
applied to the source sentence to infer the corresponding target sentence with
a pre-trained Siamese-based encoder. The fine-grained relational similarity
scores can be computed from learned embeddings. We benchmark our method on 19
datasets covering a wide range of tasks, including semantic textual similarity,
transfer, and domain-specific tasks. Experimental results show that our method
is effective and flexible in modeling sentence relations and outperforms a
series of state-of-the-art sentence embedding methods.
https://github.com/BinWang28/RSE"
15068,"is a cross-lingual task, so aligning the MT model’s
In this section, we present a further study on the re-  representations to multilingual PLMs’ takes less ef-
lationship between choices of PLMs and PRED’s           fort than to monolingual PLMs’.","We
ments over baselines with the help of XLM-R, a          hypothesize the reason is that machine translation
multilingual PLM pre-trained with MLM objective.","This is conﬁrmed
translation performance.",2022-12-17 08:34:20+00:00,"Better Datastore, Better Translation: Generating Datastores from Pre-Trained Models for Nearest Neural Machine Translation",cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiahuan Li'), arxiv.Result.Author('Shanbo Cheng'), arxiv.Result.Author('Zewei Sun'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shujian Huang')]","Nearest Neighbor Machine Translation (kNNMT) is a simple and effective method
of augmenting neural machine translation (NMT) with a token-level nearest
neighbor retrieval mechanism. The effectiveness of kNNMT directly depends on
the quality of retrieved neighbors. However, original kNNMT builds datastores
based on representations from NMT models, which would result in poor retrieval
accuracy when NMT models are not good enough, leading to sub-optimal
translation performance. In this paper, we propose PRED, a framework that
leverages Pre-trained models for Datastores in kNN-MT. Better representations
from pre-trained models allow us to build datastores of better quality. We also
design a novel contrastive alignment objective to mitigate the representation
gap between the NMT model and pre-trained models, enabling the NMT model to
retrieve from better datastores. We conduct extensive experiments on both
bilingual and multilingual translation benchmarks, including WMT17 English
$\leftrightarrow$ Chinese, WMT14 English $\leftrightarrow$ German, IWSLT14
German $\leftrightarrow$ English, and IWSLT14 multilingual datasets. Empirical
results demonstrate the effectiveness of PRED."
15075,"Moreover, we
ments in writing support and automatic phrasing in     expect that further research can ensure that the pro-
debating systems.","Applications include suggesting improve-    fully automatically, as intended.","We have presented an approach        duced claims are of decent quality by being more
that generates multiple candidate optimizations of a   attentive to the veracity of claims.",2022-12-17 16:30:27+00:00,Claim Optimization in Computational Argumentation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Gabriella Skitalinskaya'), arxiv.Result.Author('Maximilian Spliethöver'), arxiv.Result.Author('Henning Wachsmuth')]","An optimal delivery of arguments is key to persuasion in any debate, both for
humans and for AI systems. This requires the use of clear and fluent claims
relevant to the given debate. Prior work has studied the automatic assessment
of argument quality extensively. Yet, no approach actually improves the quality
so far. Our work is the first step towards filling this gap. We propose the
task of claim optimization: to rewrite argumentative claims to optimize their
delivery. As an initial approach, we first generate a candidate set of
optimized claims using a sequence-to-sequence model, such as BART, while taking
into account contextual information. Our key idea is then to rerank generated
candidates with respect to different quality metrics to find the best
optimization. In automatic and human evaluation, we outperform different
reranking baselines on an English corpus, improving 60% of all claims
(worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our
approach often specifies claims with details, whereas it adds less evidence
than humans do. Moreover, its capabilities generalize well to other domains,
such as instructional texts."
15100,"To cal-  Hard Prompt, all outperform MISC, demonstrating
culate this score, rather than dividing the number    the importance of seekers’ persona and highlight-
of unique n-grams by the total number of n-grams,     ing the need for further research into how to better
as done in the original Distinct score, we would use  leverage such information in addition to common-
the model’s vocabulary size as the denominator.","It is worth noting that all models
as the Distinct score has been shown to be biased     with persona information, PAL, PAL (α = 0), and
towards longer sentences (Liu et al., 2022).",sense reasoning.,2022-12-19 04:12:54+00:00,PAL: Persona-Augmented Emotional Support Conversation Generation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jiale Cheng'), arxiv.Result.Author('Sahand Sabour'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Zhuang Chen'), arxiv.Result.Author('Minlie Huang')]","Due to the lack of human resources for mental health support, there is an
increasing demand for employing conversational agents for support. Recent work
has demonstrated the effectiveness of dialogue models in providing emotional
support. As previous studies have demonstrated that seekers' persona is an
important factor for effective support, we investigate whether there are
benefits to modeling such information in dialogue models for support. In this
paper, our empirical analysis verifies that persona has an important impact on
emotional support. Therefore, we propose a framework for dynamically inferring
and modeling seekers' persona. We first train a model for inferring the
seeker's persona from the conversation history. Accordingly, we propose PAL, a
model that leverages persona information and, in conjunction with our
strategy-based controllable generation method, provides personalized emotional
support. Automatic and manual evaluations demonstrate that our proposed model,
PAL, achieves state-of-the-art results, outperforming the baselines on the
studied benchmark. Our code and data are publicly available at
https://github.com/chengjl19/PAL."
15113,"The
achievements of Natural Language Processing (NLP) could accelerate this procedure
and improve the reliability for their further research, in the meantime, the linguistics
could take advantages of the large, well-curated resources as well [5], beneﬁting the
both groups.","Researchers are facing enormous amount of materials to review, which requires con-
siderable energy to investigate and organize crucial information with difﬁculty.","Considering the importance of DDIs in humans health, industry and the econ-
omy, and the substantial amount of cost and time of experimental approaches [1, 8],
it is appropriate to introduce the automatic methodologies into this ﬁeld.",2022-12-19 12:24:32+00:00,An Efficient Drug-Drug Interactions Prediction Technology for Molecularly Intelligent Manufacturing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peng Gao'), arxiv.Result.Author('Feng Gao'), arxiv.Result.Author('Jian-Cheng Ni'), arxiv.Result.Author('Hamido Fujita')]","Drug-Drug Interactions (DDIs) prediction is an essential issue in the
molecular field. Traditional methods of observing DDIs in medical experiments
require plenty of resources and labor. In this paper, we present a
computational model dubbed MedKGQA based on Graph Neural Networks to
automatically predict the DDIs after reading multiple medical documents in the
form of multi-hop machine reading comprehension. We introduced a knowledge
fusion system to obtain the complete nature of drugs and proteins and exploited
a graph reasoning system to infer the drugs and proteins contained in the
documents. Our model significantly improves the performance compared to
previous state-of-the-art models on the QANGAROO MedHop dataset, which obtained
a 4.5% improvement in terms of DDIs prediction accuracy."
15114,"Alter-
ation of in vivo enzyme and/or transporter activities of a new molecular entity by
other concomitant drugs may lead to altered response (safety or efﬁcacy) [12], which
enlightens that we are supposed to follow the biological and chemical facts and man-
age to ﬁnd a more rigorous, explicit, and explicable reading and reasoning process,
and provide the calculation data for further research.","Reading and reasoning abilities on medical literature need
to pay more attention to the logics behind biological or chemical reactions.","In addition, it is crucial to supplement the natures of entities, such as drugs
and proteins, mentioned in scope-limited and independent reading materials.",2022-12-19 12:24:32+00:00,An Efficient Drug-Drug Interactions Prediction Technology for Molecularly Intelligent Manufacturing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peng Gao'), arxiv.Result.Author('Feng Gao'), arxiv.Result.Author('Jian-Cheng Ni'), arxiv.Result.Author('Hamido Fujita')]","Drug-Drug Interactions (DDIs) prediction is an essential issue in the
molecular field. Traditional methods of observing DDIs in medical experiments
require plenty of resources and labor. In this paper, we present a
computational model dubbed MedKGQA based on Graph Neural Networks to
automatically predict the DDIs after reading multiple medical documents in the
form of multi-hop machine reading comprehension. We introduced a knowledge
fusion system to obtain the complete nature of drugs and proteins and exploited
a graph reasoning system to infer the drugs and proteins contained in the
documents. Our model significantly improves the performance compared to
previous state-of-the-art models on the QANGAROO MedHop dataset, which obtained
a 4.5% improvement in terms of DDIs prediction accuracy."
15115,"In addition, there is still much blank area for further research.","This is a small attempt that com-
bine the molecular task and NLP technologies, and we hope that we can transfer this
simple methodology to other ﬁelds, such as legal and physical, where have numerous
well-established KBs and documents resources.","We consider that
the larger training datasets can help the model promote its performance, which could
be addressed by data augmentation or negative sampling.",2022-12-19 12:24:32+00:00,An Efficient Drug-Drug Interactions Prediction Technology for Molecularly Intelligent Manufacturing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peng Gao'), arxiv.Result.Author('Feng Gao'), arxiv.Result.Author('Jian-Cheng Ni'), arxiv.Result.Author('Hamido Fujita')]","Drug-Drug Interactions (DDIs) prediction is an essential issue in the
molecular field. Traditional methods of observing DDIs in medical experiments
require plenty of resources and labor. In this paper, we present a
computational model dubbed MedKGQA based on Graph Neural Networks to
automatically predict the DDIs after reading multiple medical documents in the
form of multi-hop machine reading comprehension. We introduced a knowledge
fusion system to obtain the complete nature of drugs and proteins and exploited
a graph reasoning system to infer the drugs and proteins contained in the
documents. Our model significantly improves the performance compared to
previous state-of-the-art models on the QANGAROO MedHop dataset, which obtained
a 4.5% improvement in terms of DDIs prediction accuracy."
15116,"The
achievements of Natural Language Processing (NLP) could accelerate this procedure
and improve the reliability for their further research, in the meantime, the linguistics
could take advantages of the large, well-curated resources as well [5], beneﬁting the
both groups.","Researchers are facing enormous amount of materials to review, which requires con-
siderable energy to investigate and organize crucial information with difﬁculty.","Considering the importance of DDIs in humans health, industry and the econ-
omy, and the substantial amount of cost and time of experimental approaches [1, 8],
it is appropriate to introduce the automatic methodologies into this ﬁeld.",2022-12-19 12:24:32+00:00,An Efficient Drug-Drug Interactions Prediction Technology for Molecularly Intelligent Manufacturing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peng Gao'), arxiv.Result.Author('Feng Gao'), arxiv.Result.Author('Jian-Cheng Ni')]","Drug-Drug Interactions (DDIs) prediction is an essential issue in the
molecular field. Traditional methods of observing DDIs in medical experiments
require plenty of resources and labor. In this paper, we present a
computational model dubbed MedKGQA based on Graph Neural Networks to
automatically predict the DDIs after reading multiple medical documents in the
form of multi-hop machine reading comprehension. We introduced a knowledge
fusion system to obtain the complete nature of drugs and proteins and exploited
a graph reasoning system to infer the drugs and proteins contained in the
documents. Our model significantly improves the performance compared to
previous state-of-the-art models on the QANGAROO MedHop dataset, which obtained
a 4.5% improvement in terms of DDIs prediction accuracy."
15117,"Alter-
ation of in vivo enzyme and/or transporter activities of a new molecular entity by
other concomitant drugs may lead to altered response (safety or efﬁcacy) [12], which
enlightens that we are supposed to follow the biological and chemical facts and man-
age to ﬁnd a more rigorous, explicit, and explicable reading and reasoning process,
and provide the calculation data for further research.","Reading and reasoning abilities on medical literature need
to pay more attention to the logics behind biological or chemical reactions.","In addition, it is crucial to supplement the natures of entities, such as drugs
and proteins, mentioned in scope-limited and independent reading materials.",2022-12-19 12:24:32+00:00,An Efficient Drug-Drug Interactions Prediction Technology for Molecularly Intelligent Manufacturing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peng Gao'), arxiv.Result.Author('Feng Gao'), arxiv.Result.Author('Jian-Cheng Ni')]","Drug-Drug Interactions (DDIs) prediction is an essential issue in the
molecular field. Traditional methods of observing DDIs in medical experiments
require plenty of resources and labor. In this paper, we present a
computational model dubbed MedKGQA based on Graph Neural Networks to
automatically predict the DDIs after reading multiple medical documents in the
form of multi-hop machine reading comprehension. We introduced a knowledge
fusion system to obtain the complete nature of drugs and proteins and exploited
a graph reasoning system to infer the drugs and proteins contained in the
documents. Our model significantly improves the performance compared to
previous state-of-the-art models on the QANGAROO MedHop dataset, which obtained
a 4.5% improvement in terms of DDIs prediction accuracy."
15118,"In addition, there is still much blank area for further research.","This is a small attempt that com-
bine the molecular task and NLP technologies, and we hope that we can transfer this
simple methodology to other ﬁelds, such as legal and physical, where have numerous
well-established KBs and documents resources.","We consider that
the larger training datasets can help the model promote its performance, which could
be addressed by data augmentation or negative sampling.",2022-12-19 12:24:32+00:00,An Efficient Drug-Drug Interactions Prediction Technology for Molecularly Intelligent Manufacturing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Peng Gao'), arxiv.Result.Author('Feng Gao'), arxiv.Result.Author('Jian-Cheng Ni')]","Drug-Drug Interactions (DDIs) prediction is an essential issue in the
molecular field. Traditional methods of observing DDIs in medical experiments
require plenty of resources and labor. In this paper, we present a
computational model dubbed MedKGQA based on Graph Neural Networks to
automatically predict the DDIs after reading multiple medical documents in the
form of multi-hop machine reading comprehension. We introduced a knowledge
fusion system to obtain the complete nature of drugs and proteins and exploited
a graph reasoning system to infer the drugs and proteins contained in the
documents. Our model significantly improves the performance compared to
previous state-of-the-art models on the QANGAROO MedHop dataset, which obtained
a 4.5% improvement in terms of DDIs prediction accuracy."
15123,"able for researchers and practitioners working on
                                                        MRC and MHQA tasks and hope they will inspire
5.5 Error Analysis                                      further research in this area.","We believe our ﬁndings will be valu-
niﬁcantly improve the performance of the model.","To better understand the role of label smoothing
for the overall architecture, we conducted an error
analysis following the approach of S2G (Wu et al.,
2021b) on our C2FM and C2FM-F1 model.",2022-12-19 14:48:08+00:00,Rethinking Label Smoothing on Multi-hop Question Answering,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zhangyue Yin'), arxiv.Result.Author('Yuxin Wang'), arxiv.Result.Author('Yiguang Wu'), arxiv.Result.Author('Hang Yan'), arxiv.Result.Author('Xiannian Hu'), arxiv.Result.Author('Xinyu Zhang'), arxiv.Result.Author('Zhao Cao'), arxiv.Result.Author('Xuanjing Huang'), arxiv.Result.Author('Xipeng Qiu')]","Label smoothing is a regularization technique widely used in supervised
learning to improve the generalization of models on various tasks, such as
image classification and machine translation. However, the effectiveness of
label smoothing in multi-hop question answering (MHQA) has yet to be well
studied. In this paper, we systematically analyze the role of label smoothing
on various modules of MHQA and propose F1 smoothing, a novel label smoothing
technique specifically designed for machine reading comprehension (MRC) tasks.
We evaluate our method on the HotpotQA dataset and demonstrate its superiority
over several strong baselines, including models that utilize complex attention
mechanisms. Our results suggest that label smoothing can be effective in MHQA,
but the choice of smoothing strategy can significantly affect performance."
15126,"Appropriate Time [30]
                                   The diversity of research in the field in the e-commerce market in
Accurate and prompt answering      terms of expanding the range of the automatic response system,
framework based on customer        further research is expected, such as studies on comparing various
reviews and question-answer        items or recommending items that match them.","Towards Building an Intelligent    Study how to improve the performance of dialogue systems by
Chatbot for Customer Service:      integrating the MRTM model with the state-of-the-art open-source
Learning to Respond at the         dialogue systems.","pairs [21]
                                   The possibility of evaluating the proposed method using reviews in
Generative Feature Language        other natural languages would be an interesting direction that
Models for Mining Implicit         warrants further investigation.",2022-12-16 18:17:07+00:00,Natural Language Processing in Customer Service: A Systematic Review,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Malak Mashaabi'), arxiv.Result.Author('Areej Alotaibi'), arxiv.Result.Author('Hala Qudaih'), arxiv.Result.Author('Raghad Alnashwan'), arxiv.Result.Author('Hend Al-Khalifa')]","Artificial intelligence and natural language processing (NLP) are
increasingly being used in customer service to interact with users and answer
their questions. The goal of this systematic review is to examine existing
research on the use of NLP technology in customer service, including the
research domain, applications, datasets used, and evaluation methods. The
review also looks at the future direction of the field and any significant
limitations. The review covers the time period from 2015 to 2022 and includes
papers from five major scientific databases. Chatbots and question-answering
systems were found to be used in 10 main fields, with the most common use in
general, social networking, and e-commerce areas. Twitter was the second most
commonly used dataset, with most research also using their own original
datasets. Accuracy, precision, recall, and F1 were the most common evaluation
methods. Future work aims to improve the performance and understanding of user
behavior and emotions, and address limitations such as the volume, diversity,
and quality of datasets. This review includes research on different spoken
languages and models and techniques."
15130,"(2022) utilize wav2vec 2.0       for further research and investigation on linguis-
model (Baevski et al., 2020) as a starting model        tics and NLP.","In the speech         explored and unknown styles, which can be useful
domain, Lovenia et al.","Therefore, one future direction is to
before ﬁne-tuning the model on the downstream           broaden the language scope of CSW research.",2022-12-19 17:42:07+00:00,The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges,cs.CL,['cs.CL'],"[arxiv.Result.Author('Genta Indra Winata'), arxiv.Result.Author('Alham Fikri Aji'), arxiv.Result.Author('Zheng-Xin Yong'), arxiv.Result.Author('Thamar Solorio')]","Code-Switching, a common phenomenon in written text and conversation, has
been studied over decades by the natural language processing (NLP) research
community. Initially, code-switching is intensively explored by leveraging
linguistic theories and, currently, more machine-learning oriented approaches
to develop models. We introduce a comprehensive systematic survey on
code-switching research in natural language processing to understand the
progress of the past decades and conceptualize the challenges and tasks on the
code-switching topic. Finally, we summarize the trends and findings and
conclude with a discussion for future direction and open questions for further
investigation."
15131,"The low-resource experiments on Java and Python
4.1.2 Results on MultiCC                                     aim to further study the effectiveness of Multi-
                                                             Coder on low-resource PLs.","Model                   Go           Java       JavaScript       PHP         Python         Ruby           Overall
                   Acc ES        Acc ES         Acc ES       Acc ES        Acc ES        Acc ES          Acc ES

CodeGPT (ours) 74.25 82.54 70.41 77.16 66.48 74.48 73.19 79.28 64.60 77.34 55.76 71.58 67.45 77.06

MultiCoder         74.77  82.91  71.47  77.85  66.99  74.85  74.20  79.89  65.29  77.75  57.59  72.39  68.39    77.61
  - Shared expert  74.07  82.45  70.80  77.29  66.36  74.43  73.51  79.33  64.68  77.41  57.26  72.04  67.78    77.16
  - PL-MoE         73.80  82.16  70.57  77.00  65.93  74.20  73.34  79.00  64.49  77.22  56.78  71.49  67.49    76.85
  - MoE            73.27  81.76  70.29  76.74  65.73  73.93  72.97  78.86  64.47  77.20  56.54  71.70  67.19    76.67

of MultiPL pre-training over the MonoPL pre-                 4.2 Low-Resource Code Completion
training.","Comparing the Ta-
As shown in Table 5, the advantage of the Multi-             ble 6 with Table 5, the MultiCoder shows stronger
Coder is veriﬁed not only on Java and Python but             effectiveness on the low-resource settings than the
also on Go, JavaScript, PHP and Ruby.",2022-12-19 17:50:05+00:00,MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code Completion,cs.CL,['cs.CL'],"[arxiv.Result.Author('Zi Gong'), arxiv.Result.Author('Yinpeng Guo'), arxiv.Result.Author('Pingyi Zhou'), arxiv.Result.Author('Cuiyun Gao'), arxiv.Result.Author('Yasheng Wang'), arxiv.Result.Author('Zenglin Xu')]","Code completion is a valuable topic in both academia and industry. Recently,
large-scale mono-programming-lingual (MonoPL) pre-training models have been
proposed to boost the performance of code completion. However, the code
completion on low-resource programming languages (PL) is difficult for the
data-driven paradigm, while there are plenty of developers using low-resource
PLs. On the other hand, there are few studies exploring the effects of
multi-programming-lingual (MultiPL) pre-training for the code completion,
especially the impact on low-resource programming languages. To this end, we
propose the MultiCoder to enhance the low-resource code completion via MultiPL
pre-training and MultiPL Mixture-of-Experts (MoE) layers. We further propose a
novel PL-level MoE routing strategy (PL-MoE) for improving the code completion
on all PLs. Experimental results on CodeXGLUE and MultiCC demonstrate that 1)
the proposed MultiCoder significantly outperforms the MonoPL baselines on
low-resource programming languages, and 2) the PL-MoE module further boosts the
performance on six programming languages. In addition, we analyze the effects
of the proposed method in details and explore the effectiveness of our method
in a variety of scenarios."
15133,"Since the present study
required judges to provide short justifications of their decisions, we anticipate the
data we collected can be used to further study this question in future work.","But are
there deeper relationships between the rationales used by judges in evaluating
interpretive arguments and their ultimate decisions?","References
LICATO, FIELDS, AND MARJI

Licato, J., Marji, Z., & Abraham, S. (2019).",2022-12-19 18:30:09+00:00,Resoling Open-textured Rules with Templated Interpretive Arguments,cs.CL,['cs.CL'],"[arxiv.Result.Author('John Licato'), arxiv.Result.Author('Logan Fields'), arxiv.Result.Author('Zaid Marji')]","Open-textured terms in written rules are typically settled through
interpretive argumentation. Ongoing work has attempted to catalogue the schemes
used in such interpretive argumentation. But how can the use of these schemes
affect the way in which people actually use and reason over the proper
interpretations of open-textured terms? Using the interpretive
argument-eliciting game Aporia as our framework, we carried out an empirical
study to answer this question. Differing from previous work, we did not allow
participants to argue for interpretations arbitrarily, but to only use
arguments that fit with a given set of interpretive argument templates.
Finally, we analyze the results captured by this new dataset, specifically
focusing on practical implications for the development of
interpretation-capable artificial reasoners."
15134,"(2021),     further study.","The most techni-           Our work sets the stage for several avenues of
cally related work to ours is Kojima et al.","There is signiﬁcant potential for
who focuses on instruction generation and contin-       learning from implicit signals that do not require
ual learning using implicit feedback.",2022-12-19 18:39:43+00:00,Continual Learning for Instruction Following from Realtime Feedback,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Alane Suhr'), arxiv.Result.Author('Yoav Artzi')]","We study the problem of continually training an instruction-following agent
through feedback provided by users during collaborative interactions. During
interaction, human users instruct an agent using natural language, and provide
realtime binary feedback as they observe the agent's instruction execution. We
cast learning as a contextual bandit problem, converting the user feedback to
immediate reward. We evaluate through multiple rounds of human-agent
interactions, demonstrating 15.4% absolute improvement in instruction execution
over time. We also show our approach is robust to several design variations,
and that the feedback signal is roughly equivalent to the learning signal of
supervised demonstration data."
15136,"With this work, we lay down foundations for further research in this space, so that the
strategies here do not just beneﬁt DSI, but the continual learning community in general.","Though DSI models are prone to catastrophic forgetting while we index new docu-
ments, we have shown through extensive experiments that our two proposed solutions to optimize
for the ﬂatter loss basins using SAM and using generative memory alleviate forgetting to a signiﬁ-
cant extent.","ETHICS STATEMENT

Training large models is expensive, and has a detrimental impact on the environment (Strubell et al.,
2019).",2022-12-19 18:59:34+00:00,DSI++: Updating Transformer Memory with New Documents,cs.CL,"['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']","[arxiv.Result.Author('Sanket Vaibhav Mehta'), arxiv.Result.Author('Jai Gupta'), arxiv.Result.Author('Yi Tay'), arxiv.Result.Author('Mostafa Dehghani'), arxiv.Result.Author('Vinh Q. Tran'), arxiv.Result.Author('Jinfeng Rao'), arxiv.Result.Author('Marc Najork'), arxiv.Result.Author('Emma Strubell'), arxiv.Result.Author('Donald Metzler')]","Differentiable Search Indices (DSIs) encode a corpus of documents in the
parameters of a model and use the same model to map queries directly to
relevant document identifiers. Despite the strong performance of DSI models,
deploying them in situations where the corpus changes over time is
computationally expensive because reindexing the corpus requires re-training
the model. In this work, we introduce DSI++, a continual learning challenge for
DSI to incrementally index new documents while being able to answer queries
related to both previously and newly indexed documents. Across different model
scales and document identifier representations, we show that continual indexing
of new documents leads to considerable forgetting of previously indexed
documents. We also hypothesize and verify that the model experiences forgetting
events during training, leading to unstable learning. To mitigate these issues,
we investigate two approaches. The first focuses on modifying the training
dynamics. Flatter minima implicitly alleviate forgetting, so we optimize for
flatter loss basins and show that the model stably memorizes more documents
(+12\%). Next, we introduce a generative memory to sample pseudo-queries for
documents and supplement them during continual indexing to prevent forgetting
for the retrieval task. Extensive experiments on novel continual indexing
benchmarks based on Natural Questions (NQ) and MS MARCO demonstrate that our
proposed solution mitigates forgetting by a significant margin. Concretely, it
improves the average Hits@10 by $+21.1\%$ over competitive baselines for NQ and
requires $6$ times fewer model updates compared to re-training the DSI model
for incrementally indexing five corpora in a sequence."
15137,"We recommend further study
                                                                of the relationship between speciﬁcity and incli-
    5Throughout the paper, results are denoted by * if mod-     nation to interact with an LM, and in particular
els had a signiﬁcant effect relative to TextDavinci, * if sig-  whether speciﬁcity affects reuse of the LM once
niﬁcant relative to TextBabbage, * if signiﬁcant relative to    the initial novelty of this interactive experience has
Davinci, and * if signiﬁcant relative to Jumbo at the p = 0.05  faded.",what users had said.,level using a Tukey-Kramer test.,2022-12-19 18:59:45+00:00,Evaluating Human-Language Model Interaction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mina Lee'), arxiv.Result.Author('Megha Srivastava'), arxiv.Result.Author('Amelia Hardy'), arxiv.Result.Author('John Thickstun'), arxiv.Result.Author('Esin Durmus'), arxiv.Result.Author('Ashwin Paranjape'), arxiv.Result.Author('Ines Gerard-Ursin'), arxiv.Result.Author('Xiang Lisa Li'), arxiv.Result.Author('Faisal Ladhak'), arxiv.Result.Author('Frieda Rong'), arxiv.Result.Author('Rose E. Wang'), arxiv.Result.Author('Minae Kwon'), arxiv.Result.Author('Joon Sung Park'), arxiv.Result.Author('Hancheng Cao'), arxiv.Result.Author('Tony Lee'), arxiv.Result.Author('Rishi Bommasani'), arxiv.Result.Author('Michael Bernstein'), arxiv.Result.Author('Percy Liang')]","Many real-world applications of language models (LMs), such as code
autocomplete and writing assistance, involve human-LM interaction, but the main
LM benchmarks are non-interactive, where a system produces output without human
intervention. To evaluate human-LM interaction, we develop a framework,
Human-AI Language-based Interaction Evaluation (H-LINE), that expands
non-interactive evaluation along three dimensions, capturing (i) the
interactive process, not only the final output; (ii) the first-person
subjective experience, not just a third-party assessment; and (iii) notions of
preference beyond quality. We then design five tasks ranging from goal-oriented
to open-ended to capture different forms of interaction. On four
state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21's J1-Jumbo), we
find that non-interactive performance does not always result in better human-LM
interaction and that first-person and third-party metrics can diverge,
suggesting the importance of examining the nuances of human-LM interaction."
15138,"Longer prompts that
setup the appropriate context might mitigate toxic      Figure 8: [Text summarization] The system’s state
outputs, but its effectiveness merits further study.","Other
safety concerns we observed included generating         ȧ͔ʇ˧˗̓ ^3UHVVDNH\WRPRGLI\XVHULQSXW
personal phone numbers and YouTube accounts                          &OLFNWKHQH[WEXWWRQ
belonging to non-celebrities, as well as discussing                  5HVSRQGWRVXUYH\TXHVWLRQV
more sensitive topics, which we describe further                     )LQLVKWKHVHVVLRQ`
in Appendix Section B.4.8.","consists of a document, user summary history, user in-
                                                        put, and survey questions.",2022-12-19 18:59:45+00:00,Evaluating Human-Language Model Interaction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mina Lee'), arxiv.Result.Author('Megha Srivastava'), arxiv.Result.Author('Amelia Hardy'), arxiv.Result.Author('John Thickstun'), arxiv.Result.Author('Esin Durmus'), arxiv.Result.Author('Ashwin Paranjape'), arxiv.Result.Author('Ines Gerard-Ursin'), arxiv.Result.Author('Xiang Lisa Li'), arxiv.Result.Author('Faisal Ladhak'), arxiv.Result.Author('Frieda Rong'), arxiv.Result.Author('Rose E. Wang'), arxiv.Result.Author('Minae Kwon'), arxiv.Result.Author('Joon Sung Park'), arxiv.Result.Author('Hancheng Cao'), arxiv.Result.Author('Tony Lee'), arxiv.Result.Author('Rishi Bommasani'), arxiv.Result.Author('Michael Bernstein'), arxiv.Result.Author('Percy Liang')]","Many real-world applications of language models (LMs), such as code
autocomplete and writing assistance, involve human-LM interaction, but the main
LM benchmarks are non-interactive, where a system produces output without human
intervention. To evaluate human-LM interaction, we develop a framework,
Human-AI Language-based Interaction Evaluation (H-LINE), that expands
non-interactive evaluation along three dimensions, capturing (i) the
interactive process, not only the final output; (ii) the first-person
subjective experience, not just a third-party assessment; and (iii) notions of
preference beyond quality. We then design five tasks ranging from goal-oriented
to open-ended to capture different forms of interaction. On four
state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21's J1-Jumbo), we
find that non-interactive performance does not always result in better human-LM
interaction and that first-person and third-party metrics can diverge,
suggesting the importance of examining the nuances of human-LM interaction."
15139,"We recommend further study of
and inclination, respectively.","Independently, users               than TextDavinci on all metrics except speci-
may have their own subjective views on whether                  ﬁcity and inclination, we hypothesize that users ex-
system responses are interesting and make them                  pressed an inclination to interact with Davinci be-
want to continue interacting with the system, which             cause its responses were the most speciﬁc to what
are measured by preference metrics interestingness              users had said.","the relationship between speciﬁcity and inclination
                                                                to interact with an LM, and in particular, whether
 1 Instruction tuning improves performance                      speciﬁcity affects reuse of the LM once the initial
on most quality metrics, but not speciﬁcity.",2022-12-19 18:59:45+00:00,Evaluating Human-Language Model Interaction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mina Lee'), arxiv.Result.Author('Megha Srivastava'), arxiv.Result.Author('Amelia Hardy'), arxiv.Result.Author('John Thickstun'), arxiv.Result.Author('Esin Durmus'), arxiv.Result.Author('Ashwin Paranjape'), arxiv.Result.Author('Ines Gerard-Ursin'), arxiv.Result.Author('Xiang Lisa Li'), arxiv.Result.Author('Faisal Ladhak'), arxiv.Result.Author('Frieda Rong'), arxiv.Result.Author('Rose E. Wang'), arxiv.Result.Author('Minae Kwon'), arxiv.Result.Author('Joon Sung Park'), arxiv.Result.Author('Hancheng Cao'), arxiv.Result.Author('Tony Lee'), arxiv.Result.Author('Rishi Bommasani'), arxiv.Result.Author('Michael Bernstein'), arxiv.Result.Author('Percy Liang')]","Many real-world applications of language models (LMs), such as code
autocomplete and writing assistance, involve human-LM interaction. However, the
main LM benchmarks are non-interactive in that a system produces output without
human involvement. To evaluate human-LM interaction, we develop a new
framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands
non-interactive evaluation along three dimensions, capturing (i) the
interactive process, not only the final output; (ii) the first-person
subjective experience, not just a third-party assessment; and (iii) notions of
preference beyond quality. We then design five tasks ranging from goal-oriented
to open-ended to capture different forms of interaction. On four
state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21's J1-Jumbo), we
find that non-interactive performance does not always result in better human-LM
interaction and that first-person and third-party metrics can diverge,
suggesting the importance of examining the nuances of human-LM interaction."
15140,"Longer prompts that                 8VHULQSXW6XUYH\TXHVWLRQV
setup the appropriate context might mitigate toxic
outputs, but its effectiveness merits further study.","One hypothesis for this behavior is that    personal phone numbers and YouTube accounts
these models are capable of providing conﬁdent        belonging to non-celebrities, as well as discussing
more sensitive topics, which we describe further        ƍ͔ǹ͔Ʉ  'RFXPHQW8VHUVXPPDU\KLVWRU\
in Appendix Section B.4.8.","ȧ͔ʇ˧˗̓ ^3UHVVDNH\WRPRGLI\XVHULQSXW
                                                                     &OLFNWKHQH[WEXWWRQ
 3 Users demonstrate diverse engagement be-                          5HVSRQGWRVXUYH\TXHVWLRQV
havior.",2022-12-19 18:59:45+00:00,Evaluating Human-Language Model Interaction,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mina Lee'), arxiv.Result.Author('Megha Srivastava'), arxiv.Result.Author('Amelia Hardy'), arxiv.Result.Author('John Thickstun'), arxiv.Result.Author('Esin Durmus'), arxiv.Result.Author('Ashwin Paranjape'), arxiv.Result.Author('Ines Gerard-Ursin'), arxiv.Result.Author('Xiang Lisa Li'), arxiv.Result.Author('Faisal Ladhak'), arxiv.Result.Author('Frieda Rong'), arxiv.Result.Author('Rose E. Wang'), arxiv.Result.Author('Minae Kwon'), arxiv.Result.Author('Joon Sung Park'), arxiv.Result.Author('Hancheng Cao'), arxiv.Result.Author('Tony Lee'), arxiv.Result.Author('Rishi Bommasani'), arxiv.Result.Author('Michael Bernstein'), arxiv.Result.Author('Percy Liang')]","Many real-world applications of language models (LMs), such as code
autocomplete and writing assistance, involve human-LM interaction. However, the
main LM benchmarks are non-interactive in that a system produces output without
human involvement. To evaluate human-LM interaction, we develop a new
framework, Human-AI Language-based Interaction Evaluation (HALIE), that expands
non-interactive evaluation along three dimensions, capturing (i) the
interactive process, not only the final output; (ii) the first-person
subjective experience, not just a third-party assessment; and (iii) notions of
preference beyond quality. We then design five tasks ranging from goal-oriented
to open-ended to capture different forms of interaction. On four
state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21's J1-Jumbo), we
find that non-interactive performance does not always result in better human-LM
interaction and that first-person and third-party metrics can diverge,
suggesting the importance of examining the nuances of human-LM interaction."
15144,"tems, and may drive further research for zero-shot
                                                          TOD.","We hope that the release of the STARV2
in full-shot settings, it also achieves high accuracy     dataset can serve as a new benchmark for TOD sys-
in zero-shot setups.","Overview of ANYTOD To adhere to a given
program, ANYTOD adopts a neuro-symbolic ap-               2 Related Work
proach (Figure 1).",2022-12-20 01:23:01+00:00,AnyTOD: A Programmable Task-Oriented Dialog System,cs.CL,['cs.CL'],"[arxiv.Result.Author('Jeffrey Zhao'), arxiv.Result.Author('Yuan Cao'), arxiv.Result.Author('Raghav Gupta'), arxiv.Result.Author('Harrison Lee'), arxiv.Result.Author('Abhinav Rastogi'), arxiv.Result.Author('Mingqiu Wang'), arxiv.Result.Author('Hagen Soltau'), arxiv.Result.Author('Izhak Shafran'), arxiv.Result.Author('Yonghui Wu')]","We propose AnyTOD, an end-to-end task-oriented dialog (TOD) system with
zero-shot capability for unseen tasks. We view TOD as a program executed by a
language model (LM), where program logic and ontology is provided by a designer
in the form of a schema. To enable generalization onto unseen schemas and
programs without prior training, AnyTOD adopts a neuro-symbolic approach. A
neural LM keeps track of events that occur during a conversation, and a
symbolic program implementing the dialog policy is executed to recommend next
actions AnyTOD should take. This approach drastically reduces data annotation
and model training requirements, addressing a long-standing challenge in TOD
research: rapidly adapting a TOD system to unseen tasks and domains. We
demonstrate state-of-the-art results on the STAR and ABCD benchmarks, as well
as AnyTOD's strong zero-shot transfer capability in low-resource settings. In
addition, we release STARv2, an updated version of the STAR dataset with richer
data annotations, for benchmarking zero-shot end-to-end TOD models."
15148,"As a comparison, we further study multi-task learn-    Now, we study the importance of the [SUM] to-
ing that encourages embedding relational informa-      ken for a better representation of cross-ﬁle context.",context to summarize their information (Figure 3).,tion into entity representations.,2022-12-20 05:48:09+00:00,CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context,cs.CL,"['cs.CL', 'cs.SE']","[arxiv.Result.Author('Yangruibo Ding'), arxiv.Result.Author('Zijian Wang'), arxiv.Result.Author('Wasi Uddin Ahmad'), arxiv.Result.Author('Murali Krishna Ramanathan'), arxiv.Result.Author('Ramesh Nallapati'), arxiv.Result.Author('Parminder Bhatia'), arxiv.Result.Author('Dan Roth'), arxiv.Result.Author('Bing Xiang')]","While pre-trained language models (LM) for code have achieved great success
in code completion, they generate code conditioned only on the contents within
the file, i.e., in-file context, but ignore the rich semantics in other files
within the same project, i.e., cross-file context, a critical source of
information that is especially useful in modern modular software development.
Such overlooking constrains code language models' capacity in code completion,
leading to unexpected behaviors such as generating hallucinated class member
functions or function calls with unexpected arguments. In this work, we develop
a cross-file context finder tool, CCFINDER, that effectively locates and
retrieves the most relevant cross-file context. We propose CoCoMIC, a framework
that incorporates cross-file context to learn the in-file and cross-file
context jointly on top of pretrained code LMs. CoCoMIC successfully improves
the existing code LM with a 19.30% relative increase in exact match and a
15.41% relative increase in identifier matching for code completion when the
cross-file context is provided."
15149,We further study the effectiveness            open-source autoregressive language model.,"GPT-NeoX-20B: An
only baseline.","In Pro-
of various components in COCOMIC.",2022-12-20 05:48:09+00:00,CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context,cs.CL,"['cs.CL', 'cs.SE']","[arxiv.Result.Author('Yangruibo Ding'), arxiv.Result.Author('Zijian Wang'), arxiv.Result.Author('Wasi Uddin Ahmad'), arxiv.Result.Author('Murali Krishna Ramanathan'), arxiv.Result.Author('Ramesh Nallapati'), arxiv.Result.Author('Parminder Bhatia'), arxiv.Result.Author('Dan Roth'), arxiv.Result.Author('Bing Xiang')]","While pre-trained language models (LM) for code have achieved great success
in code completion, they generate code conditioned only on the contents within
the file, i.e., in-file context, but ignore the rich semantics in other files
within the same project, i.e., cross-file context, a critical source of
information that is especially useful in modern modular software development.
Such overlooking constrains code language models' capacity in code completion,
leading to unexpected behaviors such as generating hallucinated class member
functions or function calls with unexpected arguments. In this work, we develop
a cross-file context finder tool, CCFINDER, that effectively locates and
retrieves the most relevant cross-file context. We propose CoCoMIC, a framework
that incorporates cross-file context to learn the in-file and cross-file
context jointly on top of pretrained code LMs. CoCoMIC successfully improves
the existing code LM with a 19.30% relative increase in exact match and a
15.41% relative increase in identifier matching for code completion when the
cross-file context is provided."
15155,"there is still a signiﬁcant gap in the abductive
                                             reasoning abilities of LLMs and highlights the     2 Related Work
                                             need for further research in this area.",This indicates that       this benchmark.,"Our work
                                             provides a challenging benchmark for future        Mostafazadeh et al.",2022-12-20 09:34:43+00:00,True Detective: A Challenging Benchmark for Deep Abductive Reasoning \\in Foundation Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Maksym Del'), arxiv.Result.Author('Mark Fishel')]","Large language models (LLMs) have demonstrated strong performance in
zero-shot reasoning tasks, including abductive reasoning. This is reflected in
their ability to perform well on current benchmarks in this area. However, to
truly test the limits of LLMs in abductive reasoning, a more challenging
benchmark is needed. In this paper, we present such a benchmark, consisting of
191 long-form mystery stories, each approximately 1200 words in length and
presented in the form of detective puzzles. Each puzzle includes a
multiple-choice question for evaluation sourced from the ""5 Minute Mystery""
platform. Our results show that state-of-the-art GPT models perform
significantly worse than human solvers on this benchmark, with an accuracy of
28\% compared to 47\% for humans. This indicates that there is still a
significant gap in the abductive reasoning abilities of LLMs and highlights the
need for further research in this area. Our work provides a challenging
benchmark for future studies on reasoning in language models and contributes to
a better understanding of the limits of LLMs' abilities."
15157,will help promote further research in this area.,"We hope that our dataset and analysis     Dravidian languages, which exhibit agglutination.","Furthermore, they have relatively free-word order
                                                                                               as compared to European languages which means
                                        1 Introduction                                         that metrics such as BLEU may not always be unre-
                                                                                               liable.",2022-12-20 11:37:22+00:00,IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ananya B. Sai'), arxiv.Result.Author('Vignesh Nagarajan'), arxiv.Result.Author('Tanay Dixit'), arxiv.Result.Author('Raj Dabre'), arxiv.Result.Author('Anoop Kunchukuttan'), arxiv.Result.Author('Pratyush Kumar'), arxiv.Result.Author('Mitesh M. Khapra')]","The rapid growth of machine translation (MT) systems has necessitated
comprehensive studies to meta-evaluate evaluation metrics being used, which
enables a better selection of metrics that best reflect MT quality.
Unfortunately, most of the research focuses on high-resource languages, mainly
English, the observations for which may not always apply to other languages.
Indian languages, having over a billion speakers, are linguistically different
from English, and to date, there has not been a systematic study of evaluating
MT systems from English into Indian languages. In this paper, we fill this gap
by creating an MQM dataset consisting of 7000 fine-grained annotations,
spanning 5 Indian languages and 7 MT systems, and use it to establish
correlations between annotator scores and scores obtained using existing
automatic metrics. Our results show that pre-trained metrics, such as COMET,
have the highest correlations with annotator scores. Additionally, we find that
the metrics do not adequately capture fluency-based errors in Indian languages,
and there is a need to develop metrics focused on Indian languages. We hope
that our dataset and analysis will help promote further research in this area."
15158,promote further research in this area.,"gacheva, Christof Monz, Matteo Negri, Matt Post,
We hope that our dataset and analysis will help            Raphael Rubino, Lucia Specia, and Marco Turchi.",2017.,2022-12-20 11:37:22+00:00,IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages,cs.CL,['cs.CL'],"[arxiv.Result.Author('Ananya B. Sai'), arxiv.Result.Author('Vignesh Nagarajan'), arxiv.Result.Author('Tanay Dixit'), arxiv.Result.Author('Raj Dabre'), arxiv.Result.Author('Anoop Kunchukuttan'), arxiv.Result.Author('Pratyush Kumar'), arxiv.Result.Author('Mitesh M. Khapra')]","The rapid growth of machine translation (MT) systems has necessitated
comprehensive studies to meta-evaluate evaluation metrics being used, which
enables a better selection of metrics that best reflect MT quality.
Unfortunately, most of the research focuses on high-resource languages, mainly
English, the observations for which may not always apply to other languages.
Indian languages, having over a billion speakers, are linguistically different
from English, and to date, there has not been a systematic study of evaluating
MT systems from English into Indian languages. In this paper, we fill this gap
by creating an MQM dataset consisting of 7000 fine-grained annotations,
spanning 5 Indian languages and 7 MT systems, and use it to establish
correlations between annotator scores and scores obtained using existing
automatic metrics. Our results show that pre-trained metrics, such as COMET,
have the highest correlations with annotator scores. Additionally, we find that
the metrics do not adequately capture fluency-based errors in Indian languages,
and there is a need to develop metrics focused on Indian languages. We hope
that our dataset and analysis will help promote further research in this area."
15174,"comprehensive analysis show that 2/3-Triplet
makes a strong baseline and EMMT can be a               Jesse Dodge, Suchin Gururangan, Dallas Card, Roy
promising benchmark for further research.",Experimental results and             Computational Linguistics.,"Schwartz, and Noah A. Smith.",2022-12-20 15:02:38+00:00,Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation,cs.CL,['cs.CL'],"[arxiv.Result.Author('Yaoming Zhu'), arxiv.Result.Author('Zewei Sun'), arxiv.Result.Author('Shanbo Cheng'), arxiv.Result.Author('Yuyang Huang'), arxiv.Result.Author('Liwei Wu'), arxiv.Result.Author('Mingxuan Wang')]","Multimodal machine translation (MMT) aims to improve translation quality by
incorporating information from other modalities, such as vision. Previous MMT
systems mainly focus on better access and use of visual information and tend to
validate their methods on image-related datasets. These studies face two
challenges. First, they can only utilize triple data (bilingual texts with
images), which is scarce; second, current benchmarks are relatively restricted
and do not correspond to realistic scenarios. Therefore, this paper
correspondingly establishes new methods and new datasets for MMT. First, we
propose a framework 2/3-Triplet with two new approaches to enhance MMT by
utilizing large-scale non-triple data: monolingual image-text data and parallel
text-only data. Second, we construct an English-Chinese {e}-commercial
{m}ulti{m}odal {t}ranslation dataset (including training and testing), named
EMMT, where its test set is carefully selected as some words are ambiguous and
shall be translated mistakenly without the help of images. Experiments show
that our method is more suitable for real-world scenarios and can significantly
improve translation performance by using more non-triple data. In addition, our
model also rivals various SOTA models in conventional multimodal translation
benchmarks."
15178,"The sampling is         these tasks holds immense prospects, and therefore
balanced across labels, i.e., each class will have 8       remains an interesting ﬁeld of further research.","The frustration of TopK on
randomly select 500 test samples (due to compu-            multi-choice tasks suggests that applying ICL to
tation limitation) for evaluation.","samples for binary classiﬁcation, and 5 for three-
class classiﬁcation.",2022-12-20 15:55:21+00:00,Self-adaptive In-context Learning,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Zhiyong Wu'), arxiv.Result.Author('Yaoxiang Wang'), arxiv.Result.Author('Jiacheng Ye'), arxiv.Result.Author('Lingpeng Kong')]","Despite the surprising few-shot performance of in-context learning (ICL), it
is still a common practice to randomly sample examples to serve as context.
This paper advocates a new principle for ICL: self-adaptive in-context
learning. The self-adaption mechanism is introduced to help each sample find an
in-context example permutation (i.e., selection and ordering) that can derive
the correct prediction, thus maximizing performance. To validate the
effectiveness of self-adaptive ICL, we propose a general select-then-rank
framework and instantiate it with new selection and ranking algorithms. Upon
extensive evaluation on eight different NLP datasets, our self-adaptive ICL
method achieves a 40% relative improvement over the common practice setting.
Further analysis reveals the enormous potential of self-adaptive ICL that it
might be able to close the gap between ICL and finetuning given more advanced
algorithms. Our code is released to facilitate future research in this area:
https://github.com/Shark-NLP/self-adaptive-ICL"
15185,"Language models are few-shot
not only for further research on the task but also         learners.",2020.,"Advances in neural information processing
in human writing practices.",2022-12-20 17:42:16+00:00,Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models,cs.CL,['cs.CL'],"[arxiv.Result.Author('Evgeniia Razumovskaia'), arxiv.Result.Author('Joshua Maynez'), arxiv.Result.Author('Annie Louis'), arxiv.Result.Author('Mirella Lapata'), arxiv.Result.Author('Shashi Narayan')]","We consider the problem of automatically generating stories in multiple
languages. Compared to prior work in monolingual story generation, crosslingual
story generation allows for more universal research on story planning. We
propose to use Prompting Large Language Models with Plans to study which plan
is optimal for story generation. We consider 4 types of plans and
systematically analyse how the outputs differ for different planning
strategies. The study demonstrates that formulating the plans as
question-answer pairs leads to more coherent generated stories while the plan
gives more control to the story creators."
15189,"Section 4.3 and 4.4 describe the              Regarding Section 3.1, WebTOD systems can infer
techniques that require further research along with      which slot is required by looking at the GUI form
important related works of WebTOD.","Section 4.2 describes the connection             4.2.1 Free from Slot Design
between challenges in SF-TOD and the strength
of WebTOD.",of the API.,2022-12-20 18:18:41+00:00,Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?,cs.CL,['cs.CL'],"[arxiv.Result.Author('Sang-Woo Lee'), arxiv.Result.Author('Sungdong Kim'), arxiv.Result.Author('Donghyeon Ko'), arxiv.Result.Author('Donghoon Ham'), arxiv.Result.Author('Youngki Hong'), arxiv.Result.Author('Shin Ah Oh'), arxiv.Result.Author('Hyunhoon Jung'), arxiv.Result.Author('Wangkyo Jung'), arxiv.Result.Author('Kyunghyun Cho'), arxiv.Result.Author('Donghyun Kwak'), arxiv.Result.Author('Hyungsuk Noh'), arxiv.Result.Author('Woomyoung Park')]","Task-oriented dialogue (TOD) systems are mainly based on the
slot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down
into smaller, controllable units (i.e., slots) to fulfill a specific task. A
series of approaches based on this framework achieved remarkable success on
various TOD benchmarks. However, we argue that the current TOD benchmarks are
limited to surrogate real-world scenarios and that the current TOD models are
still a long way from unraveling the scenarios. In this position paper, we
first identify current status and limitations of SF-TOD systems. After that, we
explore the WebTOD framework, the alternative direction for building a scalable
TOD system when a web/mobile interface is available. In WebTOD, the dialogue
system learns how to understand the web/mobile interface that the human agent
interacts with, powered by a large-scale language model."
15195,"be preﬁxed with “Task:” or not, the input can be
preﬁxed with “Input:” or not, “Output:” can be              We further study how the generated instructions
appended at the end of the prompt, and diﬀerent          diﬀer from the seed instructions that are used to
numbers of break lines can be put in the middle,         prompt the generation.","For example, the instruction can         these instructions.","For each generated instruc-
etc.",2022-12-20 18:59:19+00:00,Self-Instruct: Aligning Language Model with Self Generated Instructions,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Yizhong Wang'), arxiv.Result.Author('Yeganeh Kordi'), arxiv.Result.Author('Swaroop Mishra'), arxiv.Result.Author('Alisa Liu'), arxiv.Result.Author('Noah A. Smith'), arxiv.Result.Author('Daniel Khashabi'), arxiv.Result.Author('Hannaneh Hajishirzi')]","Large ""instruction-tuned"" language models (finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is limited in quantity, diversity, and creativity, therefore hindering the
generality of the tuned model. We introduce Self-Instruct, a framework for
improving the instruction-following capabilities of pretrained language models
by bootstrapping off its own generations. Our pipeline generates instruction,
input, and output samples from a language model, then prunes them before using
them to finetune the original model. Applying our method to vanilla GPT3, we
demonstrate a 33% absolute improvement over the original model on
Super-NaturalInstructions, on par with the performance of InstructGPT_001,
which is trained with private user data and human annotations. For further
evaluation, we curate a set of expert-written instructions for novel tasks, and
show through human evaluation that tuning GPT3 with Self-Instruct outperforms
using existing public instruction datasets by a large margin, leaving only a 5%
absolute gap behind InstructGPT_001. Self-Instruct provides an almost
annotation-free method for aligning pre-trained language models with
instructions, and we release our large synthetic dataset to facilitate future
studies on instruction tuning."
15208,"Meanwhile, we design some met-
Perez et al., 2022), which leads us to further study  rics to measure the morality performance based on
morality modeling in languages.","Meanwhile, previous   cussions to form a moral dialogue dataset, which
works discover some safety defects about moral-       makes dialogue systems learn morality in a very
ity in large language models (Brown et al., 2020;     natural manner.",our framework.,2022-12-21 02:21:37+00:00,MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Constructing Moral Discussions,cs.CL,['cs.CL'],"[arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Zhexin Zhang'), arxiv.Result.Author('Fei Mi'), arxiv.Result.Author('Yasheng Wang'), arxiv.Result.Author('Wei Liu'), arxiv.Result.Author('Jianwei Cui'), arxiv.Result.Author('Bin Wang'), arxiv.Result.Author('Qun Liu'), arxiv.Result.Author('Minlie Huang')]","Morality in dialogue systems has raised great attention in research recently.
A moral dialogue system could better connect users and enhance conversation
engagement by gaining users' trust. In this paper, we propose a framework,
MoralDial to train and evaluate moral dialogue systems. In our framework, we
first explore the communication mechanisms of morality and resolve expressed
morality into four sub-modules. The sub-modules indicate the roadmap for
building a moral dialogue system. Based on that, we design a simple yet
effective method: constructing moral discussions from Rules of Thumb (RoTs)
between simulated specific users and the dialogue system. The constructed
discussion consists of expressing, explaining, and revising the moral views in
dialogue exchanges, which makes conversational models learn morality well in a
natural manner. Furthermore, we propose a novel evaluation method in the
framework. We evaluate the multiple aspects of morality by judging the relation
between dialogue responses and RoTs in discussions, where the multifaceted
nature of morality is particularly considered. Automatic and manual experiments
demonstrate that our framework is promising to train and evaluate moral
dialogue systems."
15216,"with code for experimenting with them, hop-
                                                                     ing our will trigger further research in under-
   We evaluate JASMINE extensively, both intrinsi-                   standing autoregressive models in the Arabic
cally (using perplexity) and extrinsically (on zero-,                context.","We aim to responsibly and gradually release
representing all of Arabic’s varieties to allow our                  our models with interested researchers, along
models to serve wider communities.","one-, and few-shot settings).",2022-12-21 04:21:46+00:00,JASMINE: Arabic GPT Models for Few-Shot Learning,cs.CL,['cs.CL'],"[arxiv.Result.Author('El Moatez Billah Nagoudi'), arxiv.Result.Author('Muhammad Abdul-Mageed'), arxiv.Result.Author('AbdelRahim Elmadany'), arxiv.Result.Author('Alcides Alcoba Inciarte'), arxiv.Result.Author('Md Tawkat Islam Khondaker')]","Task agnostic generative pretraining (GPT) has recently proved promising for
zero- and few-shot learning, gradually diverting attention from the expensive
supervised learning paradigm. Although the community is accumulating knowledge
as to capabilities of English-language autoregressive models such as GPT-3
adopting this generative approach, scholarship about these models remains
acutely Anglocentric. Consequently, the community currently has serious gaps in
its understanding of this class of models, their potential, and their societal
impacts in diverse settings, linguistic traditions, and cultures. To alleviate
this issue for Arabic, a collection of diverse languages and language varieties
with more than $400$ million population, we introduce JASMINE, a suite of
powerful Arabic autoregressive Transformer language models ranging in size
between 300 million-13 billion parameters. We pretrain our new models with
large amounts of diverse data (400GB of text) from different Arabic varieties
and domains. We evaluate JASMINE extensively in both intrinsic and extrinsic
settings, using a comprehensive benchmark for zero- and few-shot learning
across a wide range of NLP tasks. We also carefully develop and release a novel
benchmark for both automated and human evaluation of Arabic autoregressive
models focused at investigating potential social biases, harms, and toxicity in
these models. We aim to responsibly release our models with interested
researchers, along with code for experimenting with them"
15222,"We manually analyzed         provement on existing approaches, this also encour-
40 incorrectly predicted development set examples       ages further research on this important problem.","While an im-
proved upon in the future.","In
per domain.",2022-12-21 11:22:38+00:00,Resolving Indirect Referring Expressions for Entity Selection,cs.CL,['cs.CL'],"[arxiv.Result.Author('Mohammad Javad Hosseini'), arxiv.Result.Author('Filip Radlinski'), arxiv.Result.Author('Silvia Pareti'), arxiv.Result.Author('Annie Louis')]","Recent advances in language modeling have enabled new conversational systems.
In particular, it is often desirable for people to make choices among specified
options when using such systems. We address the problem of reference
resolution, when people use natural expressions to choose between real world
entities. For example, given the choice `Should we make a Simnel cake or a
Pandan cake?' a natural response from a non-expert may be indirect: `let's make
the green one'. Reference resolution has been little studied with natural
expressions, thus robustly understanding such language has large potential for
improving naturalness in dialog, recommendation, and search systems. We create
AltEntities (Alternative Entities), a new public dataset of entity pairs and
utterances, and develop models for the disambiguation problem. Consisting of
42K indirect referring expressions across three domains, it enables for the
first time the study of how large language models can be adapted to this task.
We find they achieve 82%-87% accuracy in realistic settings, which while
reasonable also invites further advances."
15291,"Finally, we
                                              discuss related work on incremental learning and directions for further research.","We
                                              examine how well the three variants place synonyms together and keep homonyms apart, their abil-
                                              ity to recall synonyms as a function of training set size, and their training efﬁciency.",1.,2022-12-22 18:16:58+00:00,Efficient Induction of Language Models Via Probabilistic Concept Formation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Christopher J. MacLellan'), arxiv.Result.Author('Peter Matsakis'), arxiv.Result.Author('Pat Langley')]","This paper presents a novel approach to the acquisition of language models
from corpora. The framework builds on Cobweb, an early system for constructing
taxonomic hierarchies of probabilistic concepts that used a tabular,
attribute-value encoding of training cases and concepts, making it unsuitable
for sequential input like language. In response, we explore three new
extensions to Cobweb -- the Word, Leaf, and Path variants. These systems encode
each training case as an anchor word and surrounding context words, and they
store probabilistic descriptions of concepts as distributions over anchor and
context information. As in the original Cobweb, a performance element sorts a
new instance downward through the hierarchy and uses the final node to predict
missing features. Learning is interleaved with performance, updating concept
probabilities and hierarchy structure as classification occurs. Thus, the new
approaches process training cases in an incremental, online manner that it very
different from most methods for statistical language learning. We examine how
well the three variants place synonyms together and keep homonyms apart, their
ability to recall synonyms as a function of training set size, and their
training efficiency. Finally, we discuss related work on incremental learning
and directions for further research."
15292,"We believe the
current work sets the stage for innovative approaches to statistical language learning that differ con-
siderably from current language inducers and we hope that it inspires further research on human-like
learning.","The Path variant also offers a novel method for
dynamically updating the underlying representation during incremental learning.","Just as Word2Vec led, over the past decade, to large language models with impressive abil-
ities, we hope that our contextual extensions to Cobweb will develop into large-scale human-like
language systems that exhibit efﬁcient, incremental, and continual learning.",2022-12-22 18:16:58+00:00,Efficient Induction of Language Models Via Probabilistic Concept Formation,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Christopher J. MacLellan'), arxiv.Result.Author('Peter Matsakis'), arxiv.Result.Author('Pat Langley')]","This paper presents a novel approach to the acquisition of language models
from corpora. The framework builds on Cobweb, an early system for constructing
taxonomic hierarchies of probabilistic concepts that used a tabular,
attribute-value encoding of training cases and concepts, making it unsuitable
for sequential input like language. In response, we explore three new
extensions to Cobweb -- the Word, Leaf, and Path variants. These systems encode
each training case as an anchor word and surrounding context words, and they
store probabilistic descriptions of concepts as distributions over anchor and
context information. As in the original Cobweb, a performance element sorts a
new instance downward through the hierarchy and uses the final node to predict
missing features. Learning is interleaved with performance, updating concept
probabilities and hierarchy structure as classification occurs. Thus, the new
approaches process training cases in an incremental, online manner that it very
different from most methods for statistical language learning. We examine how
well the three variants place synonyms together and keep homonyms apart, their
ability to recall synonyms as a function of training set size, and their
training efficiency. Finally, we discuss related work on incremental learning
and directions for further research."
15333,"All generations and annotations used in this pa-      is shown a starting sentence which they are told comes from
per are made publicly available to encourage further study of     a real human-written document.","In each round, the player
the future.","They are then shown sub-
the detectability of machine-generated text1.",2022-12-24 06:40:25+00:00,Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,cs.CL,"['cs.CL', 'cs.AI', 'cs.HC', 'I.2.7']","[arxiv.Result.Author('Liam Dugan'), arxiv.Result.Author('Daphne Ippolito'), arxiv.Result.Author('Arun Kirubarajan'), arxiv.Result.Author('Sherry Shi'), arxiv.Result.Author('Chris Callison-Burch')]","As text generated by large language models proliferates, it becomes vital to
understand how humans engage with such text, and whether or not they are able
to detect when the text they are reading did not originate with a human writer.
Prior work on human detection of generated text focuses on the case where an
entire passage is either human-written or machine-generated. In this paper, we
study a more realistic setting where text begins as human-written and
transitions to being generated by state-of-the-art neural language models. We
show that, while annotators often struggle at this task, there is substantial
variance in annotator skill and that given proper incentives, annotators can
improve at this task over time. Furthermore, we conduct a detailed comparison
study and analyze how a variety of variables (model size, decoding strategy,
fine-tuning, prompt genre, etc.) affect human detection performance. Finally,
we collect error annotations from our participants and use them to show that
certain textual genres influence models to make different types of errors and
that certain sentence-level features correlate highly with annotator selection.
We release the RoFT dataset: a collection of over 21,000 human annotations
paired with error classifications to encourage future work in human detection
and evaluation of generated text."
15338,"However, error propagation may be an issue, and further research is needed to improve the extraction of
        entities with complex semantic meanings and low-resource entities using external knowledge.","This approach can potentially improve the understanding and tracking of SDoHs in clinical
        settings.","1 BACKGROUND AND SIGNIFICANCE

Social determinants of health (SDoH) are non-clinical factors inﬂuencing health, functioning, and quality of life
outcomes and risks.",2022-12-24 18:40:23+00:00,A Marker-based Neural Network System for Extracting Social Determinants of Health,cs.CL,['cs.CL'],"[arxiv.Result.Author('Xingmeng Zhao'), arxiv.Result.Author('Anthony Rios')]","Objective. The impact of social determinants of health (SDoH) on patients'
healthcare quality and the disparity is well-known. Many SDoH items are not
coded in structured forms in electronic health records. These items are often
captured in free-text clinical notes, but there are limited methods for
automatically extracting them. We explore a multi-stage pipeline involving
named entity recognition (NER), relation classification (RC), and text
classification methods to extract SDoH information from clinical notes
automatically.
  Materials and Methods. The study uses the N2C2 Shared Task data, which was
collected from two sources of clinical notes: MIMIC-III and University of
Washington Harborview Medical Centers. It contains 4480 social history sections
with full annotation for twelve SDoHs. In order to handle the issue of
overlapping entities, we developed a novel marker-based NER model. We used it
in a multi-stage pipeline to extract SDoH information from clinical notes.
  Results. Our marker-based system outperformed the state-of-the-art span-based
models at handling overlapping entities based on the overall Micro-F1 score
performance. It also achieved state-of-the-art performance compared to the
shared task methods.
  Conclusion. The major finding of this study is that the multi-stage pipeline
effectively extracts SDoH information from clinical notes. This approach can
potentially improve the understanding and tracking of SDoHs in clinical
settings. However, error propagation may be an issue, and further research is
needed to improve the extraction of entities with complex semantic meanings and
low-resource entities using external knowledge."
15366,"Under these
contexts further research could explore the independent inﬂuence of variation in lay raters’ education level,
medical conditions, caregiver status, experience with health care, education level or other relevant factors
on their perceptions of the quality of model outputs.","Further, substantial user experience (UX) and human-computer interaction
(HCI) studies using community-based participatory research methods are necessary before any real world use,
and would be speciﬁc to a developed tool that is beyond the scope of our exploratory research.","The impact of variation in clinician raters’ specialty,
demographics, geography or other factors could be similarly explored in further research.",2022-12-26 14:28:24+00:00,Large Language Models Encode Clinical Knowledge,cs.CL,['cs.CL'],"[arxiv.Result.Author('Karan Singhal'), arxiv.Result.Author('Shekoofeh Azizi'), arxiv.Result.Author('Tao Tu'), arxiv.Result.Author('S. Sara Mahdavi'), arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Hyung Won Chung'), arxiv.Result.Author('Nathan Scales'), arxiv.Result.Author('Ajay Tanwani'), arxiv.Result.Author('Heather Cole-Lewis'), arxiv.Result.Author('Stephen Pfohl'), arxiv.Result.Author('Perry Payne'), arxiv.Result.Author('Martin Seneviratne'), arxiv.Result.Author('Paul Gamble'), arxiv.Result.Author('Chris Kelly'), arxiv.Result.Author('Nathaneal Scharli'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Philip Mansfield'), arxiv.Result.Author('Blaise Aguera y Arcas'), arxiv.Result.Author('Dale Webster'), arxiv.Result.Author('Greg S. Corrado'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Katherine Chou'), arxiv.Result.Author('Juraj Gottweis'), arxiv.Result.Author('Nenad Tomasev'), arxiv.Result.Author('Yun Liu'), arxiv.Result.Author('Alvin Rajkomar'), arxiv.Result.Author('Joelle Barral'), arxiv.Result.Author('Christopher Semturs'), arxiv.Result.Author('Alan Karthikesalingam'), arxiv.Result.Author('Vivek Natarajan')]","Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but the quality bar for medical
and clinical applications is high. Today, attempts to assess models' clinical
knowledge typically rely on automated evaluations on limited benchmarks. There
is no standard to evaluate model predictions and reasoning across a breadth of
tasks. To address this, we present MultiMedQA, a benchmark combining six
existing open question answering datasets spanning professional medical exams,
research, and consumer queries; and HealthSearchQA, a new free-response dataset
of medical questions searched online. We propose a framework for human
evaluation of model answers along multiple axes including factuality,
precision, possible harm, and bias. In addition, we evaluate PaLM (a
540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on
MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves
state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,
MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US
Medical License Exam questions), surpassing prior state-of-the-art by over 17%.
However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve
this we introduce instruction prompt tuning, a parameter-efficient approach for
aligning LLMs to new domains using a few exemplars. The resulting model,
Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show
that comprehension, recall of knowledge, and medical reasoning improve with
model scale and instruction prompt tuning, suggesting the potential utility of
LLMs in medicine. Our human evaluations reveal important limitations of today's
models, reinforcing the importance of both evaluation frameworks and method
development in creating safe, helpful LLM models for clinical applications."
15367,"The impact of variation in clinician raters’ specialty,
demographics, geography or other factors could be similarly explored in further research.","Under these
contexts further research could explore the independent inﬂuence of variation in lay raters’ education level,
medical conditions, caregiver status, experience with health care, education level or other relevant factors
on their perceptions of the quality of model outputs.","6.4 Fairness and equity considerations

Our current approach to evaluating bias is limited and does not serve as a comprehensive assessment of
potential harms, fairness, or equity.",2022-12-26 14:28:24+00:00,Large Language Models Encode Clinical Knowledge,cs.CL,['cs.CL'],"[arxiv.Result.Author('Karan Singhal'), arxiv.Result.Author('Shekoofeh Azizi'), arxiv.Result.Author('Tao Tu'), arxiv.Result.Author('S. Sara Mahdavi'), arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Hyung Won Chung'), arxiv.Result.Author('Nathan Scales'), arxiv.Result.Author('Ajay Tanwani'), arxiv.Result.Author('Heather Cole-Lewis'), arxiv.Result.Author('Stephen Pfohl'), arxiv.Result.Author('Perry Payne'), arxiv.Result.Author('Martin Seneviratne'), arxiv.Result.Author('Paul Gamble'), arxiv.Result.Author('Chris Kelly'), arxiv.Result.Author('Nathaneal Scharli'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Philip Mansfield'), arxiv.Result.Author('Blaise Aguera y Arcas'), arxiv.Result.Author('Dale Webster'), arxiv.Result.Author('Greg S. Corrado'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Katherine Chou'), arxiv.Result.Author('Juraj Gottweis'), arxiv.Result.Author('Nenad Tomasev'), arxiv.Result.Author('Yun Liu'), arxiv.Result.Author('Alvin Rajkomar'), arxiv.Result.Author('Joelle Barral'), arxiv.Result.Author('Christopher Semturs'), arxiv.Result.Author('Alan Karthikesalingam'), arxiv.Result.Author('Vivek Natarajan')]","Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but the quality bar for medical
and clinical applications is high. Today, attempts to assess models' clinical
knowledge typically rely on automated evaluations on limited benchmarks. There
is no standard to evaluate model predictions and reasoning across a breadth of
tasks. To address this, we present MultiMedQA, a benchmark combining six
existing open question answering datasets spanning professional medical exams,
research, and consumer queries; and HealthSearchQA, a new free-response dataset
of medical questions searched online. We propose a framework for human
evaluation of model answers along multiple axes including factuality,
precision, possible harm, and bias. In addition, we evaluate PaLM (a
540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on
MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves
state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,
MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US
Medical License Exam questions), surpassing prior state-of-the-art by over 17%.
However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve
this we introduce instruction prompt tuning, a parameter-efficient approach for
aligning LLMs to new domains using a few exemplars. The resulting model,
Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show
that comprehension, recall of knowledge, and medical reasoning improve with
model scale and instruction prompt tuning, suggesting the potential utility of
LLMs in medicine. Our human evaluations reveal important limitations of today's
models, reinforcing the importance of both evaluation frameworks and method
development in creating safe, helpful LLM models for clinical applications."
15368,"Although recent studies have surprisingly suggested that the validity of reasoning
within a chain-of-thought prompt only contributes a small extent to the impact of this strategy on LLM
performance in multi-step reasoning challenges [87], further research could signiﬁcantly expand the range of
clinicians engaged in prompt construction and the selection of exemplar answers and thereby explore how
variation in multiple axes of the types of clinician participating in this activity impact LLM behavior; for
example clinician demographics, geography, specialism, lived experience and more.","In this study we worked with a panel of four qualiﬁed clinicians to identify the best-demonstration examples
and craft few-shot prompts, all based in either the US or UK, with expertise in internal medicine, pediatrics,
surgery and primary care.","6.5 Ethical considerations

This research demonstrates the potential of LLMs for future use in healthcare.",2022-12-26 14:28:24+00:00,Large Language Models Encode Clinical Knowledge,cs.CL,['cs.CL'],"[arxiv.Result.Author('Karan Singhal'), arxiv.Result.Author('Shekoofeh Azizi'), arxiv.Result.Author('Tao Tu'), arxiv.Result.Author('S. Sara Mahdavi'), arxiv.Result.Author('Jason Wei'), arxiv.Result.Author('Hyung Won Chung'), arxiv.Result.Author('Nathan Scales'), arxiv.Result.Author('Ajay Tanwani'), arxiv.Result.Author('Heather Cole-Lewis'), arxiv.Result.Author('Stephen Pfohl'), arxiv.Result.Author('Perry Payne'), arxiv.Result.Author('Martin Seneviratne'), arxiv.Result.Author('Paul Gamble'), arxiv.Result.Author('Chris Kelly'), arxiv.Result.Author('Nathaneal Scharli'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Philip Mansfield'), arxiv.Result.Author('Blaise Aguera y Arcas'), arxiv.Result.Author('Dale Webster'), arxiv.Result.Author('Greg S. Corrado'), arxiv.Result.Author('Yossi Matias'), arxiv.Result.Author('Katherine Chou'), arxiv.Result.Author('Juraj Gottweis'), arxiv.Result.Author('Nenad Tomasev'), arxiv.Result.Author('Yun Liu'), arxiv.Result.Author('Alvin Rajkomar'), arxiv.Result.Author('Joelle Barral'), arxiv.Result.Author('Christopher Semturs'), arxiv.Result.Author('Alan Karthikesalingam'), arxiv.Result.Author('Vivek Natarajan')]","Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but the quality bar for medical
and clinical applications is high. Today, attempts to assess models' clinical
knowledge typically rely on automated evaluations on limited benchmarks. There
is no standard to evaluate model predictions and reasoning across a breadth of
tasks. To address this, we present MultiMedQA, a benchmark combining six
existing open question answering datasets spanning professional medical exams,
research, and consumer queries; and HealthSearchQA, a new free-response dataset
of medical questions searched online. We propose a framework for human
evaluation of model answers along multiple axes including factuality,
precision, possible harm, and bias. In addition, we evaluate PaLM (a
540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on
MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves
state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,
MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US
Medical License Exam questions), surpassing prior state-of-the-art by over 17%.
However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve
this we introduce instruction prompt tuning, a parameter-efficient approach for
aligning LLMs to new domains using a few exemplars. The resulting model,
Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show
that comprehension, recall of knowledge, and medical reasoning improve with
model scale and instruction prompt tuning, suggesting the potential utility of
LLMs in medicine. Our human evaluations reveal important limitations of today's
models, reinforcing the importance of both evaluation frameworks and method
development in creating safe, helpful LLM models for clinical applications."
15390,"The early research focuses on mi-             to handle heterogeneous data, which requires the system to
                                              grating other QA task methods to HybridQA, while           model these two types of evidence and makes it harder to
                                              with further research, more and more HybridQA-             obtain the correct answers.","Unlike classic QA tasks, HybridQA systems need
                                              tiﬁc domain.",speciﬁc methods have been present.,2022-12-27 12:34:57+00:00,"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions",cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Dingzirui Wang'), arxiv.Result.Author('Longxu Dou'), arxiv.Result.Author('Wanxiang Che')]","Table-and-text hybrid question answering (HybridQA) is a widely used and
challenging NLP task commonly applied in the financial and scientific domain.
The early research focuses on migrating other QA task methods to HybridQA,
while with further research, more and more HybridQA-specific methods have been
present. With the rapid development of HybridQA, the systematic survey is still
under-explored to summarize the main techniques and advance further research.
So we present this work to summarize the current HybridQA benchmarks and
methods, then analyze the challenges and future directions of this task. The
contributions of this paper can be summarized in three folds: (1) first survey,
to our best knowledge, including benchmarks, methods and challenges for
HybridQA; (2) systematic investigation with the reasonable comparison of the
existing systems to articulate their advantages and shortcomings; (3) detailed
analysis of challenges in four important dimensions to shed light on future
directions."
15391,"With the rapid
                                              development of HybridQA, the systematic survey                To advance these emerging and important research topics,
                                              is still under-explored to summarize the main tech-        recently, several high-quality HybridQA benchmarks have
                                              niques and advance further research.",speciﬁc methods have been present.,"So we present         been proposed, which have a variety of settings and pose par-
                                              this work to summarize the current HybridQA                ticular research challenges.",2022-12-27 12:34:57+00:00,"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions",cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Author('Dingzirui Wang'), arxiv.Result.Author('Longxu Dou'), arxiv.Result.Author('Wanxiang Che')]","Table-and-text hybrid question answering (HybridQA) is a widely used and
challenging NLP task commonly applied in the financial and scientific domain.
The early research focuses on migrating other QA task methods to HybridQA,
while with further research, more and more HybridQA-specific methods have been
present. With the rapid development of HybridQA, the systematic survey is still
under-explored to summarize the main techniques and advance further research.
So we present this work to summarize the current HybridQA benchmarks and
methods, then analyze the challenges and future directions of this task. The
contributions of this paper can be summarized in three folds: (1) first survey,
to our best knowledge, including benchmarks, methods and challenges for
HybridQA; (2) systematic investigation with the reasonable comparison of the
existing systems to articulate their advantages and shortcomings; (3) detailed
analysis of challenges in four important dimensions to shed light on future
directions."
15392,"We further study two          Currently, there are two well-known extensions of Spider:
research questions: what causes the performance drop in non-    (1) CSpider (Min and Zhang 2019) (Chinese, schema kept in
English languages?","the overall performance by about 1.8%, reducing the perfor-
mance gap by 29.5% across languages.","(Sec 5.1) and how schema augmentation        English): we improve the existing translation and translate
SAVE improves the model?",2022-12-27 13:58:30+00:00,MultiSpider: Towards Benchmarking Multilingual Text-to-SQL Semantic Parsing,cs.CL,['cs.CL'],"[arxiv.Result.Author('Longxu Dou'), arxiv.Result.Author('Yan Gao'), arxiv.Result.Author('Mingyang Pan'), arxiv.Result.Author('Dingzirui Wang'), arxiv.Result.Author('Wanxiang Che'), arxiv.Result.Author('Dechen Zhan'), arxiv.Result.Author('Jian-Guang Lou')]","Text-to-SQL semantic parsing is an important NLP task, which greatly
facilitates the interaction between users and the database and becomes the key
component in many human-computer interaction systems. Much recent progress in
text-to-SQL has been driven by large-scale datasets, but most of them are
centered on English. In this work, we present MultiSpider, the largest
multilingual text-to-SQL dataset which covers seven languages (English, German,
French, Spanish, Japanese, Chinese, and Vietnamese). Upon MultiSpider, we
further identify the lexical and structural challenges of text-to-SQL (caused
by specific language properties and dialect sayings) and their intensity across
different languages. Experimental results under three typical settings
(zero-shot, monolingual and multilingual) reveal a 6.1% absolute drop in
accuracy in non-English languages. Qualitative and quantitative analyses are
conducted to understand the reason for the performance drop of each language.
Besides the dataset, we also propose a simple schema augmentation framework
SAVe (Schema-Augmentation-with-Verification), which significantly boosts the
overall performance by about 1.8% and closes the 29.5% performance gap across
languages."
