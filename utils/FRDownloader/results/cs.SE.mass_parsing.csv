,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract
287,"However,
work focusing on multiple counterexamples particularly, eﬀective visualization
of multiple counterexamples and highlighting relevant sources for debugging of
multiple counterexamples seems to provide potential for further research.","Sim-
ilarly, the quantitative results of RQ2 show that a considerable amount of
work addresses counterexample minimization and its visualization.","Such
research could support comprehending multiple counterexamples for experts in
formal methods (U1) and domain experts with limited knowledge in formal
methods (U2).",2022-01-09 17:46:34+00:00,A systematic literature review on counterexample explanation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arut Prakash Kaleeswaran'), arxiv.Result.Author('Arne Nordmann'), arxiv.Result.Author('Thomas Vogel'), arxiv.Result.Author('Lars Grunske')]","Context: Safety is of paramount importance for cyber-physical systems in
domains such as automotive, robotics, and avionics. Formal methods such as
model checking are one way to ensure the safety of cyber-physical systems.
However, adoption of formal methods in industry is hindered by usability
issues, particularly the difficulty of understanding model checking results.
Objective: We want to provide an overview of the state of the art for
counterexample explanation by investigating the contexts, techniques, and
evaluation of research approaches in this field. This overview shall provide an
understanding of current and guide future research. Method: To provide this
overview, we conducted a systematic literature review. The survey comprises 116
publications that address counterexample explanations for model checking.
Results: Most primary studies provide counterexample explanations graphically
or as traces, minimize counterexamples to reduce complexity, localize errors in
the models expressed in the input formats of model checkers, support linear
temporal logic or computation tree logic specifications, and use model checkers
of the Symbolic Model Verifier family. Several studies evaluate their
approaches in safety-critical domains with industrial applications. Conclusion:
We notably see a lack of research on counterexample explanation that targets
probabilistic and real-time systems, leverages the explanations to
domain-specific models, and evaluates approaches in user studies. We conclude
by discussing the adequacy of different types of explanations for users with
varying domain and formal methods expertise, showing the need to support
laypersons in understanding model checking results to increase adoption of
formal methods in industry."
288,"However, the number of approaches using
real-time model checkers is very low, which indicates room for further research.","Although
there are several challenges in using probabilistic model checkers as discussed
with RQ6, in this survey we found a considerable number of publications us-
ing probabilistic model checkers.","Similarly, referring to RQ5 for speciﬁcation properties, there are plenty of con-
tributions that address safety properties, leaving room for research on counterex-
ample explanation when verifying liveness properties, especially considering the
increase in autonomy of modern systems.",2022-01-09 17:46:34+00:00,A systematic literature review on counterexample explanation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arut Prakash Kaleeswaran'), arxiv.Result.Author('Arne Nordmann'), arxiv.Result.Author('Thomas Vogel'), arxiv.Result.Author('Lars Grunske')]","Context: Safety is of paramount importance for cyber-physical systems in
domains such as automotive, robotics, and avionics. Formal methods such as
model checking are one way to ensure the safety of cyber-physical systems.
However, adoption of formal methods in industry is hindered by usability
issues, particularly the difficulty of understanding model checking results.
Objective: We want to provide an overview of the state of the art for
counterexample explanation by investigating the contexts, techniques, and
evaluation of research approaches in this field. This overview shall provide an
understanding of current and guide future research. Method: To provide this
overview, we conducted a systematic literature review. The survey comprises 116
publications that address counterexample explanations for model checking.
Results: Most primary studies provide counterexample explanations graphically
or as traces, minimize counterexamples to reduce complexity, localize errors in
the models expressed in the input formats of model checkers, support linear
temporal logic or computation tree logic specifications, and use model checkers
of the Symbolic Model Verifier family. Several studies evaluate their
approaches in safety-critical domains with industrial applications. Conclusion:
We notably see a lack of research on counterexample explanation that targets
probabilistic and real-time systems, leverages the explanations to
domain-specific models, and evaluates approaches in user studies. We conclude
by discussing the adequacy of different types of explanations for users with
varying domain and formal methods expertise, showing the need to support
laypersons in understanding model checking results to increase adoption of
formal methods in industry."
323,"However, as this decomposition was guided by delegation of
responsibilities rather than logical products, which had adverse implications on
the system qualities; this suggests the need for further research on recon-
ciling functional decomposition with organizational decomposition at
this scale.","Likewise, on the SKA project,
this practice was key in delegating the design of such a massive system to mul-
tiple consortia.","Furthermore, the communication-oriented practices adopted by the
SKA regarding ICDs, that is to say, the work towards consensus-driven solu-
tions with the architects involved at both ends of the ICDs, calls for further
research on how to achieve this communication eﬀectively, given the
potentially very diﬀerent knowledge domains involved.",2022-01-10 10:43:43+00:00,System and Software architecting harmonization practices in ultra-large-scale Systems of Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Héctor Cadavid'), arxiv.Result.Author('Vasilios Andrikopoulos'), arxiv.Result.Author('Paris Avgeriou'), arxiv.Result.Author('P. Chris Broekema')]","Context: The challenges posed by the architecting of System of Systems (SoS)
has motivated a significant number of research efforts in the area. However,
literature is lacking when it comes to the interplay between the disciplines
involved in the architecting process, a key factor in addressing these
challenges.Objective: This paper aims to contribute to this line of research by
confirming and extending previously characterized architecting harmonization
practices from Systems and Software Engineering, adopted in an
ultra-large-scale SoS. Method: We conducted a confirmatory case study on the
Square-Kilometre Array (SKA) project to evaluate and extend the findings of our
exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a
pre-study was conducted to map the findings of the previous study with respect
to the SKA context. A survey was then designed, through which the views of 46
SKA engineers were collected and analyzed. Results: The study confirmed in
various degrees the four practices identified in the exploratory case, and
provided further insights about them, namely: (1) the friction between
disciplines caused by long-term system requirements, and how they can be
ameliorated through intermediate, short-term requirements; (2) the way design
choices with a cross-cutting impact on multiple agile teams have an indirect
impact on the system architecture; (3) how these design choices are often
caused by the criteria that guided early system decomposition; (4) the
seemingly recurrent issue with the lack of details about the dynamic elements
of the interfaces; and (5) the use of machine-readable interface specifications
for aligning hardware/software development processes."
324,"Furthermore, the apparent con-
tradiction between the engineers that call for a more holistic view of the system
to make better informed decisions, and those that call for less high-level pre-
scriptive speciﬁcations for the system calls for further research on practices
for reconciling the bottom-up and top-down design approaches that
coexist in systems of this scale.","This calls practitioner to give higher priority to the negotiation

                                                  28
6.4 Subsystems interfacing  7 THREATS TO VALIDITY

of decisions with a cross-cutting impact on multiple agile teams in
the context of a scaled agile framework.","6.4 Subsystems interfacing

This study corroborated the importance of formal interface speciﬁcations as en-
ablers of eﬀective collaboration between cross-disciplinary teams [35].",2022-01-10 10:43:43+00:00,System and Software architecting harmonization practices in ultra-large-scale Systems of Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Héctor Cadavid'), arxiv.Result.Author('Vasilios Andrikopoulos'), arxiv.Result.Author('Paris Avgeriou'), arxiv.Result.Author('P. Chris Broekema')]","Context: The challenges posed by the architecting of System of Systems (SoS)
has motivated a significant number of research efforts in the area. However,
literature is lacking when it comes to the interplay between the disciplines
involved in the architecting process, a key factor in addressing these
challenges.Objective: This paper aims to contribute to this line of research by
confirming and extending previously characterized architecting harmonization
practices from Systems and Software Engineering, adopted in an
ultra-large-scale SoS. Method: We conducted a confirmatory case study on the
Square-Kilometre Array (SKA) project to evaluate and extend the findings of our
exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a
pre-study was conducted to map the findings of the previous study with respect
to the SKA context. A survey was then designed, through which the views of 46
SKA engineers were collected and analyzed. Results: The study confirmed in
various degrees the four practices identified in the exploratory case, and
provided further insights about them, namely: (1) the friction between
disciplines caused by long-term system requirements, and how they can be
ameliorated through intermediate, short-term requirements; (2) the way design
choices with a cross-cutting impact on multiple agile teams have an indirect
impact on the system architecture; (3) how these design choices are often
caused by the criteria that guided early system decomposition; (4) the
seemingly recurrent issue with the lack of details about the dynamic elements
of the interfaces; and (5) the use of machine-readable interface specifications
for aligning hardware/software development processes."
325,"This suggests the need for further research on formalisms or Domain
Speciﬁc Languages (DSLs) for the speciﬁcation of such elements, es-
pecially considering the cross-disciplinary settings of large-scale systems like
LOFAR+ and the SKA and the diﬀerences in the terminology they exhibit, as
discussed by previous studies in SE-SWE interfacing [36].","However,
it also conﬁrmed that the lack of details about the dynamic aspects of the inter-
faces (e.g., time and state-behavioral details) is a recurring phenomenon that is
seemingly causing problems (mostly) on the software side of the involved subsys-
tems.","At the same time, the study further revealed what seems to be a promising
best practice —given the perceived success within the team that adopted it—
for aligning the life cycles of hardware and software teams, as well as managing
the lower-level interfaces of such teams: the use of machine-readable interfacing
speciﬁcations as a single source of truth for both hardware and software teams,
and workﬂows for the generation of relevant artifacts for both parties.",2022-01-10 10:43:43+00:00,System and Software architecting harmonization practices in ultra-large-scale Systems of Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Héctor Cadavid'), arxiv.Result.Author('Vasilios Andrikopoulos'), arxiv.Result.Author('Paris Avgeriou'), arxiv.Result.Author('P. Chris Broekema')]","Context: The challenges posed by the architecting of System of Systems (SoS)
has motivated a significant number of research efforts in the area. However,
literature is lacking when it comes to the interplay between the disciplines
involved in the architecting process, a key factor in addressing these
challenges.Objective: This paper aims to contribute to this line of research by
confirming and extending previously characterized architecting harmonization
practices from Systems and Software Engineering, adopted in an
ultra-large-scale SoS. Method: We conducted a confirmatory case study on the
Square-Kilometre Array (SKA) project to evaluate and extend the findings of our
exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a
pre-study was conducted to map the findings of the previous study with respect
to the SKA context. A survey was then designed, through which the views of 46
SKA engineers were collected and analyzed. Results: The study confirmed in
various degrees the four practices identified in the exploratory case, and
provided further insights about them, namely: (1) the friction between
disciplines caused by long-term system requirements, and how they can be
ameliorated through intermediate, short-term requirements; (2) the way design
choices with a cross-cutting impact on multiple agile teams have an indirect
impact on the system architecture; (3) how these design choices are often
caused by the criteria that guided early system decomposition; (4) the
seemingly recurrent issue with the lack of details about the dynamic elements
of the interfaces; and (5) the use of machine-readable interface specifications
for aligning hardware/software development processes."
326,"Although
this practice is well known in the semi-conductors domain, its success in the
context of large-scale systems calls for further research on how to make it
consistent with the higher-level interfaces (i.e., ICDs) that are often
deﬁned in human-only readable formats, and from which the lower-level
interfaces are actually meant to be derived.","At the same time, the study further revealed what seems to be a promising
best practice —given the perceived success within the team that adopted it—
for aligning the life cycles of hardware and software teams, as well as managing
the lower-level interfaces of such teams: the use of machine-readable interfacing
speciﬁcations as a single source of truth for both hardware and software teams,
and workﬂows for the generation of relevant artifacts for both parties.","7 Threats to validity

In the following we discuss potential threats to the validity of this study, and
the steps taken to mitigate them.",2022-01-10 10:43:43+00:00,System and Software architecting harmonization practices in ultra-large-scale Systems of Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Héctor Cadavid'), arxiv.Result.Author('Vasilios Andrikopoulos'), arxiv.Result.Author('Paris Avgeriou'), arxiv.Result.Author('P. Chris Broekema')]","Context: The challenges posed by the architecting of System of Systems (SoS)
has motivated a significant number of research efforts in the area. However,
literature is lacking when it comes to the interplay between the disciplines
involved in the architecting process, a key factor in addressing these
challenges.Objective: This paper aims to contribute to this line of research by
confirming and extending previously characterized architecting harmonization
practices from Systems and Software Engineering, adopted in an
ultra-large-scale SoS. Method: We conducted a confirmatory case study on the
Square-Kilometre Array (SKA) project to evaluate and extend the findings of our
exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a
pre-study was conducted to map the findings of the previous study with respect
to the SKA context. A survey was then designed, through which the views of 46
SKA engineers were collected and analyzed. Results: The study confirmed in
various degrees the four practices identified in the exploratory case, and
provided further insights about them, namely: (1) the friction between
disciplines caused by long-term system requirements, and how they can be
ameliorated through intermediate, short-term requirements; (2) the way design
choices with a cross-cutting impact on multiple agile teams have an indirect
impact on the system architecture; (3) how these design choices are often
caused by the criteria that guided early system decomposition; (4) the
seemingly recurrent issue with the lack of details about the dynamic elements
of the interfaces; and (5) the use of machine-readable interface specifications
for aligning hardware/software development processes."
327,"When it comes to
the early partition of the system, and their demarcation with ICDs, this study
highlights the importance of further attention —and further research— on (1)

   12In contrast to naturally-occurring ones.","For instance, this study suggests
that having short-term intermediate requirements (derived from the long-term
ones) would improve the alignment between the system originally envisioned on
the early stages of the project (when such requirements are deﬁned), and the
system understanding that emerges from the agile process.","31
                                                                             8 CONCLUSIONS

system breakdown techniques that better align functional decomposition with
large-scale responsibilities distribution, and (2) the way software-speciﬁc design
decisions, with impact across multiple agile teams are made, as a measure to
prevent architectural smells.",2022-01-10 10:43:43+00:00,System and Software architecting harmonization practices in ultra-large-scale Systems of Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Héctor Cadavid'), arxiv.Result.Author('Vasilios Andrikopoulos'), arxiv.Result.Author('Paris Avgeriou'), arxiv.Result.Author('P. Chris Broekema')]","Context: The challenges posed by the architecting of System of Systems (SoS)
has motivated a significant number of research efforts in the area. However,
literature is lacking when it comes to the interplay between the disciplines
involved in the architecting process, a key factor in addressing these
challenges.Objective: This paper aims to contribute to this line of research by
confirming and extending previously characterized architecting harmonization
practices from Systems and Software Engineering, adopted in an
ultra-large-scale SoS. Method: We conducted a confirmatory case study on the
Square-Kilometre Array (SKA) project to evaluate and extend the findings of our
exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a
pre-study was conducted to map the findings of the previous study with respect
to the SKA context. A survey was then designed, through which the views of 46
SKA engineers were collected and analyzed. Results: The study confirmed in
various degrees the four practices identified in the exploratory case, and
provided further insights about them, namely: (1) the friction between
disciplines caused by long-term system requirements, and how they can be
ameliorated through intermediate, short-term requirements; (2) the way design
choices with a cross-cutting impact on multiple agile teams have an indirect
impact on the system architecture; (3) how these design choices are often
caused by the criteria that guided early system decomposition; (4) the
seemingly recurrent issue with the lack of details about the dynamic elements
of the interfaces; and (5) the use of machine-readable interface specifications
for aligning hardware/software development processes."
426,"The taxonomy pro-
posed in this paper paves the way for the construction of an ontology in a

                                               39
further study, to perform semantic queries (SPARQL18) on its entities (i.e.,
patterns), and forms the knowledge base of BLADE.","This study is part of a bigger project that aims to empower software ar-
chitects with a framework for the design and implementation of blockchain-
based applications [55], notably with the construction of BLADE (Blockchain
Automated Decision Engine), a decision-making tool to select the most suit-
able blockchain and patterns for a given context [56].","Building this ontology
will also be an opportunity to further explore links between patterns and
groups of patterns, an aspect partially left from outside the scope of this pa-
per.",2022-01-12 09:12:19+00:00,Blockchain software patterns for the design of decentralized applications: A systematic literature review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nicolas Six'), arxiv.Result.Author('Nicolas Herbaut'), arxiv.Result.Author('Camille Salinesi')]","A software pattern is a reusable solution to address a commonly occurring
problem within a given context when designing software. Using patterns is a
common practice for software architects to ensure software quality. Many
pattern collections have been proposed for a large number of application
domains. However, because of the technology's recentness, there are only a few
available collections with a lack of extensive testing in industrial blockchain
applications. It is also difficult for software architects to adequately apply
blockchain patterns in their applications, as it requires deep knowledge of
blockchain technology. Through a systematic literature review, this paper has
identified 120 unique blockchain-related patterns and proposes a pattern
taxonomy composed of multiple categories, built from the extracted pattern
collection. The purpose of this collection is to map, classify, and describe
all available patterns across the literature to help readers make adequate
decisions regarding blockchain pattern selection. This study also shows
potential applications of those patterns and identifies the relationships
between blockchain patterns and other non-blockchain software patterns."
482,"In this paper we discuss the identified challenges for such a                          self-decision [17, 20], allowing them to respond to physical changes
                                        reference software architecture, present its preliminary status, and                         in the production environment in various ways - by stopping ma-
                                        sketch our further research vision in this project.","AI-enabled systems in smart manufacturing
                                        in the automotive, energy systems, and precision machining do-                               are capable of self-sensing, self-adaptation, self-organization, and
                                        main.","chines, adapting production tasks, or suggesting the change of
                                                                                                                                     production parameters.",2022-01-13 10:43:49+00:00,Towards a Reference Software Architecture for Human-AI Teaming in Smart Manufacturing,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Philipp Haindl'), arxiv.Result.Author('Georg Buchgeher'), arxiv.Result.Author('Maqbool Khan'), arxiv.Result.Author('Bernhard Moser')]","With the proliferation of AI-enabled software systems in smart manufacturing,
the role of such systems moves away from a reactive to a proactive role that
provides context-specific support to manufacturing operators. In the frame of
the EU funded Teaming.AI project, we identified the monitoring of teaming
aspects in human-AI collaboration, the runtime monitoring and validation of
ethical policies, and the support for experimentation with data and machine
learning algorithms as the most relevant challenges for human-AI teaming in
smart manufacturing. Based on these challenges, we developed a reference
software architecture based on knowledge graphs, tracking and scene analysis,
and components for relational machine learning with a particular focus on its
scalability. Our approach uses knowledge graphs to capture product- and process
specific knowledge in the manufacturing process and to utilize it for
relational machine learning. This allows for context-specific recommendations
for actions in the manufacturing process for the optimization of product
quality and the prevention of physical harm. The empirical validation of this
software architecture will be conducted in cooperation with three large-scale
companies in the automotive, energy systems, and precision machining domain. In
this paper we discuss the identified challenges for such a reference software
architecture, present its preliminary status, and sketch our further research
vision in this project."
483,"With regards to collaborative AI
                                        sketch our further research vision in this project.","In this paper we discuss the identified challenges for such a                          mutual trust into the other’s capabilities, primarily resulting from
                                        reference software architecture, present its preliminary status, and                         self-sensing and self-adaption.","systems, this demands a high degree of situational awareness for
                                                                                                                                     each other actor’s needs, knowledge of the production process, and
                                        CCS CONCEPTS                                                                                 its adjustable parameters.",2022-01-13 10:43:49+00:00,Towards a Reference Software Architecture for Human-AI Teaming in Smart Manufacturing,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Philipp Haindl'), arxiv.Result.Author('Georg Buchgeher'), arxiv.Result.Author('Maqbool Khan'), arxiv.Result.Author('Bernhard Moser')]","With the proliferation of AI-enabled software systems in smart manufacturing,
the role of such systems moves away from a reactive to a proactive role that
provides context-specific support to manufacturing operators. In the frame of
the EU funded Teaming.AI project, we identified the monitoring of teaming
aspects in human-AI collaboration, the runtime monitoring and validation of
ethical policies, and the support for experimentation with data and machine
learning algorithms as the most relevant challenges for human-AI teaming in
smart manufacturing. Based on these challenges, we developed a reference
software architecture based on knowledge graphs, tracking and scene analysis,
and components for relational machine learning with a particular focus on its
scalability. Our approach uses knowledge graphs to capture product- and process
specific knowledge in the manufacturing process and to utilize it for
relational machine learning. This allows for context-specific recommendations
for actions in the manufacturing process for the optimization of product
quality and the prevention of physical harm. The empirical validation of this
software architecture will be conducted in cooperation with three large-scale
companies in the automotive, energy systems, and precision machining domain. In
this paper we discuss the identified challenges for such a reference software
architecture, present its preliminary status, and sketch our further research
vision in this project."
484,"With regards to collaborative AI
                                        sketch our further research vision in this project.","In this paper we discuss the identified challenges for such a                          mutual trust into the other’s capabilities, primarily resulting from
                                        reference software architecture, present its preliminary status, and                         self-sensing and self-adaption.","systems, this demands a high degree of situational awareness for
                                                                                                                                     each other actor’s needs, knowledge of the production process, and
                                        CCS CONCEPTS                                                                                 its adjustable parameters.",2022-01-13 10:43:49+00:00,Towards a Reference Software Architecture for Human-AI Teaming in Smart Manufacturing,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Philipp Haindl'), arxiv.Result.Author('Georg Buchgeher'), arxiv.Result.Author('Maqbool Khan'), arxiv.Result.Author('Bernhard Moser')]","With the proliferation of AI-enabled software systems in smart manufacturing,
the role of such systems moves away from a reactive to a proactive role that
provides context-specific support to manufacturing operators. In the frame of
the EU funded Teaming.AI project, we identified the monitoring of teaming
aspects in human-AI collaboration, the runtime monitoring and validation of
ethical policies, and the support for experimentation with data and machine
learning algorithms as the most relevant challenges for human-AI teaming in
smart manufacturing. Based on these challenges, we developed a reference
software architecture based on knowledge graphs, tracking and scene analysis,
and components for relational machine learning with a particular focus on its
scalability. Our approach uses knowledge graphs to capture product- and process
specific knowledge in the manufacturing process and to utilize it for
relational machine learning. This allows for context-specific recommendations
for actions in the manufacturing process for the optimization of product
quality and the prevention of physical harm. The empirical validation of this
software architecture will be conducted in cooperation with three large-scale
companies in the automotive, energy systems, and precision machining domain. In
this paper we discuss the identified challenges for such a reference software
architecture, present its preliminary status, and sketch our further research
vision in this project."
485,"With regards to collaborative AI
                                        sketch our further research vision in this project.","In this paper we discuss the identified challenges for such a                          mutual trust into the other’s capabilities, primarily resulting from
                                        reference software architecture, present its preliminary status, and                         self-sensing and self-adaption.","systems, this demands a high degree of situational awareness for
                                                                                                                                     each other actor’s needs, knowledge of the production process, and
                                        CCS CONCEPTS                                                                                 its adjustable parameters.",2022-01-13 10:43:49+00:00,Towards a Reference Software Architecture for Human-AI Teaming in Smart Manufacturing,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Philipp Haindl'), arxiv.Result.Author('Georg Buchgeher'), arxiv.Result.Author('Maqbool Khan'), arxiv.Result.Author('Bernhard Moser')]","With the proliferation of AI-enabled software systems in smart manufacturing,
the role of such systems moves away from a reactive to a proactive role that
provides context-specific support to manufacturing operators. In the frame of
the EU funded Teaming.AI project, we identified the monitoring of teaming
aspects in human-AI collaboration, the runtime monitoring and validation of
ethical policies, and the support for experimentation with data and machine
learning algorithms as the most relevant challenges for human-AI teaming in
smart manufacturing. Based on these challenges, we developed a reference
software architecture based on knowledge graphs, tracking and scene analysis,
and components for relational machine learning with a particular focus on its
scalability. Our approach uses knowledge graphs to capture product- and process
specific knowledge in the manufacturing process and to utilize it for
relational machine learning. This allows for context-specific recommendations
for actions in the manufacturing process for the optimization of product
quality and the prevention of physical harm. The empirical validation of this
software architecture will be conducted in cooperation with three large-scale
companies in the automotive, energy systems, and precision machining domain. In
this paper we discuss the identified challenges for such a reference software
architecture, present its preliminary status, and sketch our further research
vision in this project."
486,"In this paper we discuss the identified challenges for such a                          in the production environment in various ways - by stopping ma-
                                        reference software architecture, present its preliminary status, and                         chines, adapting production tasks, or suggesting the change of
                                        sketch our further research vision in this project.","AI-enabled systems in smart manufacturing
                                        will be conducted in cooperation with three large-scale companies                            are capable of self-sensing, self-adaptation, self-organization, and
                                        in the automotive, energy systems, and precision machining do-                               self-decision [17, 20], allowing them to respond to physical changes
                                        main.",production parameters.,2022-01-13 10:43:49+00:00,Towards a Reference Software Architecture for Human-AI Teaming in Smart Manufacturing,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Philipp Haindl'), arxiv.Result.Author('Georg Buchgeher'), arxiv.Result.Author('Maqbool Khan'), arxiv.Result.Author('Bernhard Moser')]","With the proliferation of AI-enabled software systems in smart manufacturing,
the role of such systems moves away from a reactive to a proactive role that
provides context-specific support to manufacturing operators. In the frame of
the EU funded Teaming.AI project, we identified the monitoring of teaming
aspects in human-AI collaboration, the runtime monitoring and validation of
ethical policies, and the support for experimentation with data and machine
learning algorithms as the most relevant challenges for human-AI teaming in
smart manufacturing. Based on these challenges, we developed a reference
software architecture based on knowledge graphs, tracking and scene analysis,
and components for relational machine learning with a particular focus on its
scalability. Our approach uses knowledge graphs to capture product- and process
specific knowledge in the manufacturing process and to utilize it for
relational machine learning. This allows for context-specific recommendations
for actions in the manufacturing process for the optimization of product
quality and the prevention of physical harm. The empirical validation of this
software architecture will be conducted in cooperation with three large-scale
companies in the automotive, energy systems, and precision machining domain. In
this paper we discuss the identified challenges for such a reference software
architecture, present its preliminary status, and sketch our further research
vision in this project."
545,"To facilitate further research in this area, we publish       name, a small description in some natural language, and some
                                        the source code of our models and one of the collected datasets.","For example, in a bug tracking system,
                                        models outperform existing models adapted to handle stack             errors are usually present in the form of a bug report: a
                                        traces.","additional meta information (the date the error was introduced,
                                                                                                              priority, severity, etc.).",2022-01-14 00:16:57+00:00,DapStep: Deep Assignee Prediction for Stack Trace Error rePresentation,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Denis Sushentsev'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The task of finding the best developer to fix a bug is called bug triage.
Most of the existing approaches consider the bug triage task as a
classification problem, however, classification is not appropriate when the
sets of classes change over time (as developers often do in a project).
Furthermore, to the best of our knowledge, all the existing models use textual
sources of information, i.e., bug descriptions, which are not always available.
  In this work, we explore the applicability of existing solutions for the bug
triage problem when stack traces are used as the main data source of bug
reports. Additionally, we reformulate this task as a ranking problem and
propose new deep learning models to solve it. The models are based on a
bidirectional recurrent neural network with attention and on a convolutional
neural network, with the weights of the models optimized using a ranking loss
function. To improve the quality of ranking, we propose using additional
information from version control system annotations. Two approaches are
proposed for extracting features from annotations: manual and using an
additional neural network. To evaluate our models, we collected two datasets of
real-world stack traces. Our experiments show that the proposed models
outperform existing models adapted to handle stack traces. To facilitate
further research in this area, we publish the source code of our models and one
of the collected datasets."
546,"The public dataset is published for
                  xjoin  =   [x  T  ;  xsim;  x  T  ;  xTf eat ],          (3)         further research and can be found in the DapStep repository:
                                 q               d                                     https://github.com/Sushentsev/DapStep.","The public dataset is
proposing to form the following vector:                                                a subset of the private dataset that contains stack traces that
                                                                                       relate to public repositories.","where xq, xd, xfeat stand for the bug embedding, the de-                                  The larger, private dataset contains a total of 11,139 bug
                                                                                       reports from the crash system from October 2018 to April
veloper embedding, and additional stack trace features de-                             2021.",2022-01-14 00:16:57+00:00,DapStep: Deep Assignee Prediction for Stack Trace Error rePresentation,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Denis Sushentsev'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The task of finding the best developer to fix a bug is called bug triage.
Most of the existing approaches consider the bug triage task as a
classification problem, however, classification is not appropriate when the
sets of classes change over time (as developers often do in a project).
Furthermore, to the best of our knowledge, all the existing models use textual
sources of information, i.e., bug descriptions, which are not always available.
  In this work, we explore the applicability of existing solutions for the bug
triage problem when stack traces are used as the main data source of bug
reports. Additionally, we reformulate this task as a ranking problem and
propose new deep learning models to solve it. The models are based on a
bidirectional recurrent neural network with attention and on a convolutional
neural network, with the weights of the models optimized using a ranking loss
function. To improve the quality of ranking, we propose using additional
information from version control system annotations. Two approaches are
proposed for extracting features from annotations: manual and using an
additional neural network. To evaluate our models, we collected two datasets of
real-world stack traces. Our experiments show that the proposed models
outperform existing models adapted to handle stack traces. To facilitate
further research in this area, we publish the source code of our models and one
of the collected datasets."
547,"We believe that this dataset can be useful for                                                               100
further research in the ﬁeld and can facilitate the development      Regularization coefﬁcient          1e-5                 ∞
of models, which work with the systems that process the                                                                       1
reports in the form of stack traces.","Logistic Regression             log
Thus, a public dataset consists of a subset of reports from a                                                               1e-5
private dataset, for which a sufﬁcient number of annotations         Loss                               log
are available.","Random Forest
                                                                                                                             64
B. Baseline Implementations                                          Number of estimators               100                  70

   To compare our stack-trace-based approach with approaches         Maximum depth                      ∞                   100
that use reports description, we implement several baseline                                                                  70
models.",2022-01-14 00:16:57+00:00,DapStep: Deep Assignee Prediction for Stack Trace Error rePresentation,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Denis Sushentsev'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The task of finding the best developer to fix a bug is called bug triage.
Most of the existing approaches consider the bug triage task as a
classification problem, however, classification is not appropriate when the
sets of classes change over time (as developers often do in a project).
Furthermore, to the best of our knowledge, all the existing models use textual
sources of information, i.e., bug descriptions, which are not always available.
  In this work, we explore the applicability of existing solutions for the bug
triage problem when stack traces are used as the main data source of bug
reports. Additionally, we reformulate this task as a ranking problem and
propose new deep learning models to solve it. The models are based on a
bidirectional recurrent neural network with attention and on a convolutional
neural network, with the weights of the models optimized using a ranking loss
function. To improve the quality of ranking, we propose using additional
information from version control system annotations. Two approaches are
proposed for extracting features from annotations: manual and using an
additional neural network. To evaluate our models, we collected two datasets of
real-world stack traces. Our experiments show that the proposed models
outperform existing models adapted to handle stack traces. To facilitate
further research in this area, we publish the source code of our models and one
of the collected datasets."
548,"To facilitate further research in this area, the source
   Our study suffers from the following threats to validity.","The public dataset is
                                                                     a subset of the private dataset that only contains stack frames
                     V. THREATS TO VALIDITY                          that relate to public repositories, with a total of 3,361 stack
                                                                     traces.","code of all the models, as well as the public dataset, are
   Subject selection bias.",2022-01-14 00:16:57+00:00,DapStep: Deep Assignee Prediction for Stack Trace Error rePresentation,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Denis Sushentsev'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The task of finding the best developer to fix a bug is called bug triage.
Most of the existing approaches consider the bug triage task as a
classification problem, however, classification is not appropriate when the
sets of classes change over time (as developers often do in a project).
Furthermore, to the best of our knowledge, all the existing models use textual
sources of information, i.e., bug descriptions, which are not always available.
  In this work, we explore the applicability of existing solutions for the bug
triage problem when stack traces are used as the main data source of bug
reports. Additionally, we reformulate this task as a ranking problem and
propose new deep learning models to solve it. The models are based on a
bidirectional recurrent neural network with attention and on a convolutional
neural network, with the weights of the models optimized using a ranking loss
function. To improve the quality of ranking, we propose using additional
information from version control system annotations. Two approaches are
proposed for extracting features from annotations: manual and using an
additional neural network. To evaluate our models, we collected two datasets of
real-world stack traces. Our experiments show that the proposed models
outperform existing models adapted to handle stack traces. To facilitate
further research in this area, we publish the source code of our models and one
of the collected datasets."
565,"These findings, together with
  the evidence that code review conflicts happen naturally and developers cannot avoid them,
  indicates that further research can be designed and conducted on constructive conflict resolution
  specific to the context of code review.","Our interviewees also propose ways of conflict
  management that lead to active involvement in situation resolution and clearing out priorities
  and next steps in the review and software development process.",Code review and its context are strongly intertwined.,2022-01-14 12:50:23+00:00,Interpersonal Conflicts During Code Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pavlína Wurzel Gonçalves'), arxiv.Result.Author('Gül Çalıklı'), arxiv.Result.Author('Alberto Bacchelli')]","Code review consists of manual inspection, discussion, and judgment of source
code by developers other than the code's author. Due to discussions around
competing ideas and group decision-making processes, interpersonal conflicts
during code reviews are expected. This study systematically investigates how
developers perceive code review conflicts and addresses interpersonal conflicts
during code reviews as a theoretical construct. Through the thematic analysis
of interviews conducted with 22 developers, we confirm that conflicts during
code reviews are commonplace, anticipated and seen as normal by developers.
Even though conflicts do happen and carry a negative impact for the review,
conflicts-if resolved constructively-can also create value and bring
improvement. Moreover, the analysis provided insights on how strongly conflicts
during code review and its context (i.e., code, developer, team, organization)
are intertwined. Finally, there are aspects specific to code review conflicts
that call for the research and application of customized conflict resolution
and management techniques, some of which are discussed in this paper. Data and
material: https://doi.org/10.5281/zenodo.5848794"
566,"Therefore, our study corroborates that further research
  on the role of the reported factors can bring new insights into the relation between community
  smells and code smells and between social and technical debt.","Moreover, team and
  organization-level factors listed in Section 4.4 can contribute to social debt, which is the additional
  cost that occurs when strained social and organizational interactions get in the way of smooth
  software development and operation [75].",Conflicts in code review are also domain-specific.,2022-01-14 12:50:23+00:00,Interpersonal Conflicts During Code Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pavlína Wurzel Gonçalves'), arxiv.Result.Author('Gül Çalıklı'), arxiv.Result.Author('Alberto Bacchelli')]","Code review consists of manual inspection, discussion, and judgment of source
code by developers other than the code's author. Due to discussions around
competing ideas and group decision-making processes, interpersonal conflicts
during code reviews are expected. This study systematically investigates how
developers perceive code review conflicts and addresses interpersonal conflicts
during code reviews as a theoretical construct. Through the thematic analysis
of interviews conducted with 22 developers, we confirm that conflicts during
code reviews are commonplace, anticipated and seen as normal by developers.
Even though conflicts do happen and carry a negative impact for the review,
conflicts-if resolved constructively-can also create value and bring
improvement. Moreover, the analysis provided insights on how strongly conflicts
during code review and its context (i.e., code, developer, team, organization)
are intertwined. Finally, there are aspects specific to code review conflicts
that call for the research and application of customized conflict resolution
and management techniques, some of which are discussed in this paper. Data and
material: https://doi.org/10.5281/zenodo.5848794"
567,"There are several areas that require further research work to understand
conflicts and mitigate their potentially negative consequences:

   (1) Quantitatively confirm the conflicts’ frequency and severity in code reviews.","5.3 Future Work

This study acted as an initial broad description of potential forms of conflicts during code review,
their context, and effects.","(2) Quantitatively investigate positive and negative consequences of conflicts with a focus on

       their relation to the fulfillment of the review goals.",2022-01-14 12:50:23+00:00,Interpersonal Conflicts During Code Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pavlína Wurzel Gonçalves'), arxiv.Result.Author('Gül Çalıklı'), arxiv.Result.Author('Alberto Bacchelli')]","Code review consists of manual inspection, discussion, and judgment of source
code by developers other than the code's author. Due to discussions around
competing ideas and group decision-making processes, interpersonal conflicts
during code reviews are expected. This study systematically investigates how
developers perceive code review conflicts and addresses interpersonal conflicts
during code reviews as a theoretical construct. Through the thematic analysis
of interviews conducted with 22 developers, we confirm that conflicts during
code reviews are commonplace, anticipated and seen as normal by developers.
Even though conflicts do happen and carry a negative impact for the review,
conflicts-if resolved constructively-can also create value and bring
improvement. Moreover, the analysis provided insights on how strongly conflicts
during code review and its context (i.e., code, developer, team, organization)
are intertwined. Finally, there are aspects specific to code review conflicts
that call for the research and application of customized conflict resolution
and management techniques, some of which are discussed in this paper. Data and
material: https://doi.org/10.5281/zenodo.5848794"
568,"We hope that the insights we have discovered and
presented in this paper will form the basis for further research on this prominent aspect of code
review and software engineering.","Developers describe a number of factors that affect interpersonal conflicts during code review, as
well as several problem-focused strategies that can be used to identify and understand conflicts,
thus leading to better conflict management.","ACKNOWLEDGMENTS

P. Wurzel Gonçalves and A. Bacchelli gratefully acknowledge the support of the Swiss National
Science Foundation through the SNF Project No.",2022-01-14 12:50:23+00:00,Interpersonal Conflicts During Code Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pavlína Wurzel Gonçalves'), arxiv.Result.Author('Gül Çalıklı'), arxiv.Result.Author('Alberto Bacchelli')]","Code review consists of manual inspection, discussion, and judgment of source
code by developers other than the code's author. Due to discussions around
competing ideas and group decision-making processes, interpersonal conflicts
during code reviews are expected. This study systematically investigates how
developers perceive code review conflicts and addresses interpersonal conflicts
during code reviews as a theoretical construct. Through the thematic analysis
of interviews conducted with 22 developers, we confirm that conflicts during
code reviews are commonplace, anticipated and seen as normal by developers.
Even though conflicts do happen and carry a negative impact for the review,
conflicts-if resolved constructively-can also create value and bring
improvement. Moreover, the analysis provided insights on how strongly conflicts
during code review and its context (i.e., code, developer, team, organization)
are intertwined. Finally, there are aspects specific to code review conflicts
that call for the research and application of customized conflict resolution
and management techniques, some of which are discussed in this paper. Data and
material: https://doi.org/10.5281/zenodo.5848794"
630,"This paper encourages further research
ments, 2.65%) are the next ones.","According to app developers’ survey responses in
Issues related to the location & language (44 issue comments, 3.54%),     [1], accessibility is often not treated as importantly as other aspects
preference (42 issue comments, 3.38%), compatibility (33 issue com-       of quality, such as security.","Inclusiveness-related issues are         for mining and exploring (1) developer-based repositories to un-
only discussed in 23 issue comments (1.85%).",2022-01-15 21:38:27+00:00,How are Diverse End-user Human-centric Issues Discussed on GitHub?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Hourieh Khalajzadeh'), arxiv.Result.Author('Mojtaba Shahin'), arxiv.Result.Author('Humphrey O. Obie'), arxiv.Result.Author('John Grundy')]","Many software systems fail to meet the needs of the diverse end-users in
society and are prone to pose problems, such as accessibility and usability
issues. Some of these problems (partially) stem from the failure to consider
the characteristics, limitations, and abilities of diverse end-users during
software development. We refer to this class of problems as human-centric
issues. Despite their importance, there is a limited understanding of the types
of human-centric issues encountered by developers. In-depth knowledge of these
human-centric issues is needed to design software systems that better meet
their diverse end-users' needs. This paper aims to provide insights for the
software development and research communities on which human-centric issues are
a topic of discussion for developers on GitHub. We conducted an empirical study
by extracting and manually analysing 1,691 issue comments from 12 diverse
projects, ranging from small to large-scale projects, including projects
designed for challenged end-users, e.g., visually impaired and dyslexic users.
Our analysis shows that eight categories of human-centric issues are discussed
by developers. These include Inclusiveness, Privacy & Security, Compatibility,
Location & Language, Preference, Satisfaction, Emotional Aspects, and
Accessibility. Guided by our findings, we highlight some implications and
possible future paths to further understand and incorporate human-centric
issues in software development to be able to design software that meets the
needs of diverse end users in society."
675,"Also, the authors of
this work conclude that the common notion of NFRs leaves out important non-engineering-related aspects that have an
impact specially on the development of the software and speciﬁcally, that further research addressing scope, notion, and
measurement of NFRs shall be done.","The remaining 26% of
NFRs at least have a deﬁnition, but for the majority of over 53% of common NFRs neither their attributes nor their
measurement methodology is deﬁned, leaving them factually ignored in the development process.",The second stream of relevant works comprises systematic literature reviews and mapping studies.,2022-01-17 16:59:28+00:00,"Focus Areas, Themes, and Objectives of Non-Functional Requirements in DevOps: A Systematic Mapping Study",cs.SE,['cs.SE'],"[arxiv.Result.Author('Philipp Haindl'), arxiv.Result.Author('Reinhold Plösch')]","Software non-functional requirements address a multitude of objectives,
expectations, and even liabilities that must be considered during development
and operation. Typically, these non-functional requirements originate from
different domains and their concrete scope, notion, and demarcation to
functional requirements is often ambiguous. In this study we seek to categorize
and analyze relevant work related to software engineering in a DevOps context
in order to clarify the different focus areas, themes, and objectives
underlying non-functional requirements and also to identify future research
directions in this field. We conducted a systematic mapping study, including
142 selected primary studies, extracted the focus areas, and synthesized the
themes and objectives of the described NFRs. In order to examine
non-engineering-focused studies related to non-functional requirements in
DevOps, we conducted a backward snowballing step and additionally included 17
primary studies. Our analysis revealed 7 recurrent focus areas and 41 themes
that characterize NFRs in DevOps, along with typical objectives for these
themes. Overall, the focus areas and themes of NFRs in DevOps are very diverse
and reflect the different perspectives required to align software engineering
with technical quality, business, compliance, and organizational
considerations. The lack of methodological support for specifying, measuring,
and evaluating fulfillment of these NFRs in DevOps-driven projects offers ample
opportunities for future research in this field. Particularly, there is a need
for empirically validated approaches for operationalizing
non-engineering-focused objectives of software."
685,"Therefore, in this RQ, we further study the impact of    the DL model input source.","For a fair comparison with prior work,
forms three different mutation strategies in tandem (as detailed in    we enforce FreeFuzz to use exactly the same models from LEMON as
Algorithm 1).","To prepare the other two input sources
each mutation strategy by disabling it.",2022-01-17 19:07:33+00:00,Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anjiang Wei'), arxiv.Result.Author('Yinlin Deng'), arxiv.Result.Author('Chenyuan Yang'), arxiv.Result.Author('Lingming Zhang')]","Deep learning (DL) systems can make our life much easier, and thus is gaining
more and more attention from both academia and industry. Meanwhile, bugs in DL
systems can be disastrous, and can even threaten human lives in safety-critical
applications. To date, a huge body of research efforts have been dedicated to
testing DL models. However, interestingly, there is still limited work for
testing the underlying DL libraries, which are the foundation for building,
optimizing, and running the DL models. One potential reason is that test
generation for the underlying DL libraries can be rather challenging since
their public APIs are mainly exposed in Python, making it even hard to
automatically determine the API input parameter types due to dynamic typing. In
this paper, we propose FreeFuzz, the first approach to fuzzing DL libraries via
mining from open source. More specifically, FreeFuzz obtains code/models from
three different sources: 1) code snippets from the library documentation, 2)
library developer tests, and 3) DL models in the wild. Then, FreeFuzz
automatically runs all the collected code/models with instrumentation to trace
the dynamic information for each covered API, including the types and values of
each parameter during invocation, and shapes of input/output tensors. Lastly,
FreeFuzz will leverage the traced dynamic information to perform fuzz testing
for each covered API. The extensive study of FreeFuzz on PyTorch and
TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able
to automatically trace valid dynamic information for fuzzing 1158 popular APIs,
9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON.
Furthermore, FreeFuzz is able to detect 35 bugs for PyTorch and TensorFlow
(with 31 confirmed by developers and 30 previously unknown)."
686,"Therefore, in this RQ, we further study the impact of    different types tend to be more different.","Second, we can also ob-
                                                                       serve that random-value and database-value mutation strategies
After tracing the initial inputs from various sources, FreeFuzz per-   perform similarly in terms of code coverage, while type mutation
forms three different mutation strategies in tandem (as detailed in    can be even more effective since the low-level implementations for
Algorithm 1).",each mutation strategy by disabling it.,2022-01-17 19:07:33+00:00,Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anjiang Wei'), arxiv.Result.Author('Yinlin Deng'), arxiv.Result.Author('Chenyuan Yang'), arxiv.Result.Author('Lingming Zhang')]","Deep learning (DL) systems can make our life much easier, and thus is gaining
more and more attention from both academia and industry. Meanwhile, bugs in DL
systems can be disastrous, and can even threaten human lives in safety-critical
applications. To date, a huge body of research efforts have been dedicated to
testing DL models. However, interestingly, there is still limited work for
testing the underlying DL libraries, which are the foundation for building,
optimizing, and running DL models. One potential reason is that test generation
for the underlying DL libraries can be rather challenging since their public
APIs are mainly exposed in Python, making it even hard to automatically
determine the API input parameter types due to dynamic typing. In this paper,
we propose FreeFuzz, the first approach to fuzzing DL libraries via mining from
open source. More specifically, FreeFuzz obtains code/models from three
different sources: 1) code snippets from the library documentation, 2) library
developer tests, and 3) DL models in the wild. Then, FreeFuzz automatically
runs all the collected code/models with instrumentation to trace the dynamic
information for each covered API, including the types and values of each
parameter during invocation, and shapes of input/output tensors. Lastly,
FreeFuzz will leverage the traced dynamic information to perform fuzz testing
for each covered API. The extensive study of FreeFuzz on PyTorch and
TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able
to automatically trace valid dynamic information for fuzzing 1158 popular APIs,
9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON. To
date, FreeFuzz has detected 49 bugs for PyTorch and TensorFlow (with 38 already
confirmed by developers as previously unknown)."
687,"Therefore, in this RQ, we further study the impact of    different types tend to be more different.","Second, we can also ob-
                                                                       serve that random-value and database-value mutation strategies
After tracing the initial inputs from various sources, FreeFuzz per-   perform similarly in terms of code coverage, while type mutation
forms three different mutation strategies in tandem (as detailed in    can be even more effective since the low-level implementations for
Algorithm 1).",each mutation strategy by disabling it.,2022-01-17 19:07:33+00:00,Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anjiang Wei'), arxiv.Result.Author('Yinlin Deng'), arxiv.Result.Author('Chenyuan Yang'), arxiv.Result.Author('Lingming Zhang')]","Deep learning (DL) systems can make our life much easier, and thus is gaining
more and more attention from both academia and industry. Meanwhile, bugs in DL
systems can be disastrous, and can even threaten human lives in safety-critical
applications. To date, a huge body of research efforts have been dedicated to
testing DL models. However, interestingly, there is still limited work for
testing the underlying DL libraries, which are the foundation for building,
optimizing, and running DL models. One potential reason is that test generation
for the underlying DL libraries can be rather challenging since their public
APIs are mainly exposed in Python, making it even hard to automatically
determine the API input parameter types due to dynamic typing. In this paper,
we propose FreeFuzz, the first approach to fuzzing DL libraries via mining from
open source. More specifically, FreeFuzz obtains code/models from three
different sources: 1) code snippets from the library documentation, 2) library
developer tests, and 3) DL models in the wild. Then, FreeFuzz automatically
runs all the collected code/models with instrumentation to trace the dynamic
information for each covered API, including the types and values of each
parameter during invocation, and shapes of input/output tensors. Lastly,
FreeFuzz will leverage the traced dynamic information to perform fuzz testing
for each covered API. The extensive study of FreeFuzz on PyTorch and
TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able
to automatically trace valid dynamic information for fuzzing 1158 popular APIs,
9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON. To
date, FreeFuzz has detected 49 bugs for PyTorch and TensorFlow (with 38 already
confirmed by developers as previously unknown)."
688,"Therefore, in this RQ, we further study the impact of    different types tend to be more different.","Second, we can also ob-
                                                                       serve that random-value and database-value mutation strategies
After tracing the initial inputs from various sources, FreeFuzz per-   perform similarly in terms of code coverage, while type mutation
forms three different mutation strategies in tandem (as detailed in    can be even more effective since the low-level implementations for
Algorithm 1).",each mutation strategy by disabling it.,2022-01-17 19:07:33+00:00,Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anjiang Wei'), arxiv.Result.Author('Yinlin Deng'), arxiv.Result.Author('Chenyuan Yang'), arxiv.Result.Author('Lingming Zhang')]","Deep learning (DL) systems can make our life much easier, and thus are
gaining more and more attention from both academia and industry. Meanwhile,
bugs in DL systems can be disastrous, and can even threaten human lives in
safety-critical applications. To date, a huge body of research efforts have
been dedicated to testing DL models. However, interestingly, there is still
limited work for testing the underlying DL libraries, which are the foundation
for building, optimizing, and running DL models. One potential reason is that
test generation for the underlying DL libraries can be rather challenging since
their public APIs are mainly exposed in Python, making it even hard to
automatically determine the API input parameter types due to dynamic typing. In
this paper, we propose FreeFuzz, the first approach to fuzzing DL libraries via
mining from open source. More specifically, FreeFuzz obtains code/models from
three different sources: 1) code snippets from the library documentation, 2)
library developer tests, and 3) DL models in the wild. Then, FreeFuzz
automatically runs all the collected code/models with instrumentation to trace
the dynamic information for each covered API, including the types and values of
each parameter during invocation, and shapes of input/output tensors. Lastly,
FreeFuzz will leverage the traced dynamic information to perform fuzz testing
for each covered API. The extensive study of FreeFuzz on PyTorch and
TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able
to automatically trace valid dynamic information for fuzzing 1158 popular APIs,
9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON. To
date, FreeFuzz has detected 49 bugs for PyTorch and TensorFlow (with 38 already
confirmed by developers as previously unknown)."
704,"To
tecting the language, especially when the comments also featured             further study this aspect, we decided to experiment with both a
code constructs in them.","Such a process was needed          also been confirmed (for other Transformer-based models) in the
since we noticed that the Google API was the most accurate in de-            context of code-related tasks such as test case generation [44].","In this scenario, the Python libraries often        pre-trained and a non pre-trained model, both of which have been
generated false negatives (i.e., classifying an English sentence as          subject to a hyperparameter tuning process.",2022-01-18 09:58:33+00:00,Using Pre-Trained Models to Boost Code Review Automation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rosalia Tufano'), arxiv.Result.Author('Simone Masiero'), arxiv.Result.Author('Antonio Mastropaolo'), arxiv.Result.Author('Luca Pascarella'), arxiv.Result.Author('Denys Poshyvanyk'), arxiv.Result.Author('Gabriele Bavota')]","Code review is a practice widely adopted in open source and industrial
projects. Given the non-negligible cost of such a process, researchers started
investigating the possibility of automating specific code review tasks. We
recently proposed Deep Learning (DL) models targeting the automation of two
tasks: the first model takes as input a code submitted for review and
implements in it changes likely to be recommended by a reviewer; the second
takes as input the submitted code and a reviewer comment posted in natural
language and automatically implements the change required by the reviewer.
While the preliminary results we achieved are encouraging, both models had been
tested in rather simple code review scenarios, substantially simplifying the
targeted problem. This was also due to the choices we made when designing both
the technique and the experiments. In this paper, we build on top of that work
by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5)
model can outperform previous DL models for automating code review tasks. Also,
we conducted our experiments on a larger and more realistic (and challenging)
dataset of code review activities."
822,"(3) Our findings advocate the demand for further research in DL-based HIDS as the recent trend of adopting DL
        approaches showed significantly better performance compared to the prevalently used traditional ML approaches.","(2) We encourage the researchers to explore the application of contextual and deep contextual embedding for
        adaptability to a huge amount of continuous data since NLP-based contextual features were dominantly used.","(4) We recommend the researchers to further focus on time and resource-effective use of ensemble classifier structure
        as it improves the detection performance compared to the dominantly used base classifier structures.",2022-01-20 09:05:34+00:00,NLP Methods in Host-based Intrusion Detection Systems: A Systematic Review and Future Directions,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zarrin Tasnim Sworna'), arxiv.Result.Author('Zahra Mousavi'), arxiv.Result.Author('Muhammad Ali Babar')]","The Host-Based Intrusion Detection Systems (HIDS) are widely used for
defending against cybersecurity attacks. An increasing number of HIDS have
started leveraging the advances in Natural Language Processing (NLP)
technologies that have shown promising results in precisely detecting low
footprint, zero-day attacks and predict attacker's next steps. We conduct a
systematic review of the literature on NLP-based HIDS in order to build a
systematized body of knowledge. We develop an NLP-based HIDS taxonomy for
comparing the features, techniques, attacks, datasets, and metrics found from
the reviewed papers. We highlight the prevalent practices and the future
research areas."
823,"Our ﬁndings advocate the demand for further research in
      DL-based NLP methods for developing HIDS as the recent                 The work has been supported by the Cyber Security Research
      practice of adopting DL-based NLP approaches showed sig-            Centre Limited whose activities are partially funded by the Aus-
      niﬁcantly better performance compared to the prevalently            tralian Government’s Cooperative Research Centres Programme.","Acknowledgement
  3.",used traditional ML and rule based approaches.,2022-01-20 09:05:34+00:00,NLP Methods in Host-based Intrusion Detection Systems: A Systematic Review and Future Directions,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zarrin Tasnim Sworna'), arxiv.Result.Author('Zahra Mousavi'), arxiv.Result.Author('Muhammad Ali Babar')]","Host based Intrusion Detection System (HIDS) is an effective last line of
defense for defending against cyber security attacks after perimeter defenses
(e.g., Network based Intrusion Detection System and Firewall) have failed or
been bypassed. HIDS is widely adopted in the industry as HIDS is ranked among
the top two most used security tools by Security Operation Centers (SOC) of
organizations. Although effective and efficient HIDS is highly desirable for
industrial organizations, the evolution of increasingly complex attack patterns
causes several challenges resulting in performance degradation of HIDS (e.g.,
high false alert rate creating alert fatigue for SOC staff). Since Natural
Language Processing (NLP) methods are better suited for identifying complex
attack patterns, an increasing number of HIDS are leveraging the advances in
NLP that have shown effective and efficient performance in precisely detecting
low footprint, zero day attacks and predicting the next steps of attackers.
This active research trend of using NLP in HIDS demands a synthesized and
comprehensive body of knowledge of NLP based HIDS. Thus, we conducted a
systematic review of the literature on the end to end pipeline of the use of
NLP in HIDS development. For the end to end NLP based HIDS development
pipeline, we identify, taxonomically categorize and systematically compare the
state of the art of NLP methods usage in HIDS, attacks detected by these NLP
methods, datasets and evaluation metrics which are used to evaluate the NLP
based HIDS. We highlight the relevant prevalent practices, considerations,
advantages and limitations to support the HIDS developers. We also outline the
future research directions for the NLP based HIDS development."
866,"The studies indicate the involvement of diﬀerent roles,
which we believe warrants further research.","In summary, we ﬁnd there is overall a lack of understanding of quality re-
quirements prioritization.","Furthermore, the lack of systematic
prioritization seems to be in line with requirements in general and not just for
quality requirements.",2022-01-21 10:26:47+00:00,A Systematic Literature Review of Empirical Research on Quality Requirements,cs.SE,['cs.SE'],"[arxiv.Result.Author('Thomas Olsson'), arxiv.Result.Author('Severine Sentilles'), arxiv.Result.Author('Efi Papatheocharous')]","Quality requirements deal with how well a product should perform the intended
functionality, such as start-up time and learnability. Researchers argue they
are important and at the same time studies indicate there are deficiencies in
practice.
  Our goal is to review the state of evidence for quality requirements. We want
to understand the empirical research on quality requirements topics as well as
evaluations of quality requirements solutions.
  We used a hybrid method for our systematic literature review. We defined a
start set based on two literature reviews combined with a keyword-based search
from selected publication venues. We snowballed based on the start set.
  We screened 530 papers and included 84 papers in our review. Case study
method is the most common (43), followed by surveys (15) and tests (13). We
found no replication studies. The two most commonly studied themes are 1)
Differentiating characteristics of quality requirements compared to other types
of requirements, 2) the importance and prevalence of quality requirements.
Quality models, QUPER, and the NFR method are evaluated in several studies,
with positive indications. Goal modeling is the only modeling approach
evaluated. However, all studies are small scale and long-term costs and impact
are not studied.
  We conclude that more research is needed as empirical research on quality
requirements is not increasing at the same rate as software engineering
research in general. We see a gap between research and practice. The solutions
proposed are usually evaluated in an academic context and surveys on quality
requirements in industry indicate unsystematic handling of quality
requirements."
867,"5 Discussion

The results of our systematic literature review indicate that there are many qual-
ity requirements engineering aspects that warrant further research.","As this is a small scale validation, more re-
search is needed to understand how it performs in a realistic setting.","The small
number of studies found – 84 papers over 30 years – point to a lack of studies.",2022-01-21 10:26:47+00:00,A Systematic Literature Review of Empirical Research on Quality Requirements,cs.SE,['cs.SE'],"[arxiv.Result.Author('Thomas Olsson'), arxiv.Result.Author('Severine Sentilles'), arxiv.Result.Author('Efi Papatheocharous')]","Quality requirements deal with how well a product should perform the intended
functionality, such as start-up time and learnability. Researchers argue they
are important and at the same time studies indicate there are deficiencies in
practice.
  Our goal is to review the state of evidence for quality requirements. We want
to understand the empirical research on quality requirements topics as well as
evaluations of quality requirements solutions.
  We used a hybrid method for our systematic literature review. We defined a
start set based on two literature reviews combined with a keyword-based search
from selected publication venues. We snowballed based on the start set.
  We screened 530 papers and included 84 papers in our review. Case study
method is the most common (43), followed by surveys (15) and tests (13). We
found no replication studies. The two most commonly studied themes are 1)
Differentiating characteristics of quality requirements compared to other types
of requirements, 2) the importance and prevalence of quality requirements.
Quality models, QUPER, and the NFR method are evaluated in several studies,
with positive indications. Goal modeling is the only modeling approach
evaluated. However, all studies are small scale and long-term costs and impact
are not studied.
  We conclude that more research is needed as empirical research on quality
requirements is not increasing at the same rate as software engineering
research in general. We see a gap between research and practice. The solutions
proposed are usually evaluated in an academic context and surveys on quality
requirements in industry indicate unsystematic handling of quality
requirements."
868,"We believe there is a need for further research on quality re-
quirements.","This is an indication that the
statement that quality requirements being an emerging area of research needs
to be nuanced.","However, with the results we got from our study, it seems that the
research on quality requirements might be less directed towards the practical
challenges facing industry.",2022-01-21 10:26:47+00:00,A Systematic Literature Review of Empirical Research on Quality Requirements,cs.SE,['cs.SE'],"[arxiv.Result.Author('Thomas Olsson'), arxiv.Result.Author('Severine Sentilles'), arxiv.Result.Author('Efi Papatheocharous')]","Quality requirements deal with how well a product should perform the intended
functionality, such as start-up time and learnability. Researchers argue they
are important and at the same time studies indicate there are deficiencies in
practice.
  Our goal is to review the state of evidence for quality requirements. We want
to understand the empirical research on quality requirements topics as well as
evaluations of quality requirements solutions.
  We used a hybrid method for our systematic literature review. We defined a
start set based on two literature reviews combined with a keyword-based search
from selected publication venues. We snowballed based on the start set.
  We screened 530 papers and included 84 papers in our review. Case study
method is the most common (43), followed by surveys (15) and tests (13). We
found no replication studies. The two most commonly studied themes are 1)
Differentiating characteristics of quality requirements compared to other types
of requirements, 2) the importance and prevalence of quality requirements.
Quality models, QUPER, and the NFR method are evaluated in several studies,
with positive indications. Goal modeling is the only modeling approach
evaluated. However, all studies are small scale and long-term costs and impact
are not studied.
  We conclude that more research is needed as empirical research on quality
requirements is not increasing at the same rate as software engineering
research in general. We see a gap between research and practice. The solutions
proposed are usually evaluated in an academic context and surveys on quality
requirements in industry indicate unsystematic handling of quality
requirements."
869,"6 Conclusion

The results of our systematic literature review indicate that there are many qual-
ity requirements engineering aspects that warrant further research.",30  T. Olsson et al.,"We judge
that 84 papers over 30 years point to a lack of studies.",2022-01-21 10:26:47+00:00,A Systematic Literature Review of Empirical Research on Quality Requirements,cs.SE,['cs.SE'],"[arxiv.Result.Author('Thomas Olsson'), arxiv.Result.Author('Severine Sentilles'), arxiv.Result.Author('Efi Papatheocharous')]","Quality requirements deal with how well a product should perform the intended
functionality, such as start-up time and learnability. Researchers argue they
are important and at the same time studies indicate there are deficiencies in
practice.
  Our goal is to review the state of evidence for quality requirements. We want
to understand the empirical research on quality requirements topics as well as
evaluations of quality requirements solutions.
  We used a hybrid method for our systematic literature review. We defined a
start set based on two literature reviews combined with a keyword-based search
from selected publication venues. We snowballed based on the start set.
  We screened 530 papers and included 84 papers in our review. Case study
method is the most common (43), followed by surveys (15) and tests (13). We
found no replication studies. The two most commonly studied themes are 1)
Differentiating characteristics of quality requirements compared to other types
of requirements, 2) the importance and prevalence of quality requirements.
Quality models, QUPER, and the NFR method are evaluated in several studies,
with positive indications. Goal modeling is the only modeling approach
evaluated. However, all studies are small scale and long-term costs and impact
are not studied.
  We conclude that more research is needed as empirical research on quality
requirements is not increasing at the same rate as software engineering
research in general. We see a gap between research and practice. The solutions
proposed are usually evaluated in an academic context and surveys on quality
requirements in industry indicate unsystematic handling of quality
requirements."
870,"This, again, is something that warrants further research to under-
stand the needs of practitioners and their relation to proposed solutions found
in the literature.","We, therefore, hypothesize that overall, there
is a lack of clear empirical evidence for what software developing organizations
should adopt.","For practitioners, there are some recommendations of what has worked in
realistic contexts.",2022-01-21 10:26:47+00:00,A Systematic Literature Review of Empirical Research on Quality Requirements,cs.SE,['cs.SE'],"[arxiv.Result.Author('Thomas Olsson'), arxiv.Result.Author('Severine Sentilles'), arxiv.Result.Author('Efi Papatheocharous')]","Quality requirements deal with how well a product should perform the intended
functionality, such as start-up time and learnability. Researchers argue they
are important and at the same time studies indicate there are deficiencies in
practice.
  Our goal is to review the state of evidence for quality requirements. We want
to understand the empirical research on quality requirements topics as well as
evaluations of quality requirements solutions.
  We used a hybrid method for our systematic literature review. We defined a
start set based on two literature reviews combined with a keyword-based search
from selected publication venues. We snowballed based on the start set.
  We screened 530 papers and included 84 papers in our review. Case study
method is the most common (43), followed by surveys (15) and tests (13). We
found no replication studies. The two most commonly studied themes are 1)
Differentiating characteristics of quality requirements compared to other types
of requirements, 2) the importance and prevalence of quality requirements.
Quality models, QUPER, and the NFR method are evaluated in several studies,
with positive indications. Goal modeling is the only modeling approach
evaluated. However, all studies are small scale and long-term costs and impact
are not studied.
  We conclude that more research is needed as empirical research on quality
requirements is not increasing at the same rate as software engineering
research in general. We see a gap between research and practice. The solutions
proposed are usually evaluated in an academic context and surveys on quality
requirements in industry indicate unsystematic handling of quality
requirements."
1167,"We will     NaviDroid in improving testing efficiency, reducing testing time
further study these screenshots in the future and realize automatic       and saving testers’ efforts.","The automated evalu-
in the process of assisting the crowdworkers to test the app, includ-     ation and user study demonstrate the accuracy and usefulness of
ing issues such as UI display issue and compatibility issues.",issue detection and repair.,2022-01-28 12:45:56+00:00,Guided Bug Crush: Assist Manual GUI Testing of Android Apps via Hint Moves,cs.SE,"['cs.SE', 'cs.HC', 'Software Engineering (cs.SE), Human-Computer Interaction (cs.HC)', 'D.2.4']","[arxiv.Result.Author('Zhe Liu'), arxiv.Result.Author('Chunyang Chen'), arxiv.Result.Author('Junjie Wang'), arxiv.Result.Author('Yuekai Huang'), arxiv.Result.Author('Jun Hu'), arxiv.Result.Author('Qing Wang')]","Mobile apps are indispensable for people's daily life. Complementing with
automated GUI testing, manual testing is the last line of defence for app
quality. However, the repeated actions and easily missing of functionalities
make manual testing time-consuming and inefficient. Inspired by the game candy
crush with flashy candies as hint moves for players, we propose an approach
named NaviDroid for navigating testers via highlighted next operations for more
effective and efficient testing. Within NaviDroid, we construct an enriched
state transition graph with the triggering actions as the edges for two
involved states. Based on it, we utilize the dynamic programming algorithm to
plan the exploration path, and augment the GUI with visualized hints for
testers to quickly explore untested activities and avoid duplicate
explorations. The automated experiments demonstrate the high coverage and
efficient path planning of NaviDroid and a user study further confirms its
usefulness. The NaviDroid can help us develop more robust software that works
in more mission-critical settings, not only by performing more thorough testing
with the same effort that has been put in before, but also by integrating these
techniques into different parts of development pipeline."
1307,"Interviewees insisted that the combination of use      The detailed results of the interviews can form a basis
case and the concrete implementation of a language         for further research into two key aspects: (i) backing
feature signiﬁcantly change how well a feature supports    up the expert opinions with empirical data and (ii) im-
properties such as comprehensibility or productivity.","transformation language capabilities and reasoned about
                                                           their implications for the investigated properties of MTL.",For  proving existing model transformation languages.,2022-01-31 16:52:59+00:00,Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Stefan Höppner'), arxiv.Result.Author('Yves Haas'), arxiv.Result.Author('Matthias Tichy'), arxiv.Result.Author('Katharina Juhnke')]","In a recent study we have shown, that a large number of claims about model
transformation languages have not yet been substantiated and are made without
much context to be able to critically asses their merit or built meaningful
empirical studies around them. The objective of our work was to elicit the
reasoning, influences and background knowledge of researchers and practitioners
that lead them to assuming benefits or drawbacks of model transformation
languages compared to general purpose languages for the task of developing
model transformations. For this we put our focus on the following 6 properties
that have strong relevance for wider adoption: Ease of writing,
Comprehensibility, Tool Support, Practical Expressiveness, Productivity, Reuse
and Maintainability. We conducted a large-scale interview study involving 56
participants from research and industry. Interviewees were presented with
claims about model transformation languages and were asked to provide reasons
as to why they believe or dispute these claims. Our interviews show, that the
general purpose expressiveness of GPLs, the domain specific capabilities of
MTLs and the tooling of MTLs all have strong influences on how people view
properties of model transformation languages. Their specific influences differ
depending on different concrete characteristics, such as, for example,
Bidirectionality or Debugging Tooling. Moreover, the choice of MTL, the use
case for which a transformation should be developed as well as the skills of
involved stakeholders have an indirect effect on MTL properties by changing the
contextual circumstances under examination. We conclude that there is a broad
body of experience of interviews that suggests positive and negative influences
for properties of MTLs. However, our qualitative data suggests that much needs
to be done in order to convey the viability of model transformation languages."
1308,"In this context, a    Lastly, we discuss the most salient factors and argue ac-
                                                          tionable results for the community and further research.",claims advantages or disadvantages.,"claimed positive eﬀect on one of the properties means
                                                              As the ﬁrst study of this type, we make the following
an advantage whereas a negative inﬂuence means a dis-     contributions:

advantage.",2022-01-31 16:52:59+00:00,Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Stefan Höppner'), arxiv.Result.Author('Yves Haas'), arxiv.Result.Author('Matthias Tichy'), arxiv.Result.Author('Katharina Juhnke')]","Model driven development envisages the use of model transformations to evolve
models. Model transformation languages, developed for this task, are touted
with many benefits over general purpose programming languages. However, a large
number of these claims have not yet been substantiated. They are also made
without the context necessary to be able to critically assess their merit or
built meaningful empirical studies around them.
  The objective of our work is to elicit the reasoning, influences and
background knowledge that lead people to assume benefits or drawbacks of model
transformation languages.
  We conducted a large-scale interview study involving 56 participants from
research and industry. Interviewees were presented with claims about model
transformation languages and were asked to provide reasons for their assessment
thereof. We qualitatively analysed the responses to find factors that influence
he properties of model transformation languages as well as explanations as to
how exactly they do so.
  Our interviews show, that general purpose expressiveness of GPLs, domain
specific capabilities of MTLs as well as tooling all have strong influences on
how people view properties of model transformation languages. Moreover, the
Choice of MTL, the Use Case for which a transformation should be developed as
well as the Skills of involved stakeholders have a moderating effect on the
influences, by changing the context to consider.
  There is a broad body of experience, that suggests positive and negative
influences for properties of MTLs. Our data suggests, that much needs to be
done in order to convey the viability of model transformation languages.
Efforts to provide more empirical substance need to be undergone and lacklustre
language capabilities and tooling need to be improved upon. We suggest several
approaches for this that can be based on the results of the presented study."
1309,"form a basis for further research into two key aspects:
For example, a language such as Henshin is intended
for cases where patterns of model elements need to             (I) backing up the expert opinions with empirical data
be matched and manipulated.","In our opinion, this stems from the use        We believe the detailed results of the interviews can
cases language developers intended the language for.","In such cases, the fea-           (II) improving existing model transformation languages
tures provided by Henshin can bring signiﬁcant advan-
tages over implementing the intended transformation              .",2022-01-31 16:52:59+00:00,Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Stefan Höppner'), arxiv.Result.Author('Yves Haas'), arxiv.Result.Author('Matthias Tichy'), arxiv.Result.Author('Katharina Juhnke')]","Model driven development envisages the use of model transformations to evolve
models. Model transformation languages, developed for this task, are touted
with many benefits over general purpose programming languages. However, a large
number of these claims have not yet been substantiated. They are also made
without the context necessary to be able to critically assess their merit or
built meaningful empirical studies around them.
  The objective of our work is to elicit the reasoning, influences and
background knowledge that lead people to assume benefits or drawbacks of model
transformation languages.
  We conducted a large-scale interview study involving 56 participants from
research and industry. Interviewees were presented with claims about model
transformation languages and were asked to provide reasons for their assessment
thereof. We qualitatively analysed the responses to find factors that influence
he properties of model transformation languages as well as explanations as to
how exactly they do so.
  Our interviews show, that general purpose expressiveness of GPLs, domain
specific capabilities of MTLs as well as tooling all have strong influences on
how people view properties of model transformation languages. Moreover, the
Choice of MTL, the Use Case for which a transformation should be developed as
well as the Skills of involved stakeholders have a moderating effect on the
influences, by changing the context to consider.
  There is a broad body of experience, that suggests positive and negative
influences for properties of MTLs. Our data suggests, that much needs to be
done in order to convey the viability of model transformation languages.
Efforts to provide more empirical substance need to be undergone and lacklustre
language capabilities and tooling need to be improved upon. We suggest several
approaches for this that can be based on the results of the presented study."
1310,"We strongly believe in open data to allow replication of our results as well as
        enabling further research.",5.,"The anonymized answers (unprocessed and pro-
        cessed) to this interview (transcribed interview and notes ) will be made availa-
        ble online to the public to accompany publications and stored in open data re-
        positories.",2022-01-31 16:52:59+00:00,Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Stefan Höppner'), arxiv.Result.Author('Yves Haas'), arxiv.Result.Author('Matthias Tichy'), arxiv.Result.Author('Katharina Juhnke')]","Model driven development envisages the use of model transformations to evolve
models. Model transformation languages, developed for this task, are touted
with many benefits over general purpose programming languages. However, a large
number of these claims have not yet been substantiated. They are also made
without the context necessary to be able to critically assess their merit or
built meaningful empirical studies around them.
  The objective of our work is to elicit the reasoning, influences and
background knowledge that lead people to assume benefits or drawbacks of model
transformation languages.
  We conducted a large-scale interview study involving 56 participants from
research and industry. Interviewees were presented with claims about model
transformation languages and were asked to provide reasons for their assessment
thereof. We qualitatively analysed the responses to find factors that influence
he properties of model transformation languages as well as explanations as to
how exactly they do so.
  Our interviews show, that general purpose expressiveness of GPLs, domain
specific capabilities of MTLs as well as tooling all have strong influences on
how people view properties of model transformation languages. Moreover, the
Choice of MTL, the Use Case for which a transformation should be developed as
well as the Skills of involved stakeholders have a moderating effect on the
influences, by changing the context to consider.
  There is a broad body of experience, that suggests positive and negative
influences for properties of MTLs. Our data suggests, that much needs to be
done in order to convey the viability of model transformation languages.
Efforts to provide more empirical substance need to be undergone and lacklustre
language capabilities and tooling need to be improved upon. We suggest several
approaches for this that can be based on the results of the presented study."
1311,"In this context, a    Lastly, we discuss the most salient factors and argue ac-
                                                          tionable results for the community and further research.",claims advantages or disadvantages.,"claimed positive eﬀect on one of the properties means
                                                              As the ﬁrst study of this type, we make the following
an advantage whereas a negative inﬂuence means a dis-     contributions:

advantage.",2022-01-31 16:52:59+00:00,Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Stefan Höppner'), arxiv.Result.Author('Yves Haas'), arxiv.Result.Author('Matthias Tichy'), arxiv.Result.Author('Katharina Juhnke')]","Model driven development envisages the use of model transformations to evolve
models. Model transformation languages, developed for this task, are touted
with many benefits over general purpose programming languages. However, a large
number of these claims have not yet been substantiated. They are also made
without the context necessary to be able to critically assess their merit or
built meaningful empirical studies around them. The objective of our work is to
elicit the reasoning, influences and background knowledge that lead people to
assume benefits or drawbacks of model transformation languages. We conducted a
large-scale interview study involving 56 participants from research and
industry. Interviewees were presented with claims about model transformation
languages and were asked to provide reasons for their assessment thereof. We
qualitatively analysed the responses to find factors that influence the
properties of model transformation languages as well as explanations as to how
exactly they do so. Our interviews show, that general purpose expressiveness of
GPLs, domain specific capabilities of MTLs as well as tooling all have strong
influences on how people view properties of model transformation languages.
Moreover, the Choice of MTL, the Use Case for which a transformation should be
developed as well as the Skills of involved stakeholders have a moderating
effect on the influences, by changing the context to consider. There is a broad
body of experience, that suggests positive and negative influences for
properties of MTLs. Our data suggests, that much needs to be done in order to
convey the viability of model transformation languages. Efforts to provide more
empirical substance need to be undergone and lackluster language capabilities
and tooling need to be improved upon. We suggest several approaches for this
that can be based on the results of the presented study."
1312,"form a basis for further research into two key aspects:
For example, a language such as Henshin is intended
for cases where patterns of model elements need to             (I) backing up the expert opinions with empirical data
be matched and manipulated.","In our opinion, this stems from the use        We believe the detailed results of the interviews can
cases language developers intended the language for.","In such cases, the fea-           (II) improving existing model transformation languages
tures provided by Henshin can bring signiﬁcant advan-
tages over implementing the intended transformation              .",2022-01-31 16:52:59+00:00,Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Stefan Höppner'), arxiv.Result.Author('Yves Haas'), arxiv.Result.Author('Matthias Tichy'), arxiv.Result.Author('Katharina Juhnke')]","Model driven development envisages the use of model transformations to evolve
models. Model transformation languages, developed for this task, are touted
with many benefits over general purpose programming languages. However, a large
number of these claims have not yet been substantiated. They are also made
without the context necessary to be able to critically assess their merit or
built meaningful empirical studies around them. The objective of our work is to
elicit the reasoning, influences and background knowledge that lead people to
assume benefits or drawbacks of model transformation languages. We conducted a
large-scale interview study involving 56 participants from research and
industry. Interviewees were presented with claims about model transformation
languages and were asked to provide reasons for their assessment thereof. We
qualitatively analysed the responses to find factors that influence the
properties of model transformation languages as well as explanations as to how
exactly they do so. Our interviews show, that general purpose expressiveness of
GPLs, domain specific capabilities of MTLs as well as tooling all have strong
influences on how people view properties of model transformation languages.
Moreover, the Choice of MTL, the Use Case for which a transformation should be
developed as well as the Skills of involved stakeholders have a moderating
effect on the influences, by changing the context to consider. There is a broad
body of experience, that suggests positive and negative influences for
properties of MTLs. Our data suggests, that much needs to be done in order to
convey the viability of model transformation languages. Efforts to provide more
empirical substance need to be undergone and lackluster language capabilities
and tooling need to be improved upon. We suggest several approaches for this
that can be based on the results of the presented study."
1313,"We strongly believe in open data to allow replication of our results as well as
        enabling further research.",5.,"The anonymized answers (unprocessed and pro-
        cessed) to this interview (transcribed interview and notes ) will be made availa-
        ble online to the public to accompany publications and stored in open data re-
        positories.",2022-01-31 16:52:59+00:00,Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Stefan Höppner'), arxiv.Result.Author('Yves Haas'), arxiv.Result.Author('Matthias Tichy'), arxiv.Result.Author('Katharina Juhnke')]","Model driven development envisages the use of model transformations to evolve
models. Model transformation languages, developed for this task, are touted
with many benefits over general purpose programming languages. However, a large
number of these claims have not yet been substantiated. They are also made
without the context necessary to be able to critically assess their merit or
built meaningful empirical studies around them. The objective of our work is to
elicit the reasoning, influences and background knowledge that lead people to
assume benefits or drawbacks of model transformation languages. We conducted a
large-scale interview study involving 56 participants from research and
industry. Interviewees were presented with claims about model transformation
languages and were asked to provide reasons for their assessment thereof. We
qualitatively analysed the responses to find factors that influence the
properties of model transformation languages as well as explanations as to how
exactly they do so. Our interviews show, that general purpose expressiveness of
GPLs, domain specific capabilities of MTLs as well as tooling all have strong
influences on how people view properties of model transformation languages.
Moreover, the Choice of MTL, the Use Case for which a transformation should be
developed as well as the Skills of involved stakeholders have a moderating
effect on the influences, by changing the context to consider. There is a broad
body of experience, that suggests positive and negative influences for
properties of MTLs. Our data suggests, that much needs to be done in order to
convey the viability of model transformation languages. Efforts to provide more
empirical substance need to be undergone and lackluster language capabilities
and tooling need to be improved upon. We suggest several approaches for this
that can be based on the results of the presented study."
1335,"To promote further research and allow others         Replace existing features
to inspect and replicate our methodology and findings, we provide
a detailed audit trail of our study artifacts, which include our survey       Other                                                                      Refactored
questionnaire, recruitment materials, codebook, anonymized survey                                                                                        Did not refactor
data, and the Jupyter notebook used to produce the figures in the
paper.","Reuse features across systems

   Study Artifacts.","Our study artifacts are available at:                                  0.0 0.1 0.2 0Fr.3action0o.f4respo0n.d5ents 0.6 0.7 0.8
https://github.com/ArchitecturePractices/lsr_survey_artifacts.",2022-02-01 01:09:31+00:00,Industry Experiences with Large-Scale Refactoring,cs.SE,['cs.SE'],"[arxiv.Result.Author('James Ivers'), arxiv.Result.Author('Robert L. Nord'), arxiv.Result.Author('Ipek Ozkaya'), arxiv.Result.Author('Chris Seifried'), arxiv.Result.Author('Christopher S. Timperley'), arxiv.Result.Author('Marouane Kessentini')]","Software refactoring plays an important role in software engineering.
Developers often turn to refactoring when they want to restructure software to
improve its quality without changing its external behavior. Studies show that
small-scale (floss) refactoring is common in industry and can often be
performed by a single developer in short sessions, even though developers do
much of this work manually instead of using refactoring tools. However, some
refactoring efforts are much larger in scale, requiring entire teams and months
of effort, and the role of tools in these efforts is not as well studied. In
this paper, we report on a survey we conducted with developers to understand
large-scale refactoring, its prevalence, and how tools support it. Our results
from 107 industry developers demonstrate that projects commonly go through
multiple large-scale refactorings, each of which requires considerable effort.
While there is often a desire to refactor, other business concerns such as
developing new features often take higher priority. Our study finds that
developers use several categories of tools to support large-scale refactoring
and rely more heavily on general-purpose tools like IDEs than on tools designed
specifically to support refactoring. Tool support varies across the different
activities, with some particularly challenging activities seeing little use of
tools in practice. Our study demonstrates a clear need for better large-scale
refactoring tools and an opportunity for refactoring researchers to make a
difference in industry. The results we summarize in this paper is one concrete
step towards this goal."
1336,"The finding that developers make lit-              vendors would benefit from further research to obtain more granu-
tle use of existing refactoring tools for large-scale refactoring was       lar data on the activities that developers spend the most time on and
unsurprising, as it mirrors studies of smaller-scale refactoring.","Researchers and tool
   Tool Support for LSR.","As        analyses of which activities they can reliably support with trusted
part of our study, we also sought to understand whether the kinds           tools.",2022-02-01 01:09:31+00:00,Industry Experiences with Large-Scale Refactoring,cs.SE,['cs.SE'],"[arxiv.Result.Author('James Ivers'), arxiv.Result.Author('Robert L. Nord'), arxiv.Result.Author('Ipek Ozkaya'), arxiv.Result.Author('Chris Seifried'), arxiv.Result.Author('Christopher S. Timperley'), arxiv.Result.Author('Marouane Kessentini')]","Software refactoring plays an important role in software engineering.
Developers often turn to refactoring when they want to restructure software to
improve its quality without changing its external behavior. Studies show that
small-scale (floss) refactoring is common in industry and can often be
performed by a single developer in short sessions, even though developers do
much of this work manually instead of using refactoring tools. However, some
refactoring efforts are much larger in scale, requiring entire teams and months
of effort, and the role of tools in these efforts is not as well studied. In
this paper, we report on a survey we conducted with developers to understand
large-scale refactoring, its prevalence, and how tools support it. Our results
from 107 industry developers demonstrate that projects commonly go through
multiple large-scale refactorings, each of which requires considerable effort.
While there is often a desire to refactor, other business concerns such as
developing new features often take higher priority. Our study finds that
developers use several categories of tools to support large-scale refactoring
and rely more heavily on general-purpose tools like IDEs than on tools designed
specifically to support refactoring. Tool support varies across the different
activities, with some particularly challenging activities seeing little use of
tools in practice. Our study demonstrates a clear need for better large-scale
refactoring tools and an opportunity for refactoring researchers to make a
difference in industry. The results we summarize in this paper is one concrete
step towards this goal."
1337,"To promote further research and allow others
to inspect and replicate our methodology and findings, we provide        Replace existing features
a detailed audit trail of our study artifacts, which include our survey
questionnaire, recruitment materials, codebook, anonymized survey             Other                                                                      Refactored
data, and the Jupyter notebook used to produce the figures in the                                                                                        Did not refactor
paper.","Reuse features across systems
   Study Artifacts.","Our study artifacts are available at:
https://github.com/ArchitecturePractices/lsr_survey_artifacts.",2022-02-01 01:09:31+00:00,Industry Experiences with Large-Scale Refactoring,cs.SE,['cs.SE'],"[arxiv.Result.Author('James Ivers'), arxiv.Result.Author('Robert L. Nord'), arxiv.Result.Author('Ipek Ozkaya'), arxiv.Result.Author('Chris Seifried'), arxiv.Result.Author('Christopher S. Timperley'), arxiv.Result.Author('Marouane Kessentini')]","Software refactoring plays an important role in software engineering.
Developers often turn to refactoring when they want to restructure software to
improve its quality without changing its external behavior. Studies show that
small-scale (floss) refactoring is common in industry and can often be
performed by a single developer in short sessions, even though developers do
much of this work manually instead of using refactoring tools. However, some
refactoring efforts are much larger in scale, requiring entire teams and months
of effort, and the role of tools in these efforts is not as well studied. In
this paper, we report on a survey we conducted with developers to understand
large-scale refactoring, its prevalence, and how tools support it. Our results
from 107 industry developers demonstrate that projects commonly go through
multiple large-scale refactorings, each of which requires considerable effort.
While there is often a desire to refactor, other business concerns such as
developing new features often take higher priority. Our study finds that
developers use several categories of tools to support large-scale refactoring
and rely more heavily on general-purpose tools like IDEs than on tools designed
specifically to support refactoring. Tool support varies across the different
activities, with some particularly challenging activities seeing little use of
tools in practice. Our study demonstrates a clear need for better large-scale
refactoring tools and an opportunity for refactoring researchers to make a
difference in industry. The results we summarize in this paper is one concrete
step towards this goal."
1338,"Researchers and tool
tle use of existing refactoring tools for large-scale refactoring was       vendors would benefit from further research to obtain more granu-
unsurprising, as it mirrors studies of smaller-scale refactoring.",The finding that developers make lit-              torings being orders of magnitude larger).,"As        lar data on the activities that developers spend the most time on and
part of our study, we also sought to understand whether the kinds           analyses of which activities they can reliably support with trusted
of tools used in large-scale refactoring differ from those used in          tools.",2022-02-01 01:09:31+00:00,Industry Experiences with Large-Scale Refactoring,cs.SE,['cs.SE'],"[arxiv.Result.Author('James Ivers'), arxiv.Result.Author('Robert L. Nord'), arxiv.Result.Author('Ipek Ozkaya'), arxiv.Result.Author('Chris Seifried'), arxiv.Result.Author('Christopher S. Timperley'), arxiv.Result.Author('Marouane Kessentini')]","Software refactoring plays an important role in software engineering.
Developers often turn to refactoring when they want to restructure software to
improve its quality without changing its external behavior. Studies show that
small-scale (floss) refactoring is common in industry and can often be
performed by a single developer in short sessions, even though developers do
much of this work manually instead of using refactoring tools. However, some
refactoring efforts are much larger in scale, requiring entire teams and months
of effort, and the role of tools in these efforts is not as well studied. In
this paper, we report on a survey we conducted with developers to understand
large-scale refactoring, its prevalence, and how tools support it. Our results
from 107 industry developers demonstrate that projects commonly go through
multiple large-scale refactorings, each of which requires considerable effort.
While there is often a desire to refactor, other business concerns such as
developing new features often take higher priority. Our study finds that
developers use several categories of tools to support large-scale refactoring
and rely more heavily on general-purpose tools like IDEs than on tools designed
specifically to support refactoring. Tool support varies across the different
activities, with some particularly challenging activities seeing little use of
tools in practice. Our study demonstrates a clear need for better large-scale
refactoring tools and an opportunity for refactoring researchers to make a
difference in industry. The results we summarize in this paper is one concrete
step towards this goal."
1438,"In fact, best practice for prompt engineering
TOTALS                                                                                                                                                                                                 to coax better responses from an LLM requires further study.","-o1 1 3 0 1 9 1 3 8 7 8 2 10 7 3 5 1 7 2                                                                                                                   in the prompt (in addition to the snippet of interest) remains
                                         -o1 -s 1 4 1 1 6 0 3 7 5 6 4 8 7 2 3 4 1 5                                                                                                                    an open problem.","(o. src) 0 0 1 4 9 1 6 7 7 3 9 5 7 5 10 9 10 7
                                         -o1 -g 2 2 1 4 5 3 5 3 1 3 5 5 6 5 10 6 8 7                                                                                                                      To glean further insight as to possible reasons why we ob-
                                            -o1 0 4 2 1 4 4 6 4 1 3 8 6 5 5 9 7 7 10                                                                                                                   served some success when using code-davinci-001, which
                                         -o1 -s 2 4 0 5 5 1 8 6 2 6 6 1 5 4 10 7 9 9                                                                                                                   generates code, we also explored OpenAI’s recently released
                                                                                                                                                                                                       embedding models [19].",2022-02-02 17:09:15+00:00,Pop Quiz! Can a Large Language Model Help With Reverse Engineering?,cs.SE,"['cs.SE', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Hammond Pearce'), arxiv.Result.Author('Benjamin Tan'), arxiv.Result.Author('Prashanth Krishnamurthy'), arxiv.Result.Author('Farshad Khorrami'), arxiv.Result.Author('Ramesh Karri'), arxiv.Result.Author('Brendan Dolan-Gavitt')]","Large language models (such as OpenAI's Codex) have demonstrated impressive
zero-shot multi-task capabilities in the software domain, including code
explanation. In this work, we examine if this ability can be used to help with
reverse engineering. Specifically, we investigate prompting Codex to identify
the purpose, capabilities, and important variable names or values from code,
even when the code is produced through decompilation. Alongside an examination
of the model's responses in answering open-ended questions, we devise a
true/false quiz framework to characterize the performance of the language
model. We present an extensive quantitative analysis of the measured
performance of the language model on a set of program purpose identification
and information extraction tasks: of the 136,260 questions we posed, it
answered 72,754 correctly. A key takeaway is that while promising, LLMs are not
yet ready for zero-shot reverse engineering."
1546,"– To enable further research in this area, we make our issue SATD dataset
    publicly available1.","The proposed approach can enable re-
    searchers to automatically identify SATD within issues and conduct studies
    on the measurement, prioritization, as well as repayment of SATD in issue
    tracking systems on a large scale.","The dataset contains 23,180 issue sections, in which
    3,277 issue sections are classiﬁed as SATD issue sections.",2022-02-04 15:15:13+00:00,Identifying Self-Admitted Technical Debt in Issue Tracking Systems using Machine Learning,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Yikun Li'), arxiv.Result.Author('Mohamed Soliman'), arxiv.Result.Author('Paris Avgeriou')]","Technical debt is a metaphor indicating sub-optimal solutions implemented for
short-term benefits by sacrificing the long-term maintainability and
evolvability of software. A special type of technical debt is explicitly
admitted by software engineers (e.g. using a TODO comment); this is called
Self-Admitted Technical Debt or SATD. Most work on automatically identifying
SATD focuses on source code comments. In addition to source code comments,
issue tracking systems have shown to be another rich source of SATD, but there
are no approaches specifically for automatically identifying SATD in issues. In
this paper, we first create a training dataset by collecting and manually
analyzing 4,200 issues (that break down to 23,180 sections of issues) from
seven open-source projects (i.e., Camel, Chromium, Gerrit, Hadoop, HBase,
Impala, and Thrift) using two popular issue tracking systems (i.e., Jira and
Google Monorail). We then propose and optimize an approach for automatically
identifying SATD in issue tracking systems using machine learning. Our findings
indicate that: 1) our approach outperforms baseline approaches by a wide margin
with regard to the F1-score; 2) transferring knowledge from suitable datasets
can improve the predictive performance of our approach; 3) extracted SATD
keywords are intuitive and potentially indicating types and indicators of SATD;
4) projects using different issue tracking systems have less common SATD
keywords compared to projects using the same issue tracking system; 5) a small
amount of training data is needed to achieve good accuracy."
1547,"However, further research can potentially improve the F1-score
   obtained in our study (e.g., through using other machine learning tech-
   niques or trying richer datasets in the software engineering domain for
   transfer learning).","Identifying SATD in Issue Tracking Systems using Machine Learning  35

– Because of the high diversity of issues and the diﬀerent forms of SATD
   in issues, SATD identiﬁcation within issues is harder than in source code
   comments.","We also propose a number of implications for software practitioners:

– Our SATD identiﬁcation approach can help software developers and es-
   pecially project managers to evaluate the quality of their project.",2022-02-04 15:15:13+00:00,Identifying Self-Admitted Technical Debt in Issue Tracking Systems using Machine Learning,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Yikun Li'), arxiv.Result.Author('Mohamed Soliman'), arxiv.Result.Author('Paris Avgeriou')]","Technical debt is a metaphor indicating sub-optimal solutions implemented for
short-term benefits by sacrificing the long-term maintainability and
evolvability of software. A special type of technical debt is explicitly
admitted by software engineers (e.g. using a TODO comment); this is called
Self-Admitted Technical Debt or SATD. Most work on automatically identifying
SATD focuses on source code comments. In addition to source code comments,
issue tracking systems have shown to be another rich source of SATD, but there
are no approaches specifically for automatically identifying SATD in issues. In
this paper, we first create a training dataset by collecting and manually
analyzing 4,200 issues (that break down to 23,180 sections of issues) from
seven open-source projects (i.e., Camel, Chromium, Gerrit, Hadoop, HBase,
Impala, and Thrift) using two popular issue tracking systems (i.e., Jira and
Google Monorail). We then propose and optimize an approach for automatically
identifying SATD in issue tracking systems using machine learning. Our findings
indicate that: 1) our approach outperforms baseline approaches by a wide margin
with regard to the F1-score; 2) transferring knowledge from suitable datasets
can improve the predictive performance of our approach; 3) extracted SATD
keywords are intuitive and potentially indicating types and indicators of SATD;
4) projects using different issue tracking systems have less common SATD
keywords compared to projects using the same issue tracking system; 5) a small
amount of training data is needed to achieve good accuracy."
1570,"In contrast, investigating the demographic impact on the lack of align-
ment as observed in the assessments of some statements is out of the scope of
this paper and would require further research.","We merely observe that general alignment with respect to the
assessment direction typically brings with it alignment between demographic
groups.","4.1 Demographics

Overall, the survey was answered by 32 subjects.",2022-02-05 12:44:06+00:00,Event Log Generation: An Industry Perspective,cs.SE,['cs.SE'],"[arxiv.Result.Author('Timotheus Kampik'), arxiv.Result.Author('Mathias Weske')]","This paper presents the results of an industry expert survey about event log
generation in process mining. It takes academic assumptions as a starting point
and elicits practitioner's assessments of statements about process execution,
process scoping, process discovery, and process analysis. The results of the
survey shed some light on challenges and perspectives around event log
generation, as well as on the relationship between process models and process
execution and derive challenges for event log generation from it. The responses
indicate that particularly relevant challenges exist around data integration
and quality, and that process mining can benefit from a systematic integration
with more traditional and wide-spread business intelligence approaches."
1571,"In contrast, investigating the demographic impact on the lack of align-
ment as observed in the assessments of some statements is out of the scope of
this paper and would require further research.","We merely observe that general alignment with respect to the
assessment direction typically brings with it alignment between demographic
groups.","4.1 Demographics

Overall, the survey was answered by 32 subjects.",2022-02-05 12:44:06+00:00,Event Log Generation: An Industry Perspective,cs.SE,['cs.SE'],"[arxiv.Result.Author('Timotheus Kampik'), arxiv.Result.Author('Mathias Weske')]","This paper presents the results of an industry expert survey about event log
generation in process mining. It takes academic assumptions as a starting point
and elicits practitioner's assessments of statements about process execution,
process scoping, process discovery, and process analysis. The results of the
survey shed some light on challenges and perspectives around event log
generation, as well as on the relationship between process models and process
execution, and derive challenges for event log generation from it. The
responses indicate that particularly relevant challenges exist around data
integration and quality, and that process mining can benefit from a systematic
integration with more traditional and wide-spread business intelligence
approaches."
1606,"While our approach examines potential leakage in each app by itself, further study can be done on potential leaks
happening through multiple apps when they communicate with each other.","A study of how the sources and sinks appear in it, as well as modification of Taint-Things’ transformation rules will be
required to accommodate that.","SmartThings apps are usually self contained
in one file and TXL can handle that easily as an input, but extending the program to handle multiple files and analyze
their inter connectivity can be challenging.",2022-02-07 00:22:19+00:00,An Automated Approach for Privacy Leakage Identification in IoT Apps,cs.SE,"['cs.SE', 'cs.CR']","[arxiv.Result.Author(""Bara' Nazzal""), arxiv.Result.Author('Manar H. Alalfi')]","This paper presents a fully automated static analysis approach and a tool,
Taint-Things, for the identification of tainted flows in SmartThings IoT apps.
Taint-Things accurately identifies all tainted flows reported by one of the
state-of-the-art tools with at least 4 times improved performance. Our approach
reports potential vulnerable tainted flows in a form of a concise security
slice, where the relevant parts of the code are given with the lines affecting
the sensitive information, which could provide security auditors with an
effective and precise tool to pinpoint security issues in SmartThings apps
under test. We also present and test ways to add precision to Taint-Things by
adding extra sensitivities; we provide different approaches for flow, path and
context sensitive analyses through modules that can be added to Taint-Things.
We present experiments to evaluate Taint-Things by running it on a SmartThings
app dataset as well as testing for precision and recall on a set generated by a
mutation framework to see how much coverage is achieved without adding false
positives. This shows an improvement in performance both in terms of speed up
to 4 folds, as well as improving the precision avoiding false positives by
providing a higher level of flow and path sensitivity analysis in comparison
with one of state of the art tools."
1648,"The second issue requires further research, for example, by
have stringent accreditation requirements because of their direct     exploring reflective instances in groups and investigating whether
impact on human welfare.",These disciplines   [62].,"However, having such tools in software       early adopters can influence other students.",2022-02-07 17:26:58+00:00,A longitudinal case study on the effects of an evidence-based software engineering training,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sebastián Pizard'), arxiv.Result.Author('Diego Vallespir'), arxiv.Result.Author('Barbara Kitchenham')]","Context: Evidence-based software engineering (EBSE) can be an effective
resource to bridge the gap between academia and industry by balancing research
of practical relevance and academic rigor. To achieve this, it seems necessary
to investigate EBSE training and its benefits for the practice. Objective: We
sought both to develop an EBSE training course for university students and to
investigate what effects it has on the attitudes and behaviors of the trainees.
Method: We conducted a longitudinal case study to study our EBSE course and its
effects. For this, we collect data at the end of each EBSE course (2017, 2018,
and 2019), and in two follow-up surveys (one after 7 months of finishing the
last course, and a second after 21 months). Results: Our EBSE courses seem to
have taught students adequately and consistently. Half of the respondents to
the surveys report making use of the new skills from the course. The
most-reported effects in both surveys indicated that EBSE concepts increase
awareness of the value of research and evidence and EBSE methods improve
information gathering skills. Conclusions: As suggested by research in other
areas, training appears to play a key role in the adoption of evidence-based
practice. Our results indicate that our training method provides an
introduction to EBSE suitable for undergraduates. However, we believe it is
necessary to continue investigating EBSE training and its impact on software
engineering practice."
1708,"The directions of further research are related to the expansion of the apparatus for
describing library functions, including support for complex data types; expansion of
classes of detected integration errors; support for different modes of joint analysis of
libraries and their approximations.","Experiments on simple
projects with artificial and industrial libraries have shown the fundamental applicability
of the approach and the operability of the tool.","The areas of improvement of the tool are related to overcoming the existing technical
limitations of the tool, creating Java Standard Library specifications, conducting more
experiments with large-scale industrial projects using popular open libraries.",2022-02-08 15:49:42+00:00,SPIDER: Specification-based Integration Defect Revealer,cs.SE,['cs.SE'],"[arxiv.Result.Author('Vladislav Feofilaktov'), arxiv.Result.Author('Vladimir Itsykson')]","Modern software design practice implies widespread use in the development of
ready-made components, usually designed as external libraries. The undoubted
advantages of reusing third-party code can be offset by integration errors that
appear in the developed software. The reason for the appearance of such errors
is mainly due to misunderstanding or incomplete understanding by the programmer
of the details of external libraries such as an internal structure and the
subtleties of functioning. The documentation provided with the libraries is
often very sparse and describes only the main intended scenarios for the
interaction of the program and the library. In this paper, we propose the
approach based on the use of formal library specifications, which allows
detecting integration errors using static analysis methods. To do this, the
external library is described using the LibSL specification language, the
resulting description is translated into the internal data structures of the
KEX analyzer. The execution of the incorrect scenarios of library usage, such
as the incorrect sequence of method calls or the violation of the API function
contract, is marked in the program model with special built-in functions of the
KEX analyzer. Later, when analyzing the program, KEX becomes able to detect
integration errors, since incorrect library usage scenarios are diagnosed as
calling marked functions. The proposed approach is implemented as SPIDER
(SPecification-based Integration Defect Revealer), which is an extension of the
Kex analyzer and has proven its efficiency by detecting integration errors of
different classes on several special-made projects, as well as on several
projects taken from open repositories."
1716,"As uni-variate correlations are not solid
statistical evidence, it is imperative to run regression analysis to further study our conceptual
model with metric data.","Spearman method was
used to compute the correlation matrix, as it can measure the strength of both linear and
non-linear association between two variables [72].","To validate our conceptual model (Figure 1) that hypothesized the relationships among
our observed variables, we developed several empirical models:

   1.",2022-02-07 19:25:46+00:00,Test Automation Maturity Improves Product Quality -- Quantitative Study of Open Source Projects Using Continuous Integration,cs.SE,['cs.SE'],"[arxiv.Result.Author('Yuqing Wang'), arxiv.Result.Author('Mika Mäntylä'), arxiv.Result.Author('Zihao Liu'), arxiv.Result.Author('Jouni Markkula')]","The popularity of continuous integration (CI) is increasing as a result of
market pressure to release product features or updates frequently. The ability
of CI to deliver quality at speed depends on reliable test automation. In this
paper, we present an empirical study to observe the effect of test automation
maturity (assessed by standard best practices in the literature) on product
quality, test automation effort, and release cycle in the CI context of open
source projects. We run our test automation maturity survey and got responses
from 37 open source java projects. We also mined software repositories of the
same projects. The main results of regression analysis reveal that, higher
levels of test automation maturity are positively associated with higher
product quality (p-value=0.000624) and shorter release cycle (p-value=0.01891);
There is no statistically significant evidence of increased test automation
effort due to higher levels of test automation maturity and product quality.
Thus, we conclude that, a potential benefit of improving test automation
maturity (using standard best practices) is product quality improvement and
release cycle acceleration in the CI context of open source projects. We
encourage future research to extend our findings by adding more datasets with
different programming languages and CI tools, closed source projects, and
large-scale industrial projects. Our recommendation to practitioners (in the
similar CI context) is to utilize standard best practices to improve test
automation maturity."
1758,"To
        foster further research in the area of topical alignment of programming languages, we also provide supplementary
        material that contains our data and trained model.","The main contributions of our paper are:

     • Methodological: We propose an approach to assess the topical alignment of the documentation of a programming
        language using semi-supervised topic models of the posts of Q&A websites and a language’s documentation.","• Technical: We perform an empirical study about the topical alignment of the Rust documentation, followed by
        a discussion of the implications of our findings for documenters of programming languages.",2022-02-08 14:45:16+00:00,Assessing the alignment between the information needs of developers and the documentation of programming languages: A case study on Rust,cs.SE,"['cs.SE', 'cs.PL']","[arxiv.Result.Author('Filipe R. Cogo'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Ahmed E. Hassan')]","Programming language documentation refers to the set of technical documents
that provide application developers with a description of the high-level
concepts of a language. Such documentation is essential to support application
developers in the effective use of a programming language. One of the
challenges faced by documenters (i.e., personnel that produce documentation) is
to ensure that documentation has relevant information that aligns with the
concrete needs of developers. In this paper, we present an automated approach
to support documenters in evaluating the differences and similarities between
the concrete information need of developers and the current state of
documentation (a problem that we refer to as the topical alignment of a
programming language documentation). Our approach leverages semi-supervised
topic modelling to assess the similarities and differences between the topics
of Q&A posts and the official documentation. To demonstrate the application of
our approach, we perform a case study on the documentation of Rust. Our results
show that there is a relatively high level of topical alignment in Rust
documentation. Still, information about specific topics is scarce in both the
Q&A websites and the documentation, particularly related topics with
programming niches such as network, game, and database development. For other
topics (e.g., related topics with language features such as structs, patterns
and matchings, and foreign function interface), information is only available
on Q&A websites while lacking in the official documentation. Finally, we
discuss implications for programming language documenters, particularly how to
leverage our approach to prioritize topics that should be added to the
documentation."
1799,"However, the observations from
our study can give hints for further research about working environment (i.e.","As the goal of the survey
is to gain industrial insights on WFH and relating practices, we do not aim
at fully developing or validating hypotheses.",WFH) and diﬀerent properties of software companies and software startups.,2022-02-10 10:59:51+00:00,"Work-from-home and its implication for project management, resilience and innovation -- a global survey on software companies",cs.SE,['cs.SE'],"[arxiv.Result.Author('Anh Nguyen-Duc'), arxiv.Result.Author('Dron Khanna'), arxiv.Result.Author('Des Greer'), arxiv.Result.Author('Xiaofeng Wang'), arxiv.Result.Author('Luciana Martinez Zaina'), arxiv.Result.Author('Gerardo Matturro'), arxiv.Result.Author('Jorge Melegati'), arxiv.Result.Author('Eduardo Guerra'), arxiv.Result.Author('Giang Huong Le'), arxiv.Result.Author('Petri Kettunen'), arxiv.Result.Author('Sami Hyrynsalmi'), arxiv.Result.Author('Henry Edison'), arxiv.Result.Author('Afonso Sales'), arxiv.Result.Author('Didzis Rutitis'), arxiv.Result.Author('Kai-Kristian Kemell'), arxiv.Result.Author('Abdullah Aldaeej'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Juan Garbajosa'), arxiv.Result.Author('Pekka Abrahamsson')]","[Context] The COVID-19 pandemic has had a disruptive impact on how people
work and collaborate across all global economic sectors, including the software
business. While remote working is not new for software engineers, forced
Work-from-home situations to come with both constraints, limitations, and
opportunities for individuals, software teams and software companies. As the
""new normal"" for working might be based on the current state of Work From Home
(WFH), it is useful to understand what has happened and learn from that.
[Objective] The goal of this study is to gain insights on how their WFH
environment impacts software projects and software companies. We are also
interested in understanding if the impact differs between software startups and
established companies. [Method] We conducted a global-scale, cross-sectional
survey during spring and summer 2021. Our results are based on quantitative and
qualitative analysis of 297 valid responses. [Results] We observed a mixed
perception of the impact of WFH on software project management, resilience, and
innovation. Certain patterns on WFH, control and coordination mechanisms and
collaborative tools are observed globally. We find that team, agility and
leadership are the three most important factors for achieving resilience during
the pandemic. Although startups do not perceive the impact of WFH differently,
there is a difference between engineers who work in a small team context and
those who work in a large team context. [Conclusion] The result suggests a
contingency approach in studying and improving WFH practices and environment in
the future software industry."
1805,pointers for further research and development in section 5.,"Finally, we conclude and give some      result in a good computational cost-accuracy trade-off [26].","2.2 Software frameworks for neural network
2 RELATED WORK
                                                                                deployment on edge devices
In this section we give an overview of some interesting technologies
that can make neural networks more efficient to use on resource           Various software platforms are being actively developed to enable
constrained edge devices.",2022-02-10 14:29:51+00:00,A VM/Containerized Approach for Scaling TinyML Applications,cs.SE,['cs.SE'],"[arxiv.Result.Author('Meelis Lootus'), arxiv.Result.Author('Kartik Thakore'), arxiv.Result.Author('Sam Leroux'), arxiv.Result.Author('Geert Trooskens'), arxiv.Result.Author('Akshay Sharma'), arxiv.Result.Author('Holly Ly')]","Although deep neural networks are typically computationally expensive to use,
technological advances in both the design of hardware platforms and of neural
network architectures, have made it possible to use powerful models on edge
devices. To enable widespread adoption of edge based machine learning, we
introduce a set of open-source tools that make it easy to deploy, update and
monitor machine learning models on a wide variety of edge devices. Our tools
bring the concept of containerization to the TinyML world. We propose to
package ML and application logic as containers called Runes to deploy onto edge
devices. The containerization allows us to target a fragmented
Internet-of-Things (IoT) ecosystem by providing a common platform for Runes to
run across devices."
1840,"To this end, further research eﬀorts
    are required to explore and propose new patterns to particularly focus on quantum
    computing attributes (e.g.","However, these are generic or classical patterns
    that can be used to design any software system.","superposition and quantum entanglement) and facilitate the
    architecture of quantum software systems.",2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems-A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1841,"There is need for further research
that goes beyond architectural design and implementation to develop methods and solutions
for architecture-centric validation, deployment, and maintenance of QSAs.","Based on the published research on architectural process for QSAs, architectural
validation and deployment activities are mostly overlooked.",6.5.,2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems-A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1842,"To this end, further research is required to i) investigate and
propose the architectural patterns for medium or large-scale quantum software systems, and
ii) investigate the architectural framework for describing the architecture of quantum software
systems.","On the other hand, we also not ﬁnd any study that discusses architectural framework
for quantum software systems.",6.7.,2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems-A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1843,"To this end, further research eﬀorts
    are required to explore and propose new patterns to particularly focus on quantum
    computing attributes (e.g.","However, these are generic or classical patterns
    that can be used to design any software system.","superposition and quantum entanglement) and facilitate the
    architecture of quantum software systems.",2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems -- A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1844,"There is need for further research
that goes beyond architectural design and implementation to develop methods and solutions
for architecture-centric validation, deployment, and maintenance of QSAs.","Based on the published research on architectural process for QSAs, architectural
validation and deployment activities are mostly overlooked.",6.5.,2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems -- A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1845,"To this end, further research is required to i) investigate and
propose the architectural patterns for medium or large-scale quantum software systems, and
ii) investigate the architectural framework for describing the architecture of quantum software
systems.","On the other hand, we also not ﬁnd any study that discusses architectural framework
for quantum software systems.",6.7.,2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems -- A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1846,"To this end, further research eﬀorts
    are required to explore and propose new patterns to particularly focus on quantum
    computing attributes (e.g.","However, these are generic or classical patterns
    that can be used to design any software system.","superposition and quantum entanglement) and facilitate the
    architecture of quantum software systems.",2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems -- A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1847,"There is need for further research
that goes beyond architectural design and implementation to develop methods and solutions
for architecture-centric validation, deployment, and maintenance of QSAs.","Based on the published research on architectural process for QSAs, architectural
validation and deployment activities are mostly overlooked.",6.5.,2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems -- A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1848,"To this end, further research is required to i) investigate and
propose the architectural patterns for medium or large-scale quantum software systems, and
ii) investigate the architectural framework for describing the architecture of quantum software
systems.","On the other hand, we also not ﬁnd any study that discusses architectural framework
for quantum software systems.",6.7.,2022-02-11 08:42:14+00:00,Software Architecture for Quantum Computing Systems -- A Systematic Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Arif Ali Khan'), arxiv.Result.Author('Aakash Ahmad'), arxiv.Result.Author('Muhammad Waseem'), arxiv.Result.Author('Peng Liang'), arxiv.Result.Author('Mahdi Fahmideh'), arxiv.Result.Author('Tommi Mikkonen'), arxiv.Result.Author('Pekka Abrahamsson')]","Quantum computing systems rely on the principles of quantum mechanics to
perform a multitude of computationally challenging tasks more efficiently than
their classical counterparts. The architecture of software-intensive systems
can empower architects who can leverage architecture-centric processes,
practices, description languages, etc., to model, develop, and evolve quantum
computing software (quantum software for short) at higher abstraction levels.
We conducted a systematic literature review (SLR) to investigate (i)
architectural process, (ii) modeling notations, (iii) architecture design
patterns, (iv) tool support, and (iv) challenging factors for quantum software
architecture. Results of the SLR indicate that quantum software represents a
new genre of software-intensive systems; however, existing processes and
notations can be tailored to derive the architecting activities and develop
modeling languages for quantum software. Quantum bits (Qubits) mapped to
Quantum gates (Qugates) can be represented as architectural components and
connectors that implement quantum software. Tool-chains can incorporate
reusable knowledge and human roles (e.g., quantum domain engineers, quantum
code developers) to automate and customize the architectural process. Results
of this SLR can facilitate researchers and practitioners to develop new
hypotheses to be tested, derive reference architectures, and leverage
architecture-centric principles and practices to engineer emerging and next
generations of quantum software."
1902,"based approach against the failed test cases in the manually       Another promising avenue of further research is using the
written test suite.","Users could also
explore the effectiveness of model-based SCRATCH testing, we       be supported by techniques like graph minimisation to provide
compare the number of found program failures of the model-         further insights into eliminating erroneous state transitions.",models to automatically derive hints and feedback for learners.,2022-02-13 09:42:13+00:00,Model-based Testing of Scratch Programs,cs.SE,"['cs.SE', '97P50', 'D.2.5; K.3.2']","[arxiv.Result.Author('Katharina Götz'), arxiv.Result.Author('Patric Feldmeier'), arxiv.Result.Author('Gordon Fraser')]","Learners are often introduced to programming via dedicated languages such as
Scratch, where block-based commands are assembled visually in order to control
the interactions of graphical sprites. Automated testing of such programs is an
important prerequisite for supporting debugging, providing hints, or assessing
learning outcomes. However, writing tests for Scratch programs can be
challenging: The game-like and randomised nature of typical Scratch programs
makes it difficult to identify specific timed input sequences used to control
the programs. Furthermore, precise test assertions to check the resulting
program states are incompatible with the fundamental principle of creative
freedom in programming in Scratch, where correct program behaviour may be
implemented with deviations in the graphical appearance or timing of the
program. The event-driven and actor-oriented nature of Scratch programs,
however, makes them a natural fit for describing program behaviour using finite
state machines. In this paper, we introduce a model-based testing approach by
extending Whisker, an automated testing framework for Scratch programs. The
model-based extension describes expected program behaviour in terms of state
machines, which makes it feasible to check the abstract behaviour of a program
independent of exact timing and pixel-precise graphical details, and to
automatically derive test inputs testing even challenging programs. A video
demonstrating model-based testing with Whisker is available at the following
URL: https://youtu.be/edgCNbGSGEY"
2236,"Ablation Study of Tzer’s Components

5.2 RQ2: Ablation Study of Tzer

In this RQ, we further study the effectiveness of Tzer’s individual components:

   (1) RQ2.1: Is coverage feedback helpful for tensor compiler fuzzing?",6.,(2) RQ2.2: Can domain-specific mutations further improve tensor compiler fuzzing?,2022-02-21 01:48:11+00:00,Coverage-Guided Tensor Compiler Fuzzing with Joint IR-Pass Mutation,cs.SE,"['cs.SE', 'cs.CR', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Jiawei Liu'), arxiv.Result.Author('Yuxiang Wei'), arxiv.Result.Author('Sen Yang'), arxiv.Result.Author('Yinlin Deng'), arxiv.Result.Author('Lingming Zhang')]","In the past decade, Deep Learning (DL) systems have been widely deployed in
various domains to facilitate our daily life. Meanwhile, it is extremely
challenging to ensure the correctness of DL systems (e.g., due to their
intrinsic nondeterminism), and bugs in DL systems can cause serious
consequences and may even threaten human lives. In the literature, researchers
have explored various techniques to test, analyze, and verify DL models, since
their quality directly affects the corresponding system behaviors. Recently,
researchers have also proposed novel techniques for testing the underlying
operator-level DL libraries (such as TensorFlow and PyTorch), which provide
general binary implementations for each high-level DL operator for running
various DL models on many platforms. However, there is still limited work
targeting the reliability of the emerging tensor compilers, which aim to
directly compile high-level tensor computation graphs into high-performance
binaries for better efficiency, portability, and scalability. In this paper, we
target the important problem of tensor compiler testing, and have proposed
Tzer, a practical fuzzing technique for the widely used TVM tensor compiler.
Tzer focuses on mutating the low-level Intermediate Representation (IR) for TVM
due to the limited mutation space for the high-level IR. More specifically,
Tzer leverages both general-purpose and tensor-compiler-specific mutators
guided by coverage feedback for evolutionary IR mutation; furthermore, Tzer
also performs pass mutation in tandem with IR mutation for more effective
fuzzing. Our results show that Tzer substantially outperforms existing fuzzing
techniques on tensor compiler testing, with 75% higher coverage and 50% more
valuable tests than the 2nd-best technique. To date, Tzer has detected 49
previously unknown bugs for TVM, with 37 bugs confirmed and 25 bugs fixed (PR
merged)."
2478,"However, perceptual and
                                                            behavioural bugs require more sophisticated mechanisms
   Aiming to support further research into ABD, with a      for identiﬁcation, the current method is to rely on human
focus on perceptual bugs, we have developed World of        testers.",developer written conditions.,"Bugs (WOB), an open platform built on top of the Unity
game engine and Unity’s ML-Agents package [7].",2022-02-25 18:50:11+00:00,Learning to Identify Perceptual Bugs in 3D Video Games,cs.SE,"['cs.SE', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Benedict Wilkins'), arxiv.Result.Author('Kostas Stathis')]","Automated Bug Detection (ABD) in video games is composed of two distinct but
complementary problems: automated game exploration and bug identification.
Automated game exploration has received much recent attention, spurred on by
developments in fields such as reinforcement learning. The complementary
problem of identifying the bugs present in a player's experience has for the
most part relied on the manual specification of rules. Although it is widely
recognised that many bugs of interest cannot be identified with such methods,
little progress has been made in this direction. In this work we show that it
is possible to identify a range of perceptual bugs using learning-based methods
by making use of only the rendered game screen as seen by the player. To
support our work, we have developed World of Bugs (WOB) an open platform for
testing ABD methods in 3D game environments."
2479,"“What went wrong: A taxonomy of video
are other directions that require further study, many              game bugs”.","audio) and other     [9] Chris Lewis, Jim Whitehead, and Noah Wardrip-
kinds of bugs beyond those presented in this work, there           Fruin.","In: FDG 2010 - Proceedings of the
of which are supported by our platform.",2022-02-25 18:50:11+00:00,Learning to Identify Perceptual Bugs in 3D Video Games,cs.SE,"['cs.SE', 'cs.AI', 'cs.CV', 'cs.LG']","[arxiv.Result.Author('Benedict Wilkins'), arxiv.Result.Author('Kostas Stathis')]","Automated Bug Detection (ABD) in video games is composed of two distinct but
complementary problems: automated game exploration and bug identification.
Automated game exploration has received much recent attention, spurred on by
developments in fields such as reinforcement learning. The complementary
problem of identifying the bugs present in a player's experience has for the
most part relied on the manual specification of rules. Although it is widely
recognised that many bugs of interest cannot be identified with such methods,
little progress has been made in this direction. In this work we show that it
is possible to identify a range of perceptual bugs using learning-based methods
by making use of only the rendered game screen as seen by the player. To
support our work, we have developed World of Bugs (WOB) an open platform for
testing ABD methods in 3D game environments."
2601,"authors use this dataset to construct and test a naming convention     Thus, while the findings in this paper are limited to Java systems,
checking library for Java systems and conducted a study to examine     there is some likelihood that some of what we discuss extends to
                                                                       projects written in other languages– further study is needed to
R the adherence to naming conventions in Java programs [10].","Thefrom a dataset of engineered projects, meaning that the projects we
                                                                       studied all followed well-known software development practices.",An        determine how well our results generalize.,2022-02-28 22:01:06+00:00,Understanding Digits in Identifier Names: An Exploratory Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anthony Peruma'), arxiv.Result.Author('Christian D. Newman')]","Before any software maintenance can occur, developers must read the
identifier names found in the code to be maintained. Thus, high-quality
identifier names are essential for productive program comprehension and
maintenance activities. With developers free to construct identifier names to
their liking, it can be difficult to automatically reason about the quality and
semantics behind an identifier name. Studying the structure of identifier names
can help alleviate this problem. Existing research focuses on studying words
within identifiers, but there are other symbols that appear in identifier names
-- such as digits. This paper explores the presence and purpose of digits in
identifier names through an empirical study of 800 open-source Java systems. We
study how digits contribute to the semantics of identifier names and how
identifier names that contain digits evolve over time through renaming. We
envision our findings improving the efficiency of name appraisal and
recommendation tools and techniques."
2602,"authors use this dataset to construct and test a naming convention     Thus, while the findings in this paper are limited to Java systems,
checking library for Java systems and conducted a study to examine     there is some likelihood that some of what we discuss extends to
                                                                       projects written in other languages– further study is needed to
R the adherence to naming conventions in Java programs [10].","Thefrom a dataset of engineered projects, meaning that the projects we
                                                                       studied all followed well-known software development practices.",An        determine how well our results generalize.,2022-02-28 22:01:06+00:00,Understanding Digits in Identifier Names: An Exploratory Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anthony Peruma'), arxiv.Result.Author('Christian D. Newman')]","Before any software maintenance can occur, developers must read the
identifier names found in the code to be maintained. Thus, high-quality
identifier names are essential for productive program comprehension and
maintenance activities. With developers free to construct identifier names to
their liking, it can be difficult to automatically reason about the quality and
semantics behind an identifier name. Studying the structure of identifier names
can help alleviate this problem. Existing research focuses on studying words
within identifiers, but there are other symbols that appear in identifier names
-- such as digits. This paper explores the presence and purpose of digits in
identifier names through an empirical study of 800 open-source Java systems. We
study how digits contribute to the semantics of identifier names and how
identifier names that contain digits evolve over time through renaming. We
envision our findings improving the efficiency of name appraisal and
recommendation tools and techniques."
2603,"name ‘testUC_3_EraseFacetClassifier_NoSource’; this is a unique          Thus, while the findings in this paper are limited to Java systems,
traceability mechanism, especially for test cases                        there is some likelihood that some of what we discuss extends to
                                                                         projects written in other languages– further study is needed to

Domain/Technology The presence of digits is also due to de- determine how well our results generalize.","Though rare, we also           from a dataset of engineered projects, meaning that the projects we
encounter the use of digits to specify a specific use case, as in the    studied all followed well-known software development practices.",velopers using domain or technology names that contain digits.,2022-02-28 22:01:06+00:00,Understanding Digits in Identifier Names: An Exploratory Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anthony Peruma'), arxiv.Result.Author('Christian D. Newman')]","Before any software maintenance can occur, developers must read the
identifier names found in the code to be maintained. Thus, high-quality
identifier names are essential for productive program comprehension and
maintenance activities. With developers free to construct identifier names to
their liking, it can be difficult to automatically reason about the quality and
semantics behind an identifier name. Studying the structure of identifier names
can help alleviate this problem. Existing research focuses on studying words
within identifiers, but there are other symbols that appear in identifier names
-- such as digits. This paper explores the presence and purpose of digits in
identifier names through an empirical study of 800 open-source Java systems. We
study how digits contribute to the semantics of identifier names and how
identifier names that contain digits evolve over time through renaming. We
envision our findings improving the efficiency of name appraisal and
recommendation tools and techniques."
2604,"traceability mechanism, especially for test cases                        Thus, while the findings in this paper are limited to Java systems,
                                                                         there is some likelihood that some of what we discuss extends to

Domain/Technology The presence of digits is also due to de- projects written in other languages– further study is needed to

velopers using domain or technology names that contain digits.","Though rare, we also           Our sample consists of open-source Java systems that were selected
encounter the use of digits to specify a specific use case, as in the    from a dataset of engineered projects, meaning that the projects we
name ‘testUC_3_EraseFacetClassifier_NoSource’; this is a unique          studied all followed well-known software development practices.",determine how well our results generalize.,2022-02-28 22:01:06+00:00,Understanding Digits in Identifier Names: An Exploratory Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anthony Peruma'), arxiv.Result.Author('Christian D. Newman')]","Before any software maintenance can occur, developers must read the
identifier names found in the code to be maintained. Thus, high-quality
identifier names are essential for productive program comprehension and
maintenance activities. With developers free to construct identifier names to
their liking, it can be difficult to automatically reason about the quality and
semantics behind an identifier name. Studying the structure of identifier names
can help alleviate this problem. Existing research focuses on studying words
within identifiers, but there are other symbols that appear in identifier names
-- such as digits. This paper explores the presence and purpose of digits in
identifier names through an empirical study of 800 open-source Java systems. We
study how digits contribute to the semantics of identifier names and how
identifier names that contain digits evolve over time through renaming. We
envision our findings improving the efficiency of name appraisal and
recommendation tools and techniques."
2628,"To mitigate the problem, multiple approaches have
need for further research assessing the role played by additional    been proposed: 1) Rerun & Disable: This is the simplest way
factors like project and organizational domains.","pointed out the         constant rate of 1.5 % of all test executions producing ﬂaky
importance of replications to corroborate their ﬁndings and the      results [6].","We are not only     to deal with ﬂakiness, but it wastes resources and leads to
following this call but go beyond existing experience reports        delay or loss of test feedback.",2022-03-01 14:29:37+00:00,A Survey on How Test Flakiness Affects Developers and What Support They Need To Address It,cs.SE,['cs.SE'],"[arxiv.Result.Author('Martin Gruber'), arxiv.Result.Author('Gordon Fraser')]","Non-deterministically passing and failing test cases, so-called flaky tests,
have recently become a focus area of software engineering research. While this
research focus has been met with some enthusiastic endorsement from industry,
prior work nevertheless mostly studied flakiness using a code-centric approach
by mining software repositories. What data extracted from software repositories
cannot tell us, however, is how developers perceive flakiness: How prevalent is
test flakiness in developers' daily routine, how does it affect them, and most
importantly: What do they want us researchers to do about it? To answer these
questions, we surveyed 335 professional software developers and testers in
different domains. The survey respondents confirm that flaky tests are a common
and serious problem, thus reinforcing ongoing research on flaky test detection.
Developers are less worried about the computational costs caused by re-running
tests and more about the loss of trust in the test outcomes. Therefore, they
would like to have IDE plugins to detect flaky code as well as better
visualizations of the problem, particularly dashboards showing test outcomes
over time; they also wish for more training and information on flakiness. These
important aspects will require the attention of researchers as well as tool
developers."
2629,"To mitigate the problem, multiple approaches have
need for further research assessing the role played by additional    been proposed: 1) Rerun & Disable: This is the simplest way
factors like project and organizational domains.","pointed out the         constant rate of 1.5 % of all test executions producing ﬂaky
importance of replications to corroborate their ﬁndings and the      results [6].","We are not only     to deal with ﬂakiness, but it wastes resources and leads to
following this call but go beyond existing experience reports        delay or loss of test feedback.",2022-03-01 14:29:37+00:00,A Survey on How Test Flakiness Affects Developers and What Support They Need To Address It,cs.SE,['cs.SE'],"[arxiv.Result.Author('Martin Gruber'), arxiv.Result.Author('Gordon Fraser')]","Non-deterministically passing and failing test cases, so-called flaky tests,
have recently become a focus area of software engineering research. While this
research focus has been met with some enthusiastic endorsement from industry,
prior work nevertheless mostly studied flakiness using a code-centric approach
by mining software repositories. What data extracted from software repositories
cannot tell us, however, is how developers perceive flakiness: How prevalent is
test flakiness in developers' daily routine, how does it affect them, and most
importantly: What do they want us researchers to do about it? To answer these
questions, we surveyed 335 professional software developers and testers in
different domains. The survey respondents confirm that flaky tests are a common
and serious problem, thus reinforcing ongoing research on flaky test detection.
Developers are less worried about the computational costs caused by re-running
tests and more about the loss of trust in the test outcomes. Therefore, they
would like to have IDE plugins to detect flaky code as well as better
visualizations of the problem, particularly dashboards showing test outcomes
over time; they also wish for more training and information on flakiness. These
important aspects will require the attention of researchers as well as tool
developers."
2630,"To mitigate the problem, multiple approaches have
need for further research assessing the role played by additional    been proposed: 1) Rerun & Disable: This is the simplest way
factors like project and organizational domains.","pointed out the         constant rate of 1.5 % of all test executions producing ﬂaky
importance of replications to corroborate their ﬁndings and the      results [6].","We are not only     to deal with ﬂakiness, but it wastes resources and leads to
following this call but go beyond existing experience reports        delay or loss of test feedback.",2022-03-01 14:29:37+00:00,A Survey on How Test Flakiness Affects Developers and What Support They Need To Address It,cs.SE,['cs.SE'],"[arxiv.Result.Author('Martin Gruber'), arxiv.Result.Author('Gordon Fraser')]","Non-deterministically passing and failing test cases, so-called flaky tests,
have recently become a focus area of software engineering research. While this
research focus has been met with some enthusiastic endorsement from industry,
prior work nevertheless mostly studied flakiness using a code-centric approach
by mining software repositories. What data extracted from software repositories
cannot tell us, however, is how developers perceive flakiness: How prevalent is
test flakiness in developers' daily routine, how does it affect them, and most
importantly: What do they want us researchers to do about it? To answer these
questions, we surveyed 335 professional software developers and testers in
different domains. The survey respondents confirm that flaky tests are a common
and serious problem, thus reinforcing ongoing research on flaky test detection.
Developers are less worried about the computational costs caused by re-running
tests and more about the loss of trust in the test outcomes. Therefore, they
would like to have IDE plugins to detect flaky code as well as better
visualizations of the problem, particularly dashboards showing test outcomes
over time; they also wish for more training and information on flakiness. These
important aspects will require the attention of researchers as well as tool
developers."
2631,"To mitigate the problem, multiple approaches have
need for further research assessing the role played by additional    been proposed: 1) Rerun & Disable: This is the simplest way
factors like project and organizational domains.","pointed out the         constant rate of 1.5 % of all test executions producing ﬂaky
importance of replications to corroborate their ﬁndings and the      results [6].","We are not only     to deal with ﬂakiness, but it wastes resources and leads to
following this call but go beyond existing experience reports        delay or loss of test feedback.",2022-03-01 14:29:37+00:00,A Survey on How Test Flakiness Affects Developers and What Support They Need To Address It,cs.SE,['cs.SE'],"[arxiv.Result.Author('Martin Gruber'), arxiv.Result.Author('Gordon Fraser')]","Non-deterministically passing and failing test cases, so-called flaky tests,
have recently become a focus area of software engineering research. While this
research focus has been met with some enthusiastic endorsement from industry,
prior work nevertheless mostly studied flakiness using a code-centric approach
by mining software repositories. What data extracted from software repositories
cannot tell us, however, is how developers perceive flakiness: How prevalent is
test flakiness in developers' daily routine, how does it affect them, and most
importantly: What do they want us researchers to do about it? To answer these
questions, we surveyed 335 professional software developers and testers in
different domains. The survey respondents confirm that flaky tests are a common
and serious problem, thus reinforcing ongoing research on flaky test detection.
Developers are less worried about the computational costs caused by re-running
tests and more about the loss of trust in the test outcomes. Therefore, they
would like to have IDE plugins to detect flaky code as well as better
visualizations of the problem, particularly dashboards showing test outcomes
over time; they also wish for more training and information on flakiness. These
important aspects will require the attention of researchers as well as tool
developers."
2658,"We recognize that further research in the area of business process modelling for Ethereum smart contracts
may benefit our results.","To
overcome this limitation of allowing only a threshold of variation in domain models during the code clone detection
method, we analysed and updated the derived domain models depending on their applicability in a specific business
process.","Threats to External Validity

The dataset analyzed in this research consisted of a subset of all the Solidity smart contracts deployed onto the Ethereum
main-net that corresponds to DApps.",2022-03-01 22:25:00+00:00,Mining Domain Models in Ethereum DApps using Code Cloning,cs.SE,['cs.SE'],"[arxiv.Result.Author('Noama Fatima Samreen'), arxiv.Result.Author('Manar H. Alalfi')]","This research study explores the use of near-miss clone detection to support
the characterization of domain models of smart contracts for each of the
popular domains in which smart contracts are being rapidly adopted. In this
paper, we leverage the code clone detection techniques to detect similarities
in functions of the smart contracts deployed onto the Ethereum blockchain
network. We analyze the clusters of code clones and the semantics of the code
fragments in the clusters in an attempt to categorize them and discover the
structural models of the patterns in code clones."
2659,We recognize that further research with a larger dataset may strengthen the correlation established in our results.,"Moreover, DApps are being built on top of other blockchain networks

Manuscript submitted to ACM
Mining Domain Models in Ethereum DApps using Code Cloning  17

as well and might represent different characteristics when compared to the smart contracts analyzed in this research.","We
highlight that the contribution of this paper is to conduct an initial study towards producing an MDE framework to
efficiently develop DApps that are maintainable and secure.",2022-03-01 22:25:00+00:00,Mining Domain Models in Ethereum DApps using Code Cloning,cs.SE,['cs.SE'],"[arxiv.Result.Author('Noama Fatima Samreen'), arxiv.Result.Author('Manar H. Alalfi')]","This research study explores the use of near-miss clone detection to support
the characterization of domain models of smart contracts for each of the
popular domains in which smart contracts are being rapidly adopted. In this
paper, we leverage the code clone detection techniques to detect similarities
in functions of the smart contracts deployed onto the Ethereum blockchain
network. We analyze the clusters of code clones and the semantics of the code
fragments in the clusters in an attempt to categorize them and discover the
structural models of the patterns in code clones."
2857,"It combines the retrieval technique and neural model
To further study the generalization of our model, we evaluate ECMG                   and generates informative commit messages.","This paper proposes a new exemplar-based commit message gener-
                                                                                     ation model.","Experimental re-
when the dataset is split by projects.",2022-03-05 10:55:15+00:00,ECMG: Exemplar-based Commit Message Generation,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ensheng Shia'), arxiv.Result.Author('Yanlin Wangb'), arxiv.Result.Author('Lun Du'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Dongmei Zhang'), arxiv.Result.Author('Hongbin Sun')]","Commit messages concisely describe the content of code diffs (i.e., code
changes) and the intent behind them. Recently, many approaches have been
proposed to generate commit messages automatically. The information
retrieval-based methods reuse the commit messages of similar code diffs, while
the neural-based methods learn the semantic connection between code diffs and
commit messages. However, the reused commit messages might not accurately
describe the content/intent of code diffs and neural-based methods tend to
generate high-frequent and repetitive tokens in the corpus. In this paper, we
combine the advantages of the two technical routes and propose a novel
exemplar-based neural commit message generation model, which treats the similar
commit message as an exemplar and leverages it to guide the neural network
model to generate an accurate commit message. We perform extensive experiments
and the results confirm the effectiveness of our model."
2858,"It combines the retrieval technique and neural model
To further study the generalization of our model, we evaluate ECMG                   and generates informative commit messages.","This paper proposes a new exemplar-based commit message gener-
                                                                                     ation model.","Experimental re-
when the dataset is split by projects.",2022-03-05 10:55:15+00:00,ECMG: Exemplar-based Commit Message Generation,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ensheng Shia'), arxiv.Result.Author('Yanlin Wang'), arxiv.Result.Author('Lun Du'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Dongmei Zhang'), arxiv.Result.Author('Hongbin Sun')]","Commit messages concisely describe the content of code diffs (i.e., code
changes) and the intent behind them. Recently, many approaches have been
proposed to generate commit messages automatically. The information
retrieval-based methods reuse the commit messages of similar code diffs, while
the neural-based methods learn the semantic connection between code diffs and
commit messages. However, the reused commit messages might not accurately
describe the content/intent of code diffs and neural-based methods tend to
generate high-frequent and repetitive tokens in the corpus. In this paper, we
combine the advantages of the two technical routes and propose a novel
exemplar-based neural commit message generation model, which treats the similar
commit message as an exemplar and leverages it to guide the neural network
model to generate an accurate commit message. We perform extensive experiments
and the results confirm the effectiveness of our model."
2859,"Our frame-
                                                                     work can improve the performance of the original
We further study whether our framework can en-                       model from 7% to 73%.",models in all programming languages.,"Especially, after applying
hance the performance of the existing Seq2Seq                        our framework, the performance of NMTGen has
neural network model in commit message gen-                          more than 20% improvement on all programming
eration.",2022-03-05 10:55:15+00:00,RACE: Retrieval-Augmented Commit Message Generation,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ensheng Shi'), arxiv.Result.Author('Yanlin Wang'), arxiv.Result.Author('Wei Tao'), arxiv.Result.Author('Lun Du'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Dongmei Zhang'), arxiv.Result.Author('Hongbin Sun')]","Commit messages are important for software development and maintenance. Many
neural network-based approaches have been proposed and shown promising results
on automatic commit message generation. However, the generated commit messages
could be repetitive or redundant. In this paper, we propose RACE, a new
retrieval-augmented neural commit message generation method, which treats the
retrieved similar commit as an exemplar and leverages it to generate an
accurate commit message. As the retrieved commit message may not always
accurately describe the content/intent of the current code diff, we also
propose an exemplar guider, which learns the semantic similarity between the
retrieved and current code diff and then guides the generation of commit
message based on the similarity. We conduct extensive experiments on a large
public dataset with five programming languages. Experimental results show that
RACE can outperform all baselines. Furthermore, RACE can boost the performance
of existing Seq2Seq models in commit message generation."
2935,"The model and our experiments are available for further research as part of our
           replication package.1
JINGO is situated in the current state of the art in the following way.","• Evaluation results using our dataset and an established benchmark of software projects with various devel-

           opment history lengths.","Wen et al.’s technique (Locus) was the ﬁrst to build
a bug localization model based on changesets, with considerable success Wen et al.",2022-03-07 17:47:08+00:00,Online Adaptable Bug Localization for Rapidly Evolving Software,cs.SE,['cs.SE'],"[arxiv.Result.Author('Agnieszka Ciborowska'), arxiv.Result.Author('Michael J. Decker'), arxiv.Result.Author('Kostadin Damevski')]","Bug localization aims to reduce debugging time by recommending program
elements that are relevant for a specific bug report. To date, researchers have
primarily addressed this problem by applying different information retrieval
techniques that leverage similarities between a given bug report and source
code. However, with modern software development trending towards increased
speed of software change and continuous delivery to the user, the current
generation of bug localization techniques, which cannot quickly adapt to the
latest version of the software, is becoming inadequate. In this paper, we
propose a technique for online bug localization, which enables rapidly
updatable bug localization models. More specifically, we propose a streaming
bug localization technique, based on an ensemble of online topic models, that
is able to adapt to both specific (with explicit code mentions) and more
abstract bug reports. By using changesets (diffs) as the input instead of a
snapshot of the source code, the model naturally integrates defect prediction
and co-change information into its prediction. Initial results indicate that
the proposed approach improves bug localization performance for 42 out of 56
evaluation projects, with an average MAP improvement of 5.9%."
3006,"Whilst we tuned the appropriate
                                                                         hyperparameters of all baselines and models, this tuning may be
We motivate the need for significant further research into this area.","However, our experiments were still
                                                                         able to show that our preliminary application of these techniques
6.2 Future Work                                                          can improve SVP performance.",sub-optimal.,2022-03-09 01:04:44+00:00,Noisy Label Learning for Security Defects,cs.SE,['cs.SE'],"[arxiv.Result.Author('Roland Croft'), arxiv.Result.Author('M. Ali Babar'), arxiv.Result.Author('Huaming Chen')]","Data-driven software engineering processes, such as vulnerability prediction
heavily rely on the quality of the data used. In this paper, we observe that it
is infeasible to obtain a noise-free security defect dataset in practice.
Despite the vulnerable class, the non-vulnerable modules are difficult to be
verified and determined as truly exploit free given the limited manual efforts
available. It results in uncertainty, introduces labeling noise in the datasets
and affects conclusion validity. To address this issue, we propose novel
learning methods that are robust to label impurities and can leverage the most
from limited label data; noisy label learning. We investigate various noisy
label learning methods applied to software vulnerability prediction.
Specifically, we propose a two-stage learning method based on noise cleaning to
identify and remediate the noisy samples, which improves AUC and recall of
baselines by up to 8.9% and 23.4%, respectively. Moreover, we discuss several
hurdles in terms of achieving a performance upper bound with semi-omniscient
knowledge of the label noise. Overall, the experimental results show that
learning from noisy labels can be effective for data-driven software and
security analytics."
3007,"6.2 Future Work
6 DISCUSSION
                                                                          We motivate the need for significant further research into this area.",manifest more commonly for particular SV types or file locations.,"Our findings suggest that NLL techniques are able to provide ad-          Overcoming data challenges is vital for the success of SVP and
ditional knowledge to SVP models that is able to improve their            other data-driven tasks.",2022-03-09 01:04:44+00:00,Noisy Label Learning for Security Defects,cs.SE,['cs.SE'],"[arxiv.Result.Author('Roland Croft'), arxiv.Result.Author('M. Ali Babar'), arxiv.Result.Author('Huaming Chen')]","Data-driven software engineering processes, such as vulnerability prediction
heavily rely on the quality of the data used. In this paper, we observe that it
is infeasible to obtain a noise-free security defect dataset in practice.
Despite the vulnerable class, the non-vulnerable modules are difficult to be
verified and determined as truly exploit free given the limited manual efforts
available. It results in uncertainty, introduces labeling noise in the datasets
and affects conclusion validity. To address this issue, we propose novel
learning methods that are robust to label impurities and can leverage the most
from limited label data; noisy label learning. We investigate various noisy
label learning methods applied to software vulnerability prediction.
Specifically, we propose a two-stage learning method based on noise cleaning to
identify and remediate the noisy samples, which improves AUC and recall of
baselines by up to 8.9% and 23.4%, respectively. Moreover, we discuss several
hurdles in terms of achieving a performance upper bound with semi-omniscient
knowledge of the label noise. Overall, the experimental results show that
learning from noisy labels can be effective for data-driven software and
security analytics."
3056,Because the majority of our participants were graduate           We believe our work opens several avenues for further research.,"the twelve combinations of our two factors that are our feature
metrics.","students without background in feature models, we cannot gener-        Among them are: studying other eye-tracking measures of cogni-
alize our ﬁndings for other groups like experienced developers or      tive load from pupil size to EEG [14], employing ﬁner grain AOIs
variability experts.",2022-03-09 22:22:48+00:00,An Empirical Eye-Tracking Study of Feature Model Comprehension,cs.SE,"['cs.SE', 'K.6.3']","[arxiv.Result.Author('Elmira Rezaei Sepasi'), arxiv.Result.Author('Kambiz Nezami Balouchi'), arxiv.Result.Author('Julien Mercier'), arxiv.Result.Author('Roberto Erick Lopez-Herrejon')]","Software Product Lines (SPLs) are families of related software systems which
are distinguished by the set of features each system provides. Feature Models
are the de facto standard for modelling the variability of SPLs because they
describe the features, their relations, and all the combinations of features
that constitute a SPL. Because of their key role, feature models are at the
core of many tasks in SPL engineering. Our work presents an empirical study on
the comprehension of feature models for the task of checking the validity of
configurations. Our study explored the relation between the number of features
and the number of cross-tree constraints with the accuracy of participants'
answers to validity checking questions, and used eye fixations for analyzing
the difficulty in interpreting fixated information and the amount of cognitive
processing of the different parts of the feature model stimuli. We found that
answer accuracy does not relate individually to the number of features or to
the number of cross-tree constrains of a feature model, but both factors do
show an interaction on accuracy. Additionally, our study identified differences
in feature models with cross-tree constraints in both number of fixations and
fixation time, but no differences in those models without cross-tree
constraints."
3122,"While it
creat                  25                        2.74%                   can be argued that the cause can be due to the datasets and mining
T merg                                           1.64%                   tools, further research in this area is warranted, especially since
rewrit                 11                        1.64%                   we did not observe a significant difference between the operations
                                                                         applied to remove technical debt and general refactoring.","That said, when comparing
chang                  26                        6.86%                   the results from prior studies, such as [12] and [30], we observe a
                                                 3.02%                   slight consensus between the reported frequent operations.","replac                 10                             %
                                                                            Our RQ2 results provide interesting insight into the association
extend                 6                                                 of refactoring and technical debt repayment and support a further
IN prior studies [9, 20], we know that clean-up is an activity associatedstudy to create a more formal and exhaustive set of causes.",2022-03-10 21:59:50+00:00,Refactoring Debt: Myth or Reality? An Exploratory Study on the Relationship Between Technical Debt and Refactoring,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anthony Peruma'), arxiv.Result.Author('Eman Abdullah AlOmar'), arxiv.Result.Author('Christian D. Newman'), arxiv.Result.Author('Mohamed Wiem Mkaouer'), arxiv.Result.Author('Ali Ouni')]","To meet project timelines or budget constraints, developers intentionally
deviate from writing optimal code to feasible code in what is known as
incurring Technical Debt (TD). Furthermore, as part of planning their
correction, developers document these deficiencies as comments in the code
(i.e., self-admitted technical debt or SATD). As a means of improving source
code quality, developers often apply a series of refactoring operations to
their codebase. In this study, we explore developers repaying this debt through
refactoring operations by examining occurrences of SATD removal in the code of
76 open-source Java systems. Our findings show that TD payment usually occurs
with refactoring activities and developers refactor their code to remove TD for
specific reasons. We envision our findings supporting vendors in providing
tools to better support developers in the automatic repayment of technical
debt."
3157,"To further study MemoRec’s perfor-
performance of MemoRec?","The decrease in accuracy related to the
                                                                adoption of D2 as shown in Table 8(a) is conﬁrmed
5.2 RQ2: How does the training data aﬀect the                   also in Table 8(b).","mance, we compute and report in Table.",2022-03-11 16:49:14+00:00,MemoRec: A Recommender System for Assisting Modelers in Specifying Metamodels,cs.SE,['cs.SE'],"[arxiv.Result.Author('Juri Di Rocco'), arxiv.Result.Author('Davide Di Ruscio'), arxiv.Result.Author('Claudio Di Sipio'), arxiv.Result.Author('Phuong T. Nguyen'), arxiv.Result.Author('Alfonso Pierantonio')]","Model Driven Engineering (MDE) has been widely applied in software
development, aiming to facilitate the coordination among various stakeholders.
Such a methodology allows for a more efficient and effective development
process. Nevertheless, modeling is a strenuous activity that requires proper
knowledge of components, attributes, and logic to reach the level of
abstraction required by the application domain. In particular, metamodels play
an important role in several paradigms, and specifying wrong entities or
attributes in metamodels can negatively impact on the quality of the produced
artifacts as well as other elements of the whole process. During the
metamodeling phase, modelers can benefit from assistance to avoid mistakes,
e.g., getting recommendations like meta-classes and structural features
relevant to the metamodel being defined. However, suitable machinery is needed
to mine data from repositories of existing modeling artifacts and compute
recommendations. In this work, we propose MemoRec, a novel approach that makes
use of a collaborative filtering strategy to recommend valuable entities
related to the metamodel under construction. Our approach can provide
suggestions related to both metaclasses and structured features that should be
added in the metamodel under definition. We assess the quality of the work with
respect to different metrics, i.e., success rate, precision, and recall. The
results demonstrate that MemoRec is capable of suggesting relevant items given
a partial metamodel and supporting modelers in their task."
3188,"[23] and Alshayban et
their further study [13], they conducted their study from             al.","In               choice to do this task, and Eler et al.",[19] did it in this way.,2022-03-12 12:31:59+00:00,Accessible or Not? An Empirical Investigation of Android App Accessibility,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sen Chen'), arxiv.Result.Author('Chunyang Chen'), arxiv.Result.Author('Lingling Fan'), arxiv.Result.Author('Mingming Fan'), arxiv.Result.Author('Xian Zhan'), arxiv.Result.Author('Yang Liu')]","Mobile apps provide new opportunities to people with disabilities to act
independently in the world. Motivated by this trend, researchers have conducted
empirical studies by using the inaccessibility issue rate of each page (i.e.,
screen level) to represent the characteristics of mobile app accessibility.
However, there still lacks an empirical investigation directly focusing on the
issues themselves (i.e., issue level) to unveil more fine-grained findings, due
to the lack of an effective issue detection method and a relatively
comprehensive dataset of issues.
  To fill in this literature gap, we first propose an automated app page
exploration tool, named Xbot, to facilitate app accessibility testing and
automatically collect accessibility issues by leveraging the instrumentation
technique and static program analysis. Owing to the relatively high activity
coverage (around 80%) achieved by Xbot when exploring apps, Xbot achieves
better performance on accessibility issue collection than existing testing
tools such as Google Monkey. With Xbot, we are able to collect a relatively
comprehensive accessibility issue dataset and finally collect 86,767 issues
from 2,270 unique apps including both closed-source and open-source apps, based
on which we further carry out an empirical study from the perspective of
accessibility issues themselves to investigate novel characteristics of
accessibility issues. Specifically, we extensively investigate these issues by
checking 1) the overall severity of issues with multiple criteria, 2) the
in-depth relation between issue types and app categories, GUI component types,
3) the frequent issue patterns quantitatively, and 4) the fixing status of
accessibility issues."
3372,"The popularity of NVD is mainly be-
                                        paving the way for further research in this direction.","Overall, we provide the initial yet                               Database (NVD) [61] has been most commonly used for building
                                        promising ML-based baselines for function-level SV assessment,                                  SV assessment models [40].","cause it has SV-specific information (e.g., CVSS metrics) and less
                                                                                                                                        noise in SV descriptions than other Issue Tracking Systems (ITSs)
                                        CCS CONCEPTS                                                                                    like Bugzilla [23].",2022-03-16 06:29:40+00:00,On the Use of Fine-grained Vulnerable Code Statements for Software Vulnerability Assessment Models,cs.SE,"['cs.SE', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Triet H. M. Le'), arxiv.Result.Author('M. Ali Babar')]","Many studies have developed Machine Learning (ML) approaches to detect
Software Vulnerabilities (SVs) in functions and fine-grained code statements
that cause such SVs. However, there is little work on leveraging such detection
outputs for data-driven SV assessment to give information about exploitability,
impact, and severity of SVs. The information is important to understand SVs and
prioritize their fixing. Using large-scale data from 1,782 functions of 429 SVs
in 200 real-world projects, we investigate ML models for automating
function-level SV assessment tasks, i.e., predicting seven Common Vulnerability
Scoring System (CVSS) metrics. We particularly study the value and use of
vulnerable statements as inputs for developing the assessment models because
SVs in functions are originated in these statements. We show that vulnerable
statements are 5.8 times smaller in size, yet exhibit 7.5-114.5% stronger
assessment performance (Matthews Correlation Coefficient (MCC)) than
non-vulnerable statements. Incorporating context of vulnerable statements
further increases the performance by up to 8.9% (0.64 MCC and 0.75 F1-Score).
Overall, we provide the initial yet promising ML-based baselines for
function-level SV assessment, paving the way for further research in this
direction."
3402,"Our future work, we will further study how individual smell instances
aﬀect the work of developers and what aspects make an instance more severe
than another.","In conclusion, this paper provides a much clearer, and backed by empirical
evidence, view on the issues experienced by practitioners in the presence of
AS.","Acknowledgements

We would like to thank the Center for Information Technology of the Univer-
sity of Groningen for their support and for providing access to the Peregrine
high performance computing cluster.",2022-03-16 15:50:19+00:00,On the evolution and impact of Architectural Smells -- An industrial case study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Darius Sas'), arxiv.Result.Author('Paris Avgeriou'), arxiv.Result.Author('Umut Uyumaz')]","Architectural smells (AS) are notorious for their long-term impact on the
Maintainability and Evolvability of software systems. The majority of research
work has investigated this topic by mining software repositories of open source
Java systems, making it hard to generalise and apply them to an industrial
context and other programming languages. To address this research gap, we
conducted an embedded multiple-case case study, in collaboration with a large
industry partner, to study how AS evolve in industrial embedded systems. We
detect and track AS in 9 C/C++ projects with over 30 releases for each project
that span over two years of development, with over 20 millions lines of code in
the last release only. In addition to these quantitative results, we also
interview 12 among the developers and architects working on these projects,
collecting over six hours of qualitative data about the usefulness of AS
analysis and the issues they experienced while maintaining and evolving
artefacts affected by AS. Our quantitative findings show how individual smell
instances evolve over time, how long they typically survive within the system,
how they overlap with instances of other smell types, and finally what the
introduction order of smell types is when they overlap. Our qualitative
findings, instead, provide insights on the effects of AS on the long-term
maintainability and evolvability of the system, supported by several excerpts
from our interviews. Practitioners also mention what parts of the AS analysis
actually provide actionable insights that they can use to plan refactoring
activities."
3482,"Since addressing these three requirements concurrently is
under-explored in the literature, the presented ﬁndings are                         [10] M. Pol, R. Teunissen, and E. Van Veenendaal, Software testing: A guide
useful for further research in this direction.","Uitgeverij kleine Uil, 2013.",Six variants of                            to the TMap approach.,2022-03-17 20:16:45+00:00,Prioritized Variable-length Test Cases Generation for Finite State Machines,cs.SE,"['cs.SE', 'cs.AI', 'cs.FL']","[arxiv.Result.Author('Vaclav Rechtberger'), arxiv.Result.Author('Miroslav Bures'), arxiv.Result.Author('Bestoun S. Ahmed'), arxiv.Result.Author('Youcef Belkhier'), arxiv.Result.Author('Jiri Nema'), arxiv.Result.Author('Hynek Schvach')]","Model-based Testing (MBT) is an effective approach for testing when parts of
a system-under-test have the characteristics of a finite state machine (FSM).
Despite various strategies in the literature on this topic, little work exists
to handle special testing situations. More specifically, when concurrently: (1)
the test paths can start and end only in defined states of the FSM, (2) a
prioritization mechanism that requires only defined states and transitions of
the FSM to be visited by test cases is required, and (3) the test paths must be
in a given length range, not necessarily of explicit uniform length. This paper
presents a test generation strategy that satisfies all these requirements. A
concurrent combination of these requirements is highly practical for real
industrial testing. Six variants of possible algorithms to implement this
strategy are described. Using a mixture of 180 problem instances from real
automotive and defense projects and artificially generated FSMs, all variants
are compared with a baseline strategy based on an established N-switch coverage
concept modification. Various properties of the generated test paths and their
potential to activate fictional defects defined in FSMs are evaluated. The
presented strategy outperforms the baseline in most problem configurations. Out
of the six analyzed variants, three give the best results even though a
universal best performer is hard to identify. Depending on the application of
the FSM, the strategy and evaluation presented in this paper are applicable
both in testing functional and non-functional software requirements."
3483,"Since addressing these three requirements concurrently is
under-explored in the literature, the presented ﬁndings are                         [10] M. Pol, R. Teunissen, and E. Van Veenendaal, Software testing: A guide
useful for further research in this direction.","Uitgeverij kleine Uil, 2013.",Six variants of                            to the TMap approach.,2022-03-17 20:16:45+00:00,Prioritized Variable-length Test Cases Generation for Finite State Machines,cs.SE,"['cs.SE', 'cs.AI', 'cs.FL']","[arxiv.Result.Author('Vaclav Rechtberger'), arxiv.Result.Author('Miroslav Bures'), arxiv.Result.Author('Bestoun S. Ahmed'), arxiv.Result.Author('Youcef Belkhier'), arxiv.Result.Author('Jiri Nema'), arxiv.Result.Author('Hynek Schvach')]","Model-based Testing (MBT) is an effective approach for testing when parts of
a system-under-test have the characteristics of a finite state machine (FSM).
Despite various strategies in the literature on this topic, little work exists
to handle special testing situations. More specifically, when concurrently: (1)
the test paths can start and end only in defined states of the FSM, (2) a
prioritization mechanism that requires only defined states and transitions of
the FSM to be visited by test cases is required, and (3) the test paths must be
in a given length range, not necessarily of explicit uniform length. This paper
presents a test generation strategy that satisfies all these requirements. A
concurrent combination of these requirements is highly practical for real
industrial testing. Six variants of possible algorithms to implement this
strategy are described. Using a mixture of 180 problem instances from real
automotive and defense projects and artificially generated FSMs, all variants
are compared with a baseline strategy based on an established N-switch coverage
concept modification. Various properties of the generated test paths and their
potential to activate fictional defects defined in FSMs are evaluated. The
presented strategy outperforms the baseline in most problem configurations. Out
of the six analyzed variants, three give the best results even though a
universal best performer is hard to identify. Depending on the application of
the FSM, the strategy and evaluation presented in this paper are applicable
both in testing functional and non-functional software requirements."
3523,"Although further research on this is needed,
the results obtained make us hypothesize that the value of θ depends on the
development practices – basically, how strict code reviewing practices are and
if commit squashing is frequently used when merging changes into the source
code.","Using 6 large FOSS projects as case studies, we have shown how the model
can be applied and ﬁne-tuned.","If so, the future applicability of our model would not require to survey
developers, as it would suﬃce to use a value of θ obtained from projects that
follow similar practices.",2022-03-18 12:20:59+00:00,Development Effort Estimation in Free/Open Source Software from Activity in Version Control Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Gregorio Robles'), arxiv.Result.Author('Andrea Capiluppi'), arxiv.Result.Author('Jesus M. Gonzalez-Barahona'), arxiv.Result.Author('Bjorn Lundell'), arxiv.Result.Author('Jonas Gamalielsson')]","Effort estimation models are a fundamental tool in software management, and
used as a forecast for resources, constraints and costs associated to software
development. For Free/Open Source Software (FOSS) projects, effort estimation
is especially complex: professional developers work alongside occasional,
volunteer developers, so the overall effort (in person-months) becomes
non-trivial to determine.
  The objective of this work it to develop a simple effort estimation model for
FOSS projects, based on the historic data of developers' effort. The model is
fed with direct developer feedback to ensure its accuracy.
  After extracting the personal development profiles of several thousands of
developers from 6 large FOSS projects, we asked them to fill in a questionnaire
to determine if they should be considered as full-time developers in the
project that they work in. Their feedback was used to fine-tune the value of an
effort threshold, above which developers can be considered as full-time.
  With the help of the over 1,000 questionnaires received, we were able to
determine, for every project in our sample, the threshold of commits that
separates full-time from non-full-time developers.%, and that minimizes the
type I and type II errors. We finally offer guidelines and a tool to apply our
model to FOSS projects that use a version control system."
3568,"Therefore, we also recommend further research to propose strategies for apps
development for diﬀerent demographic groups.","Developers need to be conscious about addressing the values that have strong

                                                    30
correlations with demographics during apps development for diﬀerent demographic
groups.","Implication 1 for Apps Development

   Bangladeshi agriculture apps developers are recommended to apply a partic-
   ular set of apps design strategies and/or approach for a speciﬁc group of the
   target end-users according to their value preferences.",2022-03-19 19:14:13+00:00,Investigating End-Users' Values in Agriculture Mobile Applications Development: An Empirical Study on Bangladeshi Female Farmers,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rifat Ara Shams'), arxiv.Result.Author('Mojtaba Shahin'), arxiv.Result.Author('Gillian Oliver'), arxiv.Result.Author('Harsha Perera'), arxiv.Result.Author('Jon Whittle'), arxiv.Result.Author('Arif Nurwidyantoro'), arxiv.Result.Author('Waqar Hussain')]","The omnipresent nature of mobile applications (apps) in all aspects of daily
lives raises the necessity of reflecting end-users values (e.g., fairness,
honesty, etc.) in apps. However, there are limited considerations of end-users
values in apps development. Value violations by apps have been reported in the
media and are responsible for end-users dissatisfaction and negative
socio-economic consequences. Value violations may bring more severe and lasting
problems for marginalized and vulnerable end-users of apps, which have been
explored less (if at all) in the software engineering community. However,
understanding the values of the end-users of apps is the essential first step
towards values-based apps development. This research aims to fill this gap by
investigating the human values of Bangladeshi female farmers as a marginalized
and vulnerable group of end-users of Bangladeshi agriculture apps. We conducted
an empirical study that collected and analyzed data from a survey with 193
Bangladeshi female farmers to explore the underlying factor structure of the
values of Bangladeshi female farmers and the significance of demographics on
their values. The results identified three underlying factors of Bangladeshi
female farmers. The first factor comprises of five values: benevolence,
security, conformity, universalism, and tradition. The second factor consists
of two values: self-direction and stimulation. The third factor includes three
values: power, achievement, and hedonism. We also identified strong influences
of demographics on some of the values of Bangladeshi female farmers. For
example, area has significant impacts on three values: hedonism, achievement,
and tradition. Similarly, there are also strong influences of household income
on power and security."
3569,"Therefore, we recommend further research to understand when reinforcing existing
value sets in apps design might have positive impacts and when negative.","For example, a subjugated population might have values that reﬂect subjugation.","As people
ubiquitously use apps in their daily activities nowadays, this is an opportunity to
use apps to discourage the negative values of the end-users and help free them from
those negative values.",2022-03-19 19:14:13+00:00,Investigating End-Users' Values in Agriculture Mobile Applications Development: An Empirical Study on Bangladeshi Female Farmers,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rifat Ara Shams'), arxiv.Result.Author('Mojtaba Shahin'), arxiv.Result.Author('Gillian Oliver'), arxiv.Result.Author('Harsha Perera'), arxiv.Result.Author('Jon Whittle'), arxiv.Result.Author('Arif Nurwidyantoro'), arxiv.Result.Author('Waqar Hussain')]","The omnipresent nature of mobile applications (apps) in all aspects of daily
lives raises the necessity of reflecting end-users values (e.g., fairness,
honesty, etc.) in apps. However, there are limited considerations of end-users
values in apps development. Value violations by apps have been reported in the
media and are responsible for end-users dissatisfaction and negative
socio-economic consequences. Value violations may bring more severe and lasting
problems for marginalized and vulnerable end-users of apps, which have been
explored less (if at all) in the software engineering community. However,
understanding the values of the end-users of apps is the essential first step
towards values-based apps development. This research aims to fill this gap by
investigating the human values of Bangladeshi female farmers as a marginalized
and vulnerable group of end-users of Bangladeshi agriculture apps. We conducted
an empirical study that collected and analyzed data from a survey with 193
Bangladeshi female farmers to explore the underlying factor structure of the
values of Bangladeshi female farmers and the significance of demographics on
their values. The results identified three underlying factors of Bangladeshi
female farmers. The first factor comprises of five values: benevolence,
security, conformity, universalism, and tradition. The second factor consists
of two values: self-direction and stimulation. The third factor includes three
values: power, achievement, and hedonism. We also identified strong influences
of demographics on some of the values of Bangladeshi female farmers. For
example, area has significant impacts on three values: hedonism, achievement,
and tradition. Similarly, there are also strong influences of household income
on power and security."
3594,We first collected posts associated   in further research in these fields.,"Regardless, the dynamics between hu-
sponse to a values violation claim, and may modify their original        man values and technical requirements should be directly addressed
post itself in light of the claim.",with the 315 comments which cited human values violations.,2022-03-20 13:33:23+00:00,Human Values Violations in Stack Overflow: An Exploratory Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sara Krishtul'), arxiv.Result.Author('Mojtaba Shahin'), arxiv.Result.Author('Humphrey O. Obie'), arxiv.Result.Author('Hourieh Khalajzadeh'), arxiv.Result.Author('Fan Gai'), arxiv.Result.Author('Ali Rezaei Nasab'), arxiv.Result.Author('John Grundy')]","A growing number of software-intensive systems are being accused of violating
or ignoring human values (e.g., privacy, inclusion, and social responsibility),
and this poses great difficulties to individuals and society. Such violations
often occur due to the solutions employed and decisions made by developers of
such systems that are misaligned with user values. Stack Overflow is the most
popular QA website among developers to share their issues, solutions (e.g.,
code snippets), and decisions during software development. We conducted an
exploratory study to investigate the occurrence of human values violations in
Stack Overflow posts. As comments under posts are often used to point out the
possible issues and weaknesses of the posts, we analyzed 2000 Stack Overflow
comments and their corresponding posts (1980 unique questions or answers) to
identify the types of human values violations and the reactions of Stack
Overflow users to such violations. Our study finds that 315 out of 2000
comments contain concerns indicating their associated posts (313 unique posts)
violate human values. Leveraging Schwartz's theory of basic human values as the
most widely used values model, we show that hedonism and benevolence are the
most violated value categories. We also find the reaction of Stack Overflow
commenters to perceived human values violations is very quick, yet the majority
of posts (76.35%) accused of human values violation do not get downvoted at
all. Finally, we find that the original posters rarely react to the concerns of
potential human values violations by editing their posts. At the same time,
they usually are receptive when responding to these comments in follow-up
comments of their own."
3689,These are just examples of the fields of sciences that could be covered by further research.,"Seeing how this kind of software is also focused on
increasing precision of the results or reducing the errors of the approach, it is possible that
the development process of the software from these fields of science follow similar cycle as
the one described in this thesis.","By analyzing the development history of variety of tools with cooperation of specialists from
different fields of science a thesis encompassing wider type of software could be posed.",2022-03-22 12:06:17+00:00,Methodology for development of scientific software and test frameworks in function of precision of the expected results,cs.SE,['cs.SE'],[arxiv.Result.Author('T. Przedzinski')],"This dissertation focuses on the development process of scientific software.
It presents a methodology that has emerged over time during development of
Monte Carlo tools for high energy physics experiments. A short description of
the physics background needed to understand the subjects presented in this
dissertation is included and the different types of software created for the
physics experiments are outlined. Challenges related to the scientific software
development are presented.
  The development process of several projects is described. The development of
subsequent milestones of these projects follow the cycle of improving the
physics model, describing the model using mathematical formalism, implementing
the model with numerical approximations, creating the software framework,
documenting and validating results. The relation between increased precision of
the results and increased complexity of tests and test frameworks is also
demonstrated based on these projects. The subject of scientific software
testing is addressed and the taxonomy of the scientific software tests is
presented including testing techniques used in the development of this
software.
  Author of this dissertation co-authored tools presented in it. Some of these
tools have been introduced into the HEP community. Some gained large user base
and are in active use by the community. Some of them are part of analyses
performed by experiments around Large Hadron Collider. The analysis of the
development process of these tools can help estimate the effort needed to
improve the design and precision of complex algorithms."
3690,"One of the necessary step of further research of the scientific software development process
would be formalization of a systematic approach to such analysis.","Such analysis could reveal the development patterns of large scientific projects and
would allow to compare and contrast these patterns with the development cycle presented
in this thesis.","This could help other re-
searches in their own analysis of different types of software.",2022-03-22 12:06:17+00:00,Methodology for development of scientific software and test frameworks in function of precision of the expected results,cs.SE,['cs.SE'],[arxiv.Result.Author('T. Przedzinski')],"This dissertation focuses on the development process of scientific software.
It presents a methodology that has emerged over time during development of
Monte Carlo tools for high energy physics experiments. A short description of
the physics background needed to understand the subjects presented in this
dissertation is included and the different types of software created for the
physics experiments are outlined. Challenges related to the scientific software
development are presented.
  The development process of several projects is described. The development of
subsequent milestones of these projects follow the cycle of improving the
physics model, describing the model using mathematical formalism, implementing
the model with numerical approximations, creating the software framework,
documenting and validating results. The relation between increased precision of
the results and increased complexity of tests and test frameworks is also
demonstrated based on these projects. The subject of scientific software
testing is addressed and the taxonomy of the scientific software tests is
presented including testing techniques used in the development of this
software.
  Author of this dissertation co-authored tools presented in it. Some of these
tools have been introduced into the HEP community. Some gained large user base
and are in active use by the community. Some of them are part of analyses
performed by experiments around Large Hadron Collider. The analysis of the
development process of these tools can help estimate the effort needed to
improve the design and precision of complex algorithms."
3752,"In section 3, we discuss challenges
and the need for further research, before concluding our paper in section 4.","This paper is structured as follows: Section 2 presents the potential of an
end-to-end continuous software engineering.","2 Continuous Software Engineering in a Nutshell

The idea of continuous software engineering originates from lean principles in
manufacturing.",2022-03-23 13:39:31+00:00,Software Engineering in the Wild,cs.SE,['cs.SE'],"[arxiv.Result.Author('Eriks Klotins'), arxiv.Result.Author('Tony Gorschek')]","Software is becoming a critical component of most products and organizational
functions. The ability to continuously improve software determines how well the
organization can respond to market opportunities. Continuous software
engineering promises numerous advantages over sprint-based or plan-driven
development. However, implementing a continuous software engineering pipeline
in an existing organization is challenging.
  In this invited position paper, we discuss the adoption challenges and argue
for a more systematic methodology to drive the adoption of continuous
engineering. Our discussion is based on ongoing work with several industrial
partners as well as experience reported in both state-of-practice and
state-of-the-art.
  We conclude that the adoption of continuous software engineering primarily
requires analysis of the organization, its goals, and constraints. One size
does not fit all purposes, meaning that many of the principles behind
continuous engineering are relevant for most organizations, but the level of
realization and the benefits may still vary. The main hindrances to continuous
flow of software arise from sub-optimal organizational structures and the lack
of alignment. Once those are removed, the organization can implement automation
to further improve the software delivery."
3753,"In section 3, we discuss challenges
and the need for further research, before concluding our paper in section 4.","This paper is structured as follows: Section 2 presents the potential of an
end-to-end continuous software engineering.","2 Continuous Software Engineering in a Nutshell

The idea of continuous software engineering originates from lean principles in
manufacturing.",2022-03-23 13:39:31+00:00,Continuous Software Engineering in the Wild,cs.SE,['cs.SE'],"[arxiv.Result.Author('Eriks Klotins'), arxiv.Result.Author('Tony Gorschek')]","Software is becoming a critical component of most products and organizational
functions. The ability to continuously improve software determines how well the
organization can respond to market opportunities. Continuous software
engineering promises numerous advantages over sprint-based or plan-driven
development. However, implementing a continuous software engineering pipeline
in an existing organization is challenging.
  In this invited position paper, we discuss the adoption challenges and argue
for a more systematic methodology to drive the adoption of continuous
engineering. Our discussion is based on ongoing work with several industrial
partners as well as experience reported in both state-of-practice and
state-of-the-art.
  We conclude that the adoption of continuous software engineering primarily
requires analysis of the organization, its goals, and constraints. One size
does not fit all purposes, meaning that many of the principles behind
continuous engineering are relevant for most organizations, but the level of
realization and the benefits may still vary. The main hindrances to continuous
flow of software arise from sub-optimal organizational structures and the lack
of alignment. Once those are removed, the organization can implement automation
to further improve the software delivery."
3779,"Overall, we found         importance in practice, it proves the fact of the existence of certain
that there is a low acceptance of SQA standards in the practice,          phenomena and provides directions for further research.","Although our results
   After comparing the solutions the developers applied to ensure         cannot reflect the real frequency of the observations, e.g., some
the quality of the system and AI-components, coding and quality           challenges mentioned by few respondents might have a higher
standards were applied only for system quality.",and testing is the main quality assurance technique.,2022-03-23 19:43:35+00:00,What is Software Quality for AI Engineers? Towards a Thinning of the Fog,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Valentina Golendukhina'), arxiv.Result.Author('Valentina Lenarduzzi'), arxiv.Result.Author('Michael Felderer')]","It is often overseen that AI-enabled systems are also software systems and
therefore rely on software quality assurance (SQA). Thus, the goal of this
study is to investigate the software quality assurance strategies adopted
during the development, integration, and maintenance of AI/ML components and
code. We conducted semi-structured interviews with representatives of ten
Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the
interview data identified 12 issues in the development of AI/ML components.
Furthermore, we identified when quality issues arise in AI/ML components and
how they are detected. The results of this study should guide future work on
software quality assurance processes and techniques for AI/ML components."
3780,"Acknowledging this gap contributes
to the further research in the area that is needed to develop new         7 CONCLUSION
solutions and achieve certain levels of maturity for SQA practices
in AI development.",quality of AI-based products.,"In this paper, we presented an exploratory study to investigate SQA
                                                                          strategies adopted during the development, integration, and main-
6 THREATS TO VALIDITY                                                     tenance of AI/ML components.",2022-03-23 19:43:35+00:00,What is Software Quality for AI Engineers? Towards a Thinning of the Fog,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Valentina Golendukhina'), arxiv.Result.Author('Valentina Lenarduzzi'), arxiv.Result.Author('Michael Felderer')]","It is often overseen that AI-enabled systems are also software systems and
therefore rely on software quality assurance (SQA). Thus, the goal of this
study is to investigate the software quality assurance strategies adopted
during the development, integration, and maintenance of AI/ML components and
code. We conducted semi-structured interviews with representatives of ten
Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the
interview data identified 12 issues in the development of AI/ML components.
Furthermore, we identified when quality issues arise in AI/ML components and
how they are detected. The results of this study should guide future work on
software quality assurance processes and techniques for AI/ML components."
3781,"Overall, further research
                                                                          in the area and training of the developers can improve the quality
                                                                          dramatically and contribute to future generations of data scientists.",maintaining the quality of AI systems.,What is Software Quality for AI Engineers?,2022-03-23 19:43:35+00:00,What is Software Quality for AI Engineers? Towards a Thinning of the Fog,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Valentina Golendukhina'), arxiv.Result.Author('Valentina Lenarduzzi'), arxiv.Result.Author('Michael Felderer')]","It is often overseen that AI-enabled systems are also software systems and
therefore rely on software quality assurance (SQA). Thus, the goal of this
study is to investigate the software quality assurance strategies adopted
during the development, integration, and maintenance of AI/ML components and
code. We conducted semi-structured interviews with representatives of ten
Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the
interview data identified 12 issues in the development of AI/ML components.
Furthermore, we identified when quality issues arise in AI/ML components and
how they are detected. The results of this study should guide future work on
software quality assurance processes and techniques for AI/ML components."
3807,"By charting the landscape of expected                         when ﬁnancial transactions are tied to delivery of physical
                                        beneﬁts, strategies, and implications of scaling agile beyond                      products; (iii) mechanical development striving for long-term
                                        teams in automotive, we enable further research and process                        predictability, whereas software development strives for short-
                                        improvements.","foreseeable implications and side-effects of scaling agile beyond                  safety; (ii) the inherently long feedback loop from customers
                                        teams in automotive.","term agility, and (iv) handling the technical complexity, and
                                                                                                                           associated expertise needed, of interdependent components in
                                           Managerial Relevance Statement—From interviews with high-                       large-scale systems.",2022-03-24 15:43:59+00:00,Agile Beyond Teams and Feedback Beyond Software in Automotive Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('S. Magnus Ågren'), arxiv.Result.Author('Rogardt Heldal'), arxiv.Result.Author('Eric Knauss'), arxiv.Result.Author('Patrizio Pelliccione')]","In order to increase the ability to build complex, software-intensive
systems, as well as to decrease time-to-market for new functionality,
automotive companies aim to scale agile methods beyond individual teams. This
is challenging, given the specifics of automotive systems that are often
safety-critical and consist of software, hardware, and mechanical components.
This paper investigates the concrete reasons for scaling agility beyond teams,
the strategies that support such scaling, and foreseeable implications that
such a drastic organizational change will entail. The investigation is based on
a qualitative case study, with data from 20 semi-structured interviews with
managers and technical experts at two automotive companies. At the core of our
findings are observations about establishing an agile vehicle-level feedback
loop beyond individual teams. (I) We find that automotive OEMs aim to decrease
lead-time of development. (II) We also identify 7 strategies that aim to enable
scaled-agile beyond teams. (III) Finally, we extract 6 foreseeable implications
and side-effects of scaling agile beyond teams in automotive. By charting the
landscape of expected benefits, strategies, and implications of scaling agile
beyond teams in automotive, we enable further research and process
improvements."
4029,"For further research, we can observe
more on polynomial regression results and measure the actual results in a speciﬁc time period of the development
and may apply other statistical techniques for a better understanding of productivity.","Most importantly we have utilized simple but effective analysis methodologies such as Polynomial regression of a
small degree 5 to predict the latency and we have observed that our method has provided many unique results, and
have shown how to predict and measure the quality of open-source software.","7 Team Membership and Attestation of Work

Seongwoo Choi, Terry Guan, Sutej Kulkarni, Jason (Jinxiao) Song, and Sairamvinay Vijayaraghavan agree to partic-
ipate and work on this project with the understanding of the project.",2022-03-25 23:11:56+00:00,Influence of Communication Among Shared Developers on the Productivity of Open Source Software Projects,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sairamvinay Vijayaraghavan'), arxiv.Result.Author('Jinxiao Song'), arxiv.Result.Author('Terry Guan'), arxiv.Result.Author('Seongwoo Choi'), arxiv.Result.Author('Sutej Kulkarni')]","Many software developers rely on open source software for developing their
applications and writing their source codes. Measuring an independent project's
overall productivity is still an open problem for many technology companies. In
this project, we address to bridge the gap of analyzing which are the most
important features for prediction of a productivity based system. We have
chosen to collect data from GitHub via their application programming interfaces
(API) and analyze the data we gathered to understand the relation between the
average time to close an issue and the features that we collected. Since most
of the data we gathered were not Gaussian, we had to preprocess the data using
outlier detection and applying transformations before statistical modeling. The
best model we observed was polynomial regression with degree 5. Overall, we
noticed that there are many aspects of software development that make
developers increase their productivity."
4113,"To facilitate further research in
the area of Jupyter notebooks, we make this dataset available to         6.1.1 Code Writing.",the properly licensed notebooks.,"This group of metrics describes the amount
the public [13].",2022-03-30 23:59:23+00:00,A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts,cs.SE,['cs.SE'],"[arxiv.Result.Author('Konstantin Grotov'), arxiv.Result.Author('Sergey Titov'), arxiv.Result.Author('Vladimir Sotnikov'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","In recent years, Jupyter notebooks have grown in popularity in several
domains of software engineering, such as data science, machine learning, and
computer science education. Their popularity has to do with their rich features
for presenting and visualizing data, however, recent studies show that
notebooks also share a lot of drawbacks: high number of code clones, low
reproducibility, etc.
  In this work, we carry out a comparison between Python code written in
Jupyter Notebooks and in traditional Python scripts. We compare the code from
two perspectives: structural and stylistic. In the first part of the analysis,
we report the difference in the number of lines, the usage of functions, as
well as various complexity metrics. In the second part, we show the difference
in the number of stylistic issues and provide an extensive overview of the 15
most frequent stylistic issues in the studied mediums. Overall, we demonstrate
that notebooks are characterized by the lower code complexity, however, their
code could be perceived as more entangled than in the scripts. As for the
style, notebooks tend to have 1.4 times more stylistic issues, but at the same
time, some of them are caused by specific coding practices in notebooks and
should be considered as false positives. With this research, we want to pave
the way to studying specific problems of notebooks that should be addressed by
the development of notebook-specific tools, and provide various insights that
can be useful in this regard."
4114,Other results point the way to a lot of potential further research:          Summary for the complexity metrics.,"unique user-defined functions, but use them more frequently.","If we combine the re-
  it seems that notebook users define functions for specific reasons,         sults for several metrics, we can say that, on average, functions
  and the practices of the API methods usage are very distinct                in the notebooks are more coupled, but they have less complexity
  for notebooks.",2022-03-30 23:59:23+00:00,A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts,cs.SE,['cs.SE'],"[arxiv.Result.Author('Konstantin Grotov'), arxiv.Result.Author('Sergey Titov'), arxiv.Result.Author('Vladimir Sotnikov'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","In recent years, Jupyter notebooks have grown in popularity in several
domains of software engineering, such as data science, machine learning, and
computer science education. Their popularity has to do with their rich features
for presenting and visualizing data, however, recent studies show that
notebooks also share a lot of drawbacks: high number of code clones, low
reproducibility, etc.
  In this work, we carry out a comparison between Python code written in
Jupyter Notebooks and in traditional Python scripts. We compare the code from
two perspectives: structural and stylistic. In the first part of the analysis,
we report the difference in the number of lines, the usage of functions, as
well as various complexity metrics. In the second part, we show the difference
in the number of stylistic issues and provide an extensive overview of the 15
most frequent stylistic issues in the studied mediums. Overall, we demonstrate
that notebooks are characterized by the lower code complexity, however, their
code could be perceived as more entangled than in the scripts. As for the
style, notebooks tend to have 1.4 times more stylistic issues, but at the same
time, some of them are caused by specific coding practices in notebooks and
should be considered as false positives. With this research, we want to pave
the way to studying specific problems of notebooks that should be addressed by
the development of notebook-specific tools, and provide various insights that
can be useful in this regard."
4115,"The
                                                                         main goal of this work is to lay the foundation for further research
   Summary for the Best Practices issues.",should be taken as possible hypotheses and not as conclusions.,"In the notebooks, we           into the specifics of computational notebooks as a medium.",2022-03-30 23:59:23+00:00,A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts,cs.SE,['cs.SE'],"[arxiv.Result.Author('Konstantin Grotov'), arxiv.Result.Author('Sergey Titov'), arxiv.Result.Author('Vladimir Sotnikov'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","In recent years, Jupyter notebooks have grown in popularity in several
domains of software engineering, such as data science, machine learning, and
computer science education. Their popularity has to do with their rich features
for presenting and visualizing data, however, recent studies show that
notebooks also share a lot of drawbacks: high number of code clones, low
reproducibility, etc.
  In this work, we carry out a comparison between Python code written in
Jupyter Notebooks and in traditional Python scripts. We compare the code from
two perspectives: structural and stylistic. In the first part of the analysis,
we report the difference in the number of lines, the usage of functions, as
well as various complexity metrics. In the second part, we show the difference
in the number of stylistic issues and provide an extensive overview of the 15
most frequent stylistic issues in the studied mediums. Overall, we demonstrate
that notebooks are characterized by the lower code complexity, however, their
code could be perceived as more entangled than in the scripts. As for the
style, notebooks tend to have 1.4 times more stylistic issues, but at the same
time, some of them are caused by specific coding practices in notebooks and
should be considered as false positives. With this research, we want to pave
the way to studying specific problems of notebooks that should be addressed by
the development of notebook-specific tools, and provide various insights that
can be useful in this regard."
4116,"development of notebook-oriented tools to support the practice         We hope that in further research, we would be able to take more
  of iterative development and create solutions that would help to       complex metrics, and the ease of extending Matroskin can be
  clean up the code after multiple iterations of changes.","In this work, we implemented a very
  Looking at these issues, we can say that it is crucial in the further  limited set of metrics in order to process a large amount of data.",helpful in this regard.,2022-03-30 23:59:23+00:00,A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts,cs.SE,['cs.SE'],"[arxiv.Result.Author('Konstantin Grotov'), arxiv.Result.Author('Sergey Titov'), arxiv.Result.Author('Vladimir Sotnikov'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","In recent years, Jupyter notebooks have grown in popularity in several
domains of software engineering, such as data science, machine learning, and
computer science education. Their popularity has to do with their rich features
for presenting and visualizing data, however, recent studies show that
notebooks also share a lot of drawbacks: high number of code clones, low
reproducibility, etc.
  In this work, we carry out a comparison between Python code written in
Jupyter Notebooks and in traditional Python scripts. We compare the code from
two perspectives: structural and stylistic. In the first part of the analysis,
we report the difference in the number of lines, the usage of functions, as
well as various complexity metrics. In the second part, we show the difference
in the number of stylistic issues and provide an extensive overview of the 15
most frequent stylistic issues in the studied mediums. Overall, we demonstrate
that notebooks are characterized by the lower code complexity, however, their
code could be perceived as more entangled than in the scripts. As for the
style, notebooks tend to have 1.4 times more stylistic issues, but at the same
time, some of them are caused by specific coding practices in notebooks and
should be considered as false positives. With this research, we want to pave
the way to studying specific problems of notebooks that should be addressed by
the development of notebook-specific tools, and provide various insights that
can be useful in this regard."
4117,"In notebooks, users less frequently       code, and will help with the development of notebook-specific tools
define functions, but when they do, they use them significantly           and further research of this medium.","We hope
                                                                          that the conclusions that we made in this paper will highlight the
   Firstly, we clearly see a major difference in how user-defined         unique properties of computational notebooks as a medium for
functions are created and used.",more.,2022-03-30 23:59:23+00:00,A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts,cs.SE,['cs.SE'],"[arxiv.Result.Author('Konstantin Grotov'), arxiv.Result.Author('Sergey Titov'), arxiv.Result.Author('Vladimir Sotnikov'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","In recent years, Jupyter notebooks have grown in popularity in several
domains of software engineering, such as data science, machine learning, and
computer science education. Their popularity has to do with their rich features
for presenting and visualizing data, however, recent studies show that
notebooks also share a lot of drawbacks: high number of code clones, low
reproducibility, etc.
  In this work, we carry out a comparison between Python code written in
Jupyter Notebooks and in traditional Python scripts. We compare the code from
two perspectives: structural and stylistic. In the first part of the analysis,
we report the difference in the number of lines, the usage of functions, as
well as various complexity metrics. In the second part, we show the difference
in the number of stylistic issues and provide an extensive overview of the 15
most frequent stylistic issues in the studied mediums. Overall, we demonstrate
that notebooks are characterized by the lower code complexity, however, their
code could be perceived as more entangled than in the scripts. As for the
style, notebooks tend to have 1.4 times more stylistic issues, but at the same
time, some of them are caused by specific coding practices in notebooks and
should be considered as false positives. With this research, we want to pave
the way to studying specific problems of notebooks that should be addressed by
the development of notebook-specific tools, and provide various insights that
can be useful in this regard."
4118,"https://docs.github.com/en/repositories/managing-your-
could facilitate further research in different domains of software              repositorys- settings- and- features/customizing- your- repository/licensing- a-
engineering.",GitHub licenses.,repository.,2022-03-30 23:59:23+00:00,A Large-Scale Comparison of Python Code in Jupyter Notebooks and Scripts,cs.SE,['cs.SE'],"[arxiv.Result.Author('Konstantin Grotov'), arxiv.Result.Author('Sergey Titov'), arxiv.Result.Author('Vladimir Sotnikov'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","In recent years, Jupyter notebooks have grown in popularity in several
domains of software engineering, such as data science, machine learning, and
computer science education. Their popularity has to do with their rich features
for presenting and visualizing data, however, recent studies show that
notebooks also share a lot of drawbacks: high number of code clones, low
reproducibility, etc.
  In this work, we carry out a comparison between Python code written in
Jupyter Notebooks and in traditional Python scripts. We compare the code from
two perspectives: structural and stylistic. In the first part of the analysis,
we report the difference in the number of lines, the usage of functions, as
well as various complexity metrics. In the second part, we show the difference
in the number of stylistic issues and provide an extensive overview of the 15
most frequent stylistic issues in the studied mediums. Overall, we demonstrate
that notebooks are characterized by the lower code complexity, however, their
code could be perceived as more entangled than in the scripts. As for the
style, notebooks tend to have 1.4 times more stylistic issues, but at the same
time, some of them are caused by specific coding practices in notebooks and
should be considered as false positives. With this research, we want to pave
the way to studying specific problems of notebooks that should be addressed by
the development of notebook-specific tools, and provide various insights that
can be useful in this regard."
4191,"The authors
motivate us to further study the textual content in Stack Overflow                concluded that there is no single superior technique for capturing
threads in order to identify additional contexts in sentences, with               essential sentences for navigation.",These findings                    were captured by only one out of the four techniques.,"The findings of their work in-
the eventual goal of helping users navigate this information.",2022-03-31 21:46:48+00:00,Does This Apply to Me? An Empirical Study of Technical Context in Stack Overflow,cs.SE,['cs.SE'],"[arxiv.Result.Author('Akalanka Galappaththi'), arxiv.Result.Author('Sarah Nadi'), arxiv.Result.Author('Christoph Treude')]","Stack Overflow has become an essential technical resource for developers.
However, given the vast amount of knowledge available on Stack Overflow,
finding the right information that is relevant for a given task is still
challenging, especially when a developer is looking for a solution that applies
to their specific requirements or technology stack. Clearly marking answers
with their technical context, i.e., the information that characterizes the
technologies and assumptions needed for this answer, is potentially one way to
improve navigation. However, there is no information about how often such
context is mentioned, and what kind of information it might offer. In this
paper, we conduct an empirical study to understand the occurrence of technical
context in Stack Overflow answers and comments, using tags as a proxy for
technical context. We specifically focus on additional context, where
answers/comments mention information that is not already discussed in the
question. Our results show that nearly half of our studied threads contain at
least one additional context. We find that almost 50% of the additional context
are either a library/framework, a programming language, a tool/application, an
API, or a database. Overall, our findings show the promise of using additional
context as navigational cues."
4222,"Finally, section
7 concludes the paper discussing further research directions.","Section 6 describes an
IIoT scenario of a Warehouse App, an AP service run on hardware with the
physical characteristics of the DecaWave board; then simulates its operation
through the FCPP 3D simulator, and presents preliminary experiments with
physical DecaWave boards running a port of the FCPP library.","2 Related work

2.1 Edge Computing in the IIoT

Distributed edge architectures have been explored in the context of Data Man-
agement for the IIoT, which involves generating, aggregating, storing, analysing
and requesting data.",2022-04-01 14:26:09+00:00,Aggregate Processes as Distributed Adaptive Services for the Industrial Internet of Things,cs.SE,['cs.SE'],"[arxiv.Result.Author('Lorenzo Testa'), arxiv.Result.Author('Giorgio Audrito'), arxiv.Result.Author('Ferruccio Damiani'), arxiv.Result.Author('Gianluca Torta')]","The Industrial Internet of Things (IIoT) promises to bring many benefits,
including increased productivity, reduced costs, and increased safety to new
generation manufacturing plants. The main ingredients of IIoT are the
connected, communicating devices directly located in the workshop floor (far
edge devices), as well as edge gateways that connect such devices to the
Internet and, in particular, to cloud servers. The field of Edge Computing
advocates that keeping computations as close as possible to the sources of data
can be an effective means of reducing latency, preserving privacy, and improve
the overall efficiency of the system, although building systems where (far)
edge and cloud nodes cooperate is quite challenging. In the present work we
propose the adoption of the Aggregate Programming (AP) paradigm (and, in
particular, the ""aggregate process"" construct) as a way to simplify building
distributed, intelligent services at the far edge of an IIoT architecture. We
demonstrate the feasibility and efficacy of the approach with simulated
experiments on FCPP (a C++ library for AP), and with some basic experiments on
physical IIoT boards running an ad-hoc porting of FCPP."
4278,"discovered the impact of language features on code clone detection,
                                                                           revealing that further research in other languages is needed.","In a case study with multiple languages, we
language extensibility of these tools is not enough.","Different from traditional detection tools, Cross-language code
clone detection aims to detect code cloning between different                 In future works, we plan to evaluate MSCCD ’s precision in
languages.",2022-04-03 08:31:07+00:00,MSCCD: Grammar Pluggable Clone Detection Based on ANTLR Parser Generation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Wenqing Zhu'), arxiv.Result.Author('Norihiro Yoshida'), arxiv.Result.Author('Toshihiro Kamiya'), arxiv.Result.Author('Eunjong Choi'), arxiv.Result.Author('Hiroaki Takada')]","For various reasons, programming languages continue to multiply and evolve.
It has become necessary to have a multilingual clone detection tool that can
easily expand supported programming languages and detect various code clones is
needed. However, research on multilingual code clone detection has not received
sufficient attention. In this study, we propose MSCCD (Multilingual Syntactic
Code Clone Detector), a grammar pluggable code clone detection tool that uses a
parser generator to generate a code block extractor for the target language.
The extractor then extracts the semantic code blocks from a parse tree. MSCCD
can detect Type-3 clones at various granularities. We evaluated MSCCD's
language extensibility by applying MSCCD to 20 modern languages. Sixteen
languages were perfectly supported, and the remaining four were provided with
the same detection capabilities at the expense of execution time. We evaluated
MSCCD's recall by using BigCloneEval and conducted a manual experiment to
evaluate precision. MSCCD achieved equivalent detection performance equivalent
to state-of-the-art tools."
4279,"discovered the impact of language features on code clone detection,
                                                                           revealing that further research in other languages is needed.","In a case study with multiple languages, we
language extensibility of these tools is not enough.","Different from traditional detection tools, Cross-language code
clone detection aims to detect code cloning between different                 In future works, we plan to evaluate MSCCD ’s precision in
languages.",2022-04-03 08:31:07+00:00,MSCCD: Grammar Pluggable Clone Detection Based on ANTLR Parser Generation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Wenqing Zhu'), arxiv.Result.Author('Norihiro Yoshida'), arxiv.Result.Author('Toshihiro Kamiya'), arxiv.Result.Author('Eunjong Choi'), arxiv.Result.Author('Hiroaki Takada')]","For various reasons, programming languages continue to multiply and evolve.
It has become necessary to have a multilingual clone detection tool that can
easily expand supported programming languages and detect various code clones is
needed. However, research on multilingual code clone detection has not received
sufficient attention. In this study, we propose MSCCD (Multilingual Syntactic
Code Clone Detector), a grammar pluggable code clone detection tool that uses a
parser generator to generate a code block extractor for the target language.
The extractor then extracts the semantic code blocks from a parse tree. MSCCD
can detect Type-3 clones at various granularities. We evaluated MSCCD's
language extensibility by applying MSCCD to 20 modern languages. Sixteen
languages were perfectly supported, and the remaining four were provided with
the same detection capabilities at the expense of execution time. We evaluated
MSCCD's recall by using BigCloneEval and conducted a manual experiment to
evaluate precision. MSCCD achieved equivalent detection performance equivalent
to state-of-the-art tools."
4321,"However, even though the activities can be
sign of adequate explanations which, in turn, favors the assumption        integrated into an existing process alongside other activities and
that user-centered methods are adequate to develop explainable             phases, we do not discard the need for further research to cover the
systems.","The heuristics were shown to be          true for the creation of explainable systems and, hence, is outside
helpful in the resolution of the pointed issues and to support the de-     the focus of this study.","On the other hand, this assumption had not yet been val-          whole lifecycle.",2022-04-04 12:43:26+00:00,How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study,cs.SE,"['cs.SE', 'D.2.0']","[arxiv.Result.Author('Larissa Chazette'), arxiv.Result.Author('Jil Klünder'), arxiv.Result.Author('Merve Balci'), arxiv.Result.Author('Kurt Schneider')]","Quality aspects such as ethics, fairness, and transparency have been proven
to be essential for trustworthy software systems. Explainability has been
identified not only as a means to achieve all these three aspects in systems,
but also as a way to foster users' sentiments of trust. Despite this, research
has only marginally focused on the activities and practices to develop
explainable systems. To close this gap, we recommend six core activities and
associated practices for the development of explainable systems based on the
results of a literature review and an interview study. First, we identified and
summarized activities and corresponding practices in the literature. To
complement these findings, we conducted interviews with 19 industry
professionals who provided recommendations for the development process of
explainable systems and reviewed the activities and practices based on their
expertise and knowledge. We compared and combined the findings of the
interviews and the literature review to recommend the activities and assess
their applicability in industry. Our findings demonstrate that the activities
and practices are not only feasible, but can also be integrated in different
development processes."
4322,"On the other hand, this assumption had not yet been val-        phases, we do not discard the need for further research to cover the
idated beyond this study, and, more importantly, the feedback of         whole lifecycle.","However, even though the activities can be
that user-centered methods are adequate to develop explainable           integrated into an existing process alongside other activities and
systems.","In either case, it was possible to identify that there
software experts on whether this approach is compatible with the         is a special focus on the requirements process and on validating
reality of the industry was still missing.",2022-04-04 12:43:26+00:00,How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study,cs.SE,"['cs.SE', 'D.2.0']","[arxiv.Result.Author('Larissa Chazette'), arxiv.Result.Author('Jil Klünder'), arxiv.Result.Author('Merve Balci'), arxiv.Result.Author('Kurt Schneider')]","Quality aspects such as ethics, fairness, and transparency have been proven
to be essential for trustworthy software systems. Explainability has been
identified not only as a means to achieve all these three aspects in systems,
but also as a way to foster users' sentiments of trust. Despite this, research
has only marginally focused on the activities and practices to develop
explainable systems. To close this gap, we recommend six core activities and
associated practices for the development of explainable systems based on the
results of a literature review and an interview study. First, we identified and
summarized activities and corresponding practices in the literature. To
complement these findings, we conducted interviews with 19 industry
professionals who provided recommendations for the development process of
explainable systems and reviewed the activities and practices based on their
expertise and knowledge. We compared and combined the findings of the
interviews and the literature review to recommend the activities and assess
their applicability in industry. Our findings demonstrate that the activities
and practices are not only feasible, but can also be integrated in different
development processes."
4493,"ous study [25], to obtain the whole sequence representations of the       We will further study the impact of these two strategies in § 5.2.2.
query/code, we insert a special token [CLS] at the beginning of the
input code/query sequence and using the embedding of [CLS] at                We denote the SoDa module as 𝐺𝑠𝑜𝑑𝑎 which performs the dy-
the last layer as the whole sequence-level representation.",Following previ-            iteration rather than only performed once as in static making [44].,Then we        namic masking operation for the given input sequence.,2022-04-07 08:49:27+00:00,Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ensheng Shi'), arxiv.Result.Author('Wenchao Gub'), arxiv.Result.Author('Yanlin Wang'), arxiv.Result.Author('Lun Du'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Dongmei Zhang'), arxiv.Result.Author('Hongbin Sun')]","Code search aims to retrieve the most semantically relevant code snippet for
a given natural language query. Recently, large-scale code pre-trained models
such as CodeBERT and GraphCodeBERT learn generic representations of source code
and have achieved substantial improvement on code search task. However, the
high-quality sequence-level representations of code snippets have not been
sufficiently explored. In this paper, we propose a new approach with multimodal
contrastive learning and soft data augmentation for code search. Multimodal
contrastive learning is used to pull together the representations of code-query
pairs and push apart the unpaired code snippets and queries. Moreover, data
augmentation is critical in contrastive learning for learning high-quality
representations. However, only semantic-preserving augmentations for source
code are considered in existing work. In this work, we propose to do soft data
augmentation by dynamically masking and replacing some tokens in code sequences
to generate code snippets that are similar but not necessarily
semantic-preserving as positive samples for paired queries. We conduct
extensive experiments to evaluate the effectiveness of our approach on a
large-scale dataset with six programming languages. The experimental results
show that our approach significantly outperforms the state-of-the-art methods.
We also adapt our techniques to several pre-trained models such as RoBERTa and
CodeBERT, and significantly boost their performance on the code search task."
4494,"We further study the performance of our approach on the other
three pre-trained models introduced in § 4.2, including a natural

                                                                      7
Conference acronym ’XX, June 03–05, 2018, Woodstock, NY                                                                                                        Shi, et al.","5.3 RQ3: What is the Performance of Our
      Approach on other Pre-trained Models?",Table 5: Results on other pre-trained models.,2022-04-07 08:49:27+00:00,Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ensheng Shi'), arxiv.Result.Author('Wenchao Gub'), arxiv.Result.Author('Yanlin Wang'), arxiv.Result.Author('Lun Du'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Dongmei Zhang'), arxiv.Result.Author('Hongbin Sun')]","Code search aims to retrieve the most semantically relevant code snippet for
a given natural language query. Recently, large-scale code pre-trained models
such as CodeBERT and GraphCodeBERT learn generic representations of source code
and have achieved substantial improvement on code search task. However, the
high-quality sequence-level representations of code snippets have not been
sufficiently explored. In this paper, we propose a new approach with multimodal
contrastive learning and soft data augmentation for code search. Multimodal
contrastive learning is used to pull together the representations of code-query
pairs and push apart the unpaired code snippets and queries. Moreover, data
augmentation is critical in contrastive learning for learning high-quality
representations. However, only semantic-preserving augmentations for source
code are considered in existing work. In this work, we propose to do soft data
augmentation by dynamically masking and replacing some tokens in code sequences
to generate code snippets that are similar but not necessarily
semantic-preserving as positive samples for paired queries. We conduct
extensive experiments to evaluate the effectiveness of our approach on a
large-scale dataset with six programming languages. The experimental results
show that our approach significantly outperforms the state-of-the-art methods.
We also adapt our techniques to several pre-trained models such as RoBERTa and
CodeBERT, and significantly boost their performance on the code search task."
4495,"representation and update mechanism of encoder and momentum              We will further study the impact of these two strategies in § 5.2.2.
encoder.","Here, dynamic means that in data processing,
                                                                         the masking and replacement operations are performed at each
In this section, we introduce the base model, input samples, output      iteration rather than only performed once as in static making [48].","As the pre-trained models such as GraphCodeBERT [17]
have achieved substantial improvement in code search, we take               We denote the SoDa module as 𝐺𝑠𝑜𝑑𝑎 which performs the dy-
GraphCodeBERT as the base code/query encoder.",2022-04-07 08:49:27+00:00,Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ensheng Shi'), arxiv.Result.Author('Wenchao Gub'), arxiv.Result.Author('Yanlin Wang'), arxiv.Result.Author('Lun Du'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Dongmei Zhang'), arxiv.Result.Author('Hongbin Sun')]","Code search aims to retrieve the most semantically relevant code snippet for
a given natural language query. Recently, large-scale code pre-trained models
such as CodeBERT and GraphCodeBERT learn generic representations of source code
and have achieved substantial improvement on code search task. However, the
high-quality sequence-level representations of code snippets have not been
sufficiently explored. In this paper, we propose a new approach with multimodal
contrastive learning and soft data augmentation for code search. Multimodal
contrastive learning is used to pull together the representations of code-query
pairs and push apart the unpaired code snippets and queries. Moreover, data
augmentation is critical in contrastive learning for learning high-quality
representations. However, only semantic-preserving augmentations for source
code are considered in existing work. In this work, we propose to do soft data
augmentation by dynamically masking and replacing some tokens in code sequences
to generate code snippets that are similar but not necessarily
semantic-preserving as positive samples for paired queries. We conduct
extensive experiments to evaluate the effectiveness of our approach on a
large-scale dataset with six programming languages. The experimental results
show that our approach significantly outperforms the state-of-the-art methods.
We also adapt our techniques to several pre-trained models such as RoBERTa and
CodeBERT, and significantly boost their performance on the code search task."
4496,"We further study the performance of our approach on the other
three pre-trained models introduced in § 4.2, including a natural

                                                                      7
Conference acronym ’XX, June 03–05, 2018, Woodstock, NY                                                                                                        Shi, et al.","5.3 RQ3: What is the Performance of Our
      Approach on other Pre-trained Models?",Table 5: Results on other pre-trained models.,2022-04-07 08:49:27+00:00,Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ensheng Shi'), arxiv.Result.Author('Wenchao Gub'), arxiv.Result.Author('Yanlin Wang'), arxiv.Result.Author('Lun Du'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Dongmei Zhang'), arxiv.Result.Author('Hongbin Sun')]","Code search aims to retrieve the most semantically relevant code snippet for
a given natural language query. Recently, large-scale code pre-trained models
such as CodeBERT and GraphCodeBERT learn generic representations of source code
and have achieved substantial improvement on code search task. However, the
high-quality sequence-level representations of code snippets have not been
sufficiently explored. In this paper, we propose a new approach with multimodal
contrastive learning and soft data augmentation for code search. Multimodal
contrastive learning is used to pull together the representations of code-query
pairs and push apart the unpaired code snippets and queries. Moreover, data
augmentation is critical in contrastive learning for learning high-quality
representations. However, only semantic-preserving augmentations for source
code are considered in existing work. In this work, we propose to do soft data
augmentation by dynamically masking and replacing some tokens in code sequences
to generate code snippets that are similar but not necessarily
semantic-preserving as positive samples for paired queries. We conduct
extensive experiments to evaluate the effectiveness of our approach on a
large-scale dataset with six programming languages. The experimental results
show that our approach significantly outperforms the state-of-the-art methods.
We also adapt our techniques to several pre-trained models such as RoBERTa and
CodeBERT, and significantly boost their performance on the code search task."
4512,"formed the groundwork for further research, obtaining
                                                                       multiple academic references [R197].","Some
fault localization [R100].","As [R116] states, their
    Along with product attributes, we also distinguish the             work on software process models “blew up the foundation
advancement of software processes.",2022-04-07 11:25:34+00:00,Impact of Software Engineering Research in Practice,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Georgios Gousios'), arxiv.Result.Author('Diomidis Spinellis')]","Existing work on the practical impact of software engineering (SE) research
examines industrial relevance rather than adoption of study results, hence the
question of how results have been practically applied remains open. To answer
this and investigate the outcomes of impactful research, we performed a
quantitative and qualitative analysis of 4,335 SE patents citing 1,668 SE
papers published between 1975-2017. Moreover, we conducted a survey study on
413 authors of 501 top-cited and awarded publications, achieving 25% response
rate. Overall, researchers have equipped practitioners with various tools,
processes, and methods, and improved many existing products. SE practice seems
to value knowledge-seeking research and is impacted by diverse
cross-disciplinary SE areas. Practitioner-oriented publication venues appear
more impactful than researcher-, while industry-related tracks in conferences
could enhance their impact. Some research works did not reach a wide footprint
due to limited funding resources or unfavorable cost-benefit tradeoff of the
proposed solutions. The need for higher funding in SE research could be
corroborated through a dedicated empirical study. In general, the assessment of
impact is subject to its definition. Therefore, academia and industry could
jointly agree on a formal description to set a common ground for subsequent
research on the topic."
4513,"Some
                                                                    conducting computational experiments [R75,R172], guide-
formed the groundwork for further research, obtaining               lines for empirical research [R161,R300,R314], design con-
multiple academic references [R5,R6,R7,R197].","12
    Various studies affected subsequent research.","As [R116]             cepts [R212], as well as contributions that “do not work on
states, their work on software process models “blew up the          real systems” [R185,R186,R315].",2022-04-07 11:25:34+00:00,Impact of Software Engineering Research in Practice: A Patent and Author Survey Analysis,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Georgios Gousios'), arxiv.Result.Author('Diomidis Spinellis')]","Existing work on the practical impact of software engineering (SE) research
examines industrial relevance rather than adoption of study results, hence the
question of how results have been practically applied remains open. To answer
this and investigate the outcomes of impactful research, we performed a
quantitative and qualitative analysis of 4,354 SE patents citing 1,690 SE
papers published in four leading SE venues between 1975-2017. Moreover, we
conducted a survey on 475 authors of 593 top-cited and awarded publications,
achieving 26% response rate. Overall, researchers have equipped practitioners
with various tools, processes, and methods, and improved many existing
products. SE practice values knowledge-seeking research and is impacted by
diverse cross-disciplinary SE areas. Practitioner-oriented publication venues
appear more impactful than researcher-oriented ones, while industry-related
tracks in conferences could enhance their impact. Some research works did not
reach a wide footprint due to limited funding resources or unfavorable
cost-benefit trade-off of the proposed solutions. The need for higher SE
research funding could be corroborated through a dedicated empirical study. In
general, the assessment of impact is subject to its definition. Therefore,
academia and industry could jointly agree on a formal description to set a
common ground for subsequent research on the topic."
4554,This motivates the need for further research towards making energy efficient mobile applications.,"They reported that
                                       smartphone’s battery life can be extended upto an hour, if apps are developed by implementing energy-aware
                                       practices.","Research efforts were initially focused on energy measurement [19], energy profiling [23], and energy bugs
                                       identification [33, 45].",2022-04-08 05:50:26+00:00,eGEN: An Energy-saving Modeling Language and Code Generator for Location-sensing of Mobile Apps,cs.SE,['cs.SE'],"[arxiv.Result.Author('Kowndinya Boyalakuntla'), arxiv.Result.Author('Marimuthu C'), arxiv.Result.Author('Sridhar Chimalakonda'), arxiv.Result.Author('Chandrasekaran K')]","The demand for reducing the energy consumption of location-based applications
has increased in recent years. The abnormal battery-draining behavior of GPS
makes it difficult for the developers to decide on battery optimization during
the development phase directly. It will reduce the burden on developers if
battery-saving strategies are considered early, and relevant battery-aware code
is generated from the design phase artifacts. Therefore, we aim to develop tool
support, eGEN, to specify and create native location-based mobile apps. eGEN
consists of Domain-specific Modeling Language (DSML) and a code generator for
location-sensing. It is developed using Xtext and Xtend as an Eclipse plug-in,
and currently, it supports native Android apps. eGEN is evaluated through
controlled experiments by instrumenting the generated code in five
location-based open-source Android applications. The experimental results show
4.35 minutes of average GPS reduction per hour and 188 mA of average reduction
in battery consumption while showing only 97 meters degrade in location
accuracy over 3 kilometers of a cycling path. Hence, we believe that code
generated by eGEN would help developers to balance between energy and accuracy
requirements of location-based applications. The source code, documentation,
tool demo video, and tool installation video are available at
https://github.com/Kowndinya2000/egen."
4555,"As observed from the results, eGEN generated code show reduced battery usage and negligible
accuracy degradation, showing initial promise and the need for further research.","Overall, the eGEN
version of the subject application shows reduction of 4.35 minutes in GPS active time and 188 mA in battery
consumption.","The rest of the paper is organized as follows: we provide the necessary background information about
location-based apps along with a motivating scenario in Section 2.",2022-04-08 05:50:26+00:00,eGEN: An Energy-saving Modeling Language and Code Generator for Location-sensing of Mobile Apps,cs.SE,['cs.SE'],"[arxiv.Result.Author('Kowndinya Boyalakuntla'), arxiv.Result.Author('Marimuthu C'), arxiv.Result.Author('Sridhar Chimalakonda'), arxiv.Result.Author('Chandrasekaran K')]","The demand for reducing the energy consumption of location-based applications
has increased in recent years. The abnormal battery-draining behavior of GPS
makes it difficult for the developers to decide on battery optimization during
the development phase directly. It will reduce the burden on developers if
battery-saving strategies are considered early, and relevant battery-aware code
is generated from the design phase artifacts. Therefore, we aim to develop tool
support, eGEN, to specify and create native location-based mobile apps. eGEN
consists of Domain-specific Modeling Language (DSML) and a code generator for
location-sensing. It is developed using Xtext and Xtend as an Eclipse plug-in,
and currently, it supports native Android apps. eGEN is evaluated through
controlled experiments by instrumenting the generated code in five
location-based open-source Android applications. The experimental results show
4.35 minutes of average GPS reduction per hour and 188 mA of average reduction
in battery consumption while showing only 97 meters degrade in location
accuracy over 3 kilometers of a cycling path. Hence, we believe that code
generated by eGEN would help developers to balance between energy and accuracy
requirements of location-based applications. The source code, documentation,
tool demo video, and tool installation video are available at
https://github.com/Kowndinya2000/egen."
4674,Our work serves as a basis for further research.,"Lecture Notes in Business
an increase in movement for some employees.","Information Processing, Vol.",2022-04-11 13:53:11+00:00,When is Good Good Enough? Context Factors for Good Remote Work of Agile Software Development Teams. The Otto Case,cs.SE,['cs.SE'],"[arxiv.Result.Author('Lisa Rometsch'), arxiv.Result.Author('Richard Wegner'), arxiv.Result.Author('Florian Brusch'), arxiv.Result.Author('Michael Neumann'), arxiv.Result.Author('Lukas Linke')]","The Covid-19 pandemic led to several challenges in everybody working life.
Many companies worldwide enabled comprehensive remote work settings for their
employees. Agile Software Development Teams are affected by the switch to
remote work as agile methods setting communication and collaboration in focus.
The well-being and motivation of software engineers and developers, which
impacting their performance, are influenced by specific context factors. This
paper aims to analyze identify specific context factors for a good remote work
setting. We designed a single case study at a German ecommerce company and
conducted an experiment using a gamification approach including eight
semi-structured interviews. Our results show, that the agile software
development team members to their health. Furthermore, most the team members
value the gamification approach to put more focus on physical activities and
the health well-being. We discuss several practical implications and provide
recommendations for other teams and companies."
4757,"potential of being a social media channel, which should be a aim
                                                                        for further research [7].","As a consequence, Scratch
and to link the topics with the project statistics and users in order   might not be just a programming platform, but might have the
to gain further insights into the interests of the Scratch users.","In addition, there are also strong negative
 RQ1 Summary.",2022-04-12 15:55:52+00:00,Scratch as Social Network: Topic Modeling and Sentiment Analysis in Scratch Projects,cs.SE,['cs.SE'],"[arxiv.Result.Author('Isabella Graßl'), arxiv.Result.Author('Gordon Fraser')]","Societal matters like the Black Lives Matter (BLM) movement influence
software engineering, as the recent debate on replacing certain discriminatory
terms such as whitelist/blacklist has shown. Identifying relevant and trending
societal matters is important, and often done using social network analysis for
traditional social media channels such as twitter. In this paper we explore
whether this type of analysis can also be used for introspection of the
software world, by looking at the thriving scene of young Scratch programmers.
The educational programming language Scratch is not only used for teaching
programming concepts, but offers a platform for young programmers to express
and share their creativity on any topics of relevance. By analyzing titles and
project comments in a dataset of 106.032 Scratch projects, we explore which
topics are common in the Scratch community, whether socially relevant events
are reflected and how how the sentiment in the comments is. It turns out that
the diversity of topics within the Scratch projects make the analysis process
challenging. Our results nevertheless show that topics from pop and net culture
in particular are present, and even recent societal events such as the Covid-19
pandemic or BLM are to some extent reflected in Scratch. The tone in the
comments is mostly positive with catchy youth language. Hence, despite the
challenges, Scratch projects can be studied in the same way as social networks,
which opens up new possibilities to improve our understanding of the behavior
and motivation of novice programmers."
4836,"Our results invite further investigations into both
further research into body synthesis; we hope to investigate this          the capability of language models, and how to harness them to
prospect in follow-up research.","This suggests            GLAD makes it a good candidate for an ensemble of APR tools to
that an even greater number of omission faults could be solved by          deploy as well.",achieve strong repair performance.,2022-04-14 06:13:11+00:00,GLAD: Neural Predicate Synthesis to Repair Omission Faults,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sungmin Kang'), arxiv.Result.Author('Shin Yoo')]","Existing template and learning-based APR tools have successfully found
patches for many benchmark faults. However, our analysis of existing results
shows that omission faults pose a significant challenge to these techniques.
For template based approaches, omission faults provide no location to apply
templates to; for learning based approaches that formulate repair as Neural
Machine Translation (NMT), omission faults similarly do not provide the faulty
code to translate. To address these issues, we propose GLAD, a novel
learning-based repair technique that specifically targets if-clause synthesis.
GLAD does not require a faulty line as it is based on generative Language
Models (LMs) instead of machine translation; consequently, it can repair
omission faults. GLAD intelligently constrains the language model using a
type-based grammar. Further, it efficiently reduces the validation cost by
performing dynamic ranking of candidate patches using a debugger. Thanks to the
shift from translation to synthesis, GLAD is highly orthogonal to existing
techniques: GLAD can correctly fix 16 Defects4J v1.2 faults that previous
NMT-based techniques could not, while maintaining a reasonable runtime cost,
underscoring its utility as an APR tool and potential to complement existing
tools in practice. An inspection of the bugs that GLAD fixes reveals that GLAD
can quickly generate expressions that would be challenging for other
techniques."
4867,"Cancel    Temporal Worker (Cancellation Worker)
   We make the following contributions: a) We port the largest pub-       Ticket       Temporal Workflow (CancelTicketWorkflow)
licly available microservice benchmark to a fault-oblivious stateful    Controller
workflow engine and report our experience; b) We experimentally          Request
evaluate debugging of 22 bugs present in the TrainTicket benchmark
using debugging process supported by Temporal; c) We compare                        setOrderCancelling  refundAmountActivity            setOrderCancelled
our results with experimental results on original benchmarks and                            Activity                                            Activity
present our observations and insights to provoke further research.","We imple-                   Temporal Ecosystem
mented a simple linter 1 to deal with such issues and our experience
in porting code from original microservices bugs benchmark to                                                                    Queue
temporal framework was fairly smooth.","The experiments are released as an open-source project 2 and can            Figure 2: cancelTicket workflow after porting to Temporal
be replicated with a simple set-up.",2022-04-14 20:13:12+00:00,A Case for Microservices Orchestration Using Workflow Engines,cs.SE,"['cs.SE', 'cs.DC']","[arxiv.Result.Author('Anas Nadeem'), arxiv.Result.Author('Muhammad Zubair Malik')]","Microservices have become the de-facto software architecture for cloud-native
applications. A contentious architectural decision in microservices is to
compose them using choreography or orchestration. In choreography, every
service works independently, whereas, in orchestration, there is a controller
that coordinates service interactions. This paper makes a case for
orchestration. The promise of microservices is that each microservice can be
independently developed, deployed, tested, upgraded, and scaled. This makes
them suitable for systems running on cloud infrastructures. However,
microservice-based systems become complicated due to the complex interactions
of various services, concurrent events, failing components, developers' lack of
global view, and configurations of the environment. This makes maintaining and
debugging such systems very challenging. We hypothesize that orchestrated
services are easier to debug and to test this we ported the largest publicly
available microservices' benchmark TrainTicket, which is implemented using
choreography, to a fault-oblivious stateful workflow framework Temporal. We
report our experience in porting the code from traditional choreographed
microservice architecture to one orchestrated by Temporal and present our
initial findings of time to debug the 22 bugs present in the benchmark. Our
findings suggest that an effort towards making a transition to orchestrated
approach is worthwhile, making the ported code easier to debug."
4894,"Finally, we combine contrastive learning with MAML to                               clones in Java, achieve the same performance for detecting code
                                        further study whether it can improve the results of MAML.","For example, can a model that is trained to detect code
                                        samples.",clones in Python?,2022-04-15 15:01:55+00:00,Evaluating few shot and Contrastive learning Methods for Code Clone Detection,cs.SE,['cs.SE'],"[arxiv.Result.Author('Mohamad Khajezade'), arxiv.Result.Author('Fatemeh Hendijani Fard'), arxiv.Result.Author('Mohamed S. Shehata')]","Context: Code Clone Detection (CCD) is a software engineering task that is
used for plagiarism detection, code search, and code comprehension. Recently,
deep learning-based models have achieved an F1 score (a metric used to assess
classifiers) of $\sim$95\% on the CodeXGLUE benchmark. These models require
many training data, mainly fine-tuned on Java or C++ datasets. However, no
previous study evaluates the generalizability of these models where a limited
amount of annotated data is available.
  Objective: The main objective of this research is to assess the ability of
the CCD models as well as few shot learning algorithms for unseen programming
problems and new languages (i.e., the model is not trained on these
problems/languages).
  Method: We assess the generalizability of the state of the art models for CCD
in few shot settings (i.e., only a few samples are available for fine-tuning)
by setting three scenarios: i) unseen problems, ii) unseen languages, iii)
combination of new languages and new problems. We choose three datasets of
BigCloneBench, POJ-104, and CodeNet and Java, C++, and Ruby languages. Then, we
employ Model Agnostic Meta-learning (MAML), where the model learns a
meta-learner capable of extracting transferable knowledge from the train set;
so that the model can be fine-tuned using a few samples. Finally, we combine
contrastive learning with MAML to further study whether it can improve the
results of MAML."
4895,"Finally, we combine contrastive learning with MAML to                               clones in Java, achieve the same performance for detecting code
                                        further study whether it can improve the results of MAML.","For example, can a model that is trained to detect code
                                        samples.",clones in Python?,2022-04-15 15:01:55+00:00,Evaluating few shot and Contrastive learning Methods for Code Clone Detection,cs.SE,['cs.SE'],"[arxiv.Result.Author('Mohamad Khajezade'), arxiv.Result.Author('Fatemeh Hendijani Fard'), arxiv.Result.Author('Mohamed S. Shehata')]","Context: Code Clone Detection (CCD) is a software engineering task that is
used for plagiarism detection, code search, and code comprehension. Recently,
deep learning-based models have achieved an F1 score (a metric used to assess
classifiers) of $\sim$95\% on the CodeXGLUE benchmark. These models require
many training data, mainly fine-tuned on Java or C++ datasets. However, no
previous study evaluates the generalizability of these models where a limited
amount of annotated data is available.
  Objective: The main objective of this research is to assess the ability of
the CCD models as well as few shot learning algorithms for unseen programming
problems and new languages (i.e., the model is not trained on these
problems/languages).
  Method: We assess the generalizability of the state of the art models for CCD
in few shot settings (i.e., only a few samples are available for fine-tuning)
by setting three scenarios: i) unseen problems, ii) unseen languages, iii)
combination of new languages and new problems. We choose three datasets of
BigCloneBench, POJ-104, and CodeNet and Java, C++, and Ruby languages. Then, we
employ Model Agnostic Meta-learning (MAML), where the model learns a
meta-learner capable of extracting transferable knowledge from the train set;
so that the model can be fine-tuned using a few samples. Finally, we combine
contrastive learning with MAML to further study whether it can improve the
results of MAML."
5014,"There are
As the founders of this project, we share our first-hand experience     4,337,250 sessions, and users spend over 2.6 minutes on average in
and lessons which may inspire further research on developing            each session.","ual users with 14,465,574 page views, as seen in Fig 2.","Among all the visitors, 21.7% of the users are returning
similar information dashboard for other emergency disasters, and        users, whereas the rest of them are new visitors.",2022-04-19 05:19:15+00:00,Software Engineers Response to Public Crisis: Lessons Learnt from Spontaneously Building an Informative COVID-19 Dashboard,cs.SE,['cs.SE'],"[arxiv.Result.Author('Han Wang'), arxiv.Result.Author('Chao Wu'), arxiv.Result.Author('Chunyang Chen'), arxiv.Result.Author('Burak Turhan'), arxiv.Result.Author('Shiping Chen'), arxiv.Result.Author('Jon Whittle')]","The Coronavirus disease 2019 (COVID-19) outbreak quickly spread around the
world, resulting in over 240 million infections and 4 million deaths by Oct
2021. While the virus is spreading from person to person silently, fear has
also been spreading around the globe. The COVID-19 information from the
Australian Government is convincing but not timely or detailed, and there is
much information on social networks with both facts and rumors. As software
engineers, we have spontaneously and rapidly constructed a COVID-19 information
dashboard aggregating reliable information semi-automatically checked from
different sources for providing one-stop information sharing site about the
latest status in Australia. Inspired by the John Hopkins University COVID-19
Map, our dashboard contains the case statistics, case distribution, government
policy, latest news, with interactive visualization. In this paper, we present
a participant's in-person observations in which the authors acted as founders
of https://covid-19-au.com/ serving more than 830K users with 14M page views
since March 2020. According to our first-hand experience, we summarize 9
lessons for developers, researchers and instructors. These lessons may inspire
the development, research and teaching in software engineer aspects for coping
with similar public crises in the future."
5015,"There has also been further research done on
ble for a person to review or interview all applications even    using a modiﬁed variant of the ant colony optimization meta-
with common areas of interest.","Standalone research has been done on the ant colony op-
                                                                 timization method to model insects and other natural phe-
    Given the large number of applications, it is not possi-     nomena [23].","There is need to divide the       heuristic for graph colouring (modiﬁed ant colony system for
work which is why panels are created to perform this task ef-    colouring graphs [24]).",2022-04-19 06:33:21+00:00,Automated Application Processing,cs.SE,['cs.SE'],"[arxiv.Result.Author('Eshita Sharma'), arxiv.Result.Author('Keshav Gupta'), arxiv.Result.Author('Lubaina Machinewala'), arxiv.Result.Author('Samaksh Dhingra'), arxiv.Result.Author('Shrey Tripathi'), arxiv.Result.Author('Shreyas V S'), arxiv.Result.Author('Sujit Kumar Chakrabarti')]","Recruitment in large organisations often involves interviewing a large number
of candidates. The process is resource intensive and complex. Therefore, it is
important to carry it out efficiently and effectively. Planning the selection
process consists of several problems, each of which maps to one or the other
well-known computing problem. Research that looks at each of these problems in
isolation is rich and mature. However, research that takes an integrated view
of the problem is not common. In this paper, we take two of the most important
aspects of the application processing problem, namely review/interview panel
creation and interview scheduling. We have implemented our approach as a
prototype system and have used it to automatically plan the interview process
of a real-life data set. Our system provides a distinctly better plan than the
existing practice, which is predominantly manual. We have explored various
algorithmic options and have customised them to solve these panel creation and
interview scheduling problems. We have evaluated these design options
experimentally on a real data set and have presented our observations. Our
prototype and experimental process and results may be a very good starting
point for a full-fledged development project for automating application
processing process."
5091,qualitative input exploitable in further research.,"However, the evaluation provided              the limited number of participants, we did not count codes.","Instead, we provided some qualitative insights of the line
                                                                      managers of the observed and comparison unit.",2022-04-20 17:05:37+00:00,Preventing technical debt with the TAP framework for Technical Debt Aware Management,cs.SE,['cs.SE'],"[arxiv.Result.Author('Marion Wiese'), arxiv.Result.Author('Paula Rachow'), arxiv.Result.Author('Matthias Riebisch'), arxiv.Result.Author('Julian Schwarze')]","Context. Technical Debt (TD) is a metaphor for technical problems that are
not visible to users and customers but hinder developers in their work, making
future changes more difficult. TD is often incurred due to tight project
deadlines and can make future changes more costly or impossible. Project
Management usually focuses on customer benefits and pays less attention to
their IT systems' internal quality. TD prevention should be preferred over TD
repayment because subsequent refactoring and re-engineering are expensive.
Objective. This paper evaluates a framework focusing on both TD prevention and
TD repayment in the context of agile-managed projects. The framework was
developed and applied in an IT unit of a publishing house. The unique
contribution of this framework is the integration of TD management into project
management. Method. The evaluation was performed as a comparative case study
based on ticket statistics and two structured surveys. The surveys were
conducted in the observed IT unit using the framework and a comparison unit not
using the framework. The first survey targeted team members, the second one IT
managers. Results. The evaluation shows that in this IT unit, the TAP framework
led to a raised awareness for the incurrence of TD. Decisions to incur TD are
intentional, and TD is repaid timelier. Unintentional TD incurred by
unconscious decisions is prevented. Furthermore, better communication and
better planning of the project pipeline can be observed. Conclusions. We
provide an insight into practitioners' ways to identify, monitor, prevent and
repay TD. The presented framework includes a feasible method for TD prevention
despite tight timelines by making TD repayment part of project management."
5093,"It also highlights and
recommends the key areas that require further research to come up with new processes and experiments.","It especially highlights the key areas and domains for the Saudi Arabian software industry where improvement is
anticipated in the future regarding the introduction of new industrial technologies and processes.","References

 [1] Mohammad Zarour, Norah Alhammad, Mamdouh Alenezi, and Khalid Alsarayrah.",2022-04-05 10:43:49+00:00,Factors Hindering the Adoption of DevOps in the Saudi Software Industry,cs.SE,['cs.SE'],[arxiv.Result.Author('Mamdouh Alenezi')],"DevOps has gained high importance in the global software industry due to the
ease of software development, testing and deployment it provides. However, the
Saudi software industry has not been able to adopt DevOps at a great pace due
to various factors. This study, thus, aimed to examine different factors that
hindered the adoption of DevOps in the Saudi software industry. Also,
recommendations are provided at the end for Saudi Arabia to enhance the
adoption of DevOps in its software industry. To accomplish the aims, this study
used a literature review and interviews to gather data and examine it to
produce the findings. The findings of the study highlight lack of support from
organizational management and lack of laws as the major factors for the
adoption of DevOps in the Saudi software industry."
5096,"We wish our study to
Avoidance      Automatic Detection   Reaction                               inspire further research on automatic QoA evaluation and
Section III.D                       Section III.C                           anti-pattern detection and beneﬁt the reliability of the cloud
                                                                            services in the long run.","trial practices of mitigating the anti-patterns by postmortem
                                                                            reactions and preventative guidelines.",Fig.,2022-04-13 16:43:13+00:00,Characterizing and Mitigating Anti-patterns of Alerts in Industrial Cloud Systems,cs.SE,"['cs.SE', 'cs.DC']","[arxiv.Result.Author('Tianyi Yang'), arxiv.Result.Author('Jiacheng Shen'), arxiv.Result.Author('Yuxin Su'), arxiv.Result.Author('Xiaoxue Ren'), arxiv.Result.Author('Yongqiang Yang'), arxiv.Result.Author('Michael R. Lyu')]","Alerts are crucial for requesting prompt human intervention upon cloud
anomalies. The quality of alerts significantly affects the cloud reliability
and the cloud provider's business revenue. In practice, we observe on-call
engineers being hindered from quickly locating and fixing faulty cloud services
because of the vast existence of misleading, non-informative, non-actionable
alerts. We call the ineffectiveness of alerts ""anti-patterns of alerts"". To
better understand the anti-patterns of alerts and provide actionable measures
to mitigate anti-patterns, in this paper, we conduct the first empirical study
on the practices of mitigating anti-patterns of alerts in an industrial cloud
system. We study the alert strategies and the alert processing procedure at
Huawei Cloud, a leading cloud provider. Our study combines the quantitative
analysis of millions of alerts in two years and a survey with eighteen
experienced engineers. As a result, we summarized four individual anti-patterns
and two collective anti-patterns of alerts. We also summarize four current
reactions to mitigate the anti-patterns of alerts, and the general preventative
guidelines for the configuration of alert strategy. Lastly, we propose to
explore the automatic evaluation of the Quality of Alerts (QoA), including the
indicativeness, precision, and handleability of alerts, as a future research
direction that assists in the automatic detection of alerts' anti-patterns. The
findings of our study are valuable for optimizing cloud monitoring systems and
improving the reliability of cloud services."
5185,"To mitigate this problem, three experts manually verified and cross-validated
the dataset, which has been made available online for further study [66].","(4) The performance of LiDetector was evaluated
on a ground-truth dataset comprising 200 popular projects, and the quality of the test set may
threaten the results.","7 RELATED WORK

7.1 Semantic Extraction

License texts are typically long and complicated, which are not straightforward for developers to
understand.",2022-04-22 05:09:50+00:00,LiDetector: License Incompatibility Detection for Open Source Software,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sihan Xu'), arxiv.Result.Author('Ya Gao'), arxiv.Result.Author('Lingling Fan'), arxiv.Result.Author('Zheli Liu'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Hua Ji')]","Open-source software (OSS) licenses dictate the conditions which should be
followed to reuse, distribute, and modify the software. Apart from widely-used
licenses such as the MIT License, developers are also allowed to customize
their own licenses (called custom licenses), whose descriptions are more
flexible. The presence of such various licenses imposes challenges to
understanding licenses and their compatibility. To avoid financial and legal
risks, it is essential to ensure license compatibility when integrating
third-party packages or reusing code accompanied with licenses. In this work,
we propose LiDetector, an effective tool that extracts and interprets OSS
licenses (including both official licenses and custom licenses), and detects
license incompatibility among these licenses. Specifically, LiDetector
introduces a learning-based method to automatically identify meaningful license
terms from an arbitrary license and employs Probabilistic Context-Free Grammar
(PCFG) to infer rights and obligations for incompatibility detection.
Experiments demonstrate that LiDetector outperforms existing methods with
93.28% precision for term identification, and 91.09% accuracy for right and
obligation inference, and can effectively detect incompatibility with a 10.06%
FP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale
empirical study on 1,846 projects reveals that 72.91% of the projects are
suffering from license incompatibility, including popular ones such as the MIT
License and the Apache License. We highlighted lessons learned from the
perspectives of different stakeholders and made all related data and the
replication package publicly available to facilitate follow-up research."
5285,"Focusing speciﬁcally on architecture
                                        the art, and (ii) identifying the needs for further research.","This work aims     as the ones mentioned above discussing sustainability in
                                        to ﬁll this gap by conducting a systematic mapping study on            software engineering, there are fewer works describing the
                                        the intersection between sustainability and software architecture      state of the art in the intersection between sustainability and
                                        research with the intention of (i) reﬂecting on the current state of   software architecture.","Our      is essential since architecting practices allow us to reason
                                        results show that, overall, existing works have focused dispropor-     and evaluate sustainability as a system quality throughout the
                                        tionately on speciﬁc aspects of sustainability, and in particular      system’s lifecycle.",2022-04-25 13:49:26+00:00,Sustainability in Software Architecture: A Systematic Mapping Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Vasilios Andrikopoulos'), arxiv.Result.Author('Rares-Dorian Boza'), arxiv.Result.Author('Carlos Perales'), arxiv.Result.Author('Patricia Lago')]","Sustainability is an increasingly-studied topic in software engineering in
general, and in software architecture in particular. There are already a number
of secondary studies addressing sustainability in software engineering, but no
such study focusing explicitly on software architecture. This work aims to fill
this gap by conducting a systematic mapping study on the intersection between
sustainability and software architecture research with the intention of (i)
reflecting on the current state of the art, and (ii) identifying the needs for
further research. Our results show that, overall, existing works have focused
disproportionately on specific aspects of sustainability, and in particular on
the most technical and ""inward facing"" ones. This comes at the expense of the
holistic perspective required to address a multi-faceted concern such as
sustainability. Furthermore, more reflection-oriented research works, and
better coverage of the activities in the architecting life cycle are required
to further the maturity of the area. Based on our findings we then propose a
research agenda for sustainability-aware software architecture."
5296,"As additional contributions, (i) the gathered data from
Nevertheless, the conclusions about the incompleteness of data      the evaluated monolith systems, using dynamic analysis, is
and required effort associated with the dynamic collection of       publicly available and can be used by third parties to do
data are evident and shows that a cost/beneﬁt relation may          further research, (ii) the data collectors were implemented to
tend for the static analysis approaches.","quality of one decomposition does not outperforms the other,
though the dynamic analysis of more monoliths is necessary.","be as conﬁgurable and extensible as possible such that they
                                                                    can handle a wider variety of code bases with different JAVA
   Due to the diversity of metrics that exist for complexity        technology stacks.",2022-04-22 19:50:31+00:00,From Monolith to Microservices: Static and Dynamic Analysis Comparison,cs.SE,['cs.SE'],"[arxiv.Result.Author('Bernardo Andrade'), arxiv.Result.Author('Samuel Santos'), arxiv.Result.Author('António Rito Silva')]","One of the most challenging problems in the migration of a monolith to a
microservices architecture is the identification of the microservices
boundaries. Several approaches have been recently proposed for the automatic
identification of microservices, which, even though following the same basic
steps, diverge on how data of the monolith system is collected and analysed. In
this paper, we compare the decompositions generated for two monolith systems
into a set of candidate microservices, when static and dynamic analysis data
collection techniques are used. The decompositions are generated using a
combination of similarity measures and are evaluated according to a complexity
metric to answer the following research question: which collection of monolith
data, static or dynamic analysis, allows to generate better decompositions? As
result of the analysis we conclude that neither of the analysis techniques,
static nor dynamic, outperforms the other, but the dynamic collection of data
requires more effort."
5406,"potential impact are aimed at fostering further research in this
        Moreover, if a greener choice is not selected initially, the    important area, the ongoing research is aimed at industry practi-
        cost, effort, and potential emissions of code refactoring at a  tioners who have a desire/mandate to incorporate sustainability
        later stage might be exceptionally high.","While the challenges and
        energy consumption), in addition to being pro-environment.",practices into their projects but are unaware of a starting point.,2022-04-27 15:12:51+00:00,Towards a Green Quotient for Software Projects,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rohit Mehra'), arxiv.Result.Author('Vibhu Saujanya Sharma'), arxiv.Result.Author('Vikrant Kaulgud'), arxiv.Result.Author('Sanjay Podder'), arxiv.Result.Author('Adam P. Burden')]","As sustainability takes center stage across businesses, green and
energy-efficient choices are more crucial than ever. While it is becoming
increasingly evident that software and the software industry are substantial
and rapidly evolving contributors to carbon emissions, there is a dearth of
approaches to create actionable awareness about this during the software
development lifecycle (SDLC). Can software teams comprehend how green are their
projects? Here we provide an industry perspective on why this is a challenging
and worthy problem that needs to be addressed. We also outline an approach to
quickly gauge the ""greenness"" of a software project based on the choices made
across different SDLC dimensions and present the initial encouraging feedback
this approach has received."
5561,"As such, further research
tify adversarial examples that make the network fail, then     advances on providing robustness guarantees for ML models
augment the training dataset with these examples and train     are needed.",prove the robustness of a neural network would be to iden-     provide approximate guarantees.,another neural network.,2022-04-30 08:47:10+00:00,Software Testing for Machine Learning,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dusica Marijan'), arxiv.Result.Author('Arnaud Gotlieb')]","Machine learning has become prevalent across a wide variety of applications.
Unfortunately, machine learning has also shown to be susceptible to deception,
leading to errors, and even fatal failures. This circumstance calls into
question the widespread use of machine learning, especially in safety-critical
applications, unless we are able to assure its correctness and trustworthiness
properties. Software verification and testing are established technique for
assuring such properties, for example by detecting errors. However, software
testing challenges for machine learning are vast and profuse - yet critical to
address. This summary talk discusses the current state-of-the-art of software
testing for machine learning. More specifically, it discusses six key challenge
areas for software testing of machine learning systems, examines current
approaches to these challenges and highlights their limitations. The paper
provides a research agenda with elaborated directions for making progress
toward advancing the state-of-the-art on testing of machine learning."
5562,"Finally, research advances on en-
lenges, and further research work focused on addressing        abling the transparency of ethical decision making process
these challenges is needed.","2http://robust.vision/benchmarks/leaderboard
      9 Summary and Future Directions                          ﬁcation approaches may leverage different formal methods,
                                                               which underlines the open challenge of interoperability be-
Software testing of ML faces a range of open research chal-    tween different methods.",We envision such further work      is required.,2022-04-30 08:47:10+00:00,Software Testing for Machine Learning,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dusica Marijan'), arxiv.Result.Author('Arnaud Gotlieb')]","Machine learning has become prevalent across a wide variety of applications.
Unfortunately, machine learning has also shown to be susceptible to deception,
leading to errors, and even fatal failures. This circumstance calls into
question the widespread use of machine learning, especially in safety-critical
applications, unless we are able to assure its correctness and trustworthiness
properties. Software verification and testing are established technique for
assuring such properties, for example by detecting errors. However, software
testing challenges for machine learning are vast and profuse - yet critical to
address. This summary talk discusses the current state-of-the-art of software
testing for machine learning. More specifically, it discusses six key challenge
areas for software testing of machine learning systems, examines current
approaches to these challenges and highlights their limitations. The paper
provides a research agenda with elaborated directions for making progress
toward advancing the state-of-the-art on testing of machine learning."
5563,"Also, further research progress is      testing.","Current approaches provide only        machine learning based image classiﬁers using metamorphic
approximate guarantees.",In Int.,2022-04-30 08:47:10+00:00,Software Testing for Machine Learning,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Dusica Marijan'), arxiv.Result.Author('Arnaud Gotlieb')]","Machine learning has become prevalent across a wide variety of applications.
Unfortunately, machine learning has also shown to be susceptible to deception,
leading to errors, and even fatal failures. This circumstance calls into
question the widespread use of machine learning, especially in safety-critical
applications, unless we are able to assure its correctness and trustworthiness
properties. Software verification and testing are established technique for
assuring such properties, for example by detecting errors. However, software
testing challenges for machine learning are vast and profuse - yet critical to
address. This summary talk discusses the current state-of-the-art of software
testing for machine learning. More specifically, it discusses six key challenge
areas for software testing of machine learning systems, examines current
approaches to these challenges and highlights their limitations. The paper
provides a research agenda with elaborated directions for making progress
toward advancing the state-of-the-art on testing of machine learning."
5564,"Since there
                                        further research in the area.","implementation of the suggested approach, and will release the
                                        newly collected industrial dataset upon acceptance to facilitate      de-duplicate them and group them by error type.","are usually a lot of reports [7], [8], it is not feasible to group
                                                                                                              them manually, as this implies a detailed study of each crash
                                                                  I.",2022-04-30 08:51:00+00:00,Aggregation of Stack Trace Similarities for Crash Report Deduplication,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nikolay Karasov'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The automatic collection of stack traces in bug tracking systems is an
integral part of many software projects and their maintenance. However, such
reports often contain a lot of duplicates, and the problem of de-duplicating
them into groups arises. In this paper, we propose a new approach to solve the
deduplication task and report on its use on the real-world data from JetBrains,
a leading developer of IDEs and other software. Unlike most of the existing
methods, which assign the incoming stack trace to a particular group in which a
single most similar stack trace is located, we use the information about all
the calculated similarities to the group, as well as the information about the
timestamp of the stack traces. This approach to aggregating all available
information shows significantly better results compared to existing solutions.
The aggregation improved the results over the state-of-the-art solutions by 15
percentage points in the Recall Rate Top-1 metric on the existing NetBeans
dataset and by 8 percentage points on the JetBrains data. Additionally, we
evaluated a simpler k-Nearest Neighbors approach to aggregation and showed that
it cannot reach the same levels of improvement. Finally, we studied what
features from the aggregation contributed the most towards better quality to
understand which of them to develop further. We publish the implementation of
the suggested approach, and will release the newly collected industrial dataset
upon acceptance to facilitate further research in the area."
5565,"Upon acceptance, we            sures: edit distance, Longest Common Subsequence (LCS),
will release this dataset to facilitate further research in the      and preﬁx match.","The authors present three similarity mea-
developer of IDEs and other software.","These algorithms are variants of classical
area.",2022-04-30 08:51:00+00:00,Aggregation of Stack Trace Similarities for Crash Report Deduplication,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nikolay Karasov'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The automatic collection of stack traces in bug tracking systems is an
integral part of many software projects and their maintenance. However, such
reports often contain a lot of duplicates, and the problem of de-duplicating
them into groups arises. In this paper, we propose a new approach to solve the
deduplication task and report on its use on the real-world data from JetBrains,
a leading developer of IDEs and other software. Unlike most of the existing
methods, which assign the incoming stack trace to a particular group in which a
single most similar stack trace is located, we use the information about all
the calculated similarities to the group, as well as the information about the
timestamp of the stack traces. This approach to aggregating all available
information shows significantly better results compared to existing solutions.
The aggregation improved the results over the state-of-the-art solutions by 15
percentage points in the Recall Rate Top-1 metric on the existing NetBeans
dataset and by 8 percentage points on the JetBrains data. Additionally, we
evaluated a simpler k-Nearest Neighbors approach to aggregation and showed that
it cannot reach the same levels of improvement. Finally, we studied what
features from the aggregation contributed the most towards better quality to
understand which of them to develop further. We publish the implementation of
the suggested approach, and will release the newly collected industrial dataset
upon acceptance to facilitate further research in the area."
5566,"This diversity makes our
used CrashGraphs [11] as a baseline, however, it cannot be       data valuable for further research.","Additionally, we          less sparse than the NetBeans data.","Let us now describe each
aggregated since it does not calculate the similarity between    dataset a bit more.",2022-04-30 08:51:00+00:00,Aggregation of Stack Trace Similarities for Crash Report Deduplication,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nikolay Karasov'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The automatic collection of stack traces in bug tracking systems is an
integral part of many software projects and their maintenance. However, such
reports often contain a lot of duplicates, and the problem of de-duplicating
them into groups arises. In this paper, we propose a new approach to solve the
deduplication task and report on its use on the real-world data from JetBrains,
a leading developer of IDEs and other software. Unlike most of the existing
methods, which assign the incoming stack trace to a particular group in which a
single most similar stack trace is located, we use the information about all
the calculated similarities to the group, as well as the information about the
timestamp of the stack traces. This approach to aggregating all available
information shows significantly better results compared to existing solutions.
The aggregation improved the results over the state-of-the-art solutions by 15
percentage points in the Recall Rate Top-1 metric on the existing NetBeans
dataset and by 8 percentage points on the JetBrains data. Additionally, we
evaluated a simpler k-Nearest Neighbors approach to aggregation and showed that
it cannot reach the same levels of improvement. Finally, we studied what
features from the aggregation contributed the most towards better quality to
understand which of them to develop further. We publish the implementation of
the suggested approach, and will release the newly collected industrial dataset
upon acceptance to facilitate further research in the area."
5567,"the best k-NN model over the given baseline, thus, to compare
This difference in the inﬂuence demonstrates its importance      our Aggregation Model and the k-NN-based approach, one
for further research: improving the aggregation or developing    needs to compare columns ∆A and ∆K .","The column ∆K shows the improvement of
the new dataset than on the open-source NetBeans dataset.",brand new methods both require representative industrial data.,2022-04-30 08:51:00+00:00,Aggregation of Stack Trace Similarities for Crash Report Deduplication,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nikolay Karasov'), arxiv.Result.Author('Aleksandr Khvorov'), arxiv.Result.Author('Roman Vasiliev'), arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Timofey Bryksin')]","The automatic collection of stack traces in bug tracking systems is an
integral part of many software projects and their maintenance. However, such
reports often contain a lot of duplicates, and the problem of de-duplicating
them into groups arises. In this paper, we propose a new approach to solve the
deduplication task and report on its use on the real-world data from JetBrains,
a leading developer of IDEs and other software. Unlike most of the existing
methods, which assign the incoming stack trace to a particular group in which a
single most similar stack trace is located, we use the information about all
the calculated similarities to the group, as well as the information about the
timestamp of the stack traces. This approach to aggregating all available
information shows significantly better results compared to existing solutions.
The aggregation improved the results over the state-of-the-art solutions by 15
percentage points in the Recall Rate Top-1 metric on the existing NetBeans
dataset and by 8 percentage points on the JetBrains data. Additionally, we
evaluated a simpler k-Nearest Neighbors approach to aggregation and showed that
it cannot reach the same levels of improvement. Finally, we studied what
features from the aggregation contributed the most towards better quality to
understand which of them to develop further. We publish the implementation of
the suggested approach, and will release the newly collected industrial dataset
upon acceptance to facilitate further research in the area."
5675,"They
                                               10 010 011 012 012 013 014 014 015 016 016 017 018 018 019 020 020 021     claimed that organizational transformation, which is associated
11-08          2        1                                                                                                 to the pains deserves further research directions.","[47] carried out a series of industrial interviews in
11-06          11 0                                                                                                       thirteen companies to study the gaps between the visions and the
                                                                                                                          reality as well as the benefits and sufferings of microservices around
11-07          2        1                        -01 -09 -05 -01 -09 -05 -01 -09 -05 -01 -09 -05 -01 -09 -05 -01 -09 -05  the nine characteristics claimed by Fowler and Lewis [14].",Bogner et al.,2022-05-03 12:12:06+00:00,"A Cross-Company Ethnographic Study on Software Teams for DevOps and Microservices: Organization, Benefits, and Issues",cs.SE,['cs.SE'],"[arxiv.Result.Author('Xin Zhou'), arxiv.Result.Author('Huang Huang'), arxiv.Result.Author('He Zhang'), arxiv.Result.Author('Xin Huang'), arxiv.Result.Author('Dong Shao'), arxiv.Result.Author('Chenxing Zhong')]","Context: DevOps and microservices are acknowledged to be important new
paradigms to tackle contemporary software demands and provide capabilities for
rapid and reliable software development. Industrial reports show that they are
quickly adopted together in massive software companies. However, because of the
technical and organizational requirements, many difficulties against efficient
implementation of the both emerge in real software teams. Objectives: This
study aims to discover the organization, benefits and issues of software teams
using DevOps & microservices from an immersive perspective. Method: An
ethnographic study was carried out in three companies with different business,
size, products, customers, and degree of globalization. All the three companies
claimed their adoption of DevOps and microservices. Seven months (cumulative)
of participant observations and nine interviews with practitioners were
conducted to collect the data of software teams related to DevOps and
microservices. A cross-company empirical investigation using grounded theory
was done by analyzing the archive data. Results: The adoption of DevOps and
microservices brings benefits to rapid delivery, ability improvements and
burden reduction, whilst the high cost and lack of practical guidance were
emerged. Moreover, our observations and interviews reflect that in software
teams, the relationship between DevOps and microservices is not significant,
which differs from the relationship described in the previous studies. Four
lessons for practitioners and four implications for researchers were discussed
based on our findings. Conclusion: Our findings contribute to the understanding
of the organization, benefits and issues of adopting DevOps and microservices
from an immersive perspective of software teams."
5777,"Indeed, correla-
heuristics in our demo tool after manually analyzing a sample of          tion between weaknesses and SATD warrant further research.","Furthermore, 55% of the SATD blocks contain
CWE issue, we refined the initial implementation of the weakness          weak-code snippets of 14 different CWE issues.","775 files sampled with 90% confidence level and 5% margin of
error.",2022-05-04 17:38:43+00:00,WeakSATD: Detecting Weak Self-admitted Technical Debt,cs.SE,"['cs.SE', 'D.2']","[arxiv.Result.Author('Barbara Russo'), arxiv.Result.Author('Matteo Camilli'), arxiv.Result.Author('Moritz Mock')]","Speeding up development may produce technical debt, i.e., not-quite-right
code for which the effort to make it right increases with time as a sort of
interest. Developers may be aware of the debt as they admit it in their code
comments. Literature reports that such a self-admitted technical debt survives
for a long time in a program, but it is not yet clear its impact on the quality
of the code in the long term. We argue that self-admitted technical debt
contains a number of different weaknesses that may affect the security of a
program. Therefore, the longer a debt is not paid back the higher is the risk
that the weaknesses can be exploited. To discuss our claim and rise the
developers' awareness of the vulnerability of the self-admitted technical debt
that is not paid back, we explore the self-admitted technical debt in the
Chromium C-code to detect any known weaknesses. In this preliminary study, we
first mine the Common Weakness Enumeration repository to define heuristics for
the automatic detection and fix of weak code. Then, we parse the C-code to find
self-admitted technical debt and the code block it refers to. Finally, we use
the heuristics to find weak code snippets associated to self-admitted technical
debt and recommend their potential mitigation to developers. Such knowledge can
be used to prioritize self-admitted technical debt for repair. A prototype has
been developed and applied to the Chromium code. Initial findings report that
55\% of self-admitted technical debt code contains weak code of 14 different
types."
5778,"However, correlations between vulnerabil-
6 switch (how)                                                            ities and technical debt indicators requires further research.","2
                                                                         3 unsigned long *src = (unsigned long const *)

                                                                                      set;
                                                                         4 unsigned long *dest = (unsigned long *) &(

                                                                                      thread.p->sigmask);
WeakSATD: Detecting Weak Self-admitted Technical Debt                                                                  MSR ’22, May 23–24, 2022, Pittsburgh, PA, USA

5                                                                         developers use technical debt concepts to discuss design limitations
                                                                          and their consequences.","The
                                                                          approach in [31] uses security bugs in issue tracking systems to
7{                                                                        identify vulnerabilities.",2022-05-04 17:38:43+00:00,WeakSATD: Detecting Weak Self-admitted Technical Debt,cs.SE,"['cs.SE', 'D.2']","[arxiv.Result.Author('Barbara Russo'), arxiv.Result.Author('Matteo Camilli'), arxiv.Result.Author('Moritz Mock')]","Speeding up development may produce technical debt, i.e., not-quite-right
code for which the effort to make it right increases with time as a sort of
interest. Developers may be aware of the debt as they admit it in their code
comments. Literature reports that such a self-admitted technical debt survives
for a long time in a program, but it is not yet clear its impact on the quality
of the code in the long term. We argue that self-admitted technical debt
contains a number of different weaknesses that may affect the security of a
program. Therefore, the longer a debt is not paid back the higher is the risk
that the weaknesses can be exploited. To discuss our claim and rise the
developers' awareness of the vulnerability of the self-admitted technical debt
that is not paid back, we explore the self-admitted technical debt in the
Chromium C-code to detect any known weaknesses. In this preliminary study, we
first mine the Common Weakness Enumeration repository to define heuristics for
the automatic detection and fix of weak code. Then, we parse the C-code to find
self-admitted technical debt and the code block it refers to. Finally, we use
the heuristics to find weak code snippets associated to self-admitted technical
debt and recommend their potential mitigation to developers. Such knowledge can
be used to prioritize self-admitted technical debt for repair. A prototype has
been developed and applied to the Chromium code. Initial findings report that
55\% of self-admitted technical debt code contains weak code of 14 different
types."
5831,"These observations motivated further research, which is reported in this contribution.","Each of those issues either (i) increased the amount of work necessary
to adapt the classiﬁer to the domain, or (ii) decreased the accuracy of the predictions.","Speciﬁcally, it is argued that the varied and demanding reusability scenarios pro-
vide the ultimate measure of the quality of an ontological design.",2022-05-05 19:13:37+00:00,Ontology Reuse: the Real Test of Ontological Design,cs.SE,"['cs.SE', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Piotr Sowinski'), arxiv.Result.Author('Katarzyna Wasielewska-Michniewska'), arxiv.Result.Author('Maria Ganzha'), arxiv.Result.Author('Marcin Paprzycki'), arxiv.Result.Author('Costin Badica')]","Reusing ontologies in practice is still very challenging, especially when
multiple ontologies are involved. Moreover, despite recent advances, systematic
ontology quality assurance remains a difficult problem. In this work, the
quality of thirty biomedical ontologies, and the Computer Science Ontology, are
investigated from the perspective of a practical use case. Special scrutiny is
given to cross-ontology references, which are vital for combining ontologies.
Diverse methods to detect the issues are proposed, including natural language
processing and network analysis. Moreover, several suggestions for improving
ontologies and their quality assurance processes are presented. It is argued
that while the advancing automatic tools for ontology quality assurance are
crucial for ontology improvement, they will not solve the problem entirely. It
is ontology reuse that is the ultimate method for continuously verifying and
improving ontology quality, as well as for guiding its future development. Many
issues can be found and fixed only through practical and diverse ontology reuse
scenarios."
5832,"These
observations motivated further research, which is reported in this contribution.","Each of those issues either (i) increased the amount of work necessary
to adapt the classiﬁer to the domain, or (ii) decreased the accuracy of predictions.","Speciﬁcally, the goal of this work is to demonstrate that the varied and demanding
reusability scenarios provide the ultimate measure of the quality of an ontological design.",2022-05-05 19:13:37+00:00,Ontology Reuse: the Real Test of Ontological Design,cs.SE,"['cs.SE', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Piotr Sowinski'), arxiv.Result.Author('Katarzyna Wasielewska-Michniewska'), arxiv.Result.Author('Maria Ganzha'), arxiv.Result.Author('Marcin Paprzycki'), arxiv.Result.Author('Costin Badica')]","Reusing ontologies in practice is still very challenging, especially when
multiple ontologies are (jointly) involved. Moreover, despite recent advances,
the realization of systematic ontology quality assurance remains a difficult
problem. In this work, the quality of thirty biomedical ontologies, and the
Computer Science Ontology are investigated, from the perspective of a practical
use case. Special scrutiny is given to cross-ontology references, which are
vital for combining ontologies. Diverse methods to detect potential issues are
proposed, including natural language processing and network analysis. Moreover,
several suggestions for improving ontologies and their quality assurance
processes are presented. It is argued that while the advancing automatic tools
for ontology quality assurance are crucial for ontology improvement, they will
not solve the problem entirely. It is ontology reuse that is the ultimate
method for continuously verifying and improving ontology quality, as well as
for guiding its future development. Specifically, multiple issues can be found
and fixed primarily through practical and diverse ontology reuse scenarios."
5960,"https:
ses, and links to further research outputs will be accessible from                            //doi.org/10.1590/2175- 35392019016464
https://osf.io//v93zt.","Psicologia Escolar e Educacional 23 (2019), 1–10.","[12] Joseph Hallett, Nikhil Patnaik, Benjamin Shreeve, and Awais Rashid.",2022-05-09 14:59:31+00:00,The Soft Skills of Software Learning Development: the Psychological Dimensions of Computing and Security Behaviours,cs.SE,"['cs.SE', 'cs.HC']",[arxiv.Result.Author('Matthew Ivory')],"When writing software code, developers typically prioritise functionality
over security, either consciously or unconsciously through biases and
heuristics. This is often attributed to tangible pressures such as client
requirements, but little is understood about the psychological dimensions
affecting security behaviours. There is an increasing demand for understanding
how psychological skills affect secure software development and to understand
how these skills themselves are developed during the learning process.
  This doctoral research explores this research space, with aims to identify
important workplace-based skills for software developers; to identify and
empirically investigate the soft skills behind these workplace skills in order
to understand how soft skills can influence security behaviours; and, to
identify ways to introduce and teach soft skills to computer science students
to prepare the future generation of software developers.
  The motivations behind this research are presented alongside the work plan.
Three distinct phases are introduced, along with planned analyses. Phase one is
currently in the data collection stage, with the second phase in planning.
Prior relevant work is highlighted, and the paper concludes with a presentation
of preliminary results and the planned next steps."
6027,"This hinders further research development for novel techniques
to achieve better results.","Without an in-depth analysis of which parts of the SUT
code is not exercised by these fuzzers, it is not possible to understand their
limitations.","To address these issues, in this paper we compare six state-of-the-art
fuzzers, namely (in alphabetic order) bBOXRT [41], EvoMaster [21],
RESTest [44], RestCT [51], RESTler [31] and RestTestGen [50].",2022-05-11 08:03:28+00:00,Open Problems in Fuzzing RESTful APIs: A Comparison of Tools,cs.SE,['cs.SE'],"[arxiv.Result.Author('Man Zhang'), arxiv.Result.Author('Andrea Arcuri')]","RESTful APIs are a type of web services that are widely used in industry. In
the last few years, a lot of effort in the research community has been spent in
designing novel techniques to automatically fuzz those APIs to find faults in
them. Many real faults were automatically found in a large variety of RESTful
APIs. However, usually the analyzed fuzzers treat the APIs as black-box, and no
analysis of what is actually covered in these systems is done. Therefore,
although these fuzzers are clearly useful for practitioners, we do not know
what are their current limitations and actual effectiveness. Solving this is a
necessary step to be able to design better, more efficient and effective
techniques. To address this issue, in this paper we compare 6 state-of-the-art
fuzzers on 10 RESTful APIs. We then analyzed the source code of which parts of
these APIs the fuzzers fail to generate tests for. This analysis points to
clear limitations of these current fuzzers, listing concrete challenges for the
research community to follow up on."
6079,"potentially controversial statements related to team member
                                                                 age differences requires further research in the future, and
       D. Practitioner making mistakes.","These
similar occasion in real, in this case a similar RC.","Practitioners them-      we elaborate this in Section 7.3.
selves making minor mistakes and ﬁnding that their mis-
takes could have been avoided through easy ﬁxes, make                      “I feel angry, anxious anytime I work with old folks
them feel both low1 in general and low2 (e.g.",2022-05-12 01:29:27+00:00,Emotion-Centric Requirements Change Handling in Software Engineering,cs.SE,['cs.SE'],"[arxiv.Result.Author('Kashumi Madampe'), arxiv.Result.Author('Rashina Hoda'), arxiv.Result.Author('John Grundy')]","Background: Requirements Changes (RCs) -- the
additions/modifications/deletions of functional/non-functional requirements in
software products -- are challenging for software practitioners to handle.
Handling some changes may significantly impact the emotions of the
practitioners. Objective: We wanted to know the key challenges that make RC
handling difficult, how these impact the emotions of software practitioners,
what influences their RC handling, and how RC handling can be made less
emotionally challenging. Method: We followed a mixed-methods approach. We
conducted two survey studies, with 40 participants and 201 participants
respectively. The presentation of key quantitative data was followed by
descriptive statistical analysis, and the qualitative data was analysed using
Strauss-Corbinian Grounded Theory, and Socio-Technical Grounded Theory analysis
techniques. Findings:We found (1) several key factors that make RC handling an
emotional challenge, (2) varying emotions that practitioners feel when it is
challenging to handle RCs, (3) how stakeholders, including practitioners
themselves, peers, managers and customers, influence the RC handling and how
practitioners feel due to the stakeholder influence, and (4) practices that can
be used to better handle RCs. Conclusion: Some challenges are technical and
some are social which also belong to aspects of agile practice, emotional
intelligence, and cognitive intelligence. Therefore, to better handle RCs with
positive emotions in socio-technical environments, agility, emotional
intelligence, and cognitive intelligence need to cooperate with each other."
6153,"The REST-Ling tool might
be of interest to academics aiming to perform further research on API quality.","The tool is built to be freely used by anyone
who aims to improve their APIs’ design quality.",Practitioners can also use the tool to assess the quality of their APIs.,2022-05-13 09:44:44+00:00,Assessing the Linguistic Quality of REST APIs for IoT Applications,cs.SE,['cs.SE'],"[arxiv.Result.Author('Francis Palma'), arxiv.Result.Author('Tobias Olsson'), arxiv.Result.Author('Anna Wingkvist'), arxiv.Result.Author('Javier Gonzalez-Huerta')]","Internet of Things (IoT) is a growing technology that relies on connected
'things' that gather data from peer devices and send data to servers via APIs
(Application Programming Interfaces). The design quality of those APIs has a
direct impact on their understandability and reusability. This study focuses on
the linguistic design quality of REST APIs for IoT applications and assesses
their linguistic quality by performing the detection of linguistic patterns and
antipatterns in REST APIs for IoT applications. Linguistic antipatterns are
considered poor practices in the naming, documentation, and choice of
identifiers. In contrast, linguistic patterns represent best practices to APIs
design. The linguistic patterns and their corresponding antipatterns are hence
contrasting pairs. We propose the SARAv2 (Semantic Analysis of REST APIs
version two) approach to perform syntactic and semantic analyses of REST APIs
for IoT applications. Based on the SARAv2 approach, we develop the REST-Ling
tool and empirically validate the detection results of nine linguistic
antipatterns. We analyse 19 REST APIs for IoT applications. Our detection
results show that the linguistic antipatterns are prevalent and the REST-Ling
tool can detect linguistic patterns and antipatterns in REST APIs for IoT
applications with an average accuracy of over 80%. Moreover, the tool performs
the detection of linguistic antipatterns on average in the order of seconds,
i.e., 8.396 seconds. We found that APIs generally follow good linguistic
practices, although the prevalence of poor practices exists."
6201,"However,       in Table I) or using the minmum and maximum frequency (not
                                        further research is needed on how to construct such visual          shown in the table).","The frequency range is either speciﬁed
                                        buttons as opposed to entering manually, or providing an            using the centre frequency and bandwidth (as for the examples
                                        immediate visual display as users input their query.","Note that the ranges for frequencies vary
                                        and interactive query interfaces in many domains, such as for       greatly from a few hertz to several gigahertz (i.e., 109 Hz).",2022-05-15 12:23:21+00:00,Querying Spatial-Temporal-Spectral Data Using a Graphical Query Builder,cs.SE,"['cs.SE', 'cs.DB']","[arxiv.Result.Author('Adela Gorczynska'), arxiv.Result.Author('Peter Fule'), arxiv.Result.Author('Christoph Treude')]","Constructing complex queries on data which combines spatial, temporal, and
spectral aspects is a challenging and error-prone process. Query interfaces of
general-purpose database management systems fall short in providing intuitive
support for users to effectively and efficiently construct queries. To address
this situation, we developed GraphicalQueryBuilder, a tool which provides
interactive and immediate feedback during query construction and visually
represents search space and queries. A user study with ten professionals showed
that users were able to complete queries on average 40% faster and 40
percentage points more accurately compared to a Microsoft Access baseline."
6242,"The study also pointed out the importance of considering software
         testing activities as a set of human-dependent tasks and emphasized the need for
         further research that examines critically individual assessments of software test-
         ers about software testing activities.","The results from China
         and India revealed that students are not very keen on taking up a software tester
         career, but the Malaysia students’ show a more positive attitude towards soft-
         ware testing.","This investigation can academics involved
         in software testing courses to understand the impacting factors on the motiva-
         tion and de-motivators of their students, as well as to try convey positive view
         of testing as challenging and requires critical thinking and innovative ideas.",2022-05-16 16:13:25+00:00,Social Aspects of Software Testing: Comparative Studies in Asia,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luiz Fernando Capretz'), arxiv.Result.Author('Jingdong Jia'), arxiv.Result.Author('Pradeep Waychal'), arxiv.Result.Author('Shuib Basri')]","This study attempts to understand motivators and de-motivators that influence
the decisions of software students to take up and sustain software testing
careers across three different Asian countries, i.e., China, India, and
Malaysia. The re-search question can be framed as How many software students
across different Asian geographies are keen to take up testing careers, and
what are the reasons for their choices? Towards an answer, we developed a
cross-sectional but simple survey-based instrument. In this work, we
investigated how software students perceived the software testing role. The
results from China and India revealed that students are not very keen on taking
up a software tester career, but the Malaysia students show a more positive
attitude towards software testing. The study also pointed out the importance of
considering software testing activities as a set of human-dependent tasks and
emphasized the need for further re-search that examines critically individual
assessments of software testers about software testing activities. This
investigation can academics involved in software testing courses to understand
the impacting factors on the motivation and de-motivators of their students, as
well as to try convey positive view of testing as challenging and requires
critical thinking and innovative ideas."
6259,"16https://www.journals.elsevier.com/journal-of-systems-and-software
   17https://www.journals.elsevier.com/information-and-software-technology   [6] B. Kitchenham, D. Budgen, and P. Brereton, “Using mapping studies
   18http://www.icse-conferences.org                                              as the basis for further research–a participant-observer case study,”
   19http://www.esem-conferences.org                                              Information and Software Technology, vol.","58–65, 2005.","53, no.",2022-05-16 17:58:11+00:00,Benefits and Drawbacks of a Graduate Course: An Experience Teaching Systematic Literature Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anderson Yoshiaki Iwazaki'), arxiv.Result.Author('Vinicius dos Santos'), arxiv.Result.Author('Katia Romero Felizardo'), arxiv.Result.Author(""/'Erica Ferreira de Souza""), arxiv.Result.Author('Natasha M. C. Valentim'), arxiv.Result.Author('Elisa Yumi Nakagawa')]","Graduate courses can provide specialized knowledge for Ph.D. and Master's
students and contribute to develop their hard and soft skills. At the same
time, Systematic Literature Review (SLR) has been increasingly adopted in the
computing area as a valuable technique to synthesize the state of the art of a
given research topic. However, there is still a poor understanding of the real
benefits and drawbacks of offering the SLR course for graduate students. This
paper reports an experience that examines such benefits and drawbacks, the
difficulties for professors (i.e., educators), and the essential SLR topics to
be taught as well as a way to better teach them. We also surveyed computer
science graduate students who attended the SLR course, which we have offered
for almost ten years for Ph.D. and Master's students in our institution. We
found the attendance to the SLR course is a valuable opportunity for graduate
students to conduct the required deep literature review of their research
topic, improve their research skills, and increase their formation. Hence, we
recommend that Ph.D. and Masters' programs offer the SLR course to contribute
to their academic achievement."
6299,"In addition, capturing customer
                                        present practical limitations that may require further research.","We discuss      more information compared to a scripted testing type that is
                                        the problem, the design and result of our approach, and we          often applied for GUI testing.",workloads has also beneﬁts compared to artiﬁcial tests.,2022-05-16 23:58:34+00:00,Automatic Error Classification and Root Cause Determination while Replaying Recorded Workload Data at SAP HANA,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Neetha Jambigi'), arxiv.Result.Author('Thomas Bach'), arxiv.Result.Author('Felix Schabernack'), arxiv.Result.Author('Michael Felderer')]","Capturing customer workloads of database systems to replay these workloads
during internal testing can be beneficial for software quality assurance.
However, we experienced that such replays can produce a large amount of false
positive alerts that make the results unreliable or time consuming to analyze.
Therefore, we design a machine learning based approach that attributes root
causes to the alerts. This provides several benefits for quality assurance and
allows for example to classify whether an alert is true positive or false
positive. Our approach considerably reduces manual effort and improves the
overall quality assurance for the database system SAP HANA. We discuss the
problem, the design and result of our approach, and we present practical
limitations that may require further research."
6444,"Finally, in
section 5 we draw some concluding remarks and propose some further research
directions.","In section 4 we propose a methodology for learning probabilistic QoS
proﬁles from data sampled from the real-world behaviour of the services, tak-
ing out the responsibility of producing it from the service provider.","2 Probabilistic QoS-aware SLA

In this section we present the main contributions of this work.",2022-05-19 21:53:37+00:00,Probabilistic Quality of Service aware Service Selection,cs.SE,"['cs.SE', 'cs.LO', 'D.2; D.2.4; D.2.1; F.3.1']","[arxiv.Result.Author('Agustín E. Martinez Suñé'), arxiv.Result.Author('Carlos G. Lopez Pombo')]","In software-as-a-service paradigms software systems are no longer monolithic
pieces of code executing within the boundaries of an organisation, on the
contrary, they are conceived as a dynamically changing collection of services,
collectively executing, in pursuit of a common business goal. An essential
aspect of service selection is determining whether the Quality of Service (QoS)
profile of a service satisfies the QoS requirements of a client.
  In realistic execution environments, such QoS values might be influenced by
external, non-controllable events, making it impossible for the service
provider to guarantee that the values characterised by a QoS profile will be
met, naturally leading to the need of a probabilistic interpretation of QoS
profile.
  In this work we propose: 1) a model for describing probabilistic QoS profiles
based on multivariate continuous probability distributions, 2) a language for
describing probabilistic QoS requirements, and 3) an automatic procedure for
assessing whether a probabilistic QoS profile satisfies a probabilistic QoS
requirement."
6524,"Figure 11 shows an example incorrect
further study whether the patch search space produced by            solution which requires two patch ingredients.","that “TBar+Codex” is the most effective among the evaluated
                                                                    combinations by producing all required patch ingredients for
      a) Combine patch space of Codex-e and APR: We                 12 incorrect solutions.","TBar generates
APR and Codex-e complement each other by evaluating                 the ﬁrst patch ingredient (i.e., removing the if-branch from
the patch ingredients produced by different tools.",2022-05-21 12:48:27+00:00,Automated Repair of Code from Language Models,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhiyu Fan'), arxiv.Result.Author('Xiang Gao'), arxiv.Result.Author('Martin Mirchev'), arxiv.Result.Author('Abhik Roychoudhury'), arxiv.Result.Author('Shin Hwei Tan')]","Large language models such as Codex, have shown the capability to produce
code for many programming tasks. However, the success rate of existing models
is low, especially for complex programming tasks. One of the reasons is that
language models lack awareness of program semantics, resulting in incorrect
programs, or even programs which do not compile. In this paper, we
systematically study whether automated program repair (APR) techniques can fix
the incorrect solutions produced by language models in LeetCode contests. The
goal is to study whether APR techniques can enhance reliability in the code
produced by large language models. Our study revealed that: (1) automatically
generated code shares common programming mistakes with human-crafted solutions,
indicating APR techniques may have potential to fix auto-generated code; (2)
given bug location information provided by a statistical fault localization
approach, the newly released Codex edit mode, which supports editing code, is
similar to or better than existing Java repair tools TBar and Recoder in fixing
incorrect solutions. By analyzing the experimental results generated by these
tools, we provide several suggestions: (1) enhancing APR tools to surpass
limitations in patch space (e.g., introducing more flexible fault localization)
is desirable; (2) as large language models can derive more fix patterns by
training on more data, future APR tools could shift focus from adding more fix
patterns to synthesis/semantics based approaches, (3) combination of language
models with APR to curate patch ingredients, is worth studying."
6682,"Engineers per-        into further reading about these drivers, while highlighting that
ceived that awareness of what they should know to properly         some of them may perhaps call for further research.","This table can be used to not only see which of the
                                                                   drivers have been reported before, but it also provides an index
D33: Awareness of necessary security skills.","adopt and apply software security practices signiﬁcantly inﬂu-
ences their motivation to adopt security practices.",2022-05-24 22:31:20+00:00,DASP: A Framework for Driving the Adoption of Software Security Practices,cs.SE,"['cs.SE', 'D.2; K.6.3']","[arxiv.Result.Author('Enrique Larios-Vargas'), arxiv.Result.Author('Omar Elazhary'), arxiv.Result.Author('Soroush Yousefi'), arxiv.Result.Author('Derek Lowlind'), arxiv.Result.Author('Michael L. W. Vliek'), arxiv.Result.Author('Margaret-Anne Storey')]","Implementing software security practices is a critical concern in modern
software development. Industry practitioners, security tool providers, and
researchers have provided standard security guidelines and sophisticated
security development tools to ensure a secure software development pipeline.
But despite these efforts, there continues to be an increase in the number of
vulnerabilities that can be exploited by malicious hackers. There is thus an
urgent need to understand why developers still introduce security
vulnerabilities into their applications and to understand what can be done to
motivate them to write more secure code. To understand and address this problem
further, we propose DASP, a framework for diagnosing and driving the adoption
of software security practices among developers. DASP was conceived by
combining behavioral science theories to shape a cross-sectional interview
study with 28 software practitioners. Our interviews lead to a framework that
consists of a comprehensive set of 33 drivers grouped into 7 higher-level
categories that represent what needs to happen or change so that the adoption
of software security practices occurs. Using the DASP framework, organizations
can design interventions suitable for developers' specific development contexts
that will motivate them to write more secure code."
6683,"Our study opens the door to further research to
      with.","In other words, when designing       capable of doing it, have the right opportunities and conditions
      security guidelines, security professionals should use a      in their work environment, and feel motivated to perform the
      technical vocabulary that developers are more familiar        target behavior.","introduce behavior change techniques to understand developer
                                                                    and other stakeholder behaviors.",2022-05-24 22:31:20+00:00,DASP: A Framework for Driving the Adoption of Software Security Practices,cs.SE,"['cs.SE', 'D.2; K.6.3']","[arxiv.Result.Author('Enrique Larios-Vargas'), arxiv.Result.Author('Omar Elazhary'), arxiv.Result.Author('Soroush Yousefi'), arxiv.Result.Author('Derek Lowlind'), arxiv.Result.Author('Michael L. W. Vliek'), arxiv.Result.Author('Margaret-Anne Storey')]","Implementing software security practices is a critical concern in modern
software development. Industry practitioners, security tool providers, and
researchers have provided standard security guidelines and sophisticated
security development tools to ensure a secure software development pipeline.
But despite these efforts, there continues to be an increase in the number of
vulnerabilities that can be exploited by malicious hackers. There is thus an
urgent need to understand why developers still introduce security
vulnerabilities into their applications and to understand what can be done to
motivate them to write more secure code. To understand and address this problem
further, we propose DASP, a framework for diagnosing and driving the adoption
of software security practices among developers. DASP was conceived by
combining behavioral science theories to shape a cross-sectional interview
study with 28 software practitioners. Our interviews lead to a framework that
consists of a comprehensive set of 33 drivers grouped into 7 higher-level
categories that represent what needs to happen or change so that the adoption
of software security practices occurs. Using the DASP framework, organizations
can design interventions suitable for developers' specific development contexts
that will motivate them to write more secure code."
6944,"The conflict
                                                                          appeared when the downstream repository merged the changes
   The large number of conflicts identified and the low percentage        from the upstream repository that have the nougat-mr1.6-release
of merge operations performed motivate further research aiming            tag, which represents the changes characterizing a new version of
to help developers perform these operations.",duces a mapping of data network types to icon groups.,"Furthermore, the high        the upstream repository.",2022-05-31 04:45:59+00:00,Do Customized Android Frameworks Keep Pace with Android?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pei Liu'), arxiv.Result.Author('Mattia Fazzini'), arxiv.Result.Author('John Grundy'), arxiv.Result.Author('Li Li')]","To satisfy varying customer needs, device vendors and OS providers often rely
on the open-source nature of the Android OS and offer customized versions of
the Android OS. When a new version of the Android OS is released, device
vendors and OS providers need to merge the changes from the Android OS into
their customizations to account for its bug fixes, security patches, and new
features. Because developers of customized OSs might have made changes to code
locations that were also modified by the developers of the Android OS, the
merge task can be characterized by conflicts, which can be time-consuming and
error-prone to resolve.
  To provide more insight into this critical aspect of the Android ecosystem,
we present an empirical study that investigates how eight open-source
customizations of the Android OS merge the changes from the Android OS into
their projects. The study analyzes how often the developers from the customized
OSs merge changes from the Android OS, how often the developers experience
textual merge conflicts, and the characteristics of these conflicts.
Furthermore, to analyze the effect of the conflicts, the study also analyzes
how the conflicts can affect a randomly selected sample of 1,000 apps. After
analyzing 1,148 merge operations, we identified that developers perform these
operations for 9.7\% of the released versions of the Android OS, developers
will encounter at least one conflict in 41.3\% of the merge operations, 58.1\%
of the conflicts required developers to change the customized OSs, and 64.4\%
of the apps considered use at least one method affected by a conflict. In
addition to detailing our results, the paper also discusses the implications of
our findings and provides insights for researchers and practitioners working
with Android and its customizations."
6945,"working as expected when running on customized OSs [11, 17, 22,
37, 45, 47]) and further research is needed to help app developers           Downstream developers needed to handle such a conflict with
handle these issues.",1c).,"The paper elaborates on our findings to help         careful attention because the part of the conflict from the upstream
researchers and practitioners who work with the Android OS and            repository contains an update.",2022-05-31 04:45:59+00:00,Do Customized Android Frameworks Keep Pace with Android?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pei Liu'), arxiv.Result.Author('Mattia Fazzini'), arxiv.Result.Author('John Grundy'), arxiv.Result.Author('Li Li')]","To satisfy varying customer needs, device vendors and OS providers often rely
on the open-source nature of the Android OS and offer customized versions of
the Android OS. When a new version of the Android OS is released, device
vendors and OS providers need to merge the changes from the Android OS into
their customizations to account for its bug fixes, security patches, and new
features. Because developers of customized OSs might have made changes to code
locations that were also modified by the developers of the Android OS, the
merge task can be characterized by conflicts, which can be time-consuming and
error-prone to resolve.
  To provide more insight into this critical aspect of the Android ecosystem,
we present an empirical study that investigates how eight open-source
customizations of the Android OS merge the changes from the Android OS into
their projects. The study analyzes how often the developers from the customized
OSs merge changes from the Android OS, how often the developers experience
textual merge conflicts, and the characteristics of these conflicts.
Furthermore, to analyze the effect of the conflicts, the study also analyzes
how the conflicts can affect a randomly selected sample of 1,000 apps. After
analyzing 1,148 merge operations, we identified that developers perform these
operations for 9.7\% of the released versions of the Android OS, developers
will encounter at least one conflict in 41.3\% of the merge operations, 58.1\%
of the conflicts required developers to change the customized OSs, and 64.4\%
of the apps considered use at least one method affected by a conflict. In
addition to detailing our results, the paper also discusses the implications of
our findings and provides insights for researchers and practitioners working
with Android and its customizations."
6946,"They conclude that the merge conflicts usually in-      further research on automated or semi-automated techniques to
volve more than two developers, which means they need to un-           support the merge operation task.","The large number of conflicts identified
semistructured merge tool over 70,047 merges from 123 Github           and the low percentage of merge operations performed motivate
Java projects.","Furthermore, the high number
derstand different branches developed by different developers to       of conflicts and the high percentage of apps using methods affected
successfully merge.",2022-05-31 04:45:59+00:00,Do Customized Android Frameworks Keep Pace with Android?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pei Liu'), arxiv.Result.Author('Mattia Fazzini'), arxiv.Result.Author('John Grundy'), arxiv.Result.Author('Li Li')]","To satisfy varying customer needs, device vendors and OS providers often rely
on the open-source nature of the Android OS and offer customized versions of
the Android OS. When a new version of the Android OS is released, device
vendors and OS providers need to merge the changes from the Android OS into
their customizations to account for its bug fixes, security patches, and new
features. Because developers of customized OSs might have made changes to code
locations that were also modified by the developers of the Android OS, the
merge task can be characterized by conflicts, which can be time-consuming and
error-prone to resolve.
  To provide more insight into this critical aspect of the Android ecosystem,
we present an empirical study that investigates how eight open-source
customizations of the Android OS merge the changes from the Android OS into
their projects. The study analyzes how often the developers from the customized
OSs merge changes from the Android OS, how often the developers experience
textual merge conflicts, and the characteristics of these conflicts.
Furthermore, to analyze the effect of the conflicts, the study also analyzes
how the conflicts can affect a randomly selected sample of 1,000 apps. After
analyzing 1,148 merge operations, we identified that developers perform these
operations for 9.7\% of the released versions of the Android OS, developers
will encounter at least one conflict in 41.3\% of the merge operations, 58.1\%
of the conflicts required developers to change the customized OSs, and 64.4\%
of the apps considered use at least one method affected by a conflict. In
addition to detailing our results, the paper also discusses the implications of
our findings and provides insights for researchers and practitioners working
with Android and its customizations."
6947,"They        issues, motivating further research in helping app developers han-
found that some of the conflicts induced by unstructured merge can     dle this type of issues.","did a comprehensive study        by conflicts indicate that these apps might experience compatibility
over more than 30,000 merges from 50 open-source projects.","As future research, we plan to present our
be auto-removed and also the merge conflicts are easier to analyze     findings to developers that work on customizations of the Android
and resolve.",2022-05-31 04:45:59+00:00,Do Customized Android Frameworks Keep Pace with Android?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Pei Liu'), arxiv.Result.Author('Mattia Fazzini'), arxiv.Result.Author('John Grundy'), arxiv.Result.Author('Li Li')]","To satisfy varying customer needs, device vendors and OS providers often rely
on the open-source nature of the Android OS and offer customized versions of
the Android OS. When a new version of the Android OS is released, device
vendors and OS providers need to merge the changes from the Android OS into
their customizations to account for its bug fixes, security patches, and new
features. Because developers of customized OSs might have made changes to code
locations that were also modified by the developers of the Android OS, the
merge task can be characterized by conflicts, which can be time-consuming and
error-prone to resolve.
  To provide more insight into this critical aspect of the Android ecosystem,
we present an empirical study that investigates how eight open-source
customizations of the Android OS merge the changes from the Android OS into
their projects. The study analyzes how often the developers from the customized
OSs merge changes from the Android OS, how often the developers experience
textual merge conflicts, and the characteristics of these conflicts.
Furthermore, to analyze the effect of the conflicts, the study also analyzes
how the conflicts can affect a randomly selected sample of 1,000 apps. After
analyzing 1,148 merge operations, we identified that developers perform these
operations for 9.7\% of the released versions of the Android OS, developers
will encounter at least one conflict in 41.3\% of the merge operations, 58.1\%
of the conflicts required developers to change the customized OSs, and 64.4\%
of the apps considered use at least one method affected by a conflict. In
addition to detailing our results, the paper also discusses the implications of
our findings and provides insights for researchers and practitioners working
with Android and its customizations."
6962,Since the       further research on this.,"However, the prediction performance        PMR approach could be transferred to other programming
                                        of the Java-based classiﬁers signiﬁcantly decreases when applied     languages, given that using CFGs supports abstracting from
                                        to functionally equivalent Python and C++ methods despite using      language and implementation details, we decided to perform
                                        only CFG features to abstract from language details.","performance improved again when the classiﬁers were retrained
                                        on the CFGs of the methods written in Python and C++, we                Replication studies are required to validate experimental
                                        conclude that the PMR approach can be generalized, but only          results achieved in prior research.",2022-05-31 13:29:36+00:00,A Replication Study on Predicting Metamorphic Relations at Unit Testing Level,cs.SE,"['cs.SE', 'D.2.5']","[arxiv.Result.Author('Alejandra Duque-Torres'), arxiv.Result.Author('Dietmar Pfahl'), arxiv.Result.Author('Rudolf Ramler'), arxiv.Result.Author('Claus Klammer')]","Metamorphic Testing (MT) addresses the test oracle problem by examining the
relations between inputs and outputs of test executions. Such relations are
known as Metamorphic Relations (MRs). In current practice, identifying and
selecting suitable MRs is usually a challenging manual task, requiring a
thorough grasp of the SUT and its application domain. Thus, Kanewala et al.
proposed the Predicting Metamorphic Relations (PMR) approach to automatically
suggest MRs from a list of six pre-defined MRs for testing newly developed
methods. PMR is based on a classification model trained on features extracted
from the control-flow graph (CFG) of 100 Java methods. In our replication
study, we explore the generalizability of PMR. First, we rebuild the entire
preprocessing and training pipeline and repeat the original study in a close
replication to verify the reported results and establish the basis for further
experiments. Second, we perform a conceptual replication to explore the
reusability of the PMR model trained on CFGs from Java methods in the first
step for functionally identical methods implemented in Python and C++. Finally,
we retrain the model on the CFGs from the Python and C++ methods to investigate
the dependence on programming language and implementation details. We were able
to successfully replicate the original study achieving comparable results for
the Java methods set. However, the prediction performance of the Java-based
classifiers significantly decreases when applied to functionally equivalent
Python and C++ methods despite using only CFG features to abstract from
language details. Since the performance improved again when the classifiers
were retrained on the CFGs of the methods written in Python and C++, we
conclude that the PMR approach can be generalized, but only when classifiers
are developed starting from code artefacts in the used programming language."
6971,"We need further study to better understand
the source of the diﬀerences between our survey and the prior work in this
area.","This result is encouraging because as research software developers become
more knowledgeable about software engineering practices, they will tend to
produce higher-quality software.","5.2 RQ2 - Practices

Research software developers have a clear goal for testing.",2022-05-31 17:40:03+00:00,Testing Research Software: A Survey,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nasir U. Eisty'), arxiv.Result.Author('Jeffrey C. Carver')]","Background: Research software plays an important role in solving real-life
problems, empowering scientific innovations, and handling emergency situations.
Therefore, the correctness and trustworthiness of research software are of
absolute importance. Software testing is an important activity for identifying
problematic code and helping to produce high-quality software. However, testing
of research software is difficult due to the complexity of the underlying
science, relatively unknown results from scientific algorithms, and the culture
of the research software community. Aims: The goal of this paper is to better
understand current testing practices, identify challenges, and provide
recommendations on how to improve the testing process for research software
development. Method: We surveyed members of the research software developer
community to collect information regarding their knowledge about and use of
software testing in their projects. Results: We analysed 120 responses and
identified that even though research software developers report they have an
average level of knowledge about software testing, they still find it difficult
due to the numerous challenges involved. However, there are a number of ways,
such as proper training, that can improve the testing process for research
software. Conclusions: Testing can be challenging for any type of software.
This difficulty is especially present in the development of research software,
where software engineering activities are typically given less attention. To
produce trustworthy results from research software, there is a need for a
culture change so that testing is valued and teams devote appropriate effort to
writing and executing tests."
6972,"Given the fact that our results were drawn from a convenience sample
and the fact that there is much diversity in the research software space, there

  3 https://www.istqb.org/
28  Nasir U. Eisty, Jeﬀrey C. Carver

is a need for further study to verify these ﬁndings in other contexts.","Providing proper training and creating a culture that values
testing could address many of these diﬃculties and improve the testing process.","In our
future work, we will study the speciﬁc technical challenges of testing research
software.",2022-05-31 17:40:03+00:00,Testing Research Software: A Survey,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nasir U. Eisty'), arxiv.Result.Author('Jeffrey C. Carver')]","Background: Research software plays an important role in solving real-life
problems, empowering scientific innovations, and handling emergency situations.
Therefore, the correctness and trustworthiness of research software are of
absolute importance. Software testing is an important activity for identifying
problematic code and helping to produce high-quality software. However, testing
of research software is difficult due to the complexity of the underlying
science, relatively unknown results from scientific algorithms, and the culture
of the research software community. Aims: The goal of this paper is to better
understand current testing practices, identify challenges, and provide
recommendations on how to improve the testing process for research software
development. Method: We surveyed members of the research software developer
community to collect information regarding their knowledge about and use of
software testing in their projects. Results: We analysed 120 responses and
identified that even though research software developers report they have an
average level of knowledge about software testing, they still find it difficult
due to the numerous challenges involved. However, there are a number of ways,
such as proper training, that can improve the testing process for research
software. Conclusions: Testing can be challenging for any type of software.
This difficulty is especially present in the development of research software,
where software engineering activities are typically given less attention. To
produce trustworthy results from research software, there is a need for a
culture change so that testing is valued and teams devote appropriate effort to
writing and executing tests."
7117,"The beneﬁts of using an open-source language
model is that they are easily accessible and thus serve as a baseline to initiate further research on
this topic.",(2020)).,"A beneﬁt of approaching the problem of translating into formal speciﬁcations with pre-
trained language models is that they are capable of generalizing from pre-trained knowledge on
natural language, such as handling unseen variable names.",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the ability of language models to translate natural language into
formal specifications with complex semantics. In particular, we fine-tune
off-the-shelf language models on three datasets consisting of structured
English sentences and their corresponding formal representation: 1) First-order
logic (FOL), commonly used in software verification and theorem proving; 2)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages; and 3) regular expressions (regex), frequently used in
programming and search. Our experiments show that, in these diverse domains,
the language models achieve competitive performance to the respective
state-of-the-art with the benefits of being easy to access, cheap to fine-tune,
and without a particular need for domain-specific reasoning. Additionally, we
show that the language models have a unique selling point: they benefit from
their generalization capabilities from pre-trained knowledge on natural
language, e.g., to generalize to unseen variable names."
7118,"The simplicity of the dataset makes it
an ideal base line for the experiments in this work and also further research.","Two randomly drawn examples are “The fans do not bring any support.” and
“No one will ever understand how continental plates form.”.","The dataset FOL-codesc consists of pairs of natural language sentences of java code snippets
and their ﬁrst-order translations.",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the ability of language models to translate natural language into
formal specifications with complex semantics. In particular, we fine-tune
off-the-shelf language models on three datasets consisting of structured
English sentences and their corresponding formal representation: 1) First-order
logic (FOL), commonly used in software verification and theorem proving; 2)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages; and 3) regular expressions (regex), frequently used in
programming and search. Our experiments show that, in these diverse domains,
the language models achieve competitive performance to the respective
state-of-the-art with the benefits of being easy to access, cheap to fine-tune,
and without a particular need for domain-specific reasoning. Additionally, we
show that the language models have a unique selling point: they benefit from
their generalization capabilities from pre-trained knowledge on natural
language, e.g., to generalize to unseen variable names."
7119,"This is still quite remarkable, considering that
further research in this direction would enable the developement of a tool that synthesizes hardware
speciﬁcations directly out of natural language.","On these large instances, the
language model achieves an accuracy of 62.5%.","In the following, we provide for the interested reader
a random sample of a successful translation.",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the ability of language models to translate natural language into
formal specifications with complex semantics. In particular, we fine-tune
off-the-shelf language models on three datasets consisting of structured
English sentences and their corresponding formal representation: 1) First-order
logic (FOL), commonly used in software verification and theorem proving; 2)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages; and 3) regular expressions (regex), frequently used in
programming and search. Our experiments show that, in these diverse domains,
the language models achieve competitive performance to the respective
state-of-the-art with the benefits of being easy to access, cheap to fine-tune,
and without a particular need for domain-specific reasoning. Additionally, we
show that the language models have a unique selling point: they benefit from
their generalization capabilities from pre-trained knowledge on natural
language, e.g., to generalize to unseen variable names."
7120,"A possible
further research direction is to explore the capabilities of decoder-only models such as the gpt-2
model family.",Another limitation of this paper is the focus on one particular class of language models.,The datasets considered in this work are purely synthetic.,2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the ability of language models to translate natural language into
formal specifications with complex semantics. In particular, we fine-tune
off-the-shelf language models on three datasets consisting of structured
English sentences and their corresponding formal representation: 1) First-order
logic (FOL), commonly used in software verification and theorem proving; 2)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages; and 3) regular expressions (regex), frequently used in
programming and search. Our experiments show that, in these diverse domains,
the language models achieve competitive performance to the respective
state-of-the-art with the benefits of being easy to access, cheap to fine-tune,
and without a particular need for domain-specific reasoning. Additionally, we
show that the language models have a unique selling point: they benefit from
their generalization capabilities from pre-trained knowledge on natural
language, e.g., to generalize to unseen variable names."
7121,"With this work, we contribute to an
open-source gathering of existing datasets to conduct further research.","A ﬁnal limitation of this work we encountered is the unfeasibility of proper comparisons with ex-
isting works, e.g., due to missing publically available datasets.","To conclude, we did a ﬁrst study on the capabilities of language models to translate natural language
to formal speciﬁcations.",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the ability of language models to translate natural language into
formal specifications with complex semantics. In particular, we fine-tune
off-the-shelf language models on three datasets consisting of structured
English sentences and their corresponding formal representation: 1) First-order
logic (FOL), commonly used in software verification and theorem proving; 2)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages; and 3) regular expressions (regex), frequently used in
programming and search. Our experiments show that, in these diverse domains,
the language models achieve competitive performance to the respective
state-of-the-art with the benefits of being easy to access, cheap to fine-tune,
and without a particular need for domain-specific reasoning. Additionally, we
show that the language models have a unique selling point: they benefit from
their generalization capabilities from pre-trained knowledge on natural
language, e.g., to generalize to unseen variable names."
7122,"We provided experiments, serving as a baseline for further research.","We contributed two new datasets for translating natural language into
First-order Logic and two new datasets for translating natural language into Linear-time Temporal
Logic.","Our experimental
results show that off-the-shelf language models achieve competitive accuracy on existing datasets
and perform well on new datasets.",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the ability of language models to translate natural language into
formal specifications with complex semantics. In particular, we fine-tune
off-the-shelf language models on three datasets consisting of structured
English sentences and their corresponding formal representation: 1) First-order
logic (FOL), commonly used in software verification and theorem proving; 2)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages; and 3) regular expressions (regex), frequently used in
programming and search. Our experiments show that, in these diverse domains,
the language models achieve competitive performance to the respective
state-of-the-art with the benefits of being easy to access, cheap to fine-tune,
and without a particular need for domain-specific reasoning. Additionally, we
show that the language models have a unique selling point: they benefit from
their generalization capabilities from pre-trained knowledge on natural
language, e.g., to generalize to unseen variable names."
7123,"A possible further research direction is to explore the capabilities of
decoder-only models such as the GPT-2 model family.","Another limitation is the focus on one particular

                              9
Preprint

class of language models.","Many datasets considered in this work are
purely synthetic (which is only natural for the considered domains).",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the generalization abilities of language models when translating
natural language into formal specifications with complex semantics. In
particular, we fine-tune language models on three datasets consisting of
English sentences and their corresponding formal representation: 1) regular
expressions (regex), frequently used in programming and search; 2) First-order
logic (FOL), commonly used in software verification and theorem proving; and 3)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages. Our experiments show that, in these diverse domains,
the language models maintain their generalization capabilities from pre-trained
knowledge of natural language to generalize, e.g., to new variable names or
operator descriptions. Additionally, they achieve competitive performance, and
even outperform the state-of-the-art for translating into regular expressions,
with the benefits of being easy to access, efficient to fine-tune, and without
a particular need for domain-specific reasoning."
7124,"With
this work, we contribute to an open-source gathering of existing datasets to conduct further research.","A ﬁnal limitation is
the unfeasibility of proper comparisons with existing works, e.g., due to unavailable datasets.","To conclude, we conducted the ﬁrst study on the generalization capabilities of ﬁne-tuned language
models to translate natural language into formal speciﬁcations, resulting in a new state-of-the-art
for translating natural language into regular expressions.",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the generalization abilities of language models when translating
natural language into formal specifications with complex semantics. In
particular, we fine-tune language models on three datasets consisting of
English sentences and their corresponding formal representation: 1) regular
expressions (regex), frequently used in programming and search; 2) First-order
logic (FOL), commonly used in software verification and theorem proving; and 3)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages. Our experiments show that, in these diverse domains,
the language models maintain their generalization capabilities from pre-trained
knowledge of natural language to generalize, e.g., to new variable names or
operator descriptions. Additionally, they achieve competitive performance, and
even outperform the state-of-the-art for translating into regular expressions,
with the benefits of being easy to access, efficient to fine-tune, and without
a particular need for domain-specific reasoning."
7125,"We provided experiments on the generalization
capabilities of the pre-trained language model T5, which serves as a baseline for further research.","We contributed two new
datasets for translating natural language into First-order Logic and two new datasets for translating
natural language into Linear-time Temporal Logic.","Our experimental results show that off-the-shelf language models can outperform approaches on
existing datasets and perform well on new datasets.",2022-06-04 10:49:30+00:00,Formal Specifications from Natural Language,cs.SE,"['cs.SE', 'cs.LG', 'cs.PL']","[arxiv.Result.Author('Christopher Hahn'), arxiv.Result.Author('Frederik Schmitt'), arxiv.Result.Author('Julia J. Tillman'), arxiv.Result.Author('Niklas Metzger'), arxiv.Result.Author('Julian Siber'), arxiv.Result.Author('Bernd Finkbeiner')]","We study the generalization abilities of language models when translating
natural language into formal specifications with complex semantics. In
particular, we fine-tune language models on three datasets consisting of
English sentences and their corresponding formal representation: 1) regular
expressions (regex), frequently used in programming and search; 2) First-order
logic (FOL), commonly used in software verification and theorem proving; and 3)
linear-time temporal logic (LTL), which forms the basis for industrial hardware
specification languages. Our experiments show that, in these diverse domains,
the language models maintain their generalization capabilities from pre-trained
knowledge of natural language to generalize, e.g., to new variable names or
operator descriptions. Additionally, they achieve competitive performance, and
even outperform the state-of-the-art for translating into regular expressions,
with the benefits of being easy to access, efficient to fine-tune, and without
a particular need for domain-specific reasoning."
7406,"This new dataset
for code comprehension, we propose a novel          provides an insightful reference for further research
Partitioning-based Graph Neural Network with        in the community.","ization ability, we construct a new dataset, where
                                                    the test set contains codes whose functionality has
   In this paper, inspired by human behaviors       never appeared in the training set.",External Knowledge (PGNN-EK).,2022-05-10 06:53:45+00:00,A Neural Network Architecture for Program Understanding Inspired by Human Behaviors,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Renyu Zhu'), arxiv.Result.Author('Lei Yuan'), arxiv.Result.Author('Xiang Li'), arxiv.Result.Author('Ming Gao'), arxiv.Result.Author('Wenyuan Cai')]","Program understanding is a fundamental task in program language processing.
Despite the success, existing works fail to take human behaviors as reference
in understanding programs. In this paper, we consider human behaviors and
propose the PGNN-EK model that consists of two main components. On the one
hand, inspired by the ""divide-and-conquer"" reading behaviors of humans, we
present a partitioning-based graph neural network model PGNN on the upgraded
AST of codes. On the other hand, to characterize human behaviors of resorting
to other resources to help code comprehension, we transform raw codes with
external knowledge and apply pre-training techniques for information
extraction. Finally, we combine the two embeddings generated from the two
components to output code embeddings. We conduct extensive experiments to show
the superior performance of PGNN-EK on the code summarization and code clone
detection tasks. In particular, to show the generalization ability of our
model, we release a new dataset that is more challenging for code clone
detection and could advance the development of the community. Our codes and
data are publicly available at https://github.com/RecklessRonan/PGNN-EK."
7482,"For each MR, we ran an
carry out the mutation analysis to further study the eﬀectiveness           experiment twice, one with the source (random) test case and an-
of MT.",We will         whether the MRs are satisﬁed or violated.,other with the follow-up ones.,2022-06-11 08:06:11+00:00,Testing Ocean Software with Metamorphic Testing,cs.SE,['cs.SE'],"[arxiv.Result.Author('Quang-Hung Luu'), arxiv.Result.Author('Huai Liu'), arxiv.Result.Author('Tsong Yueh Chen'), arxiv.Result.Author('Hai L. Vu')]","Advancing ocean science has a significant impact to the development of the
world, from operating a safe navigation for vessels to maintaining a healthy
and diverse ocean ecosystem. Various ocean software systems have been
extensively adopted for different purposes, for instance, predicting hourly sea
level elevation across shorelines, simulating large-scale ocean circulations,
as well as integrating into Earth system models for weather forecasts and
climate projections. Regardless of their significance, guaranteeing the
trustworthiness of ocean software and modelling systems is a long-standing
challenge. The testing of ocean software suffers a lot from the so-called
oracle problem, which refers to the absence of test oracles mainly due to the
nonlinear interactions of multiple physical variables and the high complexity
in computation. In the ocean, observed tidal signals are distorted by
non-deterministic physical variables, hindering us from knowing the ""true""
astronomical tidal constituents existing in the timeseries. In this paper, we
present how to test tidal analysis and prediction (TAP) software based on
metamorphic testing (MT), a simple yet effective testing approach to the oracle
problem. In particular, we construct metamorphic relations from the periodic
property of astronomical tide, and then use them to successfully detect a
real-life defect in an open-source TAP software. We also conduct a series of
experiments to further demonstrate the applicability and effectiveness of MT in
the testing of TAP software. Our study not only justifies the potential of MT
in testing more complex ocean software and modelling systems, but also can be
expanded to assess and improve the quality of a broader range of scientific
simulation software systems."
7497,"Likewise, further research can investigate
in a single day, which was one of the single largest drops              how narratives form for less popular software products and
to a single company in history [47] There are likely multiple           how those narratives impact the products.",The organization’s stock also dropped more than 20%               of a topic over time.,"factors that contributed to this large drop in valuation and daily
users that extend beyond solely privacy concerns.",2022-06-11 19:58:05+00:00,Narratives: the Unforeseen Influencer of Privacy Concerns,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ze Shi Li'), arxiv.Result.Author('Manish Sihag'), arxiv.Result.Author('Nowshin Nawar Arony'), arxiv.Result.Author('Joao Bezerra Junior'), arxiv.Result.Author('Thanh Phan'), arxiv.Result.Author('Neil Ernst'), arxiv.Result.Author('Daniela Damian')]","Privacy requirements are increasingly growing in importance as new privacy
regulations are enacted. To adequately manage privacy requirements,
organizations not only need to comply with privacy regulations, but also
consider user privacy concerns. In this exploratory study, we used Reddit as a
source to understand users' privacy concerns regarding software applications.
We collected 4.5 million posts from Reddit and classified 129075 privacy
related posts, which is a non-negligible number of privacy discussions. Next,
we clustered these posts and identified 9 main areas of privacy concerns. We
use the concept of narratives from economics (i.e., posts that can go viral) to
explain the phenomenon of what and when users change in their discussion of
privacy. We further found that privacy discussions change over time and privacy
regulatory events have a short term impact on such discussions. However,
narratives have a notable impact on what and when users discussed about
privacy. Considering narratives could guide software organizations in eliciting
the relevant privacy concerns before developing them as privacy requirements."
7498,"Next, further research is necessary to develop

                                                                     9
metrics or strategies to systematically identify narratives in            2) Construct validity: The threat applies to the manual la-
user feedback in the requirements engineering context.",If an organization               narratives.,"More            beling of posts to privacy and non-privacy which is to prepare
importantly, additional work is needed to understand identify          our ground-truth data.",2022-06-11 19:58:05+00:00,Narratives: the Unforeseen Influencer of Privacy Concerns,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ze Shi Li'), arxiv.Result.Author('Manish Sihag'), arxiv.Result.Author('Nowshin Nawar Arony'), arxiv.Result.Author('Joao Bezerra Junior'), arxiv.Result.Author('Thanh Phan'), arxiv.Result.Author('Neil Ernst'), arxiv.Result.Author('Daniela Damian')]","Privacy requirements are increasingly growing in importance as new privacy
regulations are enacted. To adequately manage privacy requirements,
organizations not only need to comply with privacy regulations, but also
consider user privacy concerns. In this exploratory study, we used Reddit as a
source to understand users' privacy concerns regarding software applications.
We collected 4.5 million posts from Reddit and classified 129075 privacy
related posts, which is a non-negligible number of privacy discussions. Next,
we clustered these posts and identified 9 main areas of privacy concerns. We
use the concept of narratives from economics (i.e., posts that can go viral) to
explain the phenomenon of what and when users change in their discussion of
privacy. We further found that privacy discussions change over time and privacy
regulatory events have a short term impact on such discussions. However,
narratives have a notable impact on what and when users discussed about
privacy. Considering narratives could guide software organizations in eliciting
the relevant privacy concerns before developing them as privacy requirements."
7499,"In particular, we need            by having two experts who are well versed, experienced, and
to further study how we can incorporate identiﬁcation and              understand privacy.","We tried to address this problem
are important areas for future work.","We calculated our Cohen’s kappa value
analysis of narratives as part of an organization’s requirements       and the inter-rater agreement levels which also reﬂects our
elicitation process to more effectively achieve an understand-         labeling efﬁciency.",2022-06-11 19:58:05+00:00,Narratives: the Unforeseen Influencer of Privacy Concerns,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ze Shi Li'), arxiv.Result.Author('Manish Sihag'), arxiv.Result.Author('Nowshin Nawar Arony'), arxiv.Result.Author('Joao Bezerra Junior'), arxiv.Result.Author('Thanh Phan'), arxiv.Result.Author('Neil Ernst'), arxiv.Result.Author('Daniela Damian')]","Privacy requirements are increasingly growing in importance as new privacy
regulations are enacted. To adequately manage privacy requirements,
organizations not only need to comply with privacy regulations, but also
consider user privacy concerns. In this exploratory study, we used Reddit as a
source to understand users' privacy concerns regarding software applications.
We collected 4.5 million posts from Reddit and classified 129075 privacy
related posts, which is a non-negligible number of privacy discussions. Next,
we clustered these posts and identified 9 main areas of privacy concerns. We
use the concept of narratives from economics (i.e., posts that can go viral) to
explain the phenomenon of what and when users change in their discussion of
privacy. We further found that privacy discussions change over time and privacy
regulatory events have a short term impact on such discussions. However,
narratives have a notable impact on what and when users discussed about
privacy. Considering narratives could guide software organizations in eliciting
the relevant privacy concerns before developing them as privacy requirements."
7500,"Future work may
which have relatively higher user counts, but our data also            leverage tools or strategies from social network analysis [49]
contain popular software products with fewer number of user            for further study of narratives.","posts and matching these with news stories from the same time
Our data is comprised with subreddits that are associated              frame may not be the best approach, but we believe our work
with popular software products such as AirBnb, or Instagram            still brings attention to this area of research.",posts.,2022-06-11 19:58:05+00:00,Narratives: the Unforeseen Influencer of Privacy Concerns,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ze Shi Li'), arxiv.Result.Author('Manish Sihag'), arxiv.Result.Author('Nowshin Nawar Arony'), arxiv.Result.Author('Joao Bezerra Junior'), arxiv.Result.Author('Thanh Phan'), arxiv.Result.Author('Neil Ernst'), arxiv.Result.Author('Daniela Damian')]","Privacy requirements are increasingly growing in importance as new privacy
regulations are enacted. To adequately manage privacy requirements,
organizations not only need to comply with privacy regulations, but also
consider user privacy concerns. In this exploratory study, we used Reddit as a
source to understand users' privacy concerns regarding software applications.
We collected 4.5 million posts from Reddit and classified 129075 privacy
related posts, which is a non-negligible number of privacy discussions. Next,
we clustered these posts and identified 9 main areas of privacy concerns. We
use the concept of narratives from economics (i.e., posts that can go viral) to
explain the phenomenon of what and when users change in their discussion of
privacy. We further found that privacy discussions change over time and privacy
regulatory events have a short term impact on such discussions. However,
narratives have a notable impact on what and when users discussed about
privacy. Considering narratives could guide software organizations in eliciting
the relevant privacy concerns before developing them as privacy requirements."
7649,"We then discuss the importance of
JFrog          58 50 35 / 2 / 48 46 / 46                         issue and link quality and outline further research directions.","NL                             V. DISCUSSION
Apache
Hyperledger    50 43 34 37 3 41 44 36 62 52                         We summarize our ﬁndings with possible interpretations and
IntelDAOS                                                        implications for practice.","Jira                                                             Finally, we outline possible threats to validity.",2022-06-14 21:37:36+00:00,Automated Detection of Typed Links in Issue Trackers,cs.SE,['cs.SE'],"[arxiv.Result.Author('Clara Marie Lüders'), arxiv.Result.Author('Tim Pietz'), arxiv.Result.Author('Walid Maalej')]","Stakeholders in software projects use issue trackers like JIRA to capture and
manage issues, including requirements and bugs. To ease issue navigation and
structure project knowledge, stakeholders manually connect issues via links of
certain types that reflect different dependencies, such as Epic-, Block-,
Duplicate-, or Relate- links. Based on a large dataset of 15 JIRA repositories,
we study how well state-of-the-art machine learning models can automatically
detect common link types. We found that a pure BERT model trained on titles and
descriptions of linked issues significantly outperforms other optimized deep
learning models, achieving an encouraging average macro F1-score of 0.64 for
detecting 9 popular link types across all repositories (weighted F1-score of
0.73). For the specific Subtask- and Epic- links, the model achieved top
F1-scores of 0.89 and 0.97, respectively. Our model does not simply learn the
textual similarity of the issues. In general, shorter issue text seems to
improve the prediction accuracy with a strong negative correlation of -0.70. We
found that Relate-links often get confused with the other links, which suggests
that they are likely used as default links in unclear cases. We also observed
significant differences across the repositories, depending on how they are used
and by whom."
7729,"As our dataset aims to facilitate further research in the domain, it would also
end up having this societal impact, albeit indirectly so.","An unintended consequence of this has been the increased
carbon footprint of deep learning research as a result of running large number of experiments to
validate hypotheses.","We would encourage users to use compute
and memory efﬁcient methods when carrying their research using this dataset.",2022-06-16 22:49:39+00:00,XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence,cs.SE,"['cs.SE', 'cs.AI', 'cs.LG']","[arxiv.Result.Author('Ming Zhu'), arxiv.Result.Author('Aneesh Jain'), arxiv.Result.Author('Karthik Suresh'), arxiv.Result.Author('Roshan Ravindran'), arxiv.Result.Author('Sindhu Tipirneni'), arxiv.Result.Author('Chandan K. Reddy')]","Recent advances in machine learning have significantly improved the
understanding of source code data and achieved good performance on a number of
downstream tasks. Open source repositories like GitHub enable this process with
rich unlabeled code data. However, the lack of high quality labeled data has
largely hindered the progress of several code related tasks, such as program
translation, summarization, synthesis, and code search. This paper introduces
XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for
cross-lingual code intelligence. Our dataset contains fine-grained parallel
data from 8 languages (7 commonly used programming languages and English), and
supports 10 cross-lingual code tasks. To the best of our knowledge, it is the
largest parallel dataset for source code both in terms of size and the number
of languages. We also provide the performance of several state-of-the-art
baseline models for each task. We believe this new dataset can be a valuable
asset for the research community and facilitate the development and validation
of new methods for cross-lingual code intelligence."
7827,"While further research is required
                                                                                                     to verify if the changes in phantom artifacts were
code review coverage(%)  100                                                                         code reviewed or not, we recommend practitioners
                          80                                                                         remain careful of accidental phantom artifacts, such
                          60                                                                         as publishing local ﬁles in the package that are not
                          40                                                                         checked into the repository.",one phantom ﬁle).,"20

                         0

                              Crates.io npm    PyPI RubyGems All                                 5.2 RQ2: Code review coverage
                                             Registry
                                                                                                 In this section, we present our ﬁndings on code review
Fig.",2022-06-19 14:48:48+00:00,Are your dependencies code reviewed?: Measuring code review coverage in dependency updates,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nasif Imtiaz'), arxiv.Result.Author('Laurie Williams')]","As modern software extensively uses free open source packages as
dependencies, developers have to regularly pull in new third-party code through
frequent updates. However, without a proper review of every incoming change,
vulnerable and malicious code can sneak into the codebase through these
dependencies. The goal of this study is to aid developers in securely accepting
dependency updates by measuring if the code changes in an update have passed
through a code review process. We implement Depdive, an update audit tool for
packages in Crates.io, npm, PyPI, and RubyGems registry. Depdive first (i)
identifies the files and the code changes in an update that cannot be traced
back to the package's source repository, i.e., \textit{phantom artifacts}; and
then (ii) measures what portion of changes in the update, excluding the phantom
artifacts, has passed through a code review process, i.e., \textit{code review
coverage}.
  Using Depdive, we present an empirical study across the latest ten updates of
the most downloaded 1000 packages in each of the four registries. We further
evaluated our results through a maintainer agreement survey. We find the
updates are typically only partially code-reviewed (52.5\% of the time).
Further, only 9.0\% of the packages had all their updates in our data set fully
code-reviewed, indicating that even the most used packages can introduce
non-reviewed code in the software supply chain. We also observe that updates
either tend to have high \textit{CRC} or low \textit{CRC}, suggesting that
packages at the opposite end of the spectrum may require a separate set of
treatments."
7848,"Therefore,
                                                                 we were interested to see how reliability has been
                                                                 investigated by researchers in a mobile context, what
                                                                 metrics are used, what reliability attributes (i.e., sub-QAs
                                                                 of reliability as defined in standards, such as ‘availability’
                                                                 and ‘fault-tolerance’) are studied and what areas need
                                                                 further research.","1 shows, our tertiary review discovered a research
                                                                 gap in that there are no previous systematic secondary
                                                                 studies related to the reliability of mobile apps.","This knowledge is important to the
                                                                 research and practitioner community who deal with the
                                                                 reliability of mobile apps, given their pervasive use.",2022-06-20 13:16:00+00:00,A Systematic Mapping Study Addressing the Reliability of Mobile Applications: The Need to Move Beyond Testing Reliability,cs.SE,['cs.SE'],"[arxiv.Result.Author('Chathrie Wimalasooriya'), arxiv.Result.Author('Sherlock A. Licorish'), arxiv.Result.Author('Daniel Alencar da Costa'), arxiv.Result.Author('Stephen G. MacDonell')]","Intense competition in the mobile apps market means it is important to
maintain high levels of app reliability to avoid losing users. Yet despite its
importance, app reliability is underexplored in the research literature. To
address this need, we identify, analyse, and classify the state-of-the-art in
the field of mobile apps' reliability through a systematic mapping study. From
the results of such a study, researchers in the field can identify pressing
research gaps, and developers can gain knowledge about existing solutions, to
potentially leverage them in practice. We found 87 relevant papers which were
then analysed and classified based on their research focus, research type,
contribution, research method, study settings, data, quality attributes and
metrics used. Results indicate that there is a lack of research on
understanding reliability with regard to context-awareness, self-healing,
ageing and rejuvenation, and runtime event handling. These aspects have rarely
been studied, or if studied, there is limited evaluation. We also identified
several other research gaps including the need to conduct more research in
real-world industrial projects. Furthermore, little attention has been paid
towards quality standards while conducting research. Outcomes here show
numerous opportunities for greater research depth and breadth on mobile app
reliability."
7869,"THREATS TO VALIDITY                           In order to enable further research, we publish data sets
                                                                    for 6 open-source projects on commit, ﬁle and method level.",VIII.,A.,2022-06-20 18:58:21+00:00,PR-SZZ: How pull requests can support the tracing of defects in software repositories,cs.SE,['cs.SE'],"[arxiv.Result.Author('Peter Bludau'), arxiv.Result.Author('Alexander Pretschner')]","The SZZ algorithm represents a standard way to identify bug fixing commits as
well as inducing counterparts. It forms the basis for data sets used in
numerous empirical studies. Since its creation, multiple extensions have been
proposed to enhance its performance. For historical reasons, related work
relies on commit messages to map bug tickets to possibly related code with no
additional data used to trace inducing commits from these fixes. Therefore, we
present an updated version of SZZ utilizing pull requests, which are widely
adopted today. We evaluate our approach in comparison to existing SZZ variants
by conducting experiments and analyzing the usage of pull requests, inner
commits, and merge strategies. We base our results on 6 open-source projects
with more than 50k commits and 35k pull requests. With respect to bug fixing
commits, on average 18% of bug tickets can be additionally mapped to a fixing
commit, resulting in an overall F-score of 0.75, an improvement of 40
percentage points. By selecting an inducing commit, we manage to reduce the
false-positives and increase precision by on average 16 percentage points in
comparison to existing approaches."
7928,"However, current re-         also concluded in further researches that code style is more
                                        cruitment processes do not consider that even a good program-      than cosmetic and that it can affect areas such as code
                                        mer may sometimes fail to solve a problem in an exam which         comprehension [6].","They
                                        ”good” that programmer is considered.",he could have solved otherwise under normal circumstances.,2022-06-22 07:44:00+00:00,Exploring the Impact of Code Style in Identifying Good Programmers,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rafed Muhammad Yasir'), arxiv.Result.Author('Ahmedul Kabir')]","Code style reflects the choice of textual representation of source code. This
study, for the first time, explores whether code style can be used to identify
good programmers with a vision that recruitment process in the software
industry can be improved. For analysis, solutions from Google Code Jam were
selected. The study used cluster analysis to find association between good
programmers and style clusters. Furthermore, supervised machine learning models
were trained with stylistic features to predict good programmers. Results
reveal that, although association between programmers with particular clusters
could not be concluded, supervised learning models can predict good
programmers."
7929,"However,
tabsLeadLines           0.163                                   the results can induce further research on how recruitment
numTabs/length          0.151                                   processes can be improved with our results.","Therefore we
newLineBeforeOpenBrace  0.282                                   cannot compare our results with existing studies.","numSpaces/length        0.103

   • new line before opening braces, tabs lead lines            TABLE IV: Prediction Results of Supervised Learning Models
   • no new line before opening braces, tabs lead lines
   • new line before opening braces, whitespace lead lines      Model   Recall  macro-F1  ROC-AUC   Balanced
   • no new line before opening braces, whitespace lead lines                                       Accuracy
                                                                BRF     0.650     0.511      0.695
Although style clusters were found, the good programmers        LR      0.641     0.523      0.692    0.645
were almost equally distributed among them.",2022-06-22 07:44:00+00:00,Exploring the Impact of Code Style in Identifying Good Programmers,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rafed Muhammad Yasir'), arxiv.Result.Author('Ahmedul Kabir')]","Code style reflects the choice of textual representation of source code. This
study, for the first time, explores whether code style can be used to identify
good programmers with a vision that recruitment process in the software
industry can be improved. For analysis, solutions from Google Code Jam were
selected. The study used cluster analysis to find association between good
programmers and style clusters. Furthermore, supervised machine learning models
were trained with stylistic features to predict good programmers. Results
reveal that, although association between programmers with particular clusters
could not be concluded, supervised learning models can predict good
programmers."
7930,"However, current re-         also concluded in further researches that code style is more
                                        cruitment processes do not consider that even a good program-      than cosmetic and that it can affect areas such as code
                                        mer may sometimes fail to solve a problem in an exam which         comprehension [6].","They
                                        ”good” that programmer is considered.",he could have solved otherwise under normal circumstances.,2022-06-22 07:44:00+00:00,Exploring the Impact of Code Style in Identifying Good Programmers,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rafed Muhammad Yasir'), arxiv.Result.Author('Dr. Ahmedul Kabir')]","Code style reflects the choice of textual representation of source code. This
study, for the first time, explores whether code style can be used to identify
good programmers with a vision that recruitment process in the software
industry can be improved. For analysis, solutions from Google Code Jam were
selected. The study used cluster analysis to find association between good
programmers and style clusters. Furthermore, supervised machine learning models
were trained with stylistic features to predict good programmers. Results
reveal that, although association between programmers with particular clusters
could not be concluded, supervised learning models can predict good
programmers."
7931,"However,
tabsLeadLines           0.163                                   the results can induce further research on how recruitment
numTabs/length          0.151                                   processes can be improved with our results.","Therefore we
newLineBeforeOpenBrace  0.282                                   cannot compare our results with existing studies.","numSpaces/length        0.103

   • new line before opening braces, tabs lead lines            TABLE IV: Prediction Results of Supervised Learning Models
   • no new line before opening braces, tabs lead lines
   • new line before opening braces, whitespace lead lines      Model   Recall  macro-F1  ROC-AUC   Balanced
   • no new line before opening braces, whitespace lead lines                                       Accuracy
                                                                BRF     0.650     0.511      0.695
Although style clusters were found, the good programmers        LR      0.641     0.523      0.692    0.645
were almost equally distributed among them.",2022-06-22 07:44:00+00:00,Exploring the Impact of Code Style in Identifying Good Programmers,cs.SE,['cs.SE'],"[arxiv.Result.Author('Rafed Muhammad Yasir'), arxiv.Result.Author('Dr. Ahmedul Kabir')]","Code style reflects the choice of textual representation of source code. This
study, for the first time, explores whether code style can be used to identify
good programmers with a vision that recruitment process in the software
industry can be improved. For analysis, solutions from Google Code Jam were
selected. The study used cluster analysis to find association between good
programmers and style clusters. Furthermore, supervised machine learning models
were trained with stylistic features to predict good programmers. Results
reveal that, although association between programmers with particular clusters
could not be concluded, supervised learning models can predict good
programmers."
7987,"However, some practitioners’ views suggest the need for further research on the
applicability of writing style rules outside of the experimental evaluation setting.","macros and
quality gates for glossaries and writing style rules) were seen as nice to have in order to improve clarity and
avoid misinterpretations.","As a result, in future work
we foresee replacing the generic writing rules currently used by the PoC with more tailored, domain-speciﬁc
ones elicited from existing documentation and practitioners’ input.",2022-06-23 12:47:47+00:00,Documentation-as-code for Interface Control Document Management in Systems of Systems: a Technical Action Research Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Héctor Cadavid'), arxiv.Result.Author('Vasilios Andrikopoulos'), arxiv.Result.Author('Paris Avgeriou')]","The architecting of Systems of Systems (SoS), that is, of systems that emerge
from the cooperation of multiple independent constituent systems, is a topic of
increasing interest in both industry and academia. However, recent empirical
studies revealed what seems to be an overlooked aspect of the architecting of
SoS that is linked to major integration and operational issues: the interplay
between the various disciplines involved in such an architecting process. This
aspect becomes particularly relevant for the management of the interfaces
between the SoS constituents, where such disciplines inevitably meet. In this
paper, we present the results of the first cycle of a Technical Action Research
(TAR) study conducted in cooperation between the authors and a group of
practitioners involved in the long-running architecting process of a
large-scale radio astronomy SoS project. This TAR is aimed at exploring
potential improvements of the document-centered interface management approach
currently followed in this project by adopting elements of the
\textit{documentation-as-code} philosophy, which is widely adopted in the
domain of software systems. As a result, a working proof-of-concept of an ICD
(Interface Control Document) management approach was developed by the
researchers and evaluated by the practitioners. The results of the study and
the corresponding lessons learned are reported in this work."
8102,"Nevertheless, further research is
        needed to improve it and to disseminate its use among the research community.","Conclusions: Our proposal for the speciﬁcation
        of replication changes seems to ﬁt the needs not only of replications in Com-
        puter Science, but also in other research areas.","Keywords: Replication changes, Templates, Linguistic patterns, Families of experiments,
         Threats to validity

1 Introduction

As in most research areas, empirical studies, especially controlled experiments, can
be used in Computer Science to rigorously evaluate technologies, methods, and tools
and help guide further research by revealing existing problems and difﬁculties [17].",2022-06-27 11:42:08+00:00,A Model-Based Approach for Specifying Changes in Replications of Empirical Studies in Computer Science,cs.SE,['cs.SE'],"[arxiv.Result.Author('Margarita Cruz'), arxiv.Result.Author('Beatriz Bernárdez'), arxiv.Result.Author('Amador Durán'), arxiv.Result.Author('Cathy Guevara-Vega'), arxiv.Result.Author('Antonio Ruiz-Cortés')]","Context: The need of replicating empirical studies in Computer Science (CS)
is widely recognized among the research community to consolidate acquired
knowledge generalizing results. It is essential to report the changes of each
replication to understand the evolution of the experimental validity across a
family of studies. Unfortunately, the lack of proposals undermines these
objectives.
  Objective. The main goal of our work is to provide researchers in CS, and in
other areas of research, with a systematic, tool-supported approach for the
reporting of changes in the replications of their empirical studies.
  Method: Applying DSR, we have developed and validated a composite artifact
consisting of (i) a metamodel of the relevant concepts of replications and
their changes; (ii) templates and linguistic patterns for reporting those
concepts; and (iii) a proof-of-concept model-based software tool that supports
the proposed approach. For its validation, we have carried out a multiple case
study including 9 families of empirical studies from CS and Agrobiology. The 9
families encompass 23 replication studies and 92 replication changes, for which
we have analyzed the suitability of our proposal.
  Results: The multiple case study revealed some initial limitations of our
approach related to threats to experimental validity or context variables.
After several improvement iterations, all the 92 replication changes could be
properly specified, including also their qualitatively estimated effects on
experimental validity across the family of experiments and its corresponding
visualization.
  Conclusions: Our proposal for the specification of replication changes seems
to fit the needs not only of replications in CS, but also in other research
areas. Nevertheless, further research is needed to improve it and to
disseminate its use among the research community."
8103,"Keywords: Replication changes, Templates, Linguistic patterns, Families of experiments,
         Threats to validity

1 Introduction

As in most research areas, empirical studies, especially controlled experiments, can
be used in Computer Science to rigorously evaluate technologies, methods, and tools
and help guide further research by revealing existing problems and difﬁculties [17].","Nevertheless, further research is
        needed to improve it and to disseminate its use among the research community.","However, for their results to be generalizable, reported experiments must be repli-
cated in different contexts, at different times, and under different conditions [15] by
means of so-called families of experiments.1 As Basili et al introduced in 1999 [8], a
family of experiments consists of a baseline or original study, followed up by a set
of replications that answer the same research questions as the original study.",2022-06-27 11:42:08+00:00,A Model-Based Approach for Specifying Changes in Replications of Empirical Studies in Computer Science,cs.SE,['cs.SE'],"[arxiv.Result.Author('Margarita Cruz'), arxiv.Result.Author('Beatriz Bernárdez'), arxiv.Result.Author('Amador Durán'), arxiv.Result.Author('Cathy Guevara-Vega'), arxiv.Result.Author('Antonio Ruiz-Cortés')]","Context: The need of replicating empirical studies in Computer Science (CS)
is widely recognized among the research community to consolidate acquired
knowledge generalizing results. It is essential to report the changes of each
replication to understand the evolution of the experimental validity across a
family of studies. Unfortunately, the lack of proposals undermines these
objectives.
  Objective. The main goal of our work is to provide researchers in CS, and in
other areas of research, with a systematic, tool-supported approach for the
reporting of changes in the replications of their empirical studies.
  Method: Applying DSR, we have developed and validated a composite artifact
consisting of (i) a metamodel of the relevant concepts of replications and
their changes; (ii) templates and linguistic patterns for reporting those
concepts; and (iii) a proof-of-concept model-based software tool that supports
the proposed approach. For its validation, we have carried out a multiple case
study including 9 families of empirical studies from CS and Agrobiology. The 9
families encompass 23 replication studies and 92 replication changes, for which
we have analyzed the suitability of our proposal.
  Results: The multiple case study revealed some initial limitations of our
approach related to threats to experimental validity or context variables.
After several improvement iterations, all the 92 replication changes could be
properly specified, including also their qualitatively estimated effects on
experimental validity across the family of experiments and its corresponding
visualization.
  Conclusions: Our proposal for the specification of replication changes seems
to fit the needs not only of replications in CS, but also in other research
areas. Nevertheless, further research is needed to improve it and to
disseminate its use among the research community."
8104,"Nevertheless, further research
is needed to validate the artifact in more replications, belonging to the Computer
Science discipline or to other different disciplines.","In order to overcome this threat, we invited
other researchers from our own research area and from other research areas to par-
ticipate in the study, thus reducing the potential bias.","In this sense, the CÆSAR tool is
available to the scientiﬁc community, so other researchers can report their replication
changes using it and provide feedback for further improvements.",2022-06-27 11:42:08+00:00,A Model-Based Approach for Specifying Changes in Replications of Empirical Studies in Computer Science,cs.SE,['cs.SE'],"[arxiv.Result.Author('Margarita Cruz'), arxiv.Result.Author('Beatriz Bernárdez'), arxiv.Result.Author('Amador Durán'), arxiv.Result.Author('Cathy Guevara-Vega'), arxiv.Result.Author('Antonio Ruiz-Cortés')]","Context: The need of replicating empirical studies in Computer Science (CS)
is widely recognized among the research community to consolidate acquired
knowledge generalizing results. It is essential to report the changes of each
replication to understand the evolution of the experimental validity across a
family of studies. Unfortunately, the lack of proposals undermines these
objectives.
  Objective. The main goal of our work is to provide researchers in CS, and in
other areas of research, with a systematic, tool-supported approach for the
reporting of changes in the replications of their empirical studies.
  Method: Applying DSR, we have developed and validated a composite artifact
consisting of (i) a metamodel of the relevant concepts of replications and
their changes; (ii) templates and linguistic patterns for reporting those
concepts; and (iii) a proof-of-concept model-based software tool that supports
the proposed approach. For its validation, we have carried out a multiple case
study including 9 families of empirical studies from CS and Agrobiology. The 9
families encompass 23 replication studies and 92 replication changes, for which
we have analyzed the suitability of our proposal.
  Results: The multiple case study revealed some initial limitations of our
approach related to threats to experimental validity or context variables.
After several improvement iterations, all the 92 replication changes could be
properly specified, including also their qualitatively estimated effects on
experimental validity across the family of experiments and its corresponding
visualization.
  Conclusions: Our proposal for the specification of replication changes seems
to fit the needs not only of replications in CS, but also in other research
areas. Nevertheless, further research is needed to improve it and to
disseminate its use among the research community."
8173,"However, in addition to the already identiﬁed de-
pendent variables, there might also be other factors that inﬂuence the results
and further research is necessary to establish causal relations.","We added several controls that might inﬂuence the independent variables to
reduce confounding factors.","7 Related Work

Previous work has investigated a variety of automation tools, including soft-
ware bots, continuous integration/delivery, and GitHub Actions.",2022-06-28 16:24:17+00:00,GitHub Actions: The Impact on the Pull Request Process,cs.SE,['cs.SE'],"[arxiv.Result.Author('Mairieli Wessel'), arxiv.Result.Author('Joseph Vargovich'), arxiv.Result.Author('Marco A. Gerosa'), arxiv.Result.Author('Christoph Treude')]","Automated tools are frequently used in social coding repositories to perform
repetitive activities that are part of the distributed software development
process. Recently, GitHub introduced GitHub Actions, a feature providing
automated workflows for repository maintainers. Understanding and anticipating
the effects of adopting such kind of technology is important for planning and
management. Our research investigates how projects use GitHub Actions, what the
communities discuss about them, and how activity indicators change after their
adoption. Our results indicate that a considerable number of projects adopt
GitHub Actions (almost 30% of our sample) and that developers frequently ask
for help with them. Our findings also suggest that the adoption of GitHub
Actions leads to more rejections of pull requests (PRs), more communication in
accepted PRs and less in rejected PRs, fewer commits in accepted PRs and more
in rejected PRs, and more time to accept a PR. We found similar results in the
Utility Actions but we found fewer rejected PRs for the Code Quality Actions.
Our results are especially relevant for practitioners to consider these effects
when adopting GitHub Actions on their projects."
8200,"We further study the statements that CodeBERT
   The key component of Transformer is the self-attention mech-                  assigns the most weights to.","• RQ2: What critical statements does CodeBERT learn
                                                                                 about?","We classify code statements
anism that represents a sequence by relating tokens in different                 into common categories such as initialization, assignment,
positions [43].",2022-06-29 04:04:38+00:00,Diet Code is Healthy: Simplifying Programs for Pre-Trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCodeBERT,
which aims at lightweight leverage of large pre-trained models for source code.
DietCodeBERT simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8201,"Intuitively, the attention weight for a statement can
simply be obtained using the average attention weights of all its                  Motivated by this finding, we further study the attention of
tokens.",statement.,"However, different tokens in a statement have different                 Java keywords.",2022-06-29 04:04:38+00:00,Diet Code is Healthy: Simplifying Programs for Pre-Trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCodeBERT,
which aims at lightweight leverage of large pre-trained models for source code.
DietCodeBERT simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8202,"We further study the statements that CodeBERT
the next layer.",Both of the outputs will be normalized before entering                  about?,assigns the most weights to.,2022-06-29 04:04:38+00:00,Diet Code is Healthy: Simplifying Programs for Pre-Trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCodeBERT,
which aims at lightweight leverage of large pre-trained models for source code.
DietCodeBERT simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8203,"Intuitively, the attention weight for a statement                  Motivated by this finding, we further study the attention of
can simply be obtained using the average attention weights of all               Java keywords.","importance of each token, we compute the attention weight for
each statement.","Figure 1 shows the attention weight of each Java
its tokens.",2022-06-29 04:04:38+00:00,Diet Code is Healthy: Simplifying Programs for Pre-Trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCodeBERT,
which aims at lightweight leverage of large pre-trained models for source code.
DietCodeBERT simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8204,"We further study the statements that CodeBERT
the next layer.",Both of the outputs will be normalized before entering                  about?,assigns the most weights to.,2022-06-29 04:04:38+00:00,Diet Code Is Healthy: Simplifying Programs for Pre-Trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCodeBERT,
which aims at lightweight leverage of large pre-trained models for source code.
DietCodeBERT simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8205,"Intuitively, the attention weight for a statement                  Motivated by this finding, we further study the attention of
can simply be obtained using the average attention weights of all               Java keywords.","importance of each token, we compute the attention weight for
each statement.","Figure 1 shows the attention weight of each Java
its tokens.",2022-06-29 04:04:38+00:00,Diet Code Is Healthy: Simplifying Programs for Pre-Trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCodeBERT,
which aims at lightweight leverage of large pre-trained models for source code.
DietCodeBERT simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8206,"We further study the statements that CodeBERT
the next layer.",Both of the outputs will be normalized before entering                  about?,assigns the most weights to.,2022-06-29 04:04:38+00:00,Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCode, which
aims at lightweight leverage of large pre-trained models for source code.
DietCode simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8207,"Intuitively, the attention weight for a statement                  Motivated by this finding, we further study the attention of
can simply be obtained using the average attention weights of all               Java keywords.","importance of each token, we compute the attention weight for
each statement.","Figure 1 shows the attention weight of each Java
its tokens.",2022-06-29 04:04:38+00:00,Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCode, which
aims at lightweight leverage of large pre-trained models for source code.
DietCode simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8208,"We further study the statements that CodeBERT
the next layer.",Both of the outputs will be normalized before entering                  about?,assigns the most weights to.,2022-06-29 04:04:38+00:00,Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCode, which
aims at lightweight leverage of large pre-trained models for source code.
DietCode simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8209,"Having obtained the
importance of each token, we compute the attention weight for                      Motivated by this finding, we further study the attention of
each statement.",3.1.2 Computing Attentions of Statements.,"Intuitively, the attention weight for a statement               Java keywords.",2022-06-29 04:04:38+00:00,Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhaowei Zhang'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Beijun Shen'), arxiv.Result.Author('Xiaodong Gu')]","Pre-trained code representation models such as CodeBERT have demonstrated
superior performance in a variety of software engineering tasks, yet they are
often heavy in complexity, quadratically with the length of the input sequence.
Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more
attention to certain types of tokens and statements such as keywords and
data-relevant statements. Based on these findings, we propose DietCode, which
aims at lightweight leverage of large pre-trained models for source code.
DietCode simplifies the input program of CodeBERT with three strategies,
namely, word dropout, frequency filtering, and an attention-based strategy
which selects statements and tokens that receive the most attention weights
during pre-training. Hence, it gives a substantial reduction in the
computational cost without hampering the model performance. Experimental
results on two downstream tasks show that DietCodeBERT provides comparable
results to CodeBERT with 40% less computational cost in fine-tuning and
testing."
8339,"To foster further research, benchmarks and experimental data are released at https://github.com/
NARv22/data.","Finally, after discussing related work in Section 7, we
conclude the paper in Section 8.",The source code is available at https://github.com/formes20/narv.,2022-07-02 07:04:20+00:00,Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Jiaxiang Liu'), arxiv.Result.Author('Yunhan Xing'), arxiv.Result.Author('Xiaomu Shi'), arxiv.Result.Author('Fu Song'), arxiv.Result.Author('Zhiwu Xu'), arxiv.Result.Author('Zhong Ming')]","As a new programming paradigm, deep neural networks (DNNs) have been
increasingly deployed in practice, but the lack of robustness hinders their
applications in safety-critical domains. While there are techniques for
verifying DNNs with formal guarantees, they are limited in scalability and
accuracy. In this paper, we present a novel abstraction-refinement approach for
scalable and exact DNN verification. Specifically, we propose a novel
abstraction to break down the size of DNNs by over-approximation. The result of
verifying the abstract DNN is always conclusive if no spurious counterexample
is reported. To eliminate spurious counterexamples introduced by abstraction,
we propose a novel counterexample-guided refinement that refines the abstract
DNN to exclude a given spurious counterexample while still over-approximating
the original one. Our approach is orthogonal to and can be integrated with many
existing verification techniques. For demonstration, we implement our approach
using two promising and exact tools Marabou and Planet as the underlying
verification engines, and evaluate on widely-used benchmarks ACAS Xu, MNIST and
CIFAR-10. The results show that our approach can boost their performance by
solving more problems and reducing up to 86.3% and 78.0% verification time,
respectively. Compared to the most relevant abstraction-refinement approach,
our approach is 11.6-26.6 times faster."
8373,"This motivates the need for further research on library
migration in Python.","We identify two main limitations of SOAR’s [37]
API mapping technique: (1) it requires that the libraries are properly documented
which is often not the case [3] and (2) it is evaluated only on two pairs of libraries
(only one pair for Python).","8.1.3 Client code transformation for library migration

In addition to API mapping, SOAR [37] also performs client code transformation.",2022-07-03 21:00:08+00:00,PyMigBench and PyMigTax: A Benchmark and Taxonomy for Python Library Migration,cs.SE,['cs.SE'],"[arxiv.Result.Author('Mohayeminul Islam'), arxiv.Result.Author('Ajay Kumar Jha'), arxiv.Result.Author('Sarah Nadi')]","Developers heavily rely on Application Programming Interfaces (APIs) from
libraries to build their projects. However, libraries might become obsolete, or
new libraries with better APIs might become available. In such cases,
developers need to replace the used libraries with alternative libraries, a
process referred to as library migration. When done manually, library migration
can be tedious, time-consuming, and error-prone. Most of the current research
on automated library migration techniques focus on Java libraries, and even
more so on version migrations of the same library. Despite the increasing
popularity of Python, limited research work has investigated migration between
Python libraries. In this paper, we investigate the nature of Python library
migrations in open-source systems. We analyze the code changes that happen
during library migration and build PyMigBench, a manually verified migration
benchmark. PyMigBench contains 436 migration-related code changes from 74
commits in 57 client repositories, and includes migrations between 34 unique
pairs of libraries. Additionally, we manually analyze the migration-related
code changes and create a taxonomy of migrations, PyMigTax, that categorizes
migrations across various dimensions. Our contributions provide the necessary
foundations for developing automated Python library migration tools and
techniques."
8397,"In Proceedings of the 5th International Workshop on Artificial
also correct for remote settings, but further research is required to         Intelligence for Requirements Engineering.",We assume that some of our results are            vant information.,IEEE.,2022-07-04 13:43:40+00:00,Meetings and Mood -- Related or Not? Insights from Student Software Projects,cs.SE,['cs.SE'],"[arxiv.Result.Author('Jil Klünder'), arxiv.Result.Author('Oliver Karras')]","Background: Teamwork, coordination, and communication are a prerequisite for
the timely completion of a software project. Meetings as a facilitator for
coordination and communication are an established medium for information
exchange. Analyses of meetings in software projects have shown that certain
interactions in these meetings, such as proactive statements followed by
supportive ones, influence the mood and motivation of a team, which in turn
affects its productivity. So far, however, research has focused only on certain
interactions at a detailed level, requiring a complex and fine-grained analysis
of a meeting itself.
  Aim: In this paper, we investigate meetings from a more abstract perspective,
focusing on the polarity of the statements, i.e., whether they appear to be
positive, negative, or neutral.
  Method: We analyze the relationship between the polarity of statements in
meetings and different social aspects, including conflicts as well as the mood
before and after a meeting.
  Results: Our results emerge from 21 student software project meetings and
show some interesting insights: (1) Positive mood before a meeting is both
related to the amount of positive statements in the beginning, as well as
throughout the whole meeting, (2) negative mood before the meeting only
influences the amount of negative statements in the first quarter of the
meeting, but not the whole meeting, and (3) the amount of positive and negative
statements during the meeting has no influence on the mood afterwards.
  Conclusions: We conclude that the behaviour in meetings might rather
influence short-term emotional states (feelings) than long-term emotional
states (mood), which are more important for the project."
8428,"In fact,
our further research investigates collaborative practices for assessing fairness in AI systems [63].","Correspondingly, by sharing machine learning performance metrics and results (e.g., word error rate)
with designers, engineers can better align model-level performance with diverse user needs and use contexts.","Despite increasing
awareness, organizational goals and resource constraints continue to pose challenges for building responsible AI
guidelines into current design practices [49, 64].",2022-07-04 23:46:08+00:00,Human-AI Guidelines in Practice: Leaky Abstractions as an Enabler in Collaborative Software Teams,cs.SE,"['cs.SE', 'cs.HC']","[arxiv.Result.Author('Hariharan Subramonyam'), arxiv.Result.Author('Jane Im'), arxiv.Result.Author('Colleen Seifert'), arxiv.Result.Author('Eytan Adar')]","In conventional software development, user experience (UX) designers and
engineers collaborate through separation of concerns (SoC): designers create
human interface specifications, and engineers build to those specifications.
However, we argue that Human-AI systems thwart SoC because human needs must
shape the design of the AI interface, the underlying AI sub-components, and
training data. How do designers and engineers currently collaborate on AI and
UX design? To find out, we interviewed 21 industry professionals (UX
researchers, AI engineers, data scientists, and managers) across 14
organizations about their collaborative work practices and associated
challenges. We find that hidden information encapsulated by SoC challenges
collaboration across design and engineering concerns. Practitioners describe
inventing ad-hoc representations exposing low-level design and implementation
details (which we characterize as leaky abstractions) to ""puncture"" SoC and
share information across expertise boundaries. We identify how leaky
abstractions are employed to collaborate at the AI-UX boundary and formalize a
process of creating and using leaky abstractions."
8528,"Microvision is a promising direction to further research for
The tasks are given in Table I.                                                microservice system analysis.","These results show that
of the microservice APIs and their connections to each other.",General questions                                                                                     VI.,2022-07-06 21:19:19+00:00,Microvision: Static analysis-based approach to visualizing microservices in augmented reality,cs.SE,['cs.SE'],"[arxiv.Result.Author('Tomas Cerny'), arxiv.Result.Author('Amr S. Abdelfattah'), arxiv.Result.Author('Vincent Bushong'), arxiv.Result.Author('Abdullah Al Maruf'), arxiv.Result.Author('Davide Taibi')]","Microservices are supporting digital transformation; however, fundamental
tools and system perspectives are missing to better observe, understand, and
manage these systems, their properties, and their dependencies. Microservices
architecture leans toward decentralization, which yields many advantages to
system operation; it, however, brings challenges to their development.
Microservices lack a system-centric perspective to better cope with system
evolution and quality assessment. In this work, we explore
microservice-specific architecture reconstruction based on static analysis.
Such reconstruction typically results in system models to visualize selected
system-centric perspectives. Conventional models are limited in utility when
the service cardinality is high. We consider an alternative data visualization
using 3D space using augmented reality. To begin testing the feasibility of
deriving such perspectives from microservice systems, we developed and
implemented prototype tools for software architecture reconstruction and
visualization of compared perspectives."
8529,"Microvision is a promising direction to further research for
The tasks are given in Table I.                                                microservice system analysis.","These results show that
of the microservice APIs and their connections to each other.",General questions                                                                                     VI.,2022-07-06 21:19:19+00:00,Microvision: Static analysis-based approach to visualizing microservices in augmented reality,cs.SE,['cs.SE'],"[arxiv.Result.Author('Tomas Cerny'), arxiv.Result.Author('Amr S. Abdelfattah'), arxiv.Result.Author('Vincent Bushong'), arxiv.Result.Author('Abdullah Al Maruf'), arxiv.Result.Author('Davide Taibi')]","Microservices are supporting digital transformation; however, fundamental
tools and system perspectives are missing to better observe, understand, and
manage these systems, their properties, and their dependencies. Microservices
architecture leans toward decentralization, which yields many advantages to
system operation; it, however, brings challenges to their development.
Microservices lack a system-centric perspective to better cope with system
evolution and quality assessment. In this work, we explore
microservice-specific architecture reconstruction based on static analysis.
Such reconstruction typically results in system models to visualize selected
system-centric perspectives. Conventional models are limited in utility when
the service cardinality is high. We consider an alternative data visualization
using 3D space using augmented reality. To begin testing the feasibility of
deriving such perspectives from microservice systems, we developed and
implemented prototype tools for software architecture reconstruction and
visualization of compared perspectives."
8551,"Since different metrics measure the functional or non-functional properties of ML software
from different aspects, we believe that the results of this study can provide insightful implications
for real-world applications as well as a foundational baseline for further research and follow-on
studies.","This gap motivates us to evaluate existing bias mitigation methods in terms of comprehensive
metrics.","We present a comprehensive study, evaluating 17 representative bias mitigation methods in 8
widely adopted benchmark tasks with 12 ML performance metrics, 4 fairness metrics, and 24 types
of fairness-performance trade-offs (i.e., the fairness-performance trade-off in terms of different ML
performance metrics and fairness metrics).",2022-07-07 13:14:49+00:00,A Comprehensive Empirical Study of Bias Mitigation Methods for Software Fairness,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Zhenpeng Chen'), arxiv.Result.Author('Jie M. Zhang'), arxiv.Result.Author('Federica Sarro'), arxiv.Result.Author('Mark Harman')]","Software bias is an increasingly important operational concern for software
engineers. We present a large-scale, comprehensive empirical evaluation of 17
representative bias mitigation methods, evaluated with 12 Machine Learning (ML)
performance metrics, 4 fairness metrics, and 24 types of fairness-performance
trade-off assessment, applied to 8 widely-adopted benchmark software
decision/prediction tasks. The empirical coverage is comprehensive, covering
the largest numbers of bias mitigation methods, evaluation metrics, and
fairness-performance trade-off measures compared to previous work on this
important operational software characteristic. We find that (1) the bias
mitigation methods significantly decrease the values reported by all ML
performance metrics (including those not considered in previous work) in a
large proportion of the scenarios studied (42%~75% according to different ML
performance metrics); (2) the bias mitigation methods achieve fairness
improvement in only approximately 50% over all scenarios and metrics (ranging
between 29%~59% according to the metric used to asses bias/fairness); (3) the
bias mitigation methods have a poor fairness-performance trade-off or even lead
to decreases in both fairness and ML performance in 37% of the scenarios; (4)
the effectiveness of the bias mitigation methods depends on tasks, models, and
fairness and ML performance metrics, and there is no 'silver bullet' bias
mitigation method demonstrated to be effective for all scenarios studied. The
best bias mitigation method that we find outperforms other methods in only 29%
of the scenarios. We have made publicly available the scripts and data used in
this study in order to allow for future replication and extension of our work."
8762,"The developers’ feedback on the                       mentation [12, 41]       finishedForLoop=False break                    1.17
rejected pull requests reveal some interesting concerns about the
readability and performance of pythonic idioms which deserve                                                   break                   else:
further study.","Our results show developers         Loop       an efficient             if n % x == 0:          if n % x == 0:
care about pythonic idiom refactorings, and our refactorings have          Else       for loop imple-
been well received in practice.","The dataset of anti-idiom code and corresponding
idiomatic code produced in this work provides the first large-scale                                            if finishedForLoop:     pass
test bed to systematically investigate such concerns.",2022-07-12 15:30:46+00:00,Making Python Code Idiomatic by Automatic Refactoring Non-Idiomatic Python Code with Pythonic Idioms,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zejun Zhang'), arxiv.Result.Author('Zhenchang Xing'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Xiwei Xu'), arxiv.Result.Author('Liming Zhu')]","Compared to other programming languages (e.g., Java), Python has more idioms
to make Python code concise and efficient. Although pythonic idioms are well
accepted in the Python community, Python programmers are often faced with many
challenges in using them, for example, being unaware of certain pythonic idioms
or do not know how to use them properly. Based on an analysis of 7,638 Python
repositories on GitHub, we find that non-idiomatic Python code that can be
implemented with pythonic idioms occurs frequently and widely. Unfortunately,
there is no tool for automatically refactoring such non-idiomatic code into
idiomatic code. In this paper, we design and implement an automatic refactoring
tool to make Python code idiomatic. We identify nine pythonic idioms by
systematically contrasting the abstract syntax grammar of Python and Java. Then
we define the syntactic patterns for detecting non-idiomatic code for each
pythonic idiom. Finally, we devise atomic AST-rewriting operations and
refactoring steps to refactor non-idiomatic code into idiomatic code. We test
and review over 4,115 refactorings applied to 1,065 Python projects from
GitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to
84 projects. These evaluations confirm the high-accuracy, practicality and
usefulness of our refactoring tool on real-world Python code. Our refactoring
tool can be accessed at 47.242.131.128:5000."
8763,"However, existing online
 performance of pythonic idioms which deserve further study.",The developers also raise concerns about readability and                                             ness and performance of pythonic idioms.,"materials are anecdotal and mostly based on personal programming
                                                                                                                experience.",2022-07-12 15:30:46+00:00,Making Python Code Idiomatic by Automatic Refactoring Non-Idiomatic Python Code with Pythonic Idioms,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zejun Zhang'), arxiv.Result.Author('Zhenchang Xing'), arxiv.Result.Author('Xin Xia'), arxiv.Result.Author('Xiwei Xu'), arxiv.Result.Author('Liming Zhu')]","Compared to other programming languages (e.g., Java), Python has more idioms
to make Python code concise and efficient. Although pythonic idioms are well
accepted in the Python community, Python programmers are often faced with many
challenges in using them, for example, being unaware of certain pythonic idioms
or do not know how to use them properly. Based on an analysis of 7,638 Python
repositories on GitHub, we find that non-idiomatic Python code that can be
implemented with pythonic idioms occurs frequently and widely. Unfortunately,
there is no tool for automatically refactoring such non-idiomatic code into
idiomatic code. In this paper, we design and implement an automatic refactoring
tool to make Python code idiomatic. We identify nine pythonic idioms by
systematically contrasting the abstract syntax grammar of Python and Java. Then
we define the syntactic patterns for detecting non-idiomatic code for each
pythonic idiom. Finally, we devise atomic AST-rewriting operations and
refactoring steps to refactor non-idiomatic code into idiomatic code. We test
and review over 4,115 refactorings applied to 1,065 Python projects from
GitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to
84 projects. These evaluations confirm the high-accuracy, practicality and
usefulness of our refactoring tool on real-world Python code. Our refactoring
tool can be accessed at 47.242.131.128:5000."
8764,"Ultimately, improved predic-        further study of the consequences of the learning approach
tion performance results in fewer bugs and better software         choice in practice.","We also encourage increased awareness and
development and testing tasks.","M. Ahmad, K. Goseva-Popstojanova, and R. Lutz: Preprint submitted to Elsevier     Page 16 of 19
The Untold Impact of Learning Approaches on Software Fault-Proneness Predictions

Acknowledgments                                                                    - 2021 47th Euromicro Conf.",2022-07-12 17:31:55+00:00,The Untold Impact of Learning Approaches on Software Fault-Proneness Predictions,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Mohammad Jamil Ahmad'), arxiv.Result.Author('Katerina Goseva-Popstojanova'), arxiv.Result.Author('Robyn R. Lutz')]","Software fault-proneness prediction is an active research area, with many
factors affecting prediction performance extensively studied. However, the
impact of the learning approach (i.e., the specifics of the data used for
training and the target variable being predicted) on the prediction performance
has not been studied, except for one initial work. This paper explores the
effects of two learning approaches, useAllPredictAll and usePrePredictPost, on
the performance of software fault-proneness prediction, both within-release and
across-releases. The empirical results are based on data extracted from 64
releases of twelve open-source projects. Results show that the learning
approach has a substantial, and typically unacknowledged, impact on the
classification performance. Specifically, using useAllPredictAll leads to
significantly better performance than using usePrePredictPost learning
approach, both within-release and across-releases. Furthermore, this paper
uncovers that, for within-release predictions, this difference in
classification performance is due to different levels of class imbalance in the
two learning approaches. When class imbalance is addressed, the performance
difference between the learning approaches is eliminated. Our findings imply
that the learning approach should always be explicitly identified and its
impact on software fault-proneness prediction considered. The paper concludes
with a discussion of potential consequences of our results for both research
and practice."
8966,"However, this requires further research.","This could be done, for example, by randomly examining the
communication data manually and calculating the balance of negative, neutral, and positive, and matching it with that
of the data set.","Another option that, however, requires further research would
be some kind of calibration to the team with its speciﬁc characteristics.",2022-07-16 14:10:29+00:00,On the Subjectivity of Emotions in Software Projects: How Reliable are Pre-Labeled Data Sets for Sentiment Analysis?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Marc Herrmann'), arxiv.Result.Author('Martin Obaidi'), arxiv.Result.Author('Larissa Chazette'), arxiv.Result.Author('Jil Klünder')]","Social aspects of software projects become increasingly important for
research and practice. Different approaches analyze the sentiment of a
development team, ranging from simply asking the team to so-called sentiment
analysis on text-based communication. These sentiment analysis tools are
trained using pre-labeled data sets from different sources, including GitHub
and Stack Overflow. In this paper, we investigate if the labels of the
statements in the data sets coincide with the perception of potential members
of a software project team. Based on an international survey, we compare the
median perception of 94 participants with the pre-labeled data sets as well as
every single participant's agreement with the predefined labels. Our results
point to three remarkable findings: (1) Although the median values coincide
with the predefined labels of the data sets in 62.5% of the cases, we observe a
huge difference between the single participant's ratings and the labels; (2)
there is not a single participant who totally agrees with the predefined
labels; and (3) the data set whose labels are based on guidelines performs
better than the ad hoc labeled data set."
8967,"Another option that, however, requires further research would
be some kind of calibration to the team with its speciﬁc characteristics.","However, this requires further research.","Do we use the tool in a team that generally has
a very good mood or is it a rather depressed team?",2022-07-16 14:10:29+00:00,On the Subjectivity of Emotions in Software Projects: How Reliable are Pre-Labeled Data Sets for Sentiment Analysis?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Marc Herrmann'), arxiv.Result.Author('Martin Obaidi'), arxiv.Result.Author('Larissa Chazette'), arxiv.Result.Author('Jil Klünder')]","Social aspects of software projects become increasingly important for
research and practice. Different approaches analyze the sentiment of a
development team, ranging from simply asking the team to so-called sentiment
analysis on text-based communication. These sentiment analysis tools are
trained using pre-labeled data sets from different sources, including GitHub
and Stack Overflow. In this paper, we investigate if the labels of the
statements in the data sets coincide with the perception of potential members
of a software project team. Based on an international survey, we compare the
median perception of 94 participants with the pre-labeled data sets as well as
every single participant's agreement with the predefined labels. Our results
point to three remarkable findings: (1) Although the median values coincide
with the predefined labels of the data sets in 62.5% of the cases, we observe a
huge difference between the single participant's ratings and the labels; (2)
there is not a single participant who totally agrees with the predefined
labels; and (3) the data set whose labels are based on guidelines performs
better than the ad hoc labeled data set."
9021,"We therefore outline ongoing challenges         explainability and freedom from discrimination, and generally
and opportunities for further research as follows:                 adapting the RE process for ML [3].","These included understanding qualitative measures
point, and the DLMs described here are simply proof-of-            speciﬁc to ML, addressing new requirement types such as
concept solutions.","While their work gen-
                                                                   erally discussed the importance of selecting suitable training
C1: Lack of automation: Forum mining aspects of RESAM              data, it did not address the challenges of multi-variate time-
      were supported only by basic search features.",2022-07-18 18:09:59+00:00,RESAM: Requirements Elicitation and Specification for Deep-Learning Anomaly Models with Applications to UAV Flight Controllers,cs.SE,"['cs.SE', 'cs.AI', 'cs.MA']","[arxiv.Result.Author('Md Nafee Al Islam'), arxiv.Result.Author('Yihong Ma'), arxiv.Result.Author('Pedro Alarcon Granadeno'), arxiv.Result.Author('Nitesh Chawla'), arxiv.Result.Author('Jane Cleland-Huang')]","CyberPhysical systems (CPS) must be closely monitored to identify and
potentially mitigate emergent problems that arise during their routine
operations. However, the multivariate time-series data which they typically
produce can be complex to understand and analyze. While formal product
documentation often provides example data plots with diagnostic suggestions,
the sheer diversity of attributes, critical thresholds, and data interactions
can be overwhelming to non-experts who subsequently seek help from discussion
forums to interpret their data logs. Deep learning models, such as Long
Short-term memory (LSTM) networks can be used to automate these tasks and to
provide clear explanations of diverse anomalies detected in real-time
multivariate data-streams. In this paper we present RESAM, a requirements
process that integrates knowledge from domain experts, discussion forums, and
formal product documentation, to discover and specify requirements and design
definitions in the form of time-series attributes that contribute to the
construction of effective deep learning anomaly detectors. We present a
case-study based on a flight control system for small Uncrewed Aerial Systems
and demonstrate that its use guides the construction of effective anomaly
detection models whilst also providing underlying support for explainability.
RESAM is relevant to domains in which open or closed online forums provide
discussion support for log analysis."
9033,"However,
the number of clusters found in each run was smaller than the total number for some SUTs, even for BCS, so the effect
of execution time needs further study in the future.","It should be mentioned that in a real-world situation, a single search run with sufﬁciently many boundary
candidates should sufﬁce to do both the clustering and extraction of summary - which will both be faster and simpler
than running many times over and building an overall clustering model over the entire set of found candidates.",We mitigate internal validity threats by doing pilot studies and testing our instrumentation.,2022-07-19 04:35:38+00:00,Automated Black-Box Boundary Value Detection,cs.SE,"['cs.SE', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Felix Dobslaw'), arxiv.Result.Author('Robert Feldt'), arxiv.Result.Author('Francisco de Oliveira Neto')]","The input domain of software systems can typically be divided into
sub-domains for which the outputs are similar. To ensure high quality it is
critical to test the software on the boundaries between these sub-domains.
Consequently, boundary value analysis and testing has been part of the toolbox
of software testers for long and is typically taught early to students.
However, despite its many argued benefits, boundary value analysis for a given
specification or piece of software is typically described in abstract terms
which allow for variation in how testers apply it.
  Here we propose an automated, black-box boundary value detection method to
support software testers in systematic boundary value analysis with consistent
results. The method builds on a metric to quantify the level of boundariness of
test inputs: the program derivative. By coupling it with search algorithms we
find and rank pairs of inputs as good boundary candidates, i.e. inputs close
together but with outputs far apart. We implement our AutoBVA approach and
evaluate it on a curated dataset of example programs. Our results indicate that
even with a simple and generic program derivative variant in combination with
broad sampling over the input space, interesting boundary candidates can be
identified."
9036,"Finally, we discuss the present such challenges and offer new perspectives where further research is
                                                  required.","In recent publications, we observed a shortage of IoT anomaly detection
                                                  methodologies, for example, when dealing with the integration of systems with various sensors,
                                                  data and concept drifts, and data augmentation where there is a shortage of Ground Truth data.",Keywords Anomaly detection · Internet of Things · IoT · review · survey · applications.,2022-07-19 06:02:22+00:00,IoT Anomaly Detection Methods and Applications: A Survey,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ayan Chatterjee'), arxiv.Result.Author('Bestoun S. Ahmed')]","Ongoing research on anomaly detection for the Internet of Things (IoT) is a
rapidly expanding field. This growth necessitates an examination of application
trends and current gaps. The vast majority of those publications are in areas
such as network and infrastructure security, sensor monitoring, smart home, and
smart city applications and are extending into even more sectors. Recent
advancements in the field have increased the necessity to study the many IoT
anomaly detection applications. This paper begins with a summary of the
detection methods and applications, accompanied by a discussion of the
categorization of IoT anomaly detection algorithms. We then discuss the current
publications to identify distinct application domains, examining papers chosen
based on our search criteria. The survey considers 64 papers among recent
publications published between January 2019 and July 2021. In recent
publications, we observed a shortage of IoT anomaly detection methodologies,
for example, when dealing with the integration of systems with various sensors,
data and concept drifts, and data augmentation where there is a shortage of
Ground Truth data. Finally, we discuss the present such challenges and offer
new perspectives where further research is required."
9037,"It also lists
the most commonly used keywords and applications and identiﬁes application domains that require further research.","Anomaly
detection algorithms are classiﬁed into four categories, which are brieﬂy summarized in this review.","Lastly, based on our search criteria, this review discusses 64 novel papers published between January 2019 and July
2021, each with a distinct application domain.",2022-07-19 06:02:22+00:00,IoT Anomaly Detection Methods and Applications: A Survey,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ayan Chatterjee'), arxiv.Result.Author('Bestoun S. Ahmed')]","Ongoing research on anomaly detection for the Internet of Things (IoT) is a
rapidly expanding field. This growth necessitates an examination of application
trends and current gaps. The vast majority of those publications are in areas
such as network and infrastructure security, sensor monitoring, smart home, and
smart city applications and are extending into even more sectors. Recent
advancements in the field have increased the necessity to study the many IoT
anomaly detection applications. This paper begins with a summary of the
detection methods and applications, accompanied by a discussion of the
categorization of IoT anomaly detection algorithms. We then discuss the current
publications to identify distinct application domains, examining papers chosen
based on our search criteria. The survey considers 64 papers among recent
publications published between January 2019 and July 2021. In recent
publications, we observed a shortage of IoT anomaly detection methodologies,
for example, when dealing with the integration of systems with various sensors,
data and concept drifts, and data augmentation where there is a shortage of
Ground Truth data. Finally, we discuss the present such challenges and offer
new perspectives where further research is required."
9045,"automatic layout feature was sub-optimal and
   requires further research to improve the display       8.1.3 Empirical Evaluation
   arrangement of the diﬀerent visual nodes.",As previously noted the                   orchestrations.,"The
   approach should ideally optimize placement based       Replication of our study would help to consolidate and
   on the type of artifact rather than considering all    increase the conﬁdence level of these results.",2022-07-19 10:14:44+00:00,Visual Notations in Container Orchestrations: An Empirical Study with Docker Compose,cs.SE,['cs.SE'],"[arxiv.Result.Author('Bruno Piedade'), arxiv.Result.Author('João Pedro Dias'), arxiv.Result.Author('Filipe F. Correia')]","Context: Container orchestration tools supporting infrastructure-as-code
allow new forms of collaboration between developers and operatives. Still,
their text-based nature permits naive mistakes and is more difficult to read as
complexity increases. We can find few examples of low-code approaches for
defining the orchestration of containers, and there seems to be a lack of
empirical studies showing the benefits and limitations of such approaches. Goal
& method: We hypothesize that a complete visual notation for Docker-based
orchestrations could reduce the effort, the error rate, and the development
time. Therefore, we developed a tool featuring such a visual notation for
Docker Compose configurations, and we empirically evaluated it in a controlled
experiment with novice developers. Results: The results show a significant
reduction in development time and error-proneness when defining Docker Compose
files, supporting our hypothesis. The participants also thought the prototype
easier to use and useful, and wanted to use it in the future."
9137,"We
also summarize the publicly accessible datasets and open            • We analyze research trends and identify promising
source tools for fairness testing                                      research opportunities for fairness testing, with the goal
                                                                       of stimulating and facilitating further research.",for the research community working on fairness testing.,There has been previous work surveying the fairness              Figure 2 illustrates the structure of this paper.,2022-07-20 22:41:38+00:00,Fairness Testing: A Comprehensive Survey and Analysis of Trends,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhenpeng Chen'), arxiv.Result.Author('Jie M. Zhang'), arxiv.Result.Author('Max Hort'), arxiv.Result.Author('Federica Sarro'), arxiv.Result.Author('Mark Harman')]","Software systems are vulnerable to fairness bugs and frequently exhibit
unfair behaviors, making software fairness an increasingly important concern
for software engineers. Research has focused on helping software engineers to
detect fairness bugs automatically. This paper provides a comprehensive survey
of existing research on fairness testing. We collect 113 papers and organise
them based on the testing workflow (i.e., the testing activities) and the
testing components (i.e., where to find fairness bugs) for conducting fairness
testing. We also analyze the research focus, trends, promising directions, as
well as widely-adopted datasets and open source tools for fairness testing."
9138,[7] and Pessach and Shmueli [21]             of stimulating and facilitating further research.,Mehrabi et al.,surveyed fairness research on ML algorithms.,2022-07-20 22:41:38+00:00,Fairness Testing: A Comprehensive Survey and Analysis of Trends,cs.SE,['cs.SE'],"[arxiv.Result.Author('Zhenpeng Chen'), arxiv.Result.Author('Jie M. Zhang'), arxiv.Result.Author('Max Hort'), arxiv.Result.Author('Federica Sarro'), arxiv.Result.Author('Mark Harman')]","Software systems are vulnerable to fairness bugs and frequently exhibit
unfair behaviors, making software fairness an increasingly important concern
for software engineers. Research has focused on helping software engineers to
detect fairness bugs automatically. This paper provides a comprehensive survey
of existing research on fairness testing. We collect 122 papers and organise
them based on the testing workflow (i.e., the testing activities) and the
testing components (i.e., where to find fairness bugs) for conducting fairness
testing. We also analyze the research focus, trends, promising directions, as
well as widely-adopted datasets and open source tools for fairness testing."
9198,"3 METHODOLOGY

• We release all our source code publically available, hoping to         In this section, we first introduce the insight and assumption of
  facilitate further research in this direction 1.","affect the performance of Aries and give the recommendation
  parameters.","Aries, then conduct preliminary studies to empirically validate our
                                                                         assumptions, and finally present the details of our technique.",2022-07-22 08:39:10+00:00,Efficient Testing of Deep Neural Networks via Decision Boundary Analysis,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Qiang Hu'), arxiv.Result.Author('Yuejun Guo'), arxiv.Result.Author('Xiaofei Xie'), arxiv.Result.Author('Maxime Cordy'), arxiv.Result.Author('Lei Ma'), arxiv.Result.Author('Mike Papadakis'), arxiv.Result.Author('Yves Le Traon')]","Deep learning plays a more and more important role in our daily life due to
its competitive performance in multiple industrial application domains. As the
core of DL-enabled systems, deep neural networks automatically learn knowledge
from carefully collected and organized training data to gain the ability to
predict the label of unseen data. Similar to the traditional software systems
that need to be comprehensively tested, DNNs also need to be carefully
evaluated to make sure the quality of the trained model meets the demand. In
practice, the de facto standard to assess the quality of DNNs in industry is to
check their performance (accuracy) on a collected set of labeled test data.
However, preparing such labeled data is often not easy partly because of the
huge labeling effort, i.e., data labeling is labor-intensive, especially with
the massive new incoming unlabeled data every day. Recent studies show that
test selection for DNN is a promising direction that tackles this issue by
selecting minimal representative data to label and using these data to assess
the model. However, it still requires human effort and cannot be automatic. In
this paper, we propose a novel technique, named Aries, that can estimate the
performance of DNNs on new unlabeled data using only the information obtained
from the original test data. The key insight behind our technique is that the
model should have similar prediction accuracy on the data which have similar
distances to the decision boundary. We performed a large-scale evaluation of
our technique on 13 types of data transformation methods. The results
demonstrate the usefulness of our technique that the estimated accuracy by
Aries is only 0.03% -- 2.60% (on average 0.61%) off the true accuracy. Besides,
Aries also outperforms the state-of-the-art selection-labeling-based methods in
most (96 out of 128) cases."
9248,To answer the third                            further research on the usage of prompt tuning.,"To answer the second RQ, we evaluate prompt
tuning in data scarcity scenarios from two aspects, including low-                     (3) We discuss the implications of our findings and suggest
resource settings and cross-domain settings.","RQ, we comprehensively study the influence of different prompt
templates and verbalizers on model performance.",2022-07-24 07:29:17+00:00,No More Fine-Tuning? An Experimental Evaluation of Prompt Tuning in Code Intelligence,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Chaozheng Wang'), arxiv.Result.Author('Yuanhang Yang'), arxiv.Result.Author('Cuiyun Gao'), arxiv.Result.Author('Yun Peng'), arxiv.Result.Author('Hongyu Zhang'), arxiv.Result.Author('Michael R. Lyu')]","Pre-trained models have been shown effective in many code intelligence tasks.
These models are pre-trained on large-scale unlabeled corpus and then
fine-tuned in downstream tasks. However, as the inputs to pre-training and
downstream tasks are in different forms, it is hard to fully explore the
knowledge of pre-trained models. Besides, the performance of fine-tuning
strongly relies on the amount of downstream data, while in practice, the
scenarios with scarce data are common. Recent studies in the natural language
processing (NLP) field show that prompt tuning, a new paradigm for tuning,
alleviates the above issues and achieves promising results in various NLP
tasks. In prompt tuning, the prompts inserted during tuning provide
task-specific knowledge, which is especially beneficial for tasks with
relatively scarce data. In this paper, we empirically evaluate the usage and
effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on
popular pre-trained models CodeBERT and CodeT5 and experiment with three code
intelligence tasks including defect prediction, code summarization, and code
translation. Our experimental results show that prompt tuning consistently
outperforms fine-tuning in all three tasks. In addition, prompt tuning shows
great potential in low-resource scenarios, e.g., improving the BLEU scores of
fine-tuning by more than 26\% on average for code summarization. Our results
suggest that instead of fine-tuning, we could adapt prompt tuning for code
intelligence tasks to achieve better performance, especially when lacking
task-specific data."
9250,"Overall,
we provide the initial yet promising ML-based baselines for function-level SV assessment,
paving the way for further research in this direction.","Incorporating context of vulnerable statements
further increases the performance by up to 8.9% (0.64 MCC and 0.75 F1-Score).",60  Chapter 4.,2022-07-24 10:22:28+00:00,Towards an Improved Understanding of Software Vulnerability Assessment Using Data-Driven Approaches,cs.SE,"['cs.SE', 'cs.CR', 'cs.LG']",[arxiv.Result.Author('Triet H. M. Le')],"The thesis advances the field of software security by providing knowledge and
automation support for software vulnerability assessment using data-driven
approaches. Software vulnerability assessment provides important and
multifaceted information to prevent and mitigate dangerous cyber-attacks in the
wild. The key contributions include a systematisation of knowledge, along with
a suite of novel data-driven techniques and practical recommendations for
researchers and practitioners in the area. The thesis results help improve the
understanding and inform the practice of assessing ever-increasing
vulnerabilities in real-world software systems. This in turn enables more
thorough and timely fixing prioritisation and planning of these critical
security issues."
9251,"Overall,
we provide the initial yet promising ML-based baselines for function-level SV assessment,
paving the way for further research in this direction.","Incorporating context of vulnerable statements
further increases the performance by up to 8.9% (0.64 MCC and 0.75 F1-Score).",60  Chapter 4.,2022-07-24 10:22:28+00:00,Towards an Improved Understanding of Software Vulnerability Assessment Using Data-Driven Approaches,cs.SE,"['cs.SE', 'cs.CR', 'cs.LG']",[arxiv.Result.Author('Triet H. M. Le')],"The thesis advances the field of software security by providing knowledge and
automation support for software vulnerability assessment using data-driven
approaches. Software vulnerability assessment provides important and
multifaceted information to prevent and mitigate dangerous cyber-attacks in the
wild. The key contributions include a systematisation of knowledge, along with
a suite of novel data-driven techniques and practical recommendations for
researchers and practitioners in the area. The thesis results help improve the
understanding and inform the practice of assessing ever-increasing
vulnerabilities in real-world software systems. This in turn enables more
thorough and timely fixing prioritisation and planning of these critical
security issues."
9273,"However, whether this is more eﬀective
and/or eﬃcient than directly only debugging a single implementation is not
clear without further study.","Due to these issues, we believe that diﬀerential testing may be used by
developers who already know the details of their implementation, to compare
their work with other implementations.","Using diﬀerential testing as pseudo-oracle (Davis
and Weyuker, 1981) to automatically drive test campaigns between tools seems
to have only a limited potential.",2022-07-25 08:27:01+00:00,Differential testing for machine learning: an analysis for classification algorithms beyond deep learning,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Steffen Herbold'), arxiv.Result.Author('Steffen Tunkel')]","Context: Differential testing is a useful approach that uses different
implementations of the same algorithms and compares the results for software
testing. In recent years, this approach was successfully used for test
campaigns of deep learning frameworks.
  Objective: There is little knowledge on the application of differential
testing beyond deep learning. Within this article, we want to close this gap
for classification algorithms.
  Method: We conduct a case study using Scikit-learn, Weka, Spark MLlib, and
Caret in which we identify the potential of differential testing by considering
which algorithms are available in multiple frameworks, the feasibility by
identifying pairs of algorithms that should exhibit the same behavior, and the
effectiveness by executing tests for the identified pairs and analyzing the
deviations.
  Results: While we found a large potential for popular algorithms, the
feasibility seems limited because often it is not possible to determine
configurations that are the same in other frameworks. The execution of the
feasible tests revealed that there is a large amount of deviations for the
scores and classes. Only a lenient approach based on statistical significance
of classes does not lead to a huge amount of test failures.
  Conclusions: The potential of differential testing beyond deep learning seems
limited for research into the quality of machine learning libraries.
Practitioners may still use the approach if they have deep knowledge about
implementations, especially if a coarse oracle that only considers significant
differences of classes is sufficient."
9333,"However, the study has made further research implications to explore the evolution of developers’ expertise
alignment with the expertise of the newcomers joining the project.","The authors have called this a “joining
script”, which implies how much expertise newcomers require before being given access to make contributions to the
projects.","2For simplicity, we will refer to developers that become less engaged with the project as leaving members through the rest of the paper.",2022-07-26 12:35:42+00:00,Balanced Knowledge Distribution among Software Development Teams -- Observations from Open-Source and Closed-Source Software Development,cs.SE,"['cs.SE', 'cs.IR']","[arxiv.Result.Author('Saad Shafiq'), arxiv.Result.Author('Christoph Mayr-Dorn'), arxiv.Result.Author('Atif Mashkoor'), arxiv.Result.Author('Alexander Egyed')]","In software development teams, developer turnover is among the primary
reasons for project failures as it leads to a great void of knowledge and
strain for the newcomers. Unfortunately, no established methods exist to
measure how knowledge is distributed among development teams. Knowing how this
knowledge evolves and is owned by key developers in a project helps managers
reduce risks caused by turnover. To this end, this paper introduces a novel,
realistic representation of domain knowledge distribution: the ConceptRealm. To
construct the ConceptRealm, we employ a latent Dirichlet allocation model to
represent textual features obtained from 300k issues and 1.3M comments from 518
open-source projects. We analyze whether the newly emerged issues and
developers share similar concepts or how aligned the developers' concepts are
with the team over time. We also investigate the impact of leaving members on
the frequency of concepts. Finally, we evaluate the soundness of our approach
to closed-source software, thus allowing the validation of the results from a
practical standpoint. We find out that the ConceptRealm can represent the
high-level domain knowledge within a team and can be utilized to predict the
alignment of developers with issues. We also observe that projects exhibit many
keepers independent of project maturity and that abruptly leaving keepers harm
the team's concept familiarity."
9489,"[10] address how closed development organizations could beneﬁt
                                        from open source practices as an area where further research is needed.",Mistrik et al.,"Though
                                        studies conducted so far are quite limited, several success stories [20], [3], [1], [8],
                                        [12] can be found of large corporations adopting open source development.",2022-07-29 18:37:10+00:00,On infrastructure for facilitation of inner source in small development teams,cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Maria Krantz'), arxiv.Result.Author('Martin Höst')]","The phenomenon of adopting open source software development practices in a
corporate environment is known by many names, one being inner source. The
objective of this study is to investigate how an organization consisting of
small development teams can benefit from adopting inner source and assess the
level of applicability. The research has been conducted as a case study at a
software development company. Data collection was carried out through
interviews and a series of focus group meetings, and then analyzed by mapping
it to an available framework. The analysis shows that the organization
possesses potential, and also identified a number of challenges and benefits of
special importance to the case company. To address these challenges, the case
study synthesized the organizational and infrastructural needs of the
organization in a requirements specification describing a technical
infrastructure, also known as a software forge, with an adapted organizational
context and work process."
9490,"How this is best done is, however,
beyond the scope of this paper, and an interesting topic of further research.","As highlighted by
CC10, creating a new community may be related to a higher cost, and comes
with a number of extra challenges [33, 75].","6 Threats to Validity

The identiﬁed contribution objectives and complexities are the result of a
multiple-case study of three case organization over the lapse of two research
cycles.",2022-07-29 19:28:26+00:00,"What to share, when, and where: balancing the objectives and complexities of open source software contributions",cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Björn Regnell')]","Context: Software-intensive organizations' rationale for sharing Open Source
Software (OSS) may be driven by both idealistic, strategic and commercial
objectives, and include both monetary as well as non-monetary benefits. To gain
the potential benefits, an organization may need to consider what they share
and how, while taking into account risks, costs and other complexities.
  Objective: This study aims to empirically investigate objectives and
complexities organizations need to consider and balance between when deciding
on what software to share as OSS, when to share it, and whether to create a new
or contribute to an existing community.
  Method: A multiple-case study of three case organizations was conducted in
two research cycles, with data gathered from interviews with 20 practitioners
from these organizations. The data was analyzed qualitatively in an inductive
and iterative coding process.
  Results: 12 contribution objectives and 15 contribution complexities were
found. Objectives include opportunities for improving reputation, managing
suppliers, managing partners and competitors, and exploiting externally
available knowledge and resources. Complexities include risk of loosing
control, risk of giving away competitive advantage, risk of creating negative
exposure, costs of contributing, and the possibility and need to contribute to
an existing or new community.
  Conclusions: Cross-case analysis and interview validation show that the
identified objectives and complexities offer organizations a possibility to
reflect on and adapt their contribution strategies based on their specific
contexts and business goals."
9493,"However, this way of measuring        pects that should be further researched and integrated with
out-degree centrality does not provide information about the      our proposed process in future research.","neighbors and is good at communicating its views relative         We do acknowledge the topic as complementary quality as-
others in the network [25].","total number of connections of a stakeholder, which may bet-
ter show the number of collaborations and opportunities to            A further threat concerns the determination of orga-
spread one’s opinions [43].",2022-07-29 20:32:38+00:00,A method for analyzing stakeholders' influence on an open source software ecosystem's requirements engineering process,cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Björn Regnell'), arxiv.Result.Author('Daniela Damian')]","For a firm in an open source software (OSS) ecosystem, the requirements
engineering (RE) process is rather multifaceted. Apart from its typical RE
process, there is a competing process, external to the firm and inherent to the
firm's ecosystem. When trying to impose an agenda in competition with other
firms, and aiming to align internal product planning with the ecosystem's RE
process, firms need to consider who and how influential the other stakeholders
are, and what their agendas are. The aim of the presented research is to help
firms identify and analyze stakeholders in OSS ecosystems, in terms of their
influence and interactions, to create awareness of their agendas, their
collaborators, and how they invest their resources. To arrive at a solution
artifact, we applied a design science research approach where we base artifact
design on the literature and earlier work. A stakeholder influence analysis
(SIA) method is proposed and demonstrated in terms of applicability and utility
through a case study on the Apache Hadoop OSS ecosystem. SIA uses social
network constructs to measure the stakeholders' influence and interactions and
considers the special characteristics of OSS RE to help firms structure their
stakeholder analysis processes in relation to an OSS ecosystem. SIA adds a
strategic aspect to the stakeholder analysis process by addressing the concepts
of influence and interactions, which are important to consider while acting in
collaborative and meritocratic RE cultures of OSS ecosystems."
9506,"We hypothesise that poor
descriptions might inﬂuence lower results for less popular repositories, but
further research is needed to corroborate this.","README ﬁles are also the starting point for developers when
encountering a new repository (Prana et al., 2019).","RQ2 results show how our classiﬁer should be used rather than manually
annotating the repositories.",2022-07-30 16:27:16+00:00,Automatically Categorising GitHub Repositories by Application Domain,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Francisco Zanartu'), arxiv.Result.Author('Christoph Treude'), arxiv.Result.Author('Bruno Cartaxo'), arxiv.Result.Author('Hudson Silva Borges'), arxiv.Result.Author('Pedro Moura'), arxiv.Result.Author('Markus Wagner'), arxiv.Result.Author('Gustavo Pinto')]","GitHub is the largest host of open source software on the Internet. This
large, freely accessible database has attracted the attention of practitioners
and researchers alike. But as GitHub's growth continues, it is becoming
increasingly hard to navigate the plethora of repositories which span a wide
range of domains. Past work has shown that taking the application domain into
account is crucial for tasks such as predicting the popularity of a repository
and reasoning about project quality. In this work, we build on a previously
annotated dataset of 5,000 GitHub repositories to design an automated
classifier for categorising repositories by their application domain. The
classifier uses state-of-the-art natural language processing techniques and
machine learning to learn from multiple data sources and catalogue repositories
according to five application domains. We contribute with (1) an automated
classifier that can assign popular repositories to each application domain with
at least 70% precision, (2) an investigation of the approach's performance on
less popular repositories, and (3) a practical application of this approach to
answer how the adoption of software engineering practices differs across
application domains. Our work aims to help the GitHub community identify
repositories of interest and opens promising avenues for future work
investigating differences between repositories from different application
domains."
9511,"As this study is limited to exploring the collaboration in four instances of
OGD ecosystems, further research is required to validate both the model and
recommendations, create a deeper understanding, and improve the external va-
lidity [37].","For each aspect, we provide recommendations that, along with
How to Enable Collaboration in Open Government Data Ecosystems  27

the model, may provide guidance for public entities acting as platform providers
can enable sharing and collaboration on both OGD as well as any boundary
resources in the form of OSS and standards.","Readers should consider the context of the platform providers and
their ecosystems as reported and adopt an analytical generalization to cases with
similar characteristics [37].",2022-07-30 20:38:04+00:00,How to Enable Collaboration in Open Government Data Ecosystems: A Public Platform Provider's Perspective,cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Per Runeson')]","Objective: Our objective is to explore how public entities in the role of
platform providers can address this issue by enabling collaboration within
their OGD ecosystems, both in terms of the OGD published on the underpinning
platform, as well as any related Open Source Software (OSS) and standards.
Method: We conducted an exploratory multiple-case study of four OGD ecosystems
with diverse characteristics. Data was collected through semi-structured
interviews, and in one of the cases through a prolonged engagement. The data
was then coded using a set of \textit{apriori} codes. Results: The study
descriptively presents each case based on the coding, along with synthesis in
the form of a conceptual model that highlights different attributes of OGD
ecosystems. For example, we observe how collaboration can be enabled through
different types of ownership of the platform provider, how the ecosystem's
scope can vary, what roles the platform provider may undertake, how to enable
open collaboration, and how to collaborate in terms of data sharing, OSS
development, and standards. For each aspect, we provide recommendations based
on the explored cases that, together with the model, may help public entities
in designing and orchestrating new or existing OGD ecosystems. Conclusions: We
conclude that enabling and facilitating collaboration in an OGD ecosystem is a
complex exercise, yet believe that it offers new ways for public entities in
how they can leverage the power of open innovation to address their goals and
directives."
9516,"[24] described a preliminary analysis of the Eclipse marketplace with a view to providing
a glance on open marketplaces as well as initiating further research.","Krüger
et al.","They proposed to mine
marketplace data to address questions such as, who contributes to successful plug-ins, in order
to identify leading developers and communities, leading to collaborations and new research
directions.",2022-07-31 01:48:19+00:00,GitHub Marketplace for Practitioners and Researchers to Date: A Systematic Analysis of the Knowledge Mobilization Gap in Open Source Software Automation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sk Golam Saroar'), arxiv.Result.Author('Waseefa Ahmed'), arxiv.Result.Author('Maleknaz Nayebi')]","Marketplaces for distributing software products and services have been
getting increasing popularity. GitHub, which is most known for its version
control functionality through Git, launched its own marketplace in 2017. GitHub
Marketplace hosts third party apps and actions to automate workflows in
software teams. Currently, this marketplace hosts 440 Apps and 7,878 Actions
across 32 different categories. Overall, 419 Third party developers released
their apps on this platform which 111 distinct customers adopted. The
popularity and accessibility of GitHub projects have made this platform and the
projects hosted on it one of the most frequent subjects for experimentation in
the software engineering research. A simple Google Scholar search shows that
24,100 Research papers have discussed GitHub within the Software Engineering
field since 2017, but none have looked into the marketplace. The GitHub
Marketplace provides a unique source of information on the tools used by the
practitioners in the Open Source Software (OSS) ecosystem for automating their
project's workflow. In this study, we (i) mine and provide a descriptive
overview of the GitHub Marketplace, (ii) perform a systematic mapping of
research studies in automation for open source software, and (iii) compare the
state of the art with the state of the practice on the automation tools. We
conclude the paper by discussing the potential of GitHub Marketplace for
knowledge mobilization and collaboration within the field. This is the first
study on the GitHub Marketplace in the field."
9553,"As such, further research is warranted since advances in locating the correct web elements impact
test script robustness and, thereby, maintenance costs.","the state of the art solutions, with our further studies we
discovered that, in certain cases, the approach still fails to find a significant number of web elements.","3 THE SIMILO APPROACH

The similarity-based web element localization (Similo) approach attempts to increase the robustness
even further than the LML approach.",2022-08-01 08:15:00+00:00,Similarity-based web element localization for robust test automation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Michel Nass'), arxiv.Result.Author('Emil Alégroth'), arxiv.Result.Author('Robert Feldt'), arxiv.Result.Author('Maurizio Leotta'), arxiv.Result.Author('Filippo Ricca')]","Non-robust (fragile) test execution is a commonly reported challenge in
GUI-based test automation, despite much research and several proposed
solutions. A test script needs to be resilient to (minor) changes in the tested
application but, at the same time, fail when detecting potential issues that
require investigation. Test script fragility is a multi-faceted problem, but
one crucial challenge is reliably identifying and locating the correct target
web elements when the website evolves between releases or otherwise fails and
reports an issue. This paper proposes and evaluates a novel approach called
similarity-based web element localization (Similo), which leverages information
from multiple web element locator parameters to identify a target element using
a weighted similarity score. The experimental study compares Similo to a
baseline approach for web element localization. To get an extensive empirical
basis, we target 40 of the most popular websites on the Internet in our
evaluation. Robustness is considered by counting the number of web elements
found in a recent website version compared to how many of these existed in an
older version. Results of the experiment show that Similo outperforms the
baseline representing the current state-of-the-art; it failed to locate the
correct target web element in 72 out of 598 considered cases compared to 146
failed cases for the baseline approach. This study presents evidence that
quantifying the similarity between multiple attributes of web elements when
trying to locate them, as in our proposed Similo approach, is beneficial. With
acceptable efficiency, Similo gives significantly higher effectiveness (i.e.,
robustness) than the baseline web element localization approach."
9620,"They further study the symptoms, causes
operators, we constructed another baseline named as MEMO_o.",To conduct an ablation study of the proposed mutation                                               of the bugs in TensorFlow.,In-                                                 and repair patterns of TensorFlow bugs [20].,2022-08-02 14:53:02+00:00,MEMO: Coverage-guided Model Generation For Deep Learning Library Testing,cs.SE,"['cs.SE', 'cs.AI', 'D.2.5; I.2.5']","[arxiv.Result.Author('Meiziniu Li'), arxiv.Result.Author('Jialun Cao'), arxiv.Result.Author('Yongqiang Tian'), arxiv.Result.Author('Tsz On Li'), arxiv.Result.Author('Ming Wen'), arxiv.Result.Author('Shing-Chi Cheung')]","Recent deep learning (DL) applications are mostly built on top of DL
libraries. The quality assurance of these libraries is critical to the
dependable deployment of DL applications. A few techniques have thereby been
proposed to test DL libraries by generating DL models as test inputs. Then
these techniques feed those DL models to DL libraries for making inferences, in
order to exercise DL libraries modules related to a DL model's execution.
However, the test effectiveness of these techniques is constrained by the
diversity of generated DL models. Our investigation finds that these techniques
can cover at most 11.7% of layer pairs (i.e., call sequence between two layer
APIs) and 55.8% of layer parameters (e.g., ""padding"" in Conv2D). As a result,
we find that many bugs arising from specific layer pairs and parameters can be
missed by existing techniques.
  In view of the limitations of existing DL library testing techniques, we
propose MEMO to efficiently generate diverse DL models by exploring layer
types, layer pairs, and layer parameters. MEMO: (1) designs an initial model
reduction technique to boost test efficiency without compromising model
diversity; and (2) designs a set of mutation operators for a customized Markov
Chain Monte Carlo (MCMC) algorithm to explore new layer types, layer pairs, and
layer parameters. We evaluate MEMO on seven popular DL libraries, including
four for model execution (TensorFlow, PyTorch and MXNet, and ONNX) and three
for model conversions (Keras-MXNet, TF2ONNX, ONNX2PyTorch). The evaluation
result shows that MEMO outperforms recent works by covering 10.3% more layer
pairs, 15.3% more layer parameters, and 2.3% library branches. Moreover, MEMO
detects 29 new bugs in the latest version of DL libraries, with 17 of them
confirmed by DL library developers, and 5 of those confirmed bugs have been
fixed."
9637,"As this
study is limited to exploring the collaboration in two instances of OGD ecosys-
tems, further research is required to validate these hypotheses, create a deeper
understanding, and improve the external validity [22].","We hypothesize that this type of collaboration can extend the potential ben-
eﬁts for the platform provider and the ecosystem, including reduced cost and
accelerated innovation, but also increased adoption and sharing of data.","Readers should consider
the context of the platform providers and their ecosystems as reported and adopt
an analytical generalization to cases with similar characteristics [22].",2022-07-31 20:40:07+00:00,Collaboration in Open Government Data Ecosystems: Open Cross-sector Sharing and Co-development of Data and Software,cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Per Runeson')]","Background: Open innovation highlights the potential benefits of external
collaboration and knowledge-sharing, often exemplified through Open Source
Software (OSS). The public sector has thus far mainly focused on the sharing of
Open Government Data (OGD), often with a supply-driven approach with limited
feedback-loops. We hypothesize that public sector organizations can extend the
open innovation benefits by also creating platforms, where OGD, related OSS,
and open standards are collaboratively developed and shared. Objective: The
objective of this study is to explore how public sector organizations in the
role of platform providers facilitate such collaboration in the form of OGD
ecosystems and how the ecosystem's governance may be structured to support the
collaboration. Method: We conduct an exploratory multiple-case study of two
such ecosystems, focused on OGD related to the Swedish labor market and public
transport sector, respectively. Data is gathered through interviews, document
studies, and prolonged engagement at one of the platform providers. Results:
The study presents governance structure and collaboration practices of the two
ecosystems and discusses how these contribute to the platform providers' goals.
The case studies highlight the need for platform providers to take an active
and multi-functional role in enabling the sharing of data and software from and
between the members of the ecosystem. Conclusions: We conclude that OGD
ecosystems offer public sector organizations a possibility to catalyze the
potential innovation output of OGD, but that it requires investment and
adoption of an open and collaborative mindset."
9662,"However, orchestrating the deployment models requires further research.","Moreover, it takes the ﬁrst steps toward enabling the conversion of ML workﬂow
model diagrams to corresponding deployment models for serverless deployment via TOSCA.","The capabilities
and advantages of using BPMN4sML are illustrated by real-life examples.",2022-08-02 10:36:00+00:00,BPMN4sML: A BPMN Extension for Serverless Machine Learning. Technology Independent and Interoperable Modeling of Machine Learning Workflows and their Serverless Deployment Orchestration,cs.SE,"['cs.SE', 'cs.LG']",[arxiv.Result.Author('Laurens Martin Tetzlaff')],"Machine learning (ML) continues to permeate all layers of academia, industry
and society. Despite its successes, mental frameworks to capture and represent
machine learning workflows in a consistent and coherent manner are lacking. For
instance, the de facto process modeling standard, Business Process Model and
Notation (BPMN), managed by the Object Management Group, is widely accepted and
applied. However, it is short of specific support to represent machine learning
workflows. Further, the number of heterogeneous tools for deployment of machine
learning solutions can easily overwhelm practitioners. Research is needed to
align the process from modeling to deploying ML workflows.
  We analyze requirements for standard based conceptual modeling for machine
learning workflows and their serverless deployment. Confronting the
shortcomings with respect to consistent and coherent modeling of ML workflows
in a technology independent and interoperable manner, we extend BPMN's
Meta-Object Facility (MOF) metamodel and the corresponding notation and
introduce BPMN4sML (BPMN for serverless machine learning). Our extension
BPMN4sML follows the same outline referenced by the Object Management Group
(OMG) for BPMN. We further address the heterogeneity in deployment by proposing
a conceptual mapping to convert BPMN4sML models to corresponding deployment
models using TOSCA.
  BPMN4sML allows technology-independent and interoperable modeling of machine
learning workflows of various granularity and complexity across the entire
machine learning lifecycle. It aids in arriving at a shared and standardized
language to communicate ML solutions. Moreover, it takes the first steps toward
enabling conversion of ML workflow model diagrams to corresponding deployment
models for serverless deployment via TOSCA."
9716,"The methodology
used in this study oﬀers an option to such an analysis but needs further research.","Knowing
about their existing collaborations, contributions, and interests in speciﬁc features oﬀer
valuable information about the competitors’ strategies and tactics [24].","Knowledge about stakeholder inﬂuence and collaboration patterns may provide im-
portant input to stakeholders’ strategies.",2022-07-31 19:59:58+00:00,How Firms Adapt and Interact in Open Source Ecosystems: Analyzing Stakeholder Influence and Collaboration Patterns,cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Patrick Rempel'), arxiv.Result.Author('Björn Regnell'), arxiv.Result.Author('Patrick Mäder')]","[Context and motivation] Ecosystems developed as Open Source Software (OSS)
are considered to be highly innovative and reactive to new market trends due to
their openness and wide-ranging contributor base. Participation in OSS often
implies opening up of the software development process and exposure towards new
stakeholders. [Question/Problem] Firms considering to engage in such an
environment should carefully consider potential opportunities and challenges
upfront. The openness may lead to higher innovation potential but also to
frictional losses for engaged firms. Further, as an ecosystem progresses, power
structures and influence on feature selection may fluctuate accordingly.
[Principal ideas/results] We analyze the Apache Hadoop ecosystem in a
quantitative longitudinal case study to investigate changing stakeholder
influence and collaboration patterns. Further, we investigate how its
innovation and time-to-market evolve at the same time. [Contribution] Findings
show collaborations between and influence shifting among rivaling and
non-competing firms. Network analysis proves valuable on how an awareness of
past, present and emerging stakeholders, in regards to power structure and
collaborations may be created. Furthermore, the ecosystem's innovation and
time-to-market show strong variations among the release history. Indications
were also found that these characteristics are influenced by the way how
stakeholders collaborate with each other."
9744,"We use paired        2 BACKGROUND
bootstrap resampling for the obtained scores to further study the
results.","[22], yielding a human “ground truth”.","For the CoNaLa dataset for any pair of evaluated models,        2.1 Code Generation
the “ground truth” grades can be used to distinguish which model is
better with > 95% confidence.",2022-08-05 13:00:16+00:00,Out of the BLEU: how should we assess quality of the Code Generation models?,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Mikhail Evtikhiev'), arxiv.Result.Author('Egor Bogomolov'), arxiv.Result.Author('Yaroslav Sokolov'), arxiv.Result.Author('Timofey Bryksin')]","In recent years, researchers have created and introduced a significant number
of various code generation models. As human evaluation of every new model
version is unfeasible, the community adopted automatic evaluation metrics such
as BLEU to approximate the results of human judgement. These metrics originate
from the machine translation domain and it is unclear whether they are
applicable for the code generation tasks and how well do they agree with the
human evaluation on this task. There also are two metrics, CodeBLEU and RUBY,
that were developed to estimate the similarity of code and take into account
the code properties. However, for these metrics there are hardly any studies on
their agreement with the human evaluation. Despite all that, minimal
differences in the metric scores are used to claim superiority of some code
generation models over the others.
  In this paper, we present a study on applicability of six metrics -- BLEU,
ROUGE-L, METEOR, ChrF, CodeBLEU, RUBY -- for evaluation of the code generation
models. We conduct a study on two different code generation datasets and use
human annotators to assess the quality of all models run on these datasets. The
results indicate that for the CoNaLa dataset of Python one-liners none of the
metrics can correctly emulate human judgement on which model is better with
$>95\%$ certainty if the difference in model scores is less than 5 points. For
the HearthStone dataset, which consists of classes of particular structure, the
difference in model scores of at least 2 points is enough to claim the
superiority of one model over the other. Using our findings, we derive several
recommendations on using metrics to estimate the model performance on the code
generation task."
9761,"Hence, further research and design cycles are needed to validate
the CSF and to improve its level of completeness.","However, as
presented in Section 2, there are numerous variations in the characteristics of
OSS communities, e.g., in regards to governance structure, demographics, and
RE process.","25
8.",2022-07-29 20:21:39+00:00,A Community Strategy Framework -- How to obtain Influence on Requirements in Meritocratic Open Source Software Communities?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Björn Regnell'), arxiv.Result.Author('Daniela Damian')]","Context: In the Requirements Engineering (RE) process of an Open Source
Software (OSS) community, an involved firm is a stakeholder among many.
Conflicting agendas may create miss-alignment with the firm's internal
requirements strategy. In communities with meritocratic governance or with
aspects thereof, a firm has the opportunity to affect the RE process in line
with their own agenda by gaining influence through active and symbiotic
engagements.
  Objective: The focus of this study has been to identify what aspects that
firms should consider when they assess their need of influencing the RE process
in an OSS community, as well as what engagement practices that should be
considered in order to gain this influence.
  Method: Using a design science approach, 21 interviews with 18 industry
professionals from 12 different software-intensive firms were conducted to
explore, design and validate an artifact for the problem context.
  Results: A Community Strategy Framework (CSF) is presented to help firms
create community strategies that describe if and why they need influence on the
RE process in a specific (meritocratic) OSS community, and how the firm could
gain it. The framework consists of aspects and engagement practices. The
aspects help determine how important an OSS project and its community is from
business and technical perspectives. A community perspective is used when
considering the feasibility and potential in gaining influence. The engagement
practices are intended as a tool-box for how a firm can engage with a community
in order to build influence needed.
  Conclusion: It is concluded from interview-based validation that the proposed
CSF may provide support for firms in creating and tailoring community
strategies and help them to focus resources on communities that matter and gain
the influence needed on their respective RE processes."
9762,"As this study uses a qualitative survey approach with a limited sampling
of interviewees, further research is needed to validate the CSF through case
studies and additional empirical work, both qualitative and quantitative.","The engagement practices address RQ2 and should
be seen as a tool-box of ways in how a ﬁrm can engage with a community to
build inﬂuence needed in the community.","Along
with such research, more theory-grounding work should be performed to fur-
ther formalize the concept of inﬂuence in OSS communities, and how it can be
gained, as exempliﬁed by the CSF.",2022-07-29 20:21:39+00:00,A Community Strategy Framework -- How to obtain Influence on Requirements in Meritocratic Open Source Software Communities?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Johan Linåker'), arxiv.Result.Author('Björn Regnell'), arxiv.Result.Author('Daniela Damian')]","Context: In the Requirements Engineering (RE) process of an Open Source
Software (OSS) community, an involved firm is a stakeholder among many.
Conflicting agendas may create miss-alignment with the firm's internal
requirements strategy. In communities with meritocratic governance or with
aspects thereof, a firm has the opportunity to affect the RE process in line
with their own agenda by gaining influence through active and symbiotic
engagements.
  Objective: The focus of this study has been to identify what aspects that
firms should consider when they assess their need of influencing the RE process
in an OSS community, as well as what engagement practices that should be
considered in order to gain this influence.
  Method: Using a design science approach, 21 interviews with 18 industry
professionals from 12 different software-intensive firms were conducted to
explore, design and validate an artifact for the problem context.
  Results: A Community Strategy Framework (CSF) is presented to help firms
create community strategies that describe if and why they need influence on the
RE process in a specific (meritocratic) OSS community, and how the firm could
gain it. The framework consists of aspects and engagement practices. The
aspects help determine how important an OSS project and its community is from
business and technical perspectives. A community perspective is used when
considering the feasibility and potential in gaining influence. The engagement
practices are intended as a tool-box for how a firm can engage with a community
in order to build influence needed.
  Conclusion: It is concluded from interview-based validation that the proposed
CSF may provide support for firms in creating and tailoring community
strategies and help them to focus resources on communities that matter and gain
the influence needed on their respective RE processes."
9830,"Although the paper focuses
space, necessitating further research into a deep learning model for   on vacuum pumping in the steel production industry, robustness
vacuum pumping predictions.","However, all       presents our best practice to date in addressing quality assurance
six models cover less than 0.11% of the whole augmented sample         challenges in an MLOps workflow.","testing methodology and general quality assurance workflow can
                                                                       be applied across multiple disciplines.",2022-08-08 11:19:41+00:00,Testing of Machine Learning Models with Limited Samples: An Industrial Vacuum Pumping Application,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ayan Chatterjee'), arxiv.Result.Author('Bestoun S. Ahmed'), arxiv.Result.Author('Erik Hallin'), arxiv.Result.Author('Anton Engman')]","There is often a scarcity of training data for machine learning (ML)
classification and regression models in industrial production, especially for
time-consuming or sparsely run manufacturing processes. A majority of the
limited ground-truth data is used for training, while a handful of samples are
left for testing. Here, the number of test samples is inadequate to properly
evaluate the robustness of the ML models under test for classification and
regression. Furthermore, the output of these ML models may be inaccurate or
even fail if the input data differ from the expected. This is the case for ML
models used in the Electroslag Remelting (ESR) process in the refined steel
industry to predict the pressure in a vacuum chamber. A vacuum pumping event
that occurs once a workday generates a few hundred samples in a year of pumping
for training and testing. In the absence of adequate training and test samples,
this paper first presents a method to generate a fresh set of augmented samples
based on vacuum pumping principles. Based on the generated augmented samples,
three test scenarios and one test oracle are presented to assess the robustness
of an ML model used for production on an industrial scale. Experiments are
conducted with real industrial production data obtained from Uddeholms AB steel
company. The evaluations indicate that Ensemble and Neural Network are the most
robust when trained on augmented data using the proposed testing strategy. The
evaluation also demonstrates the proposed method's effectiveness in checking
and improving ML algorithms' robustness in such situations. The work improves
software testing's state-of-the-art robustness testing in similar settings.
Finally, the paper presents an MLOps implementation of the proposed approach
for real-time ML model prediction and action on the edge node and automated
continuous delivery of ML software from the cloud."
10066,"The application of reviewers’ comments and further research of the method lead to the
repetition of the above described process.","At this point, the publication may enter the peer-review process that results in reviewer com-
ments.","If the new feature is researched and works well, it is in-
tegrated into the research idea branch, a new git tag is generated (see section 3.1), the source code
archive and the secondary data are updated as new versions on the data repository, the pre-print is
modiﬁed accordingly with new cross-links, a new version of the preprint is submitted to e.g.",2022-08-15 22:33:07+00:00,A Research Software Engineering Workflow for Computational Science and Engineering,cs.SE,['cs.SE'],"[arxiv.Result.Author('Tomislav Maric'), arxiv.Result.Author('Dennis Gläser'), arxiv.Result.Author('Jan-Patrick Lehr'), arxiv.Result.Author('Ioannis Papagiannidis'), arxiv.Result.Author('Benjamin Lambie'), arxiv.Result.Author('Christian Bischof'), arxiv.Result.Author('Dieter Bothe')]","University research groups in Computational Science and Engineering (CSE)
generally lack dedicated funding and personnel for Research Software
Engineering (RSE), which, combined with the pressure to maximize the number of
scientific publications, shifts the focus away from sustainable research
software development and reproducible results. The neglect of RSE in CSE at
University research groups negatively impacts the scientific output: research
data - including research software - related to a CSE publication cannot be
found, reproduced, or re-used, different ideas are not combined easily into new
ideas, and published methods must very often be re-implemented to be
investigated further. This slows down CSE research significantly, resulting in
considerable losses in time and, consequentially, public funding.
  We propose a RSE workflow for Computational Science and Engineering (CSE)
that addresses these challenges, that improves the quality of research output
in CSE. Our workflow applies established software engineering practices adapted
for CSE: software testing, result visualization, and periodical cross-linking
of software with reports/publications and data, timed by milestones in the
scientific publication process. The workflow introduces minimal work overhead,
crucial for university research groups, and delivers modular and tested
software linked to publications whose results can easily be reproduced. We
define research software quality from a perspective of a pragmatic researcher:
the ability to quickly find the publication, data, and software related to a
published research idea, quickly reproduce results, understand or re-use a CSE
method, and finally extend the method with new research ideas."
10078,"In           stand how the performance of machine learning compares to
this way, researchers will be provided with a unique database          existing approaches and, perhaps, be later used by researchers
containing various test code-related issues, which would be            to combine it with novel, more precise information ﬂows or
beneﬁcial to stimulate further research on test code quality.","These observations might be used to under-
IDoFT with additional information related to test smells.",dynamic sources of information.,2022-08-16 07:33:15+00:00,Machine Learning-Based Test Smell Detection,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Valeria Pontillo'), arxiv.Result.Author(""Dario Amoroso d'Aragona""), arxiv.Result.Author('Fabiano Pecorelli'), arxiv.Result.Author('Dario Di Nucci'), arxiv.Result.Author('Filomena Ferrucci'), arxiv.Result.Author('Fabio Palomba')]","Context: Test smells are symptoms of sub-optimal design choices adopted when
developing test cases. Previous studies have proved their harmfulness for test
code maintainability and effectiveness. Therefore, researchers have been
proposing automated, heuristic-based techniques to detect them. However, the
performance of such detectors is still limited and dependent on thresholds to
be tuned.
  Objective: We propose the design and experimentation of a novel test smell
detection approach based on machine learning to detect four test smells.
  Method: We plan to develop the largest dataset of manually-validated test
smells. This dataset will be leveraged to train six machine learners and assess
their capabilities in within- and cross-project scenarios. Finally, we plan to
compare our approach with state-of-the-art heuristic-based techniques."
10222,"Our
duced a single embedding for the repository, and add a fully-      Github crawler and curated dataset of repositories are also
connected layer completed by a sigmoid activation function to      shared to the community to facilitate further research, as well
enable multi-label tagging.","By using DistilBERT on tokenized words, they pro-       representation, source code, docstrings, and dependencies.",We differ from these approaches in     as the code for our proposed models.,2022-08-19 18:13:27+00:00,Topical: Learning Repository Embeddings from Source Code using Attention,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Agathe Lherondelle'), arxiv.Result.Author('Yash Satsangi'), arxiv.Result.Author('Fran Silavong'), arxiv.Result.Author('Shaltiel Eloul'), arxiv.Result.Author('Sean Moran')]","Machine learning on source code (MLOnCode) promises to transform how software
is delivered. By mining the context and relationship between software
artefacts, MLOnCode augments the software developers capabilities with code
auto-generation, code recommendation, code auto-tagging and other data-driven
enhancements. For many of these tasks a script level representation of code is
sufficient, however, in many cases a repository level representation that takes
into account various dependencies and repository structure is imperative, for
example, auto-tagging repositories with topics or auto-documentation of
repository code etc. Existing methods for computing repository level
representations suffer from (a) reliance on natural language documentation of
code (for example, README files) (b) naive aggregation of method/script-level
representation, for example, by concatenation or averaging. This paper
introduces Topical a deep neural network to generate repository level
embeddings of publicly available GitHub code repositories directly from source
code. Topical incorporates an attention mechanism that projects the source
code, the full dependency graph and the script level textual information into a
dense repository-level representation. To compute the repository-level
representations, Topical is trained to predict the topics associated with a
repository, on a dataset of publicly available GitHub repositories that were
crawled along with their ground truth topic tags. Our experiments show that the
embeddings computed by Topical are able to outperform multiple baselines,
including baselines that naively combine the method-level representations
through averaging or concatenation at the task of repository auto-tagging."
10357,"While detailed analysis of each category is
subject to further research, we can infer some characteristics of the them.","We inspected some of the files that belong to each category, which provided us with some
insights into what types of files belong to each group.","We observed that the largest group with 10 comments or fewer includes a number of data
initialization scripts, configurations, low-level details (e.g.",2022-08-23 23:44:09+00:00,Preprocessing Source Code Comments for Linguistic Models,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Sergey Matskevich'), arxiv.Result.Author('Colin Gordon')]","Comments are an important part of the source code and are a primary source of
documentation. This has driven interest in using large bodies of comments to
train or evaluate tools that consume or produce them -- such as generating
oracles or even code from comments, or automatically generating code summaries.
Most of this work makes strong assumptions about the structure and quality of
comments, such as assuming they consist mostly of proper English sentences.
However, we know little about the actual quality of existing comments for these
use cases. Comments often contain unique structures and elements that are not
seen in other types of text, and filtering or extracting information from them
requires some extra care. This paper explores the contents and quality of
Python comments drawn from 840 most popular open source projects from GitHub
and 8422 projects from SriLab dataset, and the impact of na\""ive vs. in-depth
filtering can have on the use of existing comments for training and evaluation
of systems that generate comments."
10358,"However, this problem of translating formulas to text is still open and hard to implement, therefore
further research is required and the full discussion of translating formulas to text is outside of
scope of this paper.",It is possible to translate it into an English phrase that will fit in the context where it’s used.,"In our experiments we have identified several problems that researchers need to be aware of if
they use comments as an input to machine learning algorithms.",2022-08-23 23:44:09+00:00,Preprocessing Source Code Comments for Linguistic Models,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Sergey Matskevich'), arxiv.Result.Author('Colin Gordon')]","Comments are an important part of the source code and are a primary source of
documentation. This has driven interest in using large bodies of comments to
train or evaluate tools that consume or produce them -- such as generating
oracles or even code from comments, or automatically generating code summaries.
Most of this work makes strong assumptions about the structure and quality of
comments, such as assuming they consist mostly of proper English sentences.
However, we know little about the actual quality of existing comments for these
use cases. Comments often contain unique structures and elements that are not
seen in other types of text, and filtering or extracting information from them
requires some extra care. This paper explores the contents and quality of
Python comments drawn from 840 most popular open source projects from GitHub
and 8422 projects from SriLab dataset, and the impact of na\""ive vs. in-depth
filtering can have on the use of existing comments for training and evaluation
of systems that generate comments."
10359,"While detailed analysis of each category is
subject to further research, we can infer some characteristics of the them.","We inspected some of the files that belong to each category, which provided us with some
insights into what types of files belong to each group.","We observed that the largest group with 10 comments or fewer includes a number of data
initialization scripts, configurations, low-level details (e.g.",2022-08-23 23:44:09+00:00,Preprocessing Source Code Comments for Linguistic Models,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Sergey Matskevich'), arxiv.Result.Author('Colin S. Gordon')]","Comments are an important part of the source code and are a primary source of
documentation. This has driven interest in using large bodies of comments to
train or evaluate tools that consume or produce them -- such as generating
oracles or even code from comments, or automatically generating code summaries.
Most of this work makes strong assumptions about the structure and quality of
comments, such as assuming they consist mostly of proper English sentences.
However, we know little about the actual quality of existing comments for these
use cases. Comments often contain unique structures and elements that are not
seen in other types of text, and filtering or extracting information from them
requires some extra care. This paper explores the contents and quality of
Python comments drawn from 840 most popular open source projects from GitHub
and 8422 projects from SriLab dataset, and the impact of na\""ive vs. in-depth
filtering can have on the use of existing comments for training and evaluation
of systems that generate comments."
10360,"However, this problem of translating formulas to text is still open and hard to implement, therefore
further research is required and the full discussion of translating formulas to text is outside of
scope of this paper.",It is possible to translate it into an English phrase that will fit in the context where it’s used.,"In our experiments we have identified several problems that researchers need to be aware of if
they use comments as an input to machine learning algorithms.",2022-08-23 23:44:09+00:00,Preprocessing Source Code Comments for Linguistic Models,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Sergey Matskevich'), arxiv.Result.Author('Colin S. Gordon')]","Comments are an important part of the source code and are a primary source of
documentation. This has driven interest in using large bodies of comments to
train or evaluate tools that consume or produce them -- such as generating
oracles or even code from comments, or automatically generating code summaries.
Most of this work makes strong assumptions about the structure and quality of
comments, such as assuming they consist mostly of proper English sentences.
However, we know little about the actual quality of existing comments for these
use cases. Comments often contain unique structures and elements that are not
seen in other types of text, and filtering or extracting information from them
requires some extra care. This paper explores the contents and quality of
Python comments drawn from 840 most popular open source projects from GitHub
and 8422 projects from SriLab dataset, and the impact of na\""ive vs. in-depth
filtering can have on the use of existing comments for training and evaluation
of systems that generate comments."
10443,"and forming teams with synched preferences for office
                                                                             presence, which are important directions for further study.","6  Psychological safety in remote work                       July 2022
tools that provide additional contextual clues (open video                   help with psychological safety are mandatory office days
rooms, status labels15).","Finally, we found that organizational norms and routines                     For now, we found one successful case in SavingsBank,
are effective at supporting team psychological safety.",2022-08-26 06:31:57+00:00,What happens to psychological safety when going remote?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Anastasiia Tkalich'), arxiv.Result.Author('Darja Smite'), arxiv.Result.Author('Nina Haugland Andersen'), arxiv.Result.Author('Nils Brede Moe')]","Psychological safety is a precondition for learning and success in software
teams. Companies such as SavingsBank, which is discussed in this article, have
developed good practices to facilitate psychological safety, most of which
depend on face-to-face interaction. However, what happens to psychological
safety when working remotely? In this article, we explore how Norwegian
software developers experienced pandemic and post-pandemic remote work and
describe simple behaviors and attitudes related to psychological safety. We pay
special attention to the hybrid work mode, in which team members alternate days
in the office with days working from home. Our key takeaway is that spontaneous
interaction in the office facilitates psychological safety, while remote work
increases the thresholds for both spontaneous interaction and psychological
safety. We recommend that software teams synchronize their office presence to
increase chances for spontaneous interaction in the office while benefitting
from focused work while at home."
10464,"To further study why higher coverage was not

achieved, we performed a manual analysis on the code coverage reports generated by the best test suite
(i.e., B) and the source code of these APIs.",In-depth analysis of the coverage reports.,"For CS4 , we found that 10 RPC functions out of the 55
functions are not accessible with the given client library.",2022-08-26 15:54:07+00:00,White-box Fuzzing RPC-based APIs with EvoMaster: An Industrial Case Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Man Zhang'), arxiv.Result.Author('Andrea Arcuri'), arxiv.Result.Author('Yonggang Li'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Kaiming Xue')]","Remote Procedure Call (RPC) is a communication protocol to support
client-server interactions among services over a network. RPC is widely applied
in industry for building large-scale distributed systems, such as
Microservices. Modern RPC frameworks include for example Thrift, gRPC, SOFARPC
and Dubbo. Testing such systems using RPC communications is very challenging,
due to the complexity of distributed systems and various RPC frameworks the
system could employ. To the best of our knowledge, there does not exist any
tool or solution that could enable automated testing of modern RPC-based
services. To fill this gap, in this paper we propose the first approach in the
literature, together with an open-source tool, for white-box fuzzing modern
RPC-based APIs with search. To assess our novel approach, we conducted an
empirical study with two artificial and four industrial web services selected
by our industrial partner. The tool has been integrated into a real industrial
pipeline, and could be applied to real industrial development process for
fuzzing RPC-based APIs. To further demonstrate its effectiveness and
application in industrial settings, we also report results of employing our
tool for fuzzing another 30 industrial APIs autonomously conducted by our
industrial partner in their testing processes. Results show that our novel
approach is capable of enabling automated test case generation for industrial
RPC-based APIs (i.e., two artificial and 34 industrial). We also compared with
a simple grey-box technique and existing manually written tests. Our white-box
solution achieves significant improvements on code coverage. Regarding fault
detection, by conducting a careful review with our industrial partner of the
tests generated by our novel approach in the selected four industrial APIs, a
total of 41 real faults were identified, which have now been fixed."
10511,"For the sake of simplicity, we
                                                                          select the small-scale test case (with the least number of invocation
4 EVALUATION                                                              sequences) for each API (i.e., 28,367 test cases) for further study.","28,367 distinct Android APIs.","By
                                                                          compiling and executing these 28,367 test cases, we further confirm
Our JUnitTestGen aims to generate unit test cases covering as many        that 5,562 of them are invalid (i.e., 22,805 of them are valid), giving
Android APIs as possible, so as to allow the discovery of more API-       a success rate of 80.4% in generating valid test cases.",2022-08-29 08:21:46+00:00,Mining Android API Usage to Generate Unit Test Cases for Pinpointing Compatibility Issues,cs.SE,['cs.SE'],"[arxiv.Result.Author('Xiaoyu Sun'), arxiv.Result.Author('Xiao Chen'), arxiv.Result.Author('Yanjie Zhao'), arxiv.Result.Author('Pei Liu'), arxiv.Result.Author('John Grundy'), arxiv.Result.Author('Li Li')]","Despite being one of the largest and most popular projects, the official
Android framework has only provided test cases for less than 30% of its APIs.
Such a poor test case coverage rate has led to many compatibility issues that
can cause apps to crash at runtime on specific Android devices, resulting in
poor user experiences for both apps and the Android ecosystem. To mitigate this
impact, various approaches have been proposed to automatically detect such
compatibility issues. Unfortunately, these approaches have only focused on
detecting signature-induced compatibility issues (i.e., a certain API does not
exist in certain Android versions), leaving other equally important types of
compatibility issues unresolved. In this work, we propose a novel prototype
tool, JUnitTestGen, to fill this gap by mining existing Android API usage to
generate unit test cases. After locating Android API usage in given real-world
Android apps, JUnitTestGen performs inter-procedural backward data-flow
analysis to generate a minimal executable code snippet (i.e., test case).
Experimental results on thousands of real-world Android apps show that
JUnitTestGen is effective in generating valid unit test cases for Android APIs.
We show that these generated test cases are indeed helpful for pinpointing
compatibility issues, including ones involving semantic code changes."
10514,"ence how girls engage with CS, thus calling for further research on
                                                                                                                                     gender-specific characteristics in online programming courses.","All of these are important factors that may influ-
                                        courses are held online.","CCS CONCEPTS
                                                                                                                                        In order to investigate possible gender-specific characteristics,
                                        • Social and professional topics → Computing education; K-                                   we conducted and observed an online programming course for
                                        12 education; Gender.",2022-08-29 09:29:37+00:00,"Gender-dependent Contribution, Code and Creativity in a Virtual Programming Course",cs.SE,"['cs.SE', '97P50', 'D.2.5; K.3.2']","[arxiv.Result.Author('Isabella Graßl'), arxiv.Result.Author('Gordon Fraser')]","Since computer science is still mainly male dominated, academia, industry and
education jointly seek ways to motivate and inspire girls, for example by
introducing them to programming at an early age. The recent COVID-19 pandemic
has forced many such endeavours to move to an online setting. While the
gender-dependent differences in programming courses have been studied
previously, for example revealing that girls may feel safer in same-sex groups,
much less is known about gender-specific differences in online programming
courses. In order to investigate whether gender-specific differences can be
observed in online courses, we conducted an online introductory programming
course for Scratch, in which we observed the gender-specific characteristics of
participants with respect to how they interact, their enjoyment, the code they
produce, and the creativity exposed by their programs. Overall, we observed no
significant differences between how girls participated in all-female vs. mixed
groups, and girls generally engaged with the course more actively than boys.
This suggests that online courses can be a useful means to avoid
gender-dependent group dynamics. However, when encouraging creative freedom in
programming, girls and boys seem to fall back to socially inherited
stereotypical behavior also in an online setting, influencing the choice of
programming concepts applied. This may inhibit learning and is a challenge that
needs to be addressed independently of whether courses are held online."
10536,Passing this task description to iTasks generates an           basis of further research as well.,"iTasks has formed the
  information.","Tonic [28] facilitates the subject
  application that prompts the user for their name.",2022-08-29 20:31:25+00:00,Creating Interactive Visualizations of TopHat Programs,cs.SE,"['cs.SE', 'cs.HC', 'cs.PL']","[arxiv.Result.Author('Mark Gerarts'), arxiv.Result.Author('Marc de Hoog'), arxiv.Result.Author('Nico Naus'), arxiv.Result.Author('Tim Steenvoorden')]","Many companies and institutions have automated their business process in
workflow management software. The novel programming paradigm Task-Oriented
Programming (TOP) provides an abstraction for such software. The largest
framework based on TOP, iTasks, has been used to develop real-world software.
  Workflow software often includes critical systems. In such cases it is
important to reason over the software to ascertain its correctness. The lack of
a formal iTasks semantics makes it unsuitable for formal reasoning. To this end
TopHat has been developed as a TOP language with a formal semantics. However,
TopHat lacks a graphical user interface (GUI), making it harder to develop
practical TopHat systems.
  In this paper we present TopHat UI. By combining an existing server framework
and user interface framework, we have developed a fully functioning proof of
concept implementation in Haskell, on top of TopHat's semantics. We show that
implementing a TOP framework is possible using a different host language than
iTasks uses. None of TopHat's formal properties have been compromised, since
the UI framework is completely separate from TopHat. We run several example
programs and evaluate their generated GUI. Having such a system improves the
quality and verifiability of TOP software in general."
10657,"C1: It is unclear how to statically infer dynamic behavioral      itate further research, which is accessible on our website [13].1
changes by source code diff, which has hardly been studied.","We are facing the following chal-
lenges.","C2:        • We carried out a study on compliance with SemVer rules in
Syntactic changes (including inter-method changes), such as the
refactoring, which hardly changes the semantics, are difficult to be      the top 21 real-world Java libraries.",2022-09-01 12:12:50+00:00,Has My Release Disobeyed Semantic Versioning? Static Detection Based on Semantic Differencing,cs.SE,['cs.SE'],"[arxiv.Result.Author('Lyuye Zhang'), arxiv.Result.Author('Chengwei Liu'), arxiv.Result.Author('Zhengzi Xu'), arxiv.Result.Author('Sen Chen'), arxiv.Result.Author('Lingling Fan'), arxiv.Result.Author('Bihuan Chen'), arxiv.Result.Author('Yang Liu')]","To enhance the compatibility in the version control of Java Third-party
Libraries (TPLs), Maven adopts Semantic Versioning (SemVer) to standardize the
underlying meaning of versions, but users could still confront abnormal
execution and crash after upgrades even if compilation and linkage succeed. It
is caused by semantic breaking (SemB) issues, such that APIs directly used by
users have identical signatures but inconsistent semantics across upgrades. To
strengthen compliance with SemVer rules, developers and users should be alerted
of such issues. Unfortunately, it is challenging to detect them statically,
because semantic changes in the internal methods of APIs are difficult to
capture. Dynamic testing can confirmingly uncover some, but it is limited by
inadequate coverage.
  To detect SemB issues over compatible upgrades (Patch and Minor) by SemVer
rules, we conduct an empirical study on 180 SemB issues to understand the root
causes, inspired by which, we propose Sembid (Semantic Breaking Issue Detector)
to statically detect such issues of TPLs for developers and users. Since APIs
are directly used by users, Sembid detects and reports SemB issues based on
APIs. For a pair of APIs, Sembid walks through the call chains originating from
the API to locate breaking changes by measuring semantic diff. Then, Sembid
checks if the breaking changes can affect API's output along call chains. The
evaluation showed Sembid achieved 90.26% recall and 81.29% precision and
outperformed other API checkers on SemB API detection. We also revealed Sembid
detected over 3 times more SemB APIs with better coverage than unit tests, the
commonly used solution. Furthermore, we carried out an empirical study on
1,629,589 APIs from 546 version pairs of top Java libraries and found there
were 2-4 times more SemB APIs than those with signature-based issues."
10738,"explicitly consider length to be a major factor, citing logical
cohesion and doing only one thing as the most important             Another line of further research is to elaborate on our
considerations.","Also, they didn’t          other considerations emerge?",results.,2022-09-02 14:57:44+00:00,How Developers Extract Functions: An Experiment,cs.SE,['cs.SE'],"[arxiv.Result.Author('Alexey Braver'), arxiv.Result.Author('Dror G. Feitelson')]","Creating functions is at the center of writing computer programs. But there
has been little empirical research on how this is done and what are the
considerations that developers use. We design an experiment in which we can
compare the decisions made by multiple developers under exactly the same
conditions. The experiment is based on taking existing production code,
""flattening"" it into a single monolithic function, and then charging developers
with the task of refactoring it to achieve a better design by extracting
functions. The results indicate that developers tend to extract functions based
on structural cues, such as 'if' or 'try' blocks. And while there are
significant correlations between the refactorings performed by different
developers, there are also significant differences in the magnitude of
refactoring done. For example, the number of functions that were extracted was
between 3 and 10, and the amount of code extracted into functions ranged from
37% to 95%."
10759,"We also suggest further research to investigate the
tion 4.1 shows that a majority of the tasks in the security patch          challenges in developing patch management tools that harness
management process are performed manually (see Figure 2).",The evidence from Sec-              achieve a goal.,We               collective human and machine intelligence.,2022-09-04 02:58:37+00:00,An Empirical Study of Automation in Software Security Patch Management,cs.SE,['cs.SE'],"[arxiv.Result.Author('Nesara Dissanayake'), arxiv.Result.Author('Asangi Jayatilaka'), arxiv.Result.Author('Mansooreh Zahedi'), arxiv.Result.Author('Muhammad Ali Babar')]","Several studies have shown that automated support for different activities of
the security patch management process has great potential for reducing delays
in installing security patches. However, it is also important to understand how
automation is used in practice, its limitations in meeting real-world needs and
what practitioners really need, an area that has not been empirically
investigated in the existing software engineering literature. This paper
reports an empirical study aimed at investigating different aspects of
automation for security patch management using semi-structured interviews with
17 practitioners from three different organisations in the healthcare domain.
The findings are focused on the role of automation in security patch management
for providing insights into the as-is state of automation in practice, the
limitations of current automation, how automation support can be enhanced to
effectively meet practitioners' needs, and the role of the human in an
automated process. Based on the findings, we have derived a set of
recommendations for directing future efforts aimed at developing automated
support for security patch management."
10813,"to build new tools and experiences or promote further study of skills
                                                                          in OSS.","comers’ and daily contributors’ discussion activity, as well as a         Contributors may use the results from this study to drive the devel-
movement to recognize all OSS contributions [13], such tools could        opment of their own skills, while researchers may extend this work
highlight mentees’ valuable non-code contributions.","To facilitate replication of this work, the survey instrument
8 THREATS TO VALIDITY                                                     and codebook are available as supplemental material [39].",2022-09-06 05:07:46+00:00,Understanding Skills for OSS Communities on GitHub,cs.SE,['cs.SE'],"[arxiv.Result.Author('Jenny T. Liang'), arxiv.Result.Author('Thomas Zimmermann'), arxiv.Result.Author('Denae Ford')]","The development of open source software (OSS) is a broad field which requires
diverse skill sets. For example, maintainers help lead the project and promote
its longevity, technical writers assist with documentation, bug reporters
identify defects in software, and developers program the software. However, it
is unknown which skills are used in OSS development as well as OSS
contributors' general attitudes towards skills in OSS. In this paper, we
address this gap by administering a survey to a diverse set of 455 OSS
contributors. Guided by these responses as well as prior literature on software
development expertise and social factors of OSS, we develop a model of skills
in OSS that considers the many contexts OSS contributors work in. This model
has 45 skills in the following 9 categories: technical skills, working styles,
problem solving, contribution types, project-specific skills, interpersonal
skills, external relations, management, and characteristics. Through a mix of
qualitative and quantitative analyses, we find that OSS contributors are
actively motivated to improve skills and perceive many benefits in sharing
their skills with others. We then use this analysis to derive a set of design
implications and best practices for those who incorporate skills into OSS tools
and platforms, such as GitHub."
10882,"Natural Language Processing (NLP)
duct further research to determine which artifacts would be more         techniques have become increasingly helpful in understanding the
helpful and how they can be properly analyzed to identify the root       semantic meaning of documents, summarizing, and extracting use-
causes of defects.",The research community can con-         the conduct of these studies.,ful information from documents.,2022-09-07 04:51:34+00:00,Reflections on Software Failure Analysis,cs.SE,['cs.SE'],"[arxiv.Result.Author('Paschal C. Amusuo'), arxiv.Result.Author('Aishwarya Sharma'), arxiv.Result.Author('Siddharth R. Rao'), arxiv.Result.Author('Abbey Vincent'), arxiv.Result.Author('James C. Davis')]","Failure studies are important in revealing the root causes, behaviors, and
life cycle of defects in software systems. These studies either focus on
understanding the characteristics of defects in specific classes of systems or
the characteristics of a specific type of defect in the systems it manifests
in. Failure studies have influenced various software engineering research
directions, especially in the area of software evolution, defect detection, and
program repair.
  In this paper, we reflect on the conduct of failure studies in software
engineering. We reviewed a sample of 52 failure study papers. We identified
several recurring problems in these studies, some of which hinder the ability
of the engineering community to trust or replicate the results. Based on our
findings, we suggest future research directions, including identifying and
analyzing failure causal chains, standardizing the conduct of failure studies,
and tool support for faster defect analysis."
10883,"Such a map would contain a taxonomy of           We encourage further research on identifying and analyzing causal
common defect types and can be extensible such that investigators        chains for defects and tool support to simplify defect analysis while
conducting failure studies for a specific system or defect classes can   recommending efforts to standardize the conduct of failure studies.",curity vulnerabilities.,"build upon existing taxonomies with defect type categories specific      With these steps, software failure studies may improve the quality
to the class of system being investigated rather than inventing a        of software engineering.",2022-09-07 04:51:34+00:00,Reflections on Software Failure Analysis,cs.SE,['cs.SE'],"[arxiv.Result.Author('Paschal C. Amusuo'), arxiv.Result.Author('Aishwarya Sharma'), arxiv.Result.Author('Siddharth R. Rao'), arxiv.Result.Author('Abbey Vincent'), arxiv.Result.Author('James C. Davis')]","Failure studies are important in revealing the root causes, behaviors, and
life cycle of defects in software systems. These studies either focus on
understanding the characteristics of defects in specific classes of systems or
the characteristics of a specific type of defect in the systems it manifests
in. Failure studies have influenced various software engineering research
directions, especially in the area of software evolution, defect detection, and
program repair.
  In this paper, we reflect on the conduct of failure studies in software
engineering. We reviewed a sample of 52 failure study papers. We identified
several recurring problems in these studies, some of which hinder the ability
of the engineering community to trust or replicate the results. Based on our
findings, we suggest future research directions, including identifying and
analyzing failure causal chains, standardizing the conduct of failure studies,
and tool support for faster defect analysis."
10884,"of a defect-type taxonomy map for software defects, similar to the     We encourage further research on identifying and analyzing causal
Common Weakness Enumeration (CWE) used for categorizing se-            chains for defects and tool support to simplify defect analysis while
curity vulnerabilities.","Second, we suggest the development         research community can support the conduct of these failure studies.",Such a map would contain a taxonomy of         recommending efforts to standardize the conduct of failure studies.,2022-09-07 04:51:34+00:00,Reflections on Software Failure Analysis,cs.SE,['cs.SE'],"[arxiv.Result.Author('Paschal C. Amusuo'), arxiv.Result.Author('Aishwarya Sharma'), arxiv.Result.Author('Siddharth R. Rao'), arxiv.Result.Author('Abbey Vincent'), arxiv.Result.Author('James C. Davis')]","Failure studies are important in revealing the root causes, behaviors, and
life cycle of defects in software systems. These studies either focus on
understanding the characteristics of defects in specific classes of systems or
the characteristics of a specific type of defect in the systems it manifests
in. Failure studies have influenced various software engineering research
directions, especially in the area of software evolution, defect detection, and
program repair.
  In this paper, we reflect on the conduct of failure studies in software
engineering. We reviewed a sample of 52 failure study papers. We identified
several recurring problems in these studies, some of which hinder the ability
of the engineering community to trust or replicate the results. Based on our
findings, we suggest future research directions, including identifying and
analyzing failure causal chains, standardizing the conduct of failure studies,
and tool support for faster defect analysis."
10903,"further research should reﬁne SZZ for the multi-commit
The percentage of deleted lines is a good predictor for good   model.","For this reason,
all contributions obtained by each feature in each model.","Our study revealed that, when applied to commit-
linkers.",2022-09-07 17:17:34+00:00,SZZ in the time of Pull Requests,cs.SE,['cs.SE'],"[arxiv.Result.Author('Fernando Petrulio'), arxiv.Result.Author('David Ackermann'), arxiv.Result.Author('Enrico Fregnan'), arxiv.Result.Author('Gül Calikli'), arxiv.Result.Author('Marco Castelluccio'), arxiv.Result.Author('Sylvestre Ledru'), arxiv.Result.Author('Calixte Denizet'), arxiv.Result.Author('Emma Humphries'), arxiv.Result.Author('Alberto Bacchelli')]","In the multi-commit development model, programmers complete tasks (e.g.,
implementing a feature) by organizing their work in several commits and
packaging them into a commit-set. Analyzing data from developers using this
model can be useful to tackle challenging developers' needs, such as knowing
which features introduce a bug as well as assessing the risk of integrating
certain features in a release. However, to do so one first needs to identify
fix-inducing commit-sets. For such an identification, the SZZ algorithm is the
most natural candidate, but its performance has not been evaluated in the
multi-commit context yet. In this study, we conduct an in-depth investigation
on the reliability and performance of SZZ in the multi-commit model. To obtain
a reliable ground truth, we consider an already existing SZZ dataset and adapt
it to the multi-commit context. Moreover, we devise a second dataset that is
more extensive and directly created by developers as well as Quality Assurance
(QA) engineers of Mozilla. Based on these datasets, we (1) test the performance
of B-SZZ and its non-language-specific SZZ variations in the context of the
multi-commit model, (2) investigate the reasons behind their specific behavior,
and (3) analyze the impact of non-relevant commits in a commit-set and
automatically detect them before using SZZ."
10922,"44, hence we used this value to further study the vector space.","This group itself
We conclude that the difference begins to stabilize at K equals    divides into several other noteable sub-groups.","Aside from analyzing high-level information about the state
   To analyze the clusters, for each cluster, we manually          of the Python open-source ecosystem (e.g., the presence of
inspected the most frequent libraries assigned to it, as well      different domains and their relative sizes), the constructed
                                                                   dendrogram can also provide more detailed insights.",2022-09-07 23:58:35+00:00,So Much in So Little: Creating Lightweight Embeddings of Python Libraries,cs.SE,['cs.SE'],"[arxiv.Result.Author('Yaroslav Golubev'), arxiv.Result.Author('Egor Bogomolov'), arxiv.Result.Author('Egor Bulychev'), arxiv.Result.Author('Timofey Bryksin')]","In software engineering, different approaches and machine learning models
leverage different types of data: source code, textual information, historical
data. An important part of any project is its dependencies. The list of
dependencies is relatively small but carries a lot of semantics with it, which
can be used to compare projects or make judgements about them.
  In this paper, we focus on Python projects and their PyPi dependencies in the
form of requirements.txt files. We compile a dataset of 7,132 Python projects
and their dependencies, as well as use Git to pull their versions from previous
years. Using this data, we build 32-dimensional embeddings of libraries by
applying Singular Value Decomposition to the co-occurrence matrix of projects
and libraries. We then cluster the embeddings and study their semantic
relations.
  To showcase the usefulness of such lightweight library embeddings, we
introduce a prototype tool for suggesting relevant libraries to a given
project. The tool computes project embeddings and uses dependencies of projects
with similar embeddings to form suggestions. To compare different library
recommenders, we have created a benchmark based on the evolution of dependency
sets in open-source projects. Approaches based on the created embeddings
significantly outperform the baseline of showing the most popular libraries in
a given year. We have also conducted a user study that showed that the
suggestions differ in quality for different project domains and that even
relevant suggestions might be not particularly useful. Finally, to facilitate
potentially more useful recommendations, we extended the recommender system
with an option to suggest rarer libraries."
11007,"One author used a keyword search to ﬁlter papers from this corpus, retaining
for further study those papers that contained any of the following time-related
keywords1: time, date, epoch, record, month, year, hour, minute, second, period,
week, chronolog, day, past, and interval.","This gave
us a corpus of 754 papers to inspect.","This retained 346 of the 754 papers
(45.89%).",2022-09-09 20:23:27+00:00,Pitfalls and Guidelines for Using Time-Based Git Data,cs.SE,['cs.SE'],"[arxiv.Result.Author('Samuel W. Flint'), arxiv.Result.Author('Jigyasa Chauhan'), arxiv.Result.Author('Robert Dyer')]","Many software engineering research papers rely on time-based data (e.g.,
commit timestamps, issue report creation/update/close dates, release dates).
Like most real-world data however, time-based data is often dirty. To date,
there are no studies that quantify how frequently such data is used by the
software engineering research community, or investigate sources of and quantify
how often such data is dirty. Depending on the research task and method used,
including such dirty data could affect the research results. This paper
presents an extended survey of papers that utilize time-based data, published
in the Mining Software Repositories (MSR) conference series. Out of the 754
technical track and data papers published in MSR 2004--2021, we saw at least
290 (38%) papers utilized time-based data. We also observed that most
time-based data used in research papers comes in the form of Git commits, often
from GitHub. Based on those results, we then used the Boa and Software Heritage
infrastructures to help identify and quantify several sources of dirty Git
timestamp data. Finally we provide guidelines/best practices for researchers
utilizing time-based data from Git repositories."
11008,"These papers already
have over 150 citations, indicating further research has utilized this potentially
bad data.","We also observed 9 (out of 11 studied) MSR data papers that exhibit bad time
data, showing that the problems identiﬁed occur in practice.","In the future, we would like to investigate potential problems in other kinds
of time data, such as issue reports.",2022-09-09 20:23:27+00:00,Pitfalls and Guidelines for Using Time-Based Git Data,cs.SE,['cs.SE'],"[arxiv.Result.Author('Samuel W. Flint'), arxiv.Result.Author('Jigyasa Chauhan'), arxiv.Result.Author('Robert Dyer')]","Many software engineering research papers rely on time-based data (e.g.,
commit timestamps, issue report creation/update/close dates, release dates).
Like most real-world data however, time-based data is often dirty. To date,
there are no studies that quantify how frequently such data is used by the
software engineering research community, or investigate sources of and quantify
how often such data is dirty. Depending on the research task and method used,
including such dirty data could affect the research results. This paper
presents an extended survey of papers that utilize time-based data, published
in the Mining Software Repositories (MSR) conference series. Out of the 754
technical track and data papers published in MSR 2004--2021, we saw at least
290 (38%) papers utilized time-based data. We also observed that most
time-based data used in research papers comes in the form of Git commits, often
from GitHub. Based on those results, we then used the Boa and Software Heritage
infrastructures to help identify and quantify several sources of dirty Git
timestamp data. Finally we provide guidelines/best practices for researchers
utilizing time-based data from Git repositories."
11018,"If these suspicions are
meshes in case of mergers, increased learning curve for newer     shown true, as in the case of the data mesh, further research
technologists, a signiﬁcant learning curve for experienced        might also explore the ability for the MMW architecture to
technologists switching projects (this diminishes their existing  drive migrations to other architectures.","data hub, data fabric, data spoke).","This includes state-of-
skill-set and making it largely unusable), or an implementation   the-art architectures.",2022-09-10 13:12:41+00:00,Mask-Mediator-Wrapper architecture as a Data Mesh driver,cs.SE,['cs.SE'],"[arxiv.Result.Author('Juraj Dončević'), arxiv.Result.Author('Krešimir Fertalj'), arxiv.Result.Author('Mario Brčić'), arxiv.Result.Author('Mihael Kovač')]","The data mesh is a novel data management concept that emphasises the
importance of a domain before technology. The concept is still in the early
stages of development and many efforts to implement and use it are expected to
have negative consequences for organizations due to a lack of technological
guidelines and best practices. To mitigate the risk of negative outcomes this
paper proposes the use of the mask-mediator-wrapper architecture as a data mesh
driver. The mask-mediator-wrapper architecture provides a set of prefabricated
configurable components that provide basic functionalities which a data mesh
requires. This paper shows how the two concepts are compatible in terms of
functionality, data modelling, evolvability and aligned capabilities. A
mask-mediator-wrapper driven data mesh facilitates: low-risk adoption trials,
rapid prototyping, standardization, and a guarantee of evolvability."
11064,"that there is a potential for optimizing the energy consumption
                                        of the data oriented stages of the machine learning pipeline and             The dataframe manipulation libraries are used in the data ori-
                                        further research is needed in the direction.",Conclusion: The results of our study indicates          such as pivot.,ented stages of the machine learning workflow.,2022-09-12 14:00:41+00:00,On the Energy Consumption of Different Dataframe Processing Libraries -- An Exploratory Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Shriram Shanbhag'), arxiv.Result.Author('Sridhar Chimalakonda')]","Background: The energy consumption of machine learning and its impact on the
environment has made energy efficient ML an emerging area of research. However,
most of the attention stays focused on the model creation and the training and
inferencing phase. Data oriented stages like preprocessing, cleaning and
exploratory analysis form a critical part of the machine learning workflow.
However, the energy efficiency of these stages have gained little attention
from the researchers. Aim: Our study aims to explore the energy consumption of
different dataframe processing libraries as a first step towards studying the
energy efficiency of the data oriented stages of the machine learning pipeline.
Method: We measure the energy consumption of 3 popular libraries used to work
with dataframes, namely Pandas, Vaex and Dask for 21 different operations
grouped under 4 categories on 2 datasets. Results: The results of our analysis
show that for a given dataframe processing operation, the choice of library can
indeed influence the energy consumption with some libraries consuming 202 times
lesser energy over others. Conclusion: The results of our study indicates that
there is a potential for optimizing the energy consumption of the data oriented
stages of the machine learning pipeline and further research is needed in the
direction."
11242,"35 The cat representation through events (occurrences)

    Of course, examining such an extensive relationship is a                             a         b                 c
continuing process that needs further research.",Fig.,"Further
research will involve experimenting with the TM approach in                    Fig.",2022-09-15 11:30:16+00:00,Conceptual Modeling of the Whole-Part Relationship,cs.SE,['cs.SE'],[arxiv.Result.Author('Sabah Al-Fedaghi')],"Conceptual models rely on structural information to describe relationships
among UML classes; among these, the whole-part (WP) relationship plays a
fundamental role. This paper explores and analyzes the WP semantics at large
with a focus on its software engineering use. The WP relationship has often
been treated as a first-class modeling construct in object-oriented analysis, a
subject of keen interest and it is considered important for UML modeling. From
the scientific and philosophical aspects, a theory of parts forming a whole is
a complex issue, loaded with controversies that are widely discussed. This
paper aims to offer a semantic assembly model that is useful to describe WP
relationships in conceptual modeling. We contribute to the WP research by
conducting an ontological analysis using UML samples that exemplify the WP
construct. The method of investigation is based on a model called a thinging
machine (TM) to explore the WP semantics through applying TM to numerous
existing UML models. The TM model uses the so-called thimacs (things/machines)
to form building blocks for describing the domain at a three levels of
description: static, events, and behavioral models. This approach contrasts the
UML method, which is infected by a multiplicity problem concerning the
integrated view of structure and behavior and how to associate diagrams with
one another. This investigation s results point to a promising contribution to
the understanding of the notion of WP relationship in UML."
11458,"It has also been extended
of how conformance checking results are visualized         beyond a pure control ﬂow view to consider, e.g.,
by process mining tools to provide a foundation for        resources and data (Gall & Rinderle-Ma, 2017;
further research on this topic.","Therefore,     conformance checking has been considerably improved
the goal of this paper is to develop an understanding      (Dunzer et al., 2019).","We conduct a systematic    Knuplesch et al., 2017).",2022-09-20 13:38:27+00:00,Process Mining Meets Visual Analytics: The Case of Conformance Checking,cs.SE,"['cs.SE', 'cs.GR']","[arxiv.Result.Author('Jana-Rebecca Rehse'), arxiv.Result.Author('Luise Pufahl'), arxiv.Result.Author('Michael Grohs'), arxiv.Result.Author('Lisa-Marie Klein')]","Conformance checking is a major function of process mining, which allows
organizations to identify and alleviate potential deviations from the intended
process behavior. To fully leverage its benefits, it is important that
conformance checking results are visualized in a way that is approachable and
understandable for non-expert users. However, the visualization of conformance
checking results has so far not been widely considered in research. Therefore,
the goal of this paper is to develop an understanding of how conformance
checking results are visualized by process mining tools to provide a foundation
for further research on this topic. We conduct a systematic study, where we
analyze the visualization capabilities of nine academic and seven commercial
tools by means of a visual analytics framework. In this study, we find that the
''Why?'' aspect of conformance checking visualization seems already be
well-defined, but the ''What?'' and ''How?'' aspects require future research."
11459,"The authors use these
represent them by applying semantic parameterization, a          SRs to further study the effect of an incomplete privacy
method that enables expressing rights and obligations in         statement (i.e., a statement that misses semantic roles) on
restricted natural language statements.","[77] extract and prioritize rights     a set of 17 semantic roles that are expected to be present in
and obligations from the US healthcare regulation and            other policies from the same domain.",The authors derive       the user’s perspective of a privacy risk.,2022-09-20 13:50:58+00:00,NLP-based Automated Compliance Checking of Data Processing Agreements against GDPR,cs.SE,['cs.SE'],"[arxiv.Result.Author('Orlando Amaral'), arxiv.Result.Author('Muhammad Ilyas Azeem'), arxiv.Result.Author('Sallam Abualhaija'), arxiv.Result.Author('Lionel C Briand')]","Processing personal data is regulated in Europe by the General Data
Protection Regulation (GDPR) through data processing agreements (DPAs).
Checking the compliance of DPAs contributes to the compliance verification of
software systems as DPAs are an important source of requirements for software
development involving the processing of personal data. However, manually
checking whether a given DPA complies with GDPR is challenging as it requires
significant time and effort for understanding and identifying DPA-relevant
compliance requirements in GDPR and then verifying these requirements in the
DPA. In this paper, we propose an automated solution to check the compliance of
a given DPA against GDPR. In close interaction with legal experts, we first
built two artifacts: (i) the ""shall"" requirements extracted from the GDPR
provisions relevant to DPA compliance and (ii) a glossary table defining the
legal concepts in the requirements. Then, we developed an automated solution
that leverages natural language processing (NLP) technologies to check the
compliance of a given DPA against these ""shall"" requirements. Specifically, our
approach automatically generates phrasal-level representations for the textual
content of the DPA and compares it against predefined representations of the
""shall"" requirements. Over a dataset of 30 actual DPAs, the approach correctly
finds 618 out of 750 genuine violations while raising 76 false violations, and
further correctly identifies 524 satisfied requirements. The approach has thus
an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%.
Compared to a baseline that relies on off-the-shelf NLP tools, our approach
provides an average accuracy gain of ~20 percentage points. The accuracy of our
approach can be improved to ~94% with limited manual verification effort."
11484,"Finally, we conclude the review with
                                        some promising future directions to drive further research innovations and developments in this promising
                                        area.","Furthermore, we discuss the diﬃculties and potential solutions associated with
                                        the use of soft computing techniques to predict software maintainability.","Keywords: Software Maintainability, Software Maintainability Prediction, Soft Computing Techniques,
                                        Software Metrics

                                        1.",2022-09-21 05:38:23+00:00,"A Systematic Literature Review of Soft Computing Techniques for Software Maintainability Prediction: State-of-the-Art, Challenges and Future Directions",cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Gokul Yenduri'), arxiv.Result.Author('Thippa Reddy Gadekallu')]","The software is changing rapidly with the invention of advanced technologies
and methodologies. The ability to rapidly and successfully upgrade software in
response to changing business requirements is more vital than ever. For the
long-term management of software products, measuring software maintainability
is crucial. The use of soft computing techniques for software maintainability
prediction has shown immense promise in software maintenance process by
providing accurate prediction of software maintainability. To better understand
the role of soft computing techniques for software maintainability prediction,
we aim to provide a systematic literature review of soft computing techniques
for software maintainability prediction. Firstly, we provide a detailed
overview of software maintainability. Following this, we explore the
fundamentals of software maintainability and the reasons for adopting soft
computing methodologies for predicting software maintainability. Later, we
examine the soft computing approaches employed in the process of software
maintainability prediction. Furthermore, we discuss the difficulties and
potential solutions associated with the use of soft computing techniques to
predict software maintainability. Finally, we conclude the review with some
promising future directions to drive further research innovations and
developments in this promising area."
11485,"It is greatly anticipated that this study will provide encourage
further research from industry and academia.","To address this gap, this study presents a complete survey of soft computing techniques
used in predicting software maintainability.",1.3.,2022-09-21 05:38:23+00:00,"A Systematic Literature Review of Soft Computing Techniques for Software Maintainability Prediction: State-of-the-Art, Challenges and Future Directions",cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Gokul Yenduri'), arxiv.Result.Author('Thippa Reddy Gadekallu')]","The software is changing rapidly with the invention of advanced technologies
and methodologies. The ability to rapidly and successfully upgrade software in
response to changing business requirements is more vital than ever. For the
long-term management of software products, measuring software maintainability
is crucial. The use of soft computing techniques for software maintainability
prediction has shown immense promise in software maintenance process by
providing accurate prediction of software maintainability. To better understand
the role of soft computing techniques for software maintainability prediction,
we aim to provide a systematic literature review of soft computing techniques
for software maintainability prediction. Firstly, we provide a detailed
overview of software maintainability. Following this, we explore the
fundamentals of software maintainability and the reasons for adopting soft
computing methodologies for predicting software maintainability. Later, we
examine the soft computing approaches employed in the process of software
maintainability prediction. Furthermore, we discuss the difficulties and
potential solutions associated with the use of soft computing techniques to
predict software maintainability. Finally, we conclude the review with some
promising future directions to drive further research innovations and
developments in this promising area."
11493,"[11] E. C. Foster and S. Godbole, Database Systems: A Pragmatic
    In further research, some algorithms for frequent subgraph                          Approach, 2nd ed.",10.1109/ICIT.2018.8352419.,"Berkeley, CA, Apress, 2016.
mining will be implemented and tested on the available                          [12] A. Davoudian, L. Chen and M. Liu, “A Survey on NoSQL Stores,”
information about brownfield manufacturing systems from the                             ACM Comput.",2022-09-21 11:08:34+00:00,A graph-based knowledge representation and pattern mining supporting the Digital Twin creation of existing manufacturing systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Dominik Braun'), arxiv.Result.Author('Timo Müller'), arxiv.Result.Author('Nada Sahlab'), arxiv.Result.Author('Nasser Jazdi'), arxiv.Result.Author('Wolfgang Schloegl'), arxiv.Result.Author('Michael Weyrich')]","The creation of a Digital Twin for existing manufacturing systems, so-called
brownfield systems, is a challenging task due to the needed expert knowledge
about the structure of brownfield systems and the effort to realize the digital
models. Several approaches and methods have already been proposed that at least
partially digitalize the information about a brownfield manufacturing system. A
Digital Twin requires linked information from multiple sources. This paper
presents a graph-based approach to merge information from heterogeneous
sources. Furthermore, the approach provides a way to automatically identify
templates using graph structure analysis to facilitate further work with the
resulting Digital Twin and its further enhancement."
11517,"Thus, further research should address strategies to
measure social debt items connected to the rest of the community smells.","Al-
though the DAHLIA framework theoretically measures forms of social debt
and estimates monetary costs, it is only helpful regarding architecture in-
communicability [20].","This SLR reported ﬁve types of research contributions to manage commu-
nity smells and mitigate social debt.",2022-09-21 21:49:40+00:00,Community Smells -- The Sources of Social Debt: A Systematic Literature Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Eduardo Cabllero-Espinosa'), arxiv.Result.Author('Jeffrey C. Carver'), arxiv.Result.Author('Kimberly Stowers')]","Context: Social debt describes the accumulation of unforeseen project costs
(or potential costs) from sub-optimal software development processes. Community
smells are sociotechnical anti-patterns and one source of social debt that
impact software teams, development processes, outcomes, and organizations.
Objective: To provide an overview of community smells based on published
literature, and describe future research. Method: We conducted a systematic
literature review (SLR) to identify properties, understand origins and
evolution, and describe the emergence of community smells. This SLR explains
the impact of community smells on teamwork and team performance. Results: We
include 25 studies. Social debt describes the impacts of poor socio-technical
decisions on work environments, people, software products, and society. For
each of the 30 identified community smells, we provide a description,
management approaches, organizational strategies, and mitigation effectiveness.
We identify five groups of management approaches: organizational strategies,
frameworks, models, tools, and guidelines. We describe 11 properties of
community smells. We develop the Community Smell Stages Framework to concisely
describe the origin and evolution of community smells. We describe the causes
and effects for each community smell. We identify and describe 8 types of
causes and 11 types of effects for community smells. Finally, we provide 8
Sankey diagrams that offer insights into threats the community smells pose to
teamwork factors and team performance. Conclusion: Community smells explain the
influence work conditions have on software developers. The literature is scarce
and focuses on a small number of community smells. Thus, community smells still
need more research. This review organizes the state of the art about community
smells and provides motivation for future research along with educational
material."
11518,"The connection between the community smells and teamwork factors

    Next, the patterns of how the causes and eﬀects of community smells
occur and the mapping between the community smells and the critical fac-
tors for eﬀective teamwork can motivate further research.",5.5.,"For example, re-
searchers can apply software risk assessment [65, 66] to extend our work.",2022-09-21 21:49:40+00:00,Community Smells -- The Sources of Social Debt: A Systematic Literature Review,cs.SE,['cs.SE'],"[arxiv.Result.Author('Eduardo Cabllero-Espinosa'), arxiv.Result.Author('Jeffrey C. Carver'), arxiv.Result.Author('Kimberly Stowers')]","Context: Social debt describes the accumulation of unforeseen project costs
(or potential costs) from sub-optimal software development processes. Community
smells are sociotechnical anti-patterns and one source of social debt that
impact software teams, development processes, outcomes, and organizations.
Objective: To provide an overview of community smells based on published
literature, and describe future research. Method: We conducted a systematic
literature review (SLR) to identify properties, understand origins and
evolution, and describe the emergence of community smells. This SLR explains
the impact of community smells on teamwork and team performance. Results: We
include 25 studies. Social debt describes the impacts of poor socio-technical
decisions on work environments, people, software products, and society. For
each of the 30 identified community smells, we provide a description,
management approaches, organizational strategies, and mitigation effectiveness.
We identify five groups of management approaches: organizational strategies,
frameworks, models, tools, and guidelines. We describe 11 properties of
community smells. We develop the Community Smell Stages Framework to concisely
describe the origin and evolution of community smells. We describe the causes
and effects for each community smell. We identify and describe 8 types of
causes and 11 types of effects for community smells. Finally, we provide 8
Sankey diagrams that offer insights into threats the community smells pose to
teamwork factors and team performance. Conclusion: Community smells explain the
influence work conditions have on software developers. The literature is scarce
and focuses on a small number of community smells. Thus, community smells still
need more research. This review organizes the state of the art about community
smells and provides motivation for future research along with educational
material."
11573,"We
                                                 conclude that further research shall be invested in clarifying further the
                                                 design principles we learned as part of this study as well as any trade-oﬀs
                                                 posed by blockchain-oriented service design and operation.","Evi-
                                                 dence shows that the resulting architecture is, in principle, not diﬀerent
                                                 than other less complex equivalents; furthermore, the architectural lim-
                                                 itations posed by the blockchain-oriented design demand a signiﬁcant
                                                 additional eﬀort to be put onto even the simplest of functionalities.","Keywords: Blockchain Software, Service-Oriented Architectures, Tech-
                                                 nology Acceptance, Case-Study Research

                                        1 Introduction

                                        Blockchain technology is heralded as a silver bullet for a wide range of problems,
                                        yet the stylistic restrictions posed on top of more classical service-oriented ar-
                                        chitectures [14] that blockchain-oriented service design forces into the equation
                                        limit the throughput and latency of blockchain transactions [16].",2022-09-22 21:19:00+00:00,Blockchain-Oriented Services Computing in Action: Insights from a User Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Giovanni Quattrocchi'), arxiv.Result.Author('Damian Andrew Tamburri'), arxiv.Result.Author('WIllem-Jan Van Den Heuvel')]","Blockchain architectures promise disruptive innovation but factually they
pose many architectural restrictions to classical service-based applications
and show considerable design, implementation, and operations overhead.
Furthermore, the relation between such overheads and user benefits is not clear
yet. To shed light on the aforementioned relations, a service-based blockchain
architecture was designed and deployed as part of a field study in real-life
experimentation. An observational approach was then performed to elaborate on
the technology-acceptance of the service-based blockchain architecture in
question. Evidence shows that the resulting architecture is, in principle, not
different than other less complex equivalents; furthermore, the architectural
limitations posed by the blockchain-oriented design demand a significant
additional effort to be put onto even the simplest of functionalities. We
conclude that further research shall be invested in clarifying further the
design principles we learned as part of this study as well as any trade-offs
posed by blockchain-oriented service design and operation."
11683,These ﬁndings again conﬁrm that the research in SoS BP is recent and further research collaboration should be consolidated.,"Most authors (19 of 27), in red circles, participated in only one study and are
not ﬁrst authors.",We also investigate which terms are addressed by the studies.,2022-09-26 20:40:20+00:00,Can Existing Approaches Manage Dynamic and Large Business Processes enacted through Systems-of-Systems?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Maria Istela Cagnin'), arxiv.Result.Author('Elisa Yumi Nakagawa')]","In the era of joint ventures (JV) and mergers & acquisitions (M&A), dynamic
and large business processes can emerge to achieve broader business goals and
are often formed from business processes of distinct organizations. Software
systems of such distinct organizations should support these larger processes
and, for this, they need to communicate among them forming the so called
Systems-of-Systems (SoS). However, the management of these larger processes and
the correspondent SoS has been currently a complicated challenge for the
alliances of organizations. In this scenario, the main contribution of this
paper is to discover what has been proposed in the literature to manage these
processes. We analyzed possibly all existing approaches and the findings point
out that many of them cannot provide understanding of the whole large processes
and, more importantly, they do not address completely the SoS inherent
characteristics. We also highlight the next research directions to make
possible the management of these dynamic, complex, interconnected business
processes that have increasingly crossed several critical domains."
11781,"This
were assured that the interviews would be anonymised,           study raises a number of further research opportunities for
there is a threat that they did not speak freely in the inter-  software engineers, including further tool and method de-
view.","technical theory of dealing with data challenges which
                                                                serves to present a holistic view of the challenges and their
    Threats and limitations: Even though the participants       associated causes, consequences, and contingencies.","Further, the ﬁrst author performed all the interviews     velopment to address data challenges and extending work
and coding of the interviews.",2022-09-27 06:47:28+00:00,Dealing with Data Challenges when Delivering Data-Intensive Software Solutions,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ulrike M. Graetsch'), arxiv.Result.Author('Hourieh Khalajzadeh'), arxiv.Result.Author('Mojtaba Shahin'), arxiv.Result.Author('Rashina Hoda'), arxiv.Result.Author('John Grundy')]","The predicted increase in demand for data-intensive solution development is
driving the need for software, data, and domain experts to effectively
collaborate in multi-disciplinary data-intensive software teams (MDSTs). We
conducted a socio-technical grounded theory study through interviews with 24
practitioners in MDSTs to better understand the challenges these teams face
when delivering data-intensive software solutions. The interviews provided
perspectives across different types of roles including domain, data and
software experts, and covered different organisational levels from team
members, team managers to executive leaders. We found that the key concern for
these teams is dealing with data-related challenges. In this paper, we present
the theory of dealing with data challenges to explain the challenges faced by
MDSTs including gaining access to data, aligning data, understanding data, and
resolving data quality issues; the context in and condition under which these
challenges occur, the causes that lead to the challenges, and the related
consequences such as having to conduct remediation activities, inability to
achieve expected outcomes and lack of trust in the delivered solutions. We also
identified contingencies or strategies applied to address the challenges
including high-level strategic approaches such as implementing data governance,
implementing new tools and techniques such as data quality visualisation and
monitoring tools, as well as building stronger teams by focusing on people
dynamics, communication skill development and cross-skilling. Our findings have
direct implications for practitioners and researchers to better understand the
landscape of data challenges and how to deal with them."
11873,"This ﬁnding highlights room for improvement
for dynamic reconﬁguration, and it calls for further research on this topic.","Towards eﬀective assessment of steady state performance in Java software  5

ecutions and distorted results.","The main contributions of this paper are:

– a statistically rigorous investigation of steady state performance in JMH
   microbenchmarks.",2022-09-30 10:42:45+00:00,Towards effective assessment of steady state performance in Java software: Are we there yet?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luca Traini'), arxiv.Result.Author('Vittorio Cortellessa'), arxiv.Result.Author('Daniele Di Pompeo'), arxiv.Result.Author('Michele Tucci')]","Microbenchmarking is a widely used form of performance testing in Java
software. A microbenchmark repeatedly executes a small chunk of code while
collecting measurements related to its performance. Due to Java Virtual Machine
optimizations, microbenchmarks are usually subject to severe performance
fluctuations in the first phase of their execution (also known as warmup). For
this reason, software developers typically discard measurements of this phase
and focus their analysis when benchmarks reach a steady state of performance.
Developers estimate the end of the warmup phase based on their expertise, and
configure their benchmarks accordingly. Unfortunately, this approach is based
on two strong assumptions: (i) benchmarks always reach a steady state of
performance and (ii) developers accurately estimate warmup. In this paper, we
show that Java microbenchmarks do not always reach a steady state, and often
developers fail to accurately estimate the end of the warmup phase. We found
that a considerable portion of studied benchmarks do not hit the steady state,
and warmup estimates provided by software developers are often inaccurate (with
a large error). This has significant implications both in terms of results
quality and time-effort. Furthermore, we found that dynamic reconfiguration
significantly improves warmup estimation accuracy, but still it induces
suboptimal warmup estimates and relevant side-effects. We envision this paper
as a starting point for supporting the introduction of more sophisticated
automated techniques that can ensure results quality in a timely fashion."
11874,"These results highlight a large space for improvement in dynamic re-
conﬁguration techniques, and call for further research on designing and devel-
oping more eﬀective dynamic reconﬁguration techniques.","Nonetheless, half of the warmup estimates of dynamic reconﬁguration tech-
niques can be reduced by at least 96%, when only considering overestimated
forks.","For example, future
research may explore the use of other stability metrics (e.g., autocorrelation
metrics, other conﬁdence interval metrics (Fieller, 1954)), or combinations of
them to more eﬀectively determine the end of the warmup phase.",2022-09-30 10:42:45+00:00,Towards effective assessment of steady state performance in Java software: Are we there yet?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luca Traini'), arxiv.Result.Author('Vittorio Cortellessa'), arxiv.Result.Author('Daniele Di Pompeo'), arxiv.Result.Author('Michele Tucci')]","Microbenchmarking is a widely used form of performance testing in Java
software. A microbenchmark repeatedly executes a small chunk of code while
collecting measurements related to its performance. Due to Java Virtual Machine
optimizations, microbenchmarks are usually subject to severe performance
fluctuations in the first phase of their execution (also known as warmup). For
this reason, software developers typically discard measurements of this phase
and focus their analysis when benchmarks reach a steady state of performance.
Developers estimate the end of the warmup phase based on their expertise, and
configure their benchmarks accordingly. Unfortunately, this approach is based
on two strong assumptions: (i) benchmarks always reach a steady state of
performance and (ii) developers accurately estimate warmup. In this paper, we
show that Java microbenchmarks do not always reach a steady state, and often
developers fail to accurately estimate the end of the warmup phase. We found
that a considerable portion of studied benchmarks do not hit the steady state,
and warmup estimates provided by software developers are often inaccurate (with
a large error). This has significant implications both in terms of results
quality and time-effort. Furthermore, we found that dynamic reconfiguration
significantly improves warmup estimation accuracy, but still it induces
suboptimal warmup estimates and relevant side-effects. We envision this paper
as a starting point for supporting the introduction of more sophisticated
automated techniques that can ensure results quality in a timely fashion."
11875,"We have made the code and the data used in our study publicly available
to encourage further research on this topic.",This is a direction we aim to investigate in future work.,"Data Availability

To aid reproducibility we provide the data and scripts needed to replicate our
ﬁndings.",2022-09-30 10:42:45+00:00,Towards effective assessment of steady state performance in Java software: Are we there yet?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luca Traini'), arxiv.Result.Author('Vittorio Cortellessa'), arxiv.Result.Author('Daniele Di Pompeo'), arxiv.Result.Author('Michele Tucci')]","Microbenchmarking is a widely used form of performance testing in Java
software. A microbenchmark repeatedly executes a small chunk of code while
collecting measurements related to its performance. Due to Java Virtual Machine
optimizations, microbenchmarks are usually subject to severe performance
fluctuations in the first phase of their execution (also known as warmup). For
this reason, software developers typically discard measurements of this phase
and focus their analysis when benchmarks reach a steady state of performance.
Developers estimate the end of the warmup phase based on their expertise, and
configure their benchmarks accordingly. Unfortunately, this approach is based
on two strong assumptions: (i) benchmarks always reach a steady state of
performance and (ii) developers accurately estimate warmup. In this paper, we
show that Java microbenchmarks do not always reach a steady state, and often
developers fail to accurately estimate the end of the warmup phase. We found
that a considerable portion of studied benchmarks do not hit the steady state,
and warmup estimates provided by software developers are often inaccurate (with
a large error). This has significant implications both in terms of results
quality and time-effort. Furthermore, we found that dynamic reconfiguration
significantly improves warmup estimation accuracy, but still it induces
suboptimal warmup estimates and relevant side-effects. We envision this paper
as a starting point for supporting the introduction of more sophisticated
automated techniques that can ensure results quality in a timely fashion."
11876,"This ﬁnding highlights room for improvement
for dynamic reconﬁguration, and it calls for further research on this topic.","Towards eﬀective assessment of steady state performance in Java software  5

ecutions and distorted results.","The main contributions of this paper are:

– a statistically rigorous investigation of steady state performance in JMH
   microbenchmarks.",2022-09-30 10:42:45+00:00,Towards effective assessment of steady state performance in Java software: Are we there yet?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luca Traini'), arxiv.Result.Author('Vittorio Cortellessa'), arxiv.Result.Author('Daniele Di Pompeo'), arxiv.Result.Author('Michele Tucci')]","Microbenchmarking is a widely used form of performance testing in Java
software. A microbenchmark repeatedly executes a small chunk of code while
collecting measurements related to its performance. Due to Java Virtual Machine
optimizations, microbenchmarks are usually subject to severe performance
fluctuations in the first phase of their execution (also known as warmup). For
this reason, software developers typically discard measurements of this phase
and focus their analysis when benchmarks reach a steady state of performance.
Developers estimate the end of the warmup phase based on their expertise, and
configure their benchmarks accordingly. Unfortunately, this approach is based
on two strong assumptions: (i) benchmarks always reach a steady state of
performance and (ii) developers accurately estimate warmup. In this paper, we
show that Java microbenchmarks do not always reach a steady state, and often
developers fail to accurately estimate the end of the warmup phase. We found
that a considerable portion of studied benchmarks do not hit the steady state,
and warmup estimates provided by software developers are often inaccurate (with
a large error). This has significant implications both in terms of results
quality and time-effort. Furthermore, we found that dynamic reconfiguration
significantly improves warmup estimation accuracy, but still it induces
suboptimal warmup estimates and relevant side-effects. We envision this paper
as a starting point for supporting the introduction of more sophisticated
automated techniques that can ensure results quality in a timely fashion."
11877,"These results highlight a large space for improvement in dynamic re-
conﬁguration techniques, and call for further research on designing and devel-
oping more eﬀective dynamic reconﬁguration techniques.","Nonetheless, half of the warmup estimates of dynamic reconﬁguration tech-
niques can be reduced by at least 96%, when only considering overestimated
forks.","For example, future
research may explore the use of other stability metrics (e.g., autocorrelation
metrics, other conﬁdence interval metrics (Fieller, 1954)), or combinations of
them to more eﬀectively determine the end of the warmup phase.",2022-09-30 10:42:45+00:00,Towards effective assessment of steady state performance in Java software: Are we there yet?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luca Traini'), arxiv.Result.Author('Vittorio Cortellessa'), arxiv.Result.Author('Daniele Di Pompeo'), arxiv.Result.Author('Michele Tucci')]","Microbenchmarking is a widely used form of performance testing in Java
software. A microbenchmark repeatedly executes a small chunk of code while
collecting measurements related to its performance. Due to Java Virtual Machine
optimizations, microbenchmarks are usually subject to severe performance
fluctuations in the first phase of their execution (also known as warmup). For
this reason, software developers typically discard measurements of this phase
and focus their analysis when benchmarks reach a steady state of performance.
Developers estimate the end of the warmup phase based on their expertise, and
configure their benchmarks accordingly. Unfortunately, this approach is based
on two strong assumptions: (i) benchmarks always reach a steady state of
performance and (ii) developers accurately estimate warmup. In this paper, we
show that Java microbenchmarks do not always reach a steady state, and often
developers fail to accurately estimate the end of the warmup phase. We found
that a considerable portion of studied benchmarks do not hit the steady state,
and warmup estimates provided by software developers are often inaccurate (with
a large error). This has significant implications both in terms of results
quality and time-effort. Furthermore, we found that dynamic reconfiguration
significantly improves warmup estimation accuracy, but still it induces
suboptimal warmup estimates and relevant side-effects. We envision this paper
as a starting point for supporting the introduction of more sophisticated
automated techniques that can ensure results quality in a timely fashion."
11878,"We have made the code and the data used in our study publicly available
to encourage further research on this topic.",This is a direction we aim to investigate in future work.,"Data Availability

To aid reproducibility we provide the data and scripts needed to replicate our
ﬁndings.",2022-09-30 10:42:45+00:00,Towards effective assessment of steady state performance in Java software: Are we there yet?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luca Traini'), arxiv.Result.Author('Vittorio Cortellessa'), arxiv.Result.Author('Daniele Di Pompeo'), arxiv.Result.Author('Michele Tucci')]","Microbenchmarking is a widely used form of performance testing in Java
software. A microbenchmark repeatedly executes a small chunk of code while
collecting measurements related to its performance. Due to Java Virtual Machine
optimizations, microbenchmarks are usually subject to severe performance
fluctuations in the first phase of their execution (also known as warmup). For
this reason, software developers typically discard measurements of this phase
and focus their analysis when benchmarks reach a steady state of performance.
Developers estimate the end of the warmup phase based on their expertise, and
configure their benchmarks accordingly. Unfortunately, this approach is based
on two strong assumptions: (i) benchmarks always reach a steady state of
performance and (ii) developers accurately estimate warmup. In this paper, we
show that Java microbenchmarks do not always reach a steady state, and often
developers fail to accurately estimate the end of the warmup phase. We found
that a considerable portion of studied benchmarks do not hit the steady state,
and warmup estimates provided by software developers are often inaccurate (with
a large error). This has significant implications both in terms of results
quality and time-effort. Furthermore, we found that dynamic reconfiguration
significantly improves warmup estimation accuracy, but still it induces
suboptimal warmup estimates and relevant side-effects. We envision this paper
as a starting point for supporting the introduction of more sophisticated
automated techniques that can ensure results quality in a timely fashion."
12039,"For example, with the rise of NoSQL applications and non-traditional (non-relational) databases,
further research is demanded on the suitability of DooML for distributed databases.","Another question rises whether
DooML is sustainable as a modeling language that’s based on current developments in the field.","Furthermore,
more complex concepts of referential integrity are withheld from the DooML specification,
therefore the DooML specification is receptive to improvement.",2022-10-05 08:24:32+00:00,DooML: A new Database & Object-Oriented Modeling Language for database-driven web application design and development,cs.SE,"['cs.SE', 'cs.DB']",[arxiv.Result.Author('Thijs Otter')],"A database driven web application is a very common software solution to
rising business problems. Modeling the database and the software architecture
can be challenging, hence there not being one combined modeling language for
database and software architecture, specifically suited for web application
development. In this paper we present Database object-oriented Modeling
Language (DooML) and its primary Archetype Diagram: a notation for specifying
the design of a database schema and corresponding object-oriented software
architecture. It combines the syntax for drawing Entity Relationship Diagrams,
the Relational Model and Universal Modeling Language Class Diagrams as well to
create a mixed diagram, stating database design as well as software design
specifications. By default, DooML ensures that the approach of advanced web
application development is model-driven and both database-oriented as well as
object-oriented."
12046,"Together with our critical discussion, we propose the following directions
to pave the way for further research:

     • Development of new, more comprehensive DL approaches automatically capturing richer representations and
          features from heterogeneous sources (source code, bug reports and others),

     • Development of data augmentation techniques for tackling limited dataset sizes and class imbalance,
     • Identification of key source code defect attributes for defect prediction as well as exploitation of automatic feature

          extraction of DL approaches,
     • Establishing common criteria for evaluating the performance of DL-based SDP,
     • More focus on CPDP scenarios next to WPDP,
     • Better usability of SDP tools and integration into the daily practice of users,
     • Reproducibility and open science.","We have
also collected the reported challenges around the data engineering, model development aspects and SDP in general, along
with several solutions proposed by researchers.","Our results can be beneficial for both newcomers to SDP research to see the landscape of different approaches, and
established researchers to focus their efforts in the coming years.",2022-10-05 13:15:05+00:00,On the Use of Deep Learning in Software Defect Prediction,cs.SE,['cs.SE'],"[arxiv.Result.Author('Görkem Giray'), arxiv.Result.Author('Kwabena Ebo Bennin'), arxiv.Result.Author('Ömer Köksal'), arxiv.Result.Author('Önder Babur'), arxiv.Result.Author('Bedir Tekinerdogan')]","Context: Automated software defect prediction (SDP) methods are increasingly
applied, often with the use of machine learning (ML) techniques. Yet, the
existing ML-based approaches require manually extracted features, which are
cumbersome, time consuming and hardly capture the semantic information reported
in bug reporting tools. Deep learning (DL) techniques provide practitioners
with the opportunities to automatically extract and learn from more complex and
high-dimensional data. Objective: The purpose of this study is to
systematically identify, analyze, summarize, and synthesize the current state
of the utilization of DL algorithms for SDP in the literature. Method: We
systematically selected a pool of 102 peer-reviewed studies and then conducted
a quantitative and qualitative analysis using the data extracted from these
studies. Results: Main highlights include: (1) most studies applied supervised
DL; (2) two third of the studies used metrics as an input to DL algorithms; (3)
Convolutional Neural Network is the most frequently used DL algorithm.
Conclusion: Based on our findings, we propose to (1) develop more comprehensive
DL approaches that automatically capture the needed features; (2) use diverse
software artifacts other than source code; (3) adopt data augmentation
techniques to tackle the class imbalance problem; (4) publish replication
packages."
12070,"We wanted
to ﬁnd out what sorts of end user human-centric aspects they tend to encounter, which ones
they view as more important and which more challenging to address, what techniques (if any)
they currently use to address (some of) them, and where they perceive further research in this
area could be done to provide them practical support.","We wanted to ﬁnd out which are the key end user
human-centric aspects that software developers currently ﬁnd challenging to address, and how
they currently go about trying to address diverse end user human-centric aspects.","To this end we carried out a detailed
online survey of developers and development team managers, receiving 60 usable responses.",2022-10-05 20:18:29+00:00,Diverse End User Requirements,cs.SE,['cs.SE'],"[arxiv.Result.Author('John Grundy'), arxiv.Result.Author('Tanjila Kanij'), arxiv.Result.Author('Jennifer McIntosh'), arxiv.Result.Author('Hourieh Khalajzadeh'), arxiv.Result.Author('Ingo Mueller')]","As part of our larger research effort to improve support for diverse end user
human-centric aspects during software development, we wanted to better
understand how developers currently go about addressing these challenging
human-centric aspects of their end users in contemporary software development
projects. We wanted to find out which are the key end user human-centric
aspects that software developers currently find challenging to address, and how
they currently go about trying to address diverse end user human-centric
aspects. We wanted to find out what sorts of end user human-centric aspects
they tend to encounter, which ones they view as more important and which more
challenging to address, what techniques (if any) they currently use to address
(some of) them, and where they perceive further research in this area could be
done to provide them practical support. To this end we carried out a detailed
online survey of developers and development team managers, receiving 60 usable
responses. We interviewed 12 developers and managers from a range of different
practice domains, role specialisations and experience levels to explore further
details about issues."
12122,"To ﬁnd the research trends, methodology
and ﬁelds of further research in blockchain technology, Shahid et al.","[20] have utilized Topic modeling in cloud computing and to discover eﬃ-
cient cloud services, LDA is leveraged.","[21] have
                4  Joy et al.",2022-10-07 13:04:58+00:00,An Empirical Studies on How the Developers Discussed about Pandas Topics,cs.SE,"['cs.SE', 'cs.AI', 'cs.IR']","[arxiv.Result.Author('Sajib Kumar Saha Joy'), arxiv.Result.Author('Farzad Ahmed'), arxiv.Result.Author('Al Hasib Mahamud'), arxiv.Result.Author('Nibir Chandra Mandal')]","Pandas is defined as a software library which is used for data analysis in
Python programming language. As pandas is a fast, easy and open source data
analysis tool, it is rapidly used in different software engineering projects
like software development, machine learning, computer vision, natural language
processing, robotics, and others. So a huge interests are shown in software
developers regarding pandas and a huge number of discussions are now becoming
dominant in online developer forums, like Stack Overflow (SO). Such discussions
can help to understand the popularity of pandas library and also can help to
understand the importance, prevalence, difficulties of pandas topics. The main
aim of this research paper is to find the popularity and difficulty of pandas
topics. For this regard, SO posts are collected which are related to pandas
topic discussions. Topic modeling are done on the textual contents of the
posts. We found 26 topics which we further categorized into 5 board categories.
We observed that developers discuss variety of pandas topics in SO related to
error and excepting handling, visualization, External support, dataframe, and
optimization. In addition, a trend chart is generated according to the
discussion of topics in a predefined time series. The finding of this paper can
provide a path to help the developers, educators and learners. For example,
beginner developers can learn most important topics in pandas which are
essential for develop any model. Educators can understand the topics which seem
hard to learners and can build different tutorials which can make that pandas
topic understandable. From this empirical study it is possible to understand
the preferences of developers in pandas topic by processing their SO posts"
12277,"Our results also provide
                                        a starting point for further research on how contextual factors aﬀect CE and how
                                        these may be mitigated.","Conclusions: Our theory can be used by practitioners to
                                        assess an organisation’s potential for adopting CE, as well as, identifying factors
                                        which pose challenges in gaining value from CE practices.","Keywords Continuous experimentation ¨ Data-driven development ¨ A/B
                                        testing ¨ Theory building ¨ Multi-case study ¨ Empirical research

                                        Rasmus Ros
                                        Lund University, Sweden
                                        E-mail: rasmus.ros@cs.lth.se
                                        Elizabeth Bjarnason
                                        Lund University, Sweden
                                        E-mail: elizabeth.bjarnason@cs.lth.se
                                        Per Runeson
                                        Lund University, Sweden
                                        E-mail: per.runeson@cs.lth.se
2  Ros et al.",2022-10-11 06:53:24+00:00,A Theory of Factors Affecting Continuous Experimentation (FACE),cs.SE,['cs.SE'],"[arxiv.Result.Author('Rasmus Ros'), arxiv.Result.Author('Elizabeth Bjarnason'), arxiv.Result.Author('Per Runeson')]","Continuous experimentation (CE) is used by many companies with
internet-facing products to improve their software based on user data. Some
companies deliberately adopt an experiment-driven approach to software
development while some companies use CE in a more ad-hoc fashion. The goal of
the study is to identify factors that explain the variations in the utility and
efficacy of CE between different companies. We conducted a multi-case study of
12 companies involved with CE and performed 27 interviewees with practitioners
at these companies. Based on that empirical data, we then built a theory of
factors at play in CE. We introduce a theory of Factors Affecting Continuous
Experimentation (FACE). The theory includes three factors, namely 1) processes
and infrastructure for CE, 2) the user problem complexity of the product
offering, and 3) incentive structures for CE. It explains how these factors
affect the effectiveness of CE and its ability to achieve problem-solution and
product-market fit. Our theory can be used by practitioners to assess an
organisation's potential for adopting CE, as well as, identifying factors which
pose challenges in gaining value from CE practices. Our results also provide a
starting point for further research on how contextual factors affect CE and how
these may be mitigated."
12423,"Such solutions could eliminate        6.1  Limitations and further research
scenarios where people do not want to discriminate, but they
do, though unintentional bias when the AI algorithms get          The volume of data generated from edge devices creates
biased data and make biased decisions.","the use of less biased/more inclusive data such as multi-eth-
ics, and multi-gender data.","The proposed solu-         diverse challenges in developing data strategies for training
tions can be applied in some obvious examples where data          AI algorithms that can operate with lower memory require-
comes from unhealthy stereotypes.",2022-08-30 15:36:51+00:00,Advancing the cybersecurity of the healthcare system with self-optimising and self-adaptative artificial intelligence (part 2),cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Petar Radanliev'), arxiv.Result.Author('David De Roure')]","This article advances the knowledge on teaching and training new artificial
intelligence algorithms, for securing, preparing, and adapting the healthcare
system to cope with future pandemics. The core objective is to develop a
concept healthcare system supported by autonomous artificial intelligence that
can use edge health devices with real-time data. The article constructs two
case scenarios for applying cybersecurity with autonomous artificial
intelligence for (1) self-optimising predictive cyber risk analytics of
failures in healthcare systems during a Disease X event (i.e., undefined future
pandemic), and (2) self-adaptive forecasting of medical production and supply
chain bottlenecks during future pandemics. To construct the two testing
scenarios, the article uses the case of Covid-19 to synthesise data for the
algorithms i.e., for optimising and securing digital healthcare systems in
anticipation of disease X. The testing scenarios are built to tackle the
logistical challenges and disruption of complex production and supply chains
for vaccine distribution with optimisation algorithms."
12466,"Apart from that,
recommendations, shared below, for software practitioners            practitioners have mentioned other human aspects such as
involved in RE activities and the wider SE research com-             empathy, geographic distribution, gender, communication
munity for further research into the personality impact of           skills, and other factors such as project domain and work
RE/SE.","We have framed these as a set of          2, culture is the most mentioned aspect.",experience.,2022-10-13 01:25:00+00:00,Does personality impact requirements engineering Activities?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Dulaji Hidellaarachchi'), arxiv.Result.Author('John Grundy'), arxiv.Result.Author('Rashina Hoda'), arxiv.Result.Author('Ingo Mueller')]","Context: Requirements engineering (RE) is an important part of Software
Engineering (SE), consisting of various human-centric activities that require
frequent collaboration of a variety of roles. Prior research has shown that
personality is one such human aspect that has a huge impact on the success of a
software project. However, a limited number of empirical studies exist focusing
on the impact of personality on RE activities. Objective: The objective of this
study is to explore and identify the impact of personality on RE activities,
provide a better understanding of these impacts, and to provide guidance on how
to better handle these impacts in RE. Method: We used a mixed-methods approach,
including a personality test-based survey (50 participants) and an in-depth
interview study (15 participants) with software practitioners from around the
world involved in RE activities. Results: Through personality profiles, we
found a majority of the practitioners scored as statistically significant
(high-scored) on agreeableness and conscientiousness traits and average on
extraversion and neuroticism traits. Through analysis of the interviews, we
found a range of impacts related to the personality traits of software
practitioners, their team members, and external stakeholders. These impacts can
be positive or negative, depending on the RE activities, the overall software
development process, and the people involved in these activities. Moreover, we
found a set of strategies that can be applied to mitigate the negative impact
of personality on RE activities. Conclusion: Our identified impacts of
personality on RE activities and mitigation strategies serve to provide
guidance to software practitioners on handling such possible personality
impacts on RE activities and for researchers to investigate these impacts in
greater depth in future."
13065,"To facilitate further research, we have made the source code and dataset publicly available [15].","• We evaluate O2NMatcher through several experiments and results show our method can achieve a 6% improve-
        ment in detecting inlined functions for existing binary2source matching works.","2 OVERVIEW
In this section, we will introduce the workflow of O2NMatcher.",2022-10-27 03:57:12+00:00,Comparing One with Many -- Solving Binary2source Function Matching Under Function Inlining,cs.SE,['cs.SE'],"[arxiv.Result.Author('Ang Jia'), arxiv.Result.Author('Ming Fan'), arxiv.Result.Author('Xi Xu'), arxiv.Result.Author('Wuxia Jin'), arxiv.Result.Author('Haijun Wang'), arxiv.Result.Author('Qiyi Tang'), arxiv.Result.Author('Sen Nie'), arxiv.Result.Author('Shi Wu'), arxiv.Result.Author('Ting Liu')]","Binary2source function matching is a fundamental task for many security
applications, including Software Component Analysis (SCA). The ""1-to-1""
mechanism has been applied in existing binary2source matching works, in which
one binary function is matched against one source function. However, we
discovered that such mapping could be ""1-to-n"" (one query binary function maps
multiple source functions), due to the existence of function inlining.
  To help conduct binary2source function matching under function inlining, we
propose a method named O2NMatcher to generate Source Function Sets (SFSs) as
the matching target for binary functions with inlining. We first propose a
model named ECOCCJ48 for inlined call site prediction. To train this model, we
leverage the compilable OSS to generate a dataset with labeled call sites
(inlined or not), extract several features from the call sites, and design a
compiler-opt-based multi-label classifier by inspecting the inlining
correlations between different compilations. Then, we use this model to predict
the labels of call sites in the uncompilable OSS projects without compilation
and obtain the labeled function call graphs of these projects. Next, we regard
the construction of SFSs as a sub-tree generation problem and design root node
selection and edge extension rules to construct SFSs automatically. Finally,
these SFSs will be added to the corpus of source functions and compared with
binary functions with inlining. We conduct several experiments to evaluate the
effectiveness of O2NMatcher and results show our method increases the
performance of existing works by 6% and exceeds all the state-of-the-art works."
13088,"Details of this issue
                                                                            will be delayed to further research.",as a starting condition of the microwave.,Fig.,2022-10-27 13:13:07+00:00,Lupascian Non-Negativity Applied to Conceptual Modeling: Alternating Static Potentiality and Dynamic Actuality,cs.SE,['cs.SE'],[arxiv.Result.Author('Sabah Al-Fedaghi')],"In software engineering, conceptual modeling focuses on creating
representations of the world that are as faithful and rich as possible, with
the aim of guiding the development of software systems. In contrast, in the
computing realm, the notion of ontology has been characterized as being closely
related to conceptual modeling and is often viewed as a specification of a
conceptualization. Accordingly, conceptual modeling and ontology engineering
now address the same problem of representing the world in a suitable fashion. A
high-level ontology provides a means to describe concepts and their
interactions with each other and to capture structural and behavioral features
in the intended domain. This paper aims to analyze ontological concepts and
semantics of modeling notations to provide a common understanding among
software engineers. An important issue in this context concerns the question of
whether the modeled world might be stratified into ontological levels. We
introduce an abstract system of two-level domain ontology to be used as a
foundation for conceptual models. We study the two levels of staticity and
dynamics in the context of the thinging machine (TM) model using the notions of
potentiality and actuality that the Franco-Romanian philosopher Stephane
Lupasco developed in logic. He provided a quasi-universal rejection of
contradiction where every event was always associated with a no event, such
that the actualization of an event entails the potentialization of a no event
and vice versa without either ever disappearing completely. This approach is
illustrated by re-modeling UML state machines in TM modeling. The results
strengthen the semantics of a static versus dynamic levels in conceptual
modeling and sharpen the notion of events as a phenomenon without negativity
alternating between the two levels of dynamics and staticity."
13136,"These insights have implications for
                                        further research, offer concrete advice to practitioners, provide guidance for future tool design, and suggest ways of educating future
                                        software architects.","In studying these aspects,
                                        we identify 12 observations related to both technical aspects and social aspects of the meetings.","Index Terms—Software architecture, software architects, whiteboard meetings, architecture documentation, interviews, survey

                                                                                                        !",2022-10-28 12:12:17+00:00,Let's Go to the Whiteboard (Again):Perceptions from Software Architects on Whiteboard Architecture Meetings,cs.SE,['cs.SE'],"[arxiv.Result.Author('Eduardo Santana de Almeida'), arxiv.Result.Author('Iftekhar Ahmed'), arxiv.Result.Author('Andre van der Hoek')]","The whiteboard plays a crucial role in the day-to-day lives of software
architects, as they frequently will organize meetings at the whiteboard to
discuss a new architecture, some proposed changes to the architecture, a
mismatch between the architecture and the code, and more. While much has been
studied about software architects, the architectures they produce, and how they
produce them, a detailed understanding of these whiteboards meetings is still
lacking. In this paper, we contribute a mixed-methods study involving
semi-structured interviews and a subsequent survey to understand the
perceptions of software architects on whiteboard architecture meetings. We
focus on five aspects: (1) why do they hold these meetings, what is the impact
of the experience levels of the participants in these meetings, how do the
architects document the meetings, what kinds of changes are made after the
meetings have concluded and their results are moved to implementation, and what
role do digital whiteboards plays? In studying these aspects, we identify 12
observations related to both technical aspects and social aspects of the
meetings. These insights have implications for further research, offer concrete
advice to practitioners, provide guidance for future tool design, and suggest
ways of educating future software architects."
13137,"Our study is the ﬁrst study of this kind, with the
not have been sufﬁciently representative (e.g., additional as-   ﬁndings giving rise to further study, offering concrete advice
pects that change from sketch to implementation, additional      for practicing architects, providing guidance for future tool
approaches to document whiteboard software architecture          design, and suggesting new topics for educating future
meetings).","Because        they witness when the outcomes of the whiteboard meet-
they were derived from the answers from the interviews,          ings transition to implementation and the reasons for those
a possibility exists that the questions on the survey might      changes.",This was mitigated by the ability for the sur-        software architects.,2022-10-28 12:12:17+00:00,Let's Go to the Whiteboard (Again):Perceptions from Software Architects on Whiteboard Architecture Meetings,cs.SE,['cs.SE'],"[arxiv.Result.Author('Eduardo Santana de Almeida'), arxiv.Result.Author('Iftekhar Ahmed'), arxiv.Result.Author('Andre van der Hoek')]","The whiteboard plays a crucial role in the day-to-day lives of software
architects, as they frequently will organize meetings at the whiteboard to
discuss a new architecture, some proposed changes to the architecture, a
mismatch between the architecture and the code, and more. While much has been
studied about software architects, the architectures they produce, and how they
produce them, a detailed understanding of these whiteboards meetings is still
lacking. In this paper, we contribute a mixed-methods study involving
semi-structured interviews and a subsequent survey to understand the
perceptions of software architects on whiteboard architecture meetings. We
focus on five aspects: (1) why do they hold these meetings, what is the impact
of the experience levels of the participants in these meetings, how do the
architects document the meetings, what kinds of changes are made after the
meetings have concluded and their results are moved to implementation, and what
role do digital whiteboards plays? In studying these aspects, we identify 12
observations related to both technical aspects and social aspects of the
meetings. These insights have implications for further research, offer concrete
advice to practitioners, provide guidance for future tool design, and suggest
ways of educating future software architects."
13185,"Our proposed defense framework CodeDetector is also open-sourced and publicly available5 to provide
the support for further research for SE researchers and practitioners.","Through this work, we call for the attention of SE researchers and practitioners to
notice the poison attack during training new DL models for source code and design more advanced defense
techniques.",Paper Organization.,2022-10-31 03:06:40+00:00,Poison Attack and Defense on Deep Source Code Processing Models,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Zhuo Li'), arxiv.Result.Author('Huangzhao Zhang'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Zhi Jin'), arxiv.Result.Author('Xing Hu'), arxiv.Result.Author('Xin Xia')]","In the software engineering community, deep learning (DL) has recently been
applied to many source code processing tasks. Due to the poor interpretability
of DL models, their security vulnerabilities require scrutiny. Recently,
researchers have identified an emergent security threat, namely poison attack.
The attackers aim to inject insidious backdoors into models by poisoning the
training data with poison samples. Poisoned models work normally with clean
inputs but produce targeted erroneous results with poisoned inputs embedded
with triggers. By activating backdoors, attackers can manipulate the poisoned
models in security-related scenarios.
  To verify the vulnerability of existing deep source code processing models to
the poison attack, we present a poison attack framework for source code named
CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable
even human-imperceptible poison samples and attack models by poisoning the
training data with poison samples. To defend against the poison attack, we
further propose an effective defense approach named CodeDetector to detect
poison samples in the training data. CodeDetector can be applied to many model
architectures and effectively defend against multiple poison attack approaches.
We apply our CodePoisoner and CodeDetector to three tasks, including defect
detection, clone detection, and code repair. The results show that (1)
CodePoisoner achieves a high attack success rate (max: 100%) in misleading
models to targeted erroneous behaviors. It validates that existing deep source
code processing models have a strong vulnerability to the poison attack. (2)
CodeDetector effectively defends against multiple poison attack approaches by
detecting (max: 100%) poison samples in the training data. We hope this work
can help practitioners notice the poison attack and inspire the design of more
advanced defense techniques."
13186,"[60] also released a
defect detection dataset to facilitate further researchs.",Zhou et al.,"Later, some studies [15, 32, 49] further leveraged graph neural
network (GNN) to represent the control, data, and call dependencies of a program for defect detection.",2022-10-31 03:06:40+00:00,Poison Attack and Defense on Deep Source Code Processing Models,cs.SE,"['cs.SE', 'cs.AI']","[arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Zhuo Li'), arxiv.Result.Author('Huangzhao Zhang'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Zhi Jin'), arxiv.Result.Author('Xing Hu'), arxiv.Result.Author('Xin Xia')]","In the software engineering community, deep learning (DL) has recently been
applied to many source code processing tasks. Due to the poor interpretability
of DL models, their security vulnerabilities require scrutiny. Recently,
researchers have identified an emergent security threat, namely poison attack.
The attackers aim to inject insidious backdoors into models by poisoning the
training data with poison samples. Poisoned models work normally with clean
inputs but produce targeted erroneous results with poisoned inputs embedded
with triggers. By activating backdoors, attackers can manipulate the poisoned
models in security-related scenarios.
  To verify the vulnerability of existing deep source code processing models to
the poison attack, we present a poison attack framework for source code named
CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable
even human-imperceptible poison samples and attack models by poisoning the
training data with poison samples. To defend against the poison attack, we
further propose an effective defense approach named CodeDetector to detect
poison samples in the training data. CodeDetector can be applied to many model
architectures and effectively defend against multiple poison attack approaches.
We apply our CodePoisoner and CodeDetector to three tasks, including defect
detection, clone detection, and code repair. The results show that (1)
CodePoisoner achieves a high attack success rate (max: 100%) in misleading
models to targeted erroneous behaviors. It validates that existing deep source
code processing models have a strong vulnerability to the poison attack. (2)
CodeDetector effectively defends against multiple poison attack approaches by
detecting (max: 100%) poison samples in the training data. We hope this work
can help practitioners notice the poison attack and inspire the design of more
advanced defense techniques."
13350,"can help performance on specialized hardware     researchers should extend further research on mining code commits
        needed for XR applications.","Moreover,
        tracing, etc.)","and UE documentation to identify more performance issues XR
     • Framerate settings are also of great importance depending        applications
        on the XR application being developed.",2022-11-03 17:27:36+00:00,Analyzing Performance Issues of Virtual Reality Applications,cs.SE,['cs.SE'],"[arxiv.Result.Author('Jason Hogan'), arxiv.Result.Author('Aaron Salo'), arxiv.Result.Author('Dhia Elhaq Rzig'), arxiv.Result.Author('Foyzul Hassan'), arxiv.Result.Author('Bruce Maxim')]","Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)
and Mixed Reality (MR). XR is an emerging technology that simulates a realistic
environment for users. XR techniques have provided revolutionary user
experiences in various application scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). Due to the
high computational cost of rendering real-time animation in limited-resource
devices and constant interaction with user activity, XR applications often face
performance bottlenecks, and these bottlenecks create a negative impact on the
user experience of XR software. Thus, performance optimization plays an
essential role in many industry-standard XR applications. Even though
identifying performance bottlenecks in traditional software (e.g., desktop
applications) is a widely explored topic, those approaches cannot be directly
applied within XR software due to the different nature of XR applications.
Moreover, XR applications developed in different frameworks such as Unity and
Unreal Engine show different performance bottleneck patterns and thus,
bottleneck patterns of Unity projects can't be applied for Unreal Engine
(UE)-based XR projects. To fill the knowledge gap for XR performance
optimizations of Unreal Engine-based XR projects, we present the first
empirical study on performance optimizations from seven UE XR projects, 78 UE
XR discussion issues and three sources of UE documentation. Our analysis
identified 14 types of performance bugs, including 12 types of bugs related to
UE settings issues and two types of CPP source code-related issues. To further
assist developers in detecting performance bugs based on the identified bug
patterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect
performance bugs in both configuration files and source code."
13414,"We hypothesise that the
latter is the case, but further study is needed to obtain deeper insight.","Yet, another explanation may be that self-adaptation has not
yet been fully utilised in industry to deal with bigger system changes.","The four classic management tasks of self-adaptation studied by researchers (self-healing, self-
optimising, self-protecting, and self-configuring) are also relevant to practitioners.",2022-11-06 13:34:29+00:00,Self-Adaptation in Industry: A Survey,cs.SE,"['cs.SE', 'D2']","[arxiv.Result.Author('Danny Weyns'), arxiv.Result.Author('Ilias Gerostathopoulos'), arxiv.Result.Author('Nadeem Abbas'), arxiv.Result.Author('Jesper Andersson'), arxiv.Result.Author('Stefan Biffl'), arxiv.Result.Author('Premek Brada'), arxiv.Result.Author('Tomas Bures'), arxiv.Result.Author('Amleto Di Salle'), arxiv.Result.Author('Matthias Galster'), arxiv.Result.Author('Patricia Lago'), arxiv.Result.Author('Grace Lewis'), arxiv.Result.Author('Marin Litoiu'), arxiv.Result.Author('Angelika Musil'), arxiv.Result.Author('Juergen Musil'), arxiv.Result.Author('Panos Patros'), arxiv.Result.Author('Patrizio Pelliccione')]","Computing systems form the backbone of many areas in our society, from
manufacturing to traffic control, healthcare, and financial systems. When
software plays a vital role in the design, construction, and operation, these
systems are referred as software-intensive systems. Self-adaptation equips a
software-intensive system with a feedback loop that either automates tasks that
otherwise need to be performed by human operators or deals with uncertain
conditions. Such feedback loops have found their way to a variety of practical
applications; typical examples are an elastic cloud to adapt computing
resources and automated server management to respond quickly to business needs.
To gain insight into the motivations for applying self-adaptation in practice,
the problems solved using self-adaptation and how these problems are solved,
and the difficulties and risks that industry faces in adopting self-adaptation,
we performed a large-scale survey. We received 184 valid responses from
practitioners spread over 21 countries. Based on the analysis of the survey
data, we provide an empirically grounded overview of state-of-the-practice in
the application of self-adaptation. From that, we derive insights for
researchers to check their current research with industrial needs, and for
practitioners to compare their current practice in applying self-adaptation.
These insights also provide opportunities for the application of
self-adaptation in practice and pave the way for future industry-research
collaborations."
13589,"The resulting graph can be used
as an entry point for further research to better include speciﬁcs of certain plat-
forms.",Future Work and Research Directions.,"One example is the analysis of the LLVM-IR emitted by XCode for apps
written in Apple’s programming languages Swift or Objective-C. Their calling
conventions diﬀer signiﬁcantly from other programming languages.",2022-11-09 09:37:30+00:00,Representing LLVM-IR in a Code Property Graph,cs.SE,"['cs.SE', 'cs.CR', 'cs.PL']","[arxiv.Result.Author('Alexander Küchler'), arxiv.Result.Author('Christian Banse')]","In the past years, a number of static application security testing tools have
been proposed which make use of so-called code property graphs, a graph model
which keeps rich information about the source code while enabling its user to
write language-agnostic analyses. However, they suffer from several
shortcomings. They work mostly on source code and exclude the analysis of
third-party dependencies if they are only available as compiled binaries.
Furthermore, they are limited in their analysis to whether an individual
programming language is supported or not. While often support for
well-established languages such as C/C++ or Java is included, languages that
are still heavily evolving, such as Rust, are not considered because of the
constant changes in the language design. To overcome these limitations, we
extend an open source implementation of a code property graph to support
LLVM-IR which can be used as output by many compilers and binary lifters. In
this paper, we discuss how we address challenges that arise when mapping
concepts of an intermediate representation to a CPG. At the same time, we
optimize the resulting graph to be minimal and close to the representation of
equivalent source code. Our evaluation indicates that existing analyses can be
reused without modifications and that the performance requirements are
comparable to operating on source code. This makes the approach suitable for an
analysis of large-scale projects."
13590,"Hence, further research
should study which gaps still exist to apply existing tools to lifted binaries.","However, lifted or decompiled binaries still suﬀer from a lack of
information which are crucial for a security analysis [9].",Generalizability.,2022-11-09 09:37:30+00:00,Representing LLVM-IR in a Code Property Graph,cs.SE,"['cs.SE', 'cs.CR', 'cs.PL']","[arxiv.Result.Author('Alexander Küchler'), arxiv.Result.Author('Christian Banse')]","In the past years, a number of static application security testing tools have
been proposed which make use of so-called code property graphs, a graph model
which keeps rich information about the source code while enabling its user to
write language-agnostic analyses. However, they suffer from several
shortcomings. They work mostly on source code and exclude the analysis of
third-party dependencies if they are only available as compiled binaries.
Furthermore, they are limited in their analysis to whether an individual
programming language is supported or not. While often support for
well-established languages such as C/C++ or Java is included, languages that
are still heavily evolving, such as Rust, are not considered because of the
constant changes in the language design. To overcome these limitations, we
extend an open source implementation of a code property graph to support
LLVM-IR which can be used as output by many compilers and binary lifters. In
this paper, we discuss how we address challenges that arise when mapping
concepts of an intermediate representation to a CPG. At the same time, we
optimize the resulting graph to be minimal and close to the representation of
equivalent source code. Our evaluation indicates that existing analyses can be
reused without modifications and that the performance requirements are
comparable to operating on source code. This makes the approach suitable for an
analysis of large-scale projects."
13591,"The resulting graph can be used
as an entry point for further research to better include speciﬁcs of certain plat-
forms.",Future Work and Research Directions.,"One example is the analysis of the LLVM-IR emitted by XCode for apps
written in Apple’s programming languages Swift or Objective-C. Their calling
conventions diﬀer signiﬁcantly from other programming languages.",2022-11-09 09:37:30+00:00,Representing LLVM-IR in a Code Property Graph,cs.SE,"['cs.SE', 'cs.CR', 'cs.PL']","[arxiv.Result.Author('Alexander Küchler'), arxiv.Result.Author('Christian Banse')]","In the past years, a number of static application security testing tools have
been proposed which make use of so-called code property graphs, a graph model
which keeps rich information about the source code while enabling its user to
write language-agnostic analyses. However, they suffer from several
shortcomings. They work mostly on source code and exclude the analysis of
third-party dependencies if they are only available as compiled binaries.
Furthermore, they are limited in their analysis to whether an individual
programming language is supported or not. While often support for
well-established languages such as C/C++ or Java is included, languages that
are still heavily evolving, such as Rust, are not considered because of the
constant changes in the language design. To overcome these limitations, we
extend an open source implementation of a code property graph to support
LLVM-IR which can be used as output by many compilers and binary lifters. In
this paper, we discuss how we address challenges that arise when mapping
concepts of an intermediate representation to a CPG. At the same time, we
optimize the resulting graph to be minimal and close to the representation of
equivalent source code. Our evaluation indicates that existing analyses can be
reused without modifications and that the performance requirements are
comparable to operating on source code. This makes the approach suitable for an
analysis of large-scale projects."
13592,"Hence, further research
should study which gaps still exist to apply existing tools to lifted binaries.","However, lifted or decompiled binaries still suﬀer from a lack of
information which are crucial for a security analysis [9].","17
Generalizability.",2022-11-09 09:37:30+00:00,Representing LLVM-IR in a Code Property Graph,cs.SE,"['cs.SE', 'cs.CR', 'cs.PL']","[arxiv.Result.Author('Alexander Küchler'), arxiv.Result.Author('Christian Banse')]","In the past years, a number of static application security testing tools have
been proposed which make use of so-called code property graphs, a graph model
which keeps rich information about the source code while enabling its user to
write language-agnostic analyses. However, they suffer from several
shortcomings. They work mostly on source code and exclude the analysis of
third-party dependencies if they are only available as compiled binaries.
Furthermore, they are limited in their analysis to whether an individual
programming language is supported or not. While often support for
well-established languages such as C/C++ or Java is included, languages that
are still heavily evolving, such as Rust, are not considered because of the
constant changes in the language design. To overcome these limitations, we
extend an open source implementation of a code property graph to support
LLVM-IR which can be used as output by many compilers and binary lifters. In
this paper, we discuss how we address challenges that arise when mapping
concepts of an intermediate representation to a CPG. At the same time, we
optimize the resulting graph to be minimal and close to the representation of
equivalent source code. Our evaluation indicates that existing analyses can be
reused without modifications and that the performance requirements are
comparable to operating on source code. This makes the approach suitable for an
analysis of large-scale projects."
13617,"10  Suvodeep Majumder, Joymallya Chakraborty, Tim Menzies

These methods mostly use computationally-expensive deep neural networks
and are excluded from further study in this paper.","There are other models as well, including perturbation-based or Generative
model-based methods, which fall under intrinsically semi-supervised methods.","2.2.4 Graph-Based Methods:

The intuition with graph-based methods is that we do not need to label all
data.",2022-11-10 23:39:12+00:00,"When Less is More: On the Value of ""Co-training"" for Semi-Supervised Software Defect Predictors",cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Suvodeep Majumder'), arxiv.Result.Author('Joymallya Chakraborty'), arxiv.Result.Author('Tim Menzies')]","Labeling a module defective or non-defective is an expensive task. Hence,
there are often limits on how much-labeled data is available for training.
Semi-supervised classifiers use far fewer labels for training models, but there
are numerous semi-supervised methods, including self-labeling, co-training,
maximal-margin, and graph-based methods, to name a few. Only a handful of these
methods have been tested in SE for (e.g.) predicting defects and even that,
those tests have been on just a handful of projects. This paper takes a wide
range of 55 semi-supervised learners and applies these to over 714 projects. We
find that semi-supervised ""co-training methods"" work significantly better than
other approaches. However, co-training needs to be used with caution since the
specific choice of co-training methods needs to be carefully selected based on
a user's specific goals. Also, we warn that a commonly-used co-training method
(""multi-view""-- where different learners get different sets of columns) does
not improve predictions (while adding too much to the run time costs 11 hours
vs. 1.8 hours). Those cautions stated, we find using these ""co-trainers,"" we
can label just 2.5% of data, then make predictions that are competitive to
those using 100% of the data. It is an open question worthy of future work to
test if these reductions can be seen in other areas of software analytics.
  All the codes used and datasets analyzed during the current study are
available in the https://GitHub.com/Suvodeep90/Semi_Supervised_Methods."
13640,"Our proposed theory serves as the ﬁrst guide toward further research in
this area.","Nevertheless,
                                        measuring and tracking requirements engineering debt are immature in prac-

                                             ∗Corresponding author

                                        Preprint submitted to Journal of Information and Software Technology November 14, 2022
tice.","Keywords: Requirements Engineering, Requirements Engineering Debt,
Interview Study, Online Survey, Theory

1.",2022-11-11 13:24:22+00:00,An initial Theory to Understand and Manage Requirements Engineering Debt in Practice,cs.SE,['cs.SE'],"[arxiv.Result.Author('Julian Frattini'), arxiv.Result.Author('Davide Fucci'), arxiv.Result.Author('Daniel Mendez'), arxiv.Result.Author('Rodrigo Spinola'), arxiv.Result.Author('Vladimir Mandic'), arxiv.Result.Author('Nebojsa Tausan'), arxiv.Result.Author('Muhammad Ovais Ahmad'), arxiv.Result.Author('Javier Gonzalez-Huerta')]","Context: Advances in technical debt research demonstrate the benefits of
applying the financial debt metaphor to support decision-making in software
development activities. Although decision-making during requirements
engineering has significant consequences, the debt metaphor in requirements
engineering is inadequately explored. Objective: We aim to conceptualize how
the debt metaphor applies to requirements engineering by organizing concepts
related to practitioners' understanding and managing of requirements
engineering debt (RED). Method: We conducted two in-depth expert interviews to
identify key requirements engineering debt concepts and construct a survey
instrument. We surveyed 69 practitioners worldwide regarding their perception
of the concepts and developed an initial analytical theory. Results: We propose
a RED theory that aligns key concepts from technical debt research but
emphasizes the specific nature of requirements engineering. In particular, the
theory consists of 23 falsifiable propositions derived from the literature, the
interviews, and survey results. Conclusions: The concepts of requirements
engineering debt are perceived to be similar to their technical debt
counterpart. Nevertheless, measuring and tracking requirements engineering debt
are immature in practice. Our proposed theory serves as the first guide toward
further research in this area."
13641,"Third, we use
the 69 survey responses from practitioners to develop a descriptive, evidence-
based theory of RED serving as the conceptual foundation for further research.","Second, we develop
a questionnaire based on the interview results and conduct an online survey to
gauge practitioners’ perceptions of the identiﬁed RED concepts.","We make the following contributions:

   1.",2022-11-11 13:24:22+00:00,An initial Theory to Understand and Manage Requirements Engineering Debt in Practice,cs.SE,['cs.SE'],"[arxiv.Result.Author('Julian Frattini'), arxiv.Result.Author('Davide Fucci'), arxiv.Result.Author('Daniel Mendez'), arxiv.Result.Author('Rodrigo Spinola'), arxiv.Result.Author('Vladimir Mandic'), arxiv.Result.Author('Nebojsa Tausan'), arxiv.Result.Author('Muhammad Ovais Ahmad'), arxiv.Result.Author('Javier Gonzalez-Huerta')]","Context: Advances in technical debt research demonstrate the benefits of
applying the financial debt metaphor to support decision-making in software
development activities. Although decision-making during requirements
engineering has significant consequences, the debt metaphor in requirements
engineering is inadequately explored. Objective: We aim to conceptualize how
the debt metaphor applies to requirements engineering by organizing concepts
related to practitioners' understanding and managing of requirements
engineering debt (RED). Method: We conducted two in-depth expert interviews to
identify key requirements engineering debt concepts and construct a survey
instrument. We surveyed 69 practitioners worldwide regarding their perception
of the concepts and developed an initial analytical theory. Results: We propose
a RED theory that aligns key concepts from technical debt research but
emphasizes the specific nature of requirements engineering. In particular, the
theory consists of 23 falsifiable propositions derived from the literature, the
interviews, and survey results. Conclusions: The concepts of requirements
engineering debt are perceived to be similar to their technical debt
counterpart. Nevertheless, measuring and tracking requirements engineering debt
are immature in practice. Our proposed theory serves as the first guide toward
further research in this area."
13656,"This feature is not part of the requirement matrix to compare the diﬀerent tools, but we
consider a documentation of these platforms in the subsequent section as a good starting point for further research (exchange).","in a repository), it might be beneﬁcial to provide a platform that allows to publish documented workﬂows with
a search and versioning functionality.","4 SIMPLE USE CASE

A simple exemplary use case was deﬁned in order to analyze and evaluate the diﬀerent workﬂow tools with respect to the re-
quirements stated in section 3.",2022-11-10 14:11:31+00:00,"Evaluation of tools for describing, reproducing and reusing scientific workflows",cs.SE,['cs.SE'],"[arxiv.Result.Author('Philipp Diercks'), arxiv.Result.Author('Dennis Gläser'), arxiv.Result.Author('Ontje Lünsdorf'), arxiv.Result.Author('Michael Selzer'), arxiv.Result.Author('Bernd Flemisch'), arxiv.Result.Author('Jörg F. Unger')]","In the field of computational science and engineering, workflows often entail
the application of various software, for instance, for simulation or pre- and
postprocessing. Typically, these components have to be combined in arbitrarily
complex workflows to address a specific research question. In order for peer
researchers to understand, reproduce and (re)use the findings of a scientific
publication, several challenges have to be addressed. For instance, the
employed workflow has to be automated and information on all used software must
be available for a reproduction of the results. Moreover, the results must be
traceable and the workflow documented and readable to allow for external
verification and greater trust. In this paper, existing workflow management
systems (WfMSs) are discussed regarding their suitability for describing,
reproducing and reusing scientific workflows. To this end, a set of general
requirements for WfMSswere deduced from user stories that we deem relevant in
the domain of computational science and engineering. On the basis of an
exemplary workflow implementation, publicly hosted at GitHub (https://
github.com/BAMresearch/NFDI4IngScientificWorkflowRequirements), a selection of
different WfMSs is compared with respect to these requirements, to support
fellow scientists in identifying the WfMSs that best suit their requirements."
13690,"In view of this, we suggest that further research is needed to understand
the range of technical and socio-technical implications of this emerging area.","In the webinars,
however, speakers were keen on promoting AI-based tools and services [W27,
W31].","Collaborative Application Security Testing for DevSecOps  41

Gap area: Collaborative intelligence via Human-AI collaboration is an area
rapidly gaining practitioner interest.",2022-11-13 16:54:49+00:00,"Collaborative Application Security Testing for DevSecOps: An Empirical Analysis of Challenges, Best Practices and Tool Support",cs.SE,['cs.SE'],"[arxiv.Result.Author('Roshan Namal Rajapakse'), arxiv.Result.Author('Mansooreh Zahedi'), arxiv.Result.Author('Muhammad Ali Babar')]","DevSecOps is a software development paradigm that places a high emphasis on
the culture of collaboration between developers (Dev), security (Sec) and
operations (Ops) teams to deliver secure software continuously and rapidly.
Adopting this paradigm effectively, therefore, requires an understanding of the
challenges, best practices and available solutions for collaboration among
these functional teams. However, collaborative aspects related to these teams
have received very little empirical attention in the DevSecOps literature.
Hence, we present a study focusing on a key security activity, Application
Security Testing (AST), in which practitioners face difficulties performing
collaborative work in a DevSecOps environment. Our study made novel use of 48
systematically selected webinars, technical talks and panel discussions as a
data source to qualitatively analyse software practitioner discussions on the
most recent trends and emerging solutions in this highly evolving field. We
find that the lack of features that facilitate collaboration built into the AST
tools themselves is a key tool-related challenge in DevSecOps. In addition, the
lack of clarity related to role definitions, shared goals, and ownership also
hinders Collaborative AST (CoAST). We also captured a range of best practices
for collaboration (e.g., Shift-left security), emerging communication methods
(e.g., ChatOps), and new team structures (e.g., hybrid teams) for CoAST.
Finally, our study identified several requirements for new tool features and
specific gap areas for future research to provide better support for CoAST in
DevSecOps."
13691,"Based on our analysis, we point out the below speciﬁc feature requirements
for new tool developments and gap areas for further research in this area.","We also captured several existing (e.g.,
communication platforms and team collaboration software) and emerging tool
types (e.g., Automated Bots, IAST) that better facilitate CoAST.",– There are many communications and collaboration platforms available.,2022-11-13 16:54:49+00:00,"Collaborative Application Security Testing for DevSecOps: An Empirical Analysis of Challenges, Best Practices and Tool Support",cs.SE,['cs.SE'],"[arxiv.Result.Author('Roshan Namal Rajapakse'), arxiv.Result.Author('Mansooreh Zahedi'), arxiv.Result.Author('Muhammad Ali Babar')]","DevSecOps is a software development paradigm that places a high emphasis on
the culture of collaboration between developers (Dev), security (Sec) and
operations (Ops) teams to deliver secure software continuously and rapidly.
Adopting this paradigm effectively, therefore, requires an understanding of the
challenges, best practices and available solutions for collaboration among
these functional teams. However, collaborative aspects related to these teams
have received very little empirical attention in the DevSecOps literature.
Hence, we present a study focusing on a key security activity, Application
Security Testing (AST), in which practitioners face difficulties performing
collaborative work in a DevSecOps environment. Our study made novel use of 48
systematically selected webinars, technical talks and panel discussions as a
data source to qualitatively analyse software practitioner discussions on the
most recent trends and emerging solutions in this highly evolving field. We
find that the lack of features that facilitate collaboration built into the AST
tools themselves is a key tool-related challenge in DevSecOps. In addition, the
lack of clarity related to role definitions, shared goals, and ownership also
hinders Collaborative AST (CoAST). We also captured a range of best practices
for collaboration (e.g., Shift-left security), emerging communication methods
(e.g., ChatOps), and new team structures (e.g., hybrid teams) for CoAST.
Finally, our study identified several requirements for new tool features and
specific gap areas for future research to provide better support for CoAST in
DevSecOps."
13692,"In view of this, we suggest that further research is needed to understand
the range of technical and socio-technical implications of this emerging area.","In the webinars,
however, speakers were keen on promoting AI-based tools and services [W27,
W31].","Collaborative Application Security Testing for DevSecOps  41

Gap area: Collaborative intelligence via Human-AI collaboration is an area
rapidly gaining practitioner interest.",2022-11-13 16:54:49+00:00,"Collaborative Application Security Testing for DevSecOps: An Empirical Analysis of Challenges, Best Practices and Tool Support",cs.SE,['cs.SE'],"[arxiv.Result.Author('Roshan Namal Rajapakse'), arxiv.Result.Author('Mansooreh Zahedi'), arxiv.Result.Author('Muhammad Ali Babar')]","DevSecOps is a software development paradigm that places a high emphasis on
the culture of collaboration between developers (Dev), security (Sec) and
operations (Ops) teams to deliver secure software continuously and rapidly.
Adopting this paradigm effectively, therefore, requires an understanding of the
challenges, best practices and available solutions for collaboration among
these functional teams. However, collaborative aspects related to these teams
have received very little empirical attention in the DevSecOps literature.
Hence, we present a study focusing on a key security activity, Application
Security Testing (AST), in which practitioners face difficulties performing
collaborative work in a DevSecOps environment. Our study made novel use of 48
systematically selected webinars, technical talks and panel discussions as a
data source to qualitatively analyse software practitioner discussions on the
most recent trends and emerging solutions in this highly evolving field. We
find that the lack of features that facilitate collaboration built into the AST
tools themselves is a key tool-related challenge in DevSecOps. In addition, the
lack of clarity related to role definitions, shared goals, and ownership also
hinders Collaborative AST (CoAST). We also captured a range of best practices
for collaboration (e.g., Shift-left security), emerging communication methods
(e.g., ChatOps), and new team structures (e.g., hybrid teams) for CoAST.
Finally, our study identified several requirements for new tool features and
specific gap areas for future research to provide better support for CoAST in
DevSecOps."
13693,"Based on our analysis, we point out the below speciﬁc feature requirements
for new tool developments and gap areas for further research in this area.","We also captured several existing (e.g.,
communication platforms and team collaboration software) and emerging tool
types (e.g., Automated Bots, IAST) that better facilitate CoAST.",– There are many communications and collaboration platforms available.,2022-11-13 16:54:49+00:00,"Collaborative Application Security Testing for DevSecOps: An Empirical Analysis of Challenges, Best Practices and Tool Support",cs.SE,['cs.SE'],"[arxiv.Result.Author('Roshan Namal Rajapakse'), arxiv.Result.Author('Mansooreh Zahedi'), arxiv.Result.Author('Muhammad Ali Babar')]","DevSecOps is a software development paradigm that places a high emphasis on
the culture of collaboration between developers (Dev), security (Sec) and
operations (Ops) teams to deliver secure software continuously and rapidly.
Adopting this paradigm effectively, therefore, requires an understanding of the
challenges, best practices and available solutions for collaboration among
these functional teams. However, collaborative aspects related to these teams
have received very little empirical attention in the DevSecOps literature.
Hence, we present a study focusing on a key security activity, Application
Security Testing (AST), in which practitioners face difficulties performing
collaborative work in a DevSecOps environment. Our study made novel use of 48
systematically selected webinars, technical talks and panel discussions as a
data source to qualitatively analyse software practitioner discussions on the
most recent trends and emerging solutions in this highly evolving field. We
find that the lack of features that facilitate collaboration built into the AST
tools themselves is a key tool-related challenge in DevSecOps. In addition, the
lack of clarity related to role definitions, shared goals, and ownership also
hinders Collaborative AST (CoAST). We also captured a range of best practices
for collaboration (e.g., Shift-left security), emerging communication methods
(e.g., ChatOps), and new team structures (e.g., hybrid teams) for CoAST.
Finally, our study identified several requirements for new tool features and
specific gap areas for future research to provide better support for CoAST in
DevSecOps."
13697,"We hope our MLR study will
facilitate further research in Kubernetes.","Furthermore, researchers can use our findings to understand the
pain points of current Kubernetes adoptees, and pursue research efforts that
will resolve industry’s need related to Kubernetes.","A Survey of Kubernetes  41

A Appendix

Index           Table A1: List of 105 Publications for the Multi-vocal Literature Review
P1
P2     Publication
P3
P4     Medel, V´ıctor, Omer Rana, Jos´e Angel Ban˜ares, and Unai Arronategui.",2022-11-13 22:23:13+00:00,"Benefits, Challenges, and Research Topics: A Multi-vocal Literature Review of Kubernetes",cs.SE,['cs.SE'],"[arxiv.Result.Author('Shazibul Islam Shamim'), arxiv.Result.Author('Jonathan Alexander Gibson'), arxiv.Result.Author('Patrick Morrison'), arxiv.Result.Author('Akond Rahman')]","Context: Kubernetes is an open source software that helps in automated
deployment of software and orchestration of containers. With Kubernetes, IT
organizations, such as IBM, Pinterest, and Spotify have experienced an increase
in release frequency. Objective: The goal of this paper is to inform
practitioners and researchers on benefits and challenges of Kubernetes usage by
conducting a multi-vocal literature review of Kubernetes. Methodology: We
conduct a multi-vocal literature review (MLR) where we use 321
Kubernetes-related Internet artifacts to identify benefits and challenges
perceived by practitioners. In our MLR, we also analyze 105 peer-reviewed
publications to identify the research topics addressed by the research
community. Findings: We find 8 benefits that include service level objective
(SLO)-based scalability and self-healing containers. Our identified 15
challenges related to Kubernetes include unavailability of diagnostics and
security tools and attack surface reduction. We observe researchers to address
14 research topics related to Kubernetes, which includes efficient resource
utilization. We also identify 9 challenges that are under-explored in research
publications, which include cultural change, hardware compatibility, learning
curve, maintenance, and testing."
13836,"As we found negative effects for            While we applied rigor in the setup of this research,
‘sandboxes for minimum deployment’ and not much adoption           designing the survey and collecting/analyzing data, there are
among participants, we believe further research is necessary.","some participants indicate that work is less fun because of the
implementation of DevOps.",limitations to this research.,2022-11-17 07:45:59+00:00,A Study of Adoption and Effects of DevOps Practices,cs.SE,['cs.SE'],"[arxiv.Result.Author('Tyron Offerman'), arxiv.Result.Author('Robert Blinde'), arxiv.Result.Author('Christoph Johann Stettina'), arxiv.Result.Author('Joost Visser')]","Many organizations adopt DevOps practices and tools in order to break down
silos within the organization, improve software quality and delivery, and
increase customer satisfaction. However, the impact of the individual practices
on the performance of the organization is not well known. In this paper, we
collect evidence on the effects of DevOps practices and tools on organizational
performance. In an extensive literature search we identified 14 DevOps
practices, consisting of 47 subpractices. Based on these practices, we
conducted a global survey to study their effects in practice, and measure
DevOps maturity. Across 123 respondents, working in 11 different industries, we
found that 13 of the 14 DevOps practices are adopted, determined by 50\% of the
participants indicating that practices are `always', `most of the time', and
'about half of the time' applied. There is a positive correlation between the
adoption of all practices and independently measured maturity. In particular,
practices concerning sandboxes for minimum deployment, test-driven development,
and trunk based development show the lowest correlations in our data. Effects
of software delivery and organizational performance are mainly perceived
positive. Yet, DevOps is also considered by some to have a negative impact such
as respondents mentioning the predictability of product delivery has decreased
and work is less fun. Concluding, our detailed overview of DevOps practices
allows more targeted application of DevOps practices to obtain its positive
effects while minimizing any negative effects."
13839,"According to the authors, there is a need for simple comprehensive global models, due to the
distributed nature of software development, while further research should be conducted to further improve estimation
results derived with ML approaches.","Concerning accuracy metrics, there is an increasing use of Mean Magnitude of
Relative Error (MMRE), Median Magnitude of Relative Error (MdMRE), and Prediction Pred (25%), with 78% of primary
studies employing MMRE.","3 REVIEW METHODS
To conduct this tertiary review, we followed the guidelines outlined by Kitchenham and Charters [83].",2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13840,• Implications for further research and comments concerning the use of ML in SE—to answer RQ2.,"• Application domain in terms of SWEBOK KAs and subareas as well as SE tasks covered by each secondary study—to

  answer RQ1 and RQ2.",• Employed ML techniques—to answer RQ3.,2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13841,"Moreover, we extracted by hand any implications for
further research as well as comments regarding the use of ML in SE that were mentioned in the associated reviews.22 To
do this, we searched the sections of abstract, introduction, results, conclusion, and further research or future directions
(where available) of all secondary studies to identify ML-related research opportunities in each KA.","For this, we used the results of RQ1 to identify
SWEBOK KAs that are insufficiently covered by ML techniques.","Lastly, we extracted
from the same sections any identified issues or obstacles associated with the use of ML techniques in SE.",2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13842,"The sparse coverage of
certain KAs is also recognized by the authors of many secondary studies through their calls for further research on the
application of ML techniques to the associated SE tasks.","This is the case for Software
Quality, SE Process, Software Requirements, and Software Maintenance, which span various subareas, as opposed to SE
Management, which contains an equivalent number of reviews mapped to a single subarea.","In the subsequent sections we provide an extensive description
of how each KA and SE task could be better covered, as evidenced from the authors’ remarks through the process
described in Section 3.6, as well as any issues and obstacles related to the use of ML techniques in SE.",2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13843,"Furthermore, in smell detection further research is needed on: smell types [93]; smell prioritization [1]; the mutual
effect between smells to identify highly correlated ones [1, 17]; the false-positiveness of the proposed ML techniques,
which could be high due to deficient smell definitions [1, 4, 94, 144]; and the effect of data transformations on the smell
detection process [1].","there are needed more empirical analyses on the validity of cross project defect prediction datasets [67, 121, 148].",4.3.3 Software Testing.,2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13844,"Online and incremental ML-based applications in SE provide a fertile ground for further research,
   due to their computational effectiveness, rapid changeability and adaptability, and thanks to recent advances in
   incremental data retrieval methods and tools.",Implication 6.,"The general idea of experimenting with ML approaches applied in different domains and contexts is also supported
by the authors of some secondary studies (Section 4.3).",2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13845,"These include the manual and backward snowballing search processes (Section 3.2), the study
selection (Section 3.4) and quality assessment (Section 3.5), the data extraction process, the classification of the studies
using the SWEBOK KAs and the multi-axis ML scheme, the extraction of the tackled SE tasks by ML using the open
coding practice, the identification of implications for further research in ML4SE, and the detection of the employed
ML techniques in SE (Section 3.6).","Other major threats to the research validity stem from the steps during which we followed manual processes involving
subjective judgment.","The reliability of these processes was improved by engaging multiple raters, and
by basing them on standard research methods.",2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13846,"The
analysis was performed by hand and consisted of: the classification of the reviews using the SWEBOK KAs and subareas;

Manuscript submitted to ACM
ML4SE: A Tertiary Study  29

the extraction of SE tasks tackled with ML from the reviews; the extraction of SE topics for further research using ML
from the reviews; the categorization of the reviews using a four-axis ML classification scheme that was synthesized
from two sources; and the extraction of the ML techniques employed in the reviews.","To analyze
the reviews we followed established guidelines and designed a protocol that was internally agreed by all authors.","Through these manual processes
the following key findings were obtained.",2022-11-17 09:19:53+00:00,Machine Learning for Software Engineering: A Tertiary Study,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Zoe Kotti'), arxiv.Result.Author('Rafaila Galanopoulou'), arxiv.Result.Author('Diomidis Spinellis')]","Machine learning (ML) techniques increase the effectiveness of software
engineering (SE) lifecycle activities. We systematically collected,
quality-assessed, summarized, and categorized 83 reviews in ML for SE published
between 2009-2022, covering 6,117 primary studies. The SE areas most tackled
with ML are software quality and testing, while human-centered areas appear
more challenging for ML. We propose a number of ML for SE research challenges
and actions including: conducting further empirical validation and industrial
studies on ML; reconsidering deficient SE methods; documenting and automating
data collection and pipeline processes; reexamining how industrial
practitioners distribute their proprietary data; and implementing incremental
ML approaches."
13854,"SYSTEMATIC LITERATURE REVIEW
                                        dependency among different topics to have a solid foundation for

                                        accelerating further research and getting actionable practices.","investigating the synergy between (regional) gender aspects and

                                        cultural concerns and considering possible contributions and              II.","We performed a systematic literature review (SLR) to col-
                                           Index Terms—Gender, diversity, inclusiveness, software engi-     lect research and practices about gender aspects in software
                                                                                                            development and study reported by researchers and practi-
                                        neering, software development

                                                                  I.",2022-11-16 14:58:01+00:00,Systematic Literature Review of Gender and Software Engineering in Asia,cs.SE,"['cs.SE', 'cs.GL']",[arxiv.Result.Author('Hironori Washizaki')],"It is essential to discuss the role, difficulties, and opportunities
concerning people of different gender in the field of software engineering
research, education, and industry. Although some literature reviews address
software engineering and gender, it is still unclear how research and practices
in Asia exist for handling gender aspects in software development and
engineering. We conducted a systematic literature review to grasp the
comprehensive view of gender research and practices in Asia. We analyzed the 32
identified papers concerning countries and publication years among 463
publications. Researchers and practitioners from various organizations actively
work on gender research and practices in some countries, including China,
India, and Turkey. We identified topics and classified them into seven
categories varying from personal mental health and team building to
organization. Future research directions include investigating the synergy
between (regional) gender aspects and cultural concerns and considering
possible contributions and dependency among different topics to have a solid
foundation for accelerating further research and getting actionable practices."
13855,"Making a holistic view by clarifying them should be                          of diversity in software engineering: a perspective from the
another research direction, resulting in a solid foundation for                    agile methodologies,” in Proceedings of the 12th International
accelerating further research and getting actionable practices.","Furthermore, gender diversity                        https://doi.org/10.1007/s10664- 021- 09992- 2
in organizational human resource development (T6) can be a
prerequisite for building teams having proper gender diversity                [4] K. K. Silveira and R. Prikladnicki, “A systematic mapping study
(T1).","Workshop on Cooperative and Human Aspects of Software
                                                                                   Engineering, CHASE@ICSE 2019, Montre´al, QC, Canada, 27
   In addition to gender aspects, some of identiﬁed papers ad-                     May 2019, Y. Dittrich, F. Fagerholm, R. Hoda, D. Socha, and
dress various diversity such as personality types [16], cultural                   I. Steinmacher, Eds.",2022-11-16 14:58:01+00:00,Systematic Literature Review of Gender and Software Engineering in Asia,cs.SE,"['cs.SE', 'cs.GL']",[arxiv.Result.Author('Hironori Washizaki')],"It is essential to discuss the role, difficulties, and opportunities
concerning people of different gender in the field of software engineering
research, education, and industry. Although some literature reviews address
software engineering and gender, it is still unclear how research and practices
in Asia exist for handling gender aspects in software development and
engineering. We conducted a systematic literature review to grasp the
comprehensive view of gender research and practices in Asia. We analyzed the 32
identified papers concerning countries and publication years among 463
publications. Researchers and practitioners from various organizations actively
work on gender research and practices in some countries, including China,
India, and Turkey. We identified topics and classified them into seven
categories varying from personal mental health and team building to
organization. Future research directions include investigating the synergy
between (regional) gender aspects and cultural concerns and considering
possible contributions and dependency among different topics to have a solid
foundation for accelerating further research and getting actionable practices."
13856,"Furthermore,
 it is also expected to clarify contributions and dependen-                   [6] H. Washizaki, S. Ogata, A. Hazeyama, T. Okubo, E. B. Ferna´ndez, and
 cies among different topics to have a solid foundation                            N. Yoshioka, “Landscape of architecture and design patterns for iot
 for further research and actionable practices.","Comparison with other areas outside                             Available: https://doi.org/10.1109/COMPSAC54236.2022.00083
 Asia is another expected research direction.","Clarifying                         systems,” IEEE Internet Things J., vol.",2022-11-16 14:58:01+00:00,Systematic Literature Review of Gender and Software Engineering in Asia,cs.SE,"['cs.SE', 'cs.GL']",[arxiv.Result.Author('Hironori Washizaki')],"It is essential to discuss the role, difficulties, and opportunities
concerning people of different gender in the field of software engineering
research, education, and industry. Although some literature reviews address
software engineering and gender, it is still unclear how research and practices
in Asia exist for handling gender aspects in software development and
engineering. We conducted a systematic literature review to grasp the
comprehensive view of gender research and practices in Asia. We analyzed the 32
identified papers concerning countries and publication years among 463
publications. Researchers and practitioners from various organizations actively
work on gender research and practices in some countries, including China,
India, and Turkey. We identified topics and classified them into seven
categories varying from personal mental health and team building to
organization. Future research directions include investigating the synergy
between (regional) gender aspects and cultural concerns and considering
possible contributions and dependency among different topics to have a solid
foundation for accelerating further research and getting actionable practices."
14088,"https://doi.org/10.1007/978-3-642-41924-9_27
Studying TM representation of different types of negative               [10] M. Auguston, System and Software Architecture and
events is a topic for further research.",This would preserve agency in negative events.,"Workflow Modeling Language Manual (version 2),
                                                                              California: Naval Postgraduate School Monterey, April10,
                        VIII.",2022-11-22 21:02:49+00:00,Modeling System Events and Negative Events Using Thinging Machines Based on Lupascian Logic,cs.SE,['cs.SE'],[arxiv.Result.Author('Sabah Al-Fedaghi')],"This paper is an exploration of the ontological foundations of conceptual
modeling that addresses the concept of events and related notions. Development
models that convey how things change over space and time demand continued
attention in systems and software engineering. In this context, foundational
matters in modeling systems include the definition of an event, the types of
events, and the kinds of relationships that can be recognized among events.
Although a broad spectrum of research of such issues exists in various fields
of study, events have extensive applicability in computing (e.g., event-driven
programming, architecture, data modeling, automation, and surveillance). While
these computing notions are diverse, their event-based nature lets us apply
many of the same software engineering techniques to all of them. In this paper,
the focus is on addressing the dynamic concepts of system events and negative
events. Specifically, we concentrate on what computer scientists would refer to
as an event grammar and event calculus. Analyzing the concept of event would
further the understanding of the event notion and provide a sound foundation
for improving the theory and practice of conceptual modeling. An event in
computer science has many definitions (e.g., anything that happens, changes in
the properties of objects, and the occurrence of and transition between
states). This paper is based upon a different conceptualization using thinging
machines and Lupascian logic to define negative events. An event is defined as
a time penetrated domain s region, which is described in terms of things and
five-action machines. Accordingly, samples from event grammar and event
calculus are remodeled and analyzed in terms of this definition. The results
point to an enriched modeling technique with an enhanced conceptualization of
events that can benefit behavior modeling in systems."
14289,"To facilitate reproducibility and further research, source code, benchmarks and experimental
data are released at https://github.com/NTDXYG/RADAR.","More importantly, at the methodological level, this paper promotes, with solid evidence,
the importance of studying the robustness of deep learning models in neural code generation and
even software engineering in general, where they are playing an increasingly important role.",Structure.,2022-11-29 00:37:35+00:00,How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective,cs.SE,['cs.SE'],"[arxiv.Result.Author('Guang Yang'), arxiv.Result.Author('Yu Zhou'), arxiv.Result.Author('Wenhua Yang'), arxiv.Result.Author('Tao Yue'), arxiv.Result.Author('Xiang Chen'), arxiv.Result.Author('Taolue Chen')]","Pre-trained code generation models (PCGMs) have been widely applied in neural
code generation which can generate executable code from functional descriptions
in natural languages, possibly together with signatures. Despite substantial
performance improvement of PCGMs, the role of method names in neural code
generation has not been thoroughly investigated. In this paper, we study and
demonstrate the potential of benefiting from method names to enhance the
performance of PCGMs, from a model robustness perspective. Specifically, we
propose a novel approach, named RADAR (neuRAl coDe generAtor Robustifier).
RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former
attacks a PCGM by generating adversarial method names as part of the input,
which are semantic and visual similar to the original input, but may trick the
PCGM to generate completely unrelated code snippets. As a countermeasure to
such attacks, RADAR-Defense synthesizes a new method name from the functional
description and supplies it to the PCGM. Evaluation results show that
RADAR-Attack can, e.g., reduce the CodeBLEU of generated code by 19.72% to
38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5).
Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with
synthesized method names. These results highlight the importance of good method
names in neural code generation and implicate the benefits of studying model
robustness in software engineering."
14403,"Our observations suggest several bug patterns, which would         Moreover, the reusability in the prior work’s dataset is limited to
                                        greatly facilitate further research and development for reducing
                                        bugs in the pre-trained models as well as the code that reuses them.",etc.,"KEYWORDS

                                        empirical study, bugs, NLP, deep learning, model reuse

                                        1 INTRODUCTION

                                        “Hey, Alexa!",2022-11-30 20:25:50+00:00,An Empirical Study on the Bugs Found while Reusing Pre-trained Natural Language Processing Models,cs.SE,"['cs.SE', '68T50', 'D.6; D.2.5; D.2.13']","[arxiv.Result.Author('Rangeet Pan'), arxiv.Result.Author('Sumon Biswas'), arxiv.Result.Author('Mohna Chakraborty'), arxiv.Result.Author('Breno Dantas Cruz'), arxiv.Result.Author('Hridesh Rajan')]","In NLP, reusing pre-trained models instead of training from scratch has
gained popularity; however, NLP models are mostly black boxes, very large, and
often require significant resources. To ease, models trained with large corpora
are made available, and developers reuse them for different problems. In
contrast, developers mostly build their models from scratch for traditional
DL-related problems. By doing so, they have control over the choice of
algorithms, data processing, model structure, tuning hyperparameters, etc.
Whereas in NLP, due to the reuse of the pre-trained models, NLP developers are
limited to little to no control over such design decisions. They either apply
tuning or transfer learning on pre-trained models to meet their requirements.
Also, NLP models and their corresponding datasets are significantly larger than
the traditional DL models and require heavy computation. Such reasons often
lead to bugs in the system while reusing the pre-trained models. While bugs in
traditional DL software have been intensively studied, the nature of extensive
reuse and black-box structure motivates us to understand the different types of
bugs that occur while reusing NLP models? What are the root causes of those
bugs? How do these bugs affect the system? To answer these questions, We
studied the bugs reported while reusing the 11 popular NLP models. We mined
9,214 issues from GitHub repositories and identified 984 bugs. We created a
taxonomy with bug types, root causes, and impacts. Our observations led to
several findings, including limited access to model internals resulting in a
lack of robustness, lack of input validation leading to the propagation of
algorithmic and data bias, and high-resource consumption causing more crashes,
etc. Our observations suggest several bug patterns, which would greatly
facilitate further efforts in reducing bugs in pre-trained models and code
reuse."
14628,"The aforementioned ﬁnding reﬂects a tendency of microservice designers and operators towards the so-called “NoOps” trend16,
which itself deserves further study.","Team sizes are as small as the microservices catered to, typically reﬂecting 3 or less microservice “owners”.",: Polyglotism.,2022-12-06 16:12:37+00:00,Microservice Architecture Practices and Experience: a Focused Look on Docker Configuration Files,cs.SE,['cs.SE'],"[arxiv.Result.Author('Luciano Baresi'), arxiv.Result.Author('Giovanni Quattrocchi'), arxiv.Result.Author('Damian Andrew Tamburri')]","Cloud applications are more and more microservice-oriented, but a concrete
charting of the microservices architecture landscape -- namely, the space of
technical options available for microservice software architects in their
decision-making -- is still very much lacking, thereby limiting the ability of
software architects to properly evaluate their architectural decisions with
sound experiential devices and/or practical design principles. On the one hand,
Microservices are fine-grained, loosely coupled services that communicate
through lightweight protocols. On the other hand, each microservice can use a
different software stack, be deployed and scaled independently or even executed
in different containers, which provide isolation and a wide-range of
configuration options but also offer unforeseeable architectural interactions
and under-explored architecture smells, with such experience captured mainly in
software repositories where such solutions are cycled.
  This paper adopts a mining software repositories (MSR) approach to capture
the practice within the microservice architecture landscape, by eliciting and
analysing Docker configuration files, being Docker the leading technical device
to design for, and implement modern microservices. Our analysis of Docker-based
microservices gives an interesting summary of the current state of
microservices practice and experience. Conversely, observing that all our
datapoints have their own shape and characteristics, we conclude that further
comparative assessment with industrial systems is needed to better address the
recurring positive principles and patterns around microservices."
14710,"What is more, it         the weaknesses as a basis for further research.","to 10 years (ARC Advisory Group, 2011).","As the main
will even remain relevant for decades to come, due to the        contribution of this paper, we introduce SWMAT4aPS
plants’ lifetime of up to 50 years.",2022-12-07 14:21:03+00:00,Modularity and Architecture of PLC-based Software for Automated Production Systems: An analysis in industrial companies,cs.SE,['cs.SE'],"[arxiv.Result.Author('Birgit Vogel-Heuser'), arxiv.Result.Author('Juliane Fischer'), arxiv.Result.Author('Stefan Feldmann'), arxiv.Result.Author('Sebastian Ulewicz'), arxiv.Result.Author('Susanne Rösch')]","Adaptive and flexible production systems require modular and reusable
software especially considering their long term life cycle of up to 50 years.
SWMAT4aPS, an approach to measure Software Maturity for automated Production
Systems is introduced. The approach identifies weaknesses and strengths of
various companie's solutions for modularity of software in the design of
automated Production Systems (aPS). At first, a self assessed questionnaire is
used to evaluate a large number of companies concerning their software
maturity. Secondly, we analyze PLC code, architectural levels, workflows and
abilities to configure code automatically out of engineering information in
four selected companies. In this paper, the questionnaire results from 16
German world leading companies in machine and plant manufacturing and four case
studies validating the results from the detailed analyses are introduced to
prove the applicability of the approach and give a survey of the state of the
art in industry."
14711,"2, ① and ②) and to subsequently focus on these
the weaknesses as a basis for further research.",Fig.,To reach our         weaknesses in the expert evaluation (cp.,2022-12-07 14:21:03+00:00,Modularity and Architecture of PLC-based Software for Automated Production Systems: An analysis in industrial companies,cs.SE,['cs.SE'],"[arxiv.Result.Author('Birgit Vogel-Heuser'), arxiv.Result.Author('Juliane Fischer'), arxiv.Result.Author('Stefan Feldmann'), arxiv.Result.Author('Sebastian Ulewicz'), arxiv.Result.Author('Susanne Rösch')]","Adaptive and flexible production systems require modular and reusable
software especially considering their long term life cycle of up to 50 years.
SWMAT4aPS, an approach to measure Software Maturity for automated Production
Systems is introduced. The approach identifies weaknesses and strengths of
various companie's solutions for modularity of software in the design of
automated Production Systems (aPS). At first, a self assessed questionnaire is
used to evaluate a large number of companies concerning their software
maturity. Secondly, we analyze PLC code, architectural levels, workflows and
abilities to configure code automatically out of engineering information in
four selected companies. In this paper, the questionnaire results from 16
German world leading companies in machine and plant manufacturing and four case
studies validating the results from the detailed analyses are introduced to
prove the applicability of the approach and give a survey of the state of the
art in industry."
14712,"(RQ2.3)
and to identify weaknesses as a basis for further research.","Research Method
                                                                                          (RQ2.2b)
    The research goal addressed in this paper is to gain deeper
insights in the state of the art in software engineering of aPS                           Are cross-disciplinary modules used?",Three research questions were identified that shall be ad-           What are success How big is the variant and version mgmt.,2022-12-07 13:44:36+00:00,Maintainability and evolvability of control software in machine and plant manufacturing -- An industrial survey,cs.SE,"['cs.SE', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Birgit Vogel-Heuser'), arxiv.Result.Author('Felix Ocker')]","Automated Production Systems (aPS) have lifetimes of up to 30-50 years,
throughout which the desired products change ever more frequently. This
requires flexible, reusable control software that can be easily maintained and
evolved. To evaluate selected criteria that are especially relevant for
maturity in software maintainability and evolvability of aPS, the approach
SWMAT4aPS+ builds on a questionnaire with 52 questions. The three main research
questions cover updates of software modules and success factors for both
cross-disciplinary development as well as reusable models. This paper presents
the evaluation results of 68 companies from machine and plant manufacturing
(MPM). Companies providing automation devices and/or engineering tools will be
able to identify challenges their customers in MPM face. Validity is ensured
through feedback of the participating companies and an analysis of the
statistical unambiguousness of the results. From a software or systems
engineering point of view, almost all criteria are fulfilled below
expectations."
14713,"cessity for further research on the obstacles for companies to          It is assumed that the answers refer mostly to unit tests of
improve their processes on the one hand and on the improve-             modules, but the question needs to be refined (cp.",This mirrors the ne-              companies in six research projects on testing in automation.,Sec.,2022-12-07 13:44:36+00:00,Maintainability and evolvability of control software in machine and plant manufacturing -- An industrial survey,cs.SE,"['cs.SE', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Birgit Vogel-Heuser'), arxiv.Result.Author('Felix Ocker')]","Automated Production Systems (aPS) have lifetimes of up to 30-50 years,
throughout which the desired products change ever more frequently. This
requires flexible, reusable control software that can be easily maintained and
evolved. To evaluate selected criteria that are especially relevant for
maturity in software maintainability and evolvability of aPS, the approach
SWMAT4aPS+ builds on a questionnaire with 52 questions. The three main research
questions cover updates of software modules and success factors for both
cross-disciplinary development as well as reusable models. This paper presents
the evaluation results of 68 companies from machine and plant manufacturing
(MPM). Companies providing automation devices and/or engineering tools will be
able to identify challenges their customers in MPM face. Validity is ensured
through feedback of the participating companies and an analysis of the
statistical unambiguousness of the results. From a software or systems
engineering point of view, almost all criteria are fulfilled below
expectations."
14717,"8 Outlook

Multiple threads of further research were identified given the results of the evaluation and experience
gained during the experiments.","As the experts’ requirements and machines are similar to a substantial part of the authors’ industry
partners, the approach represents a significant improvement in supporting the testing process within the
production automation domain.","21
Increasing System Test Coverage in Production Automation Systems

As for the main criticism of the runtime overhead, multiple improvements could be made and will be
pursued in future work.",2022-12-07 13:55:00+00:00,Increasing System Test Coverage in Production Automation Systems,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sebastian Ulewicz'), arxiv.Result.Author('Birgit Vogel-Heuser')]","An approach is introduced, which supports a testing technician in the
identification of possibly untested behavior of control software of fully
integrated automated production systems (aPS). Based on an approach for guided
semi-automatic system testing, execution traces are recorded during testing,
allowing a subsequent coverage assessment. As the behavior of an aPS is highly
dependent on the software, omitted system behavior can be identified and
assessed for criticality. Through close cooperation with industry, this
approach represents the first coverage assessment approach for system testing
in production automation to be applied on real industrial objects and evaluated
by industrial experts."
14752,"To foster the replicability of our study, we built a large dataset of labeled
test cases [52] that can be used for replicating our results and promoting
further research.","– Integration of SDC-Scissor in an Industrial Use Case (analysis de-
    tailed Section 6): We integrated SDC-Scissor into the development context
    of the AICAS use case, demonstrating that the proposed tool can automate
    the testing process of such a large automotive company.","Furthermore, SDC-Scissor is publicly available on GitHub 2,
which can be used with the data to replicate our results.",2022-12-09 10:46:19+00:00,Machine Learning-based Test Selection for Simulation-based Testing of Self-driving Cars Software,cs.SE,['cs.SE'],"[arxiv.Result.Author('Christian Birchler'), arxiv.Result.Author('Sajad Khatiri'), arxiv.Result.Author('Bill Bosshard'), arxiv.Result.Author('Alessio Gambi'), arxiv.Result.Author('Sebastiano Panichella')]","Simulation platforms facilitate the development of emerging Cyber-Physical
Systems (CPS) like self-driving cars (SDC) because they are more efficient and
less dangerous than field operational test cases. Despite this, thoroughly
testing SDCs in simulated environments remains challenging because SDCs must be
tested in a sheer amount of long-running test cases. Past results on software
testing optimization have shown that not all the test cases contribute equally
to establishing confidence in test subjects' quality and reliability, and the
execution of ""safe and uninformative"" test cases can be skipped to reduce
testing effort. However, this problem is only partially addressed in the
context of SDC simulation platforms. In this paper, we investigate test
selection strategies to increase the cost-effectiveness of simulation-based
testing in the context of SDCs. We propose an approach called SDC-Scissor (SDC
coSt-effeCtIve teSt SelectOR) that leverages Machine Learning (ML) strategies
to identify and skip test cases that are unlikely to detect faults in SDCs
before executing them.
  Our evaluation shows that SDC-Scissor outperforms the baselines. With the
Logistic model, we achieve an accuracy of 70%, a precision of 65%, and a recall
of 80% in selecting tests leading to a fault and improved testing
cost-effectiveness. Specifically, SDC-Scissor avoided the execution of 50% of
unnecessary tests as well as outperformed two baseline strategies.
Complementary to existing work, we also integrated SDC-Scissor into the context
of an industrial organization in the automotive domain to demonstrate how it
can be used in industrial settings."
14753,"To foster the replicability of our study,
we built a large dataset of labeled test cases [52] that can be used for replicat-
ing our results and promoting further research.","Data Availability

The datasets generated during and/or analysed during the current study are
available in the Zenodo repository [52].","Furthermore, SDC-Scissor is
publicly available on GitHub 9, which can be used with the data to replicate
our results.",2022-12-09 10:46:19+00:00,Machine Learning-based Test Selection for Simulation-based Testing of Self-driving Cars Software,cs.SE,['cs.SE'],"[arxiv.Result.Author('Christian Birchler'), arxiv.Result.Author('Sajad Khatiri'), arxiv.Result.Author('Bill Bosshard'), arxiv.Result.Author('Alessio Gambi'), arxiv.Result.Author('Sebastiano Panichella')]","Simulation platforms facilitate the development of emerging Cyber-Physical
Systems (CPS) like self-driving cars (SDC) because they are more efficient and
less dangerous than field operational test cases. Despite this, thoroughly
testing SDCs in simulated environments remains challenging because SDCs must be
tested in a sheer amount of long-running test cases. Past results on software
testing optimization have shown that not all the test cases contribute equally
to establishing confidence in test subjects' quality and reliability, and the
execution of ""safe and uninformative"" test cases can be skipped to reduce
testing effort. However, this problem is only partially addressed in the
context of SDC simulation platforms. In this paper, we investigate test
selection strategies to increase the cost-effectiveness of simulation-based
testing in the context of SDCs. We propose an approach called SDC-Scissor (SDC
coSt-effeCtIve teSt SelectOR) that leverages Machine Learning (ML) strategies
to identify and skip test cases that are unlikely to detect faults in SDCs
before executing them.
  Our evaluation shows that SDC-Scissor outperforms the baselines. With the
Logistic model, we achieve an accuracy of 70%, a precision of 65%, and a recall
of 80% in selecting tests leading to a fault and improved testing
cost-effectiveness. Specifically, SDC-Scissor avoided the execution of 50% of
unnecessary tests as well as outperformed two baseline strategies.
Complementary to existing work, we also integrated SDC-Scissor into the context
of an industrial organization in the automotive domain to demonstrate how it
can be used in industrial settings."
14811,"Such a topic needs to be further researched in the
future.","Accordingly, it seems that the
specific notion of event is different from the Stoic notion of
event.",III.,2022-12-10 13:41:49+00:00,Conceptual Modeling Founded on the Stoic Ontology: Reality with Dynamic Existence and Static Subsistence,cs.SE,['cs.SE'],[arxiv.Result.Author('Sabah Al-Fedaghi')],"According to the software engineering community, the acknowledgement is
growing that a theory of software development is needed to integrate the
currently myriad popular methodologies, some of which are based on opposing
perspectives. Conceptual modeling (CM) can contribute to such a theory. CM
defines fundamental concepts to create representations of reality to achieve
ontologically sound software behavior that is characterized by truthfulness to
reality and conceptual clarity. In this context, CM is founded on theories
about the world that serve to represent a given domain. Ontologies have made
their way into CM as tools in requirements analysis, implementation
specification, and software architecture. This paper involves building a direct
connection between reality and CM by establishing mapping between reality and
modeling thinging machines (TMs). Specifically, Stoic ontology serves to define
the existence of TM things and actions in reality. Such a development would
benefit CM in addition to demonstrating that classical concepts in philosophy
can be applied to modern fields of study. The TM model includes static and
dynamic specifications. The dynamic level involves time-based events that can
be mapped to reality. The problem concerns the nature of a time-less static
description, which provides regions where the actions in events take place;
without them, the dynamic description collapses. The Stoics came up with a
brilliant move: the assumed reality to be a broader category than being.
Reality is made of things that exist and things that subsist. In this case, the
dynamic TM description is in existence, whereas the static, mapped portion of
the dynamic description is in subsistence. We apply such ontology to a contract
workflow example. The result seems to open a new avenue of CM that may enhance
the theoretical foundation for software and system development."
14812,"In this
context, the static TM model seems to provide a portrait of        [3] O. Dieste, N. Juristo, A.M. Moreno, J. Pazos, A. Sierra, ―Conceptual
―grounds of being‖ or ―reality before existence.‖ Such an                modelling in software engineering and knowledge engineering:
initial observation is worth further research.","A first impression is that the Stoic           https://doi.org/10.1007/s10838-008-9068-7
ontology fits hand-in-glove with the TM model.","This also needs           Concepts, techniques and trends,‖ Handbook of Software Engineering
further understanding of differences in assumptions and the              and Knowledge Engineering, vol.",2022-12-10 13:41:49+00:00,Conceptual Modeling Founded on the Stoic Ontology: Reality with Dynamic Existence and Static Subsistence,cs.SE,['cs.SE'],[arxiv.Result.Author('Sabah Al-Fedaghi')],"According to the software engineering community, the acknowledgement is
growing that a theory of software development is needed to integrate the
currently myriad popular methodologies, some of which are based on opposing
perspectives. Conceptual modeling (CM) can contribute to such a theory. CM
defines fundamental concepts to create representations of reality to achieve
ontologically sound software behavior that is characterized by truthfulness to
reality and conceptual clarity. In this context, CM is founded on theories
about the world that serve to represent a given domain. Ontologies have made
their way into CM as tools in requirements analysis, implementation
specification, and software architecture. This paper involves building a direct
connection between reality and CM by establishing mapping between reality and
modeling thinging machines (TMs). Specifically, Stoic ontology serves to define
the existence of TM things and actions in reality. Such a development would
benefit CM in addition to demonstrating that classical concepts in philosophy
can be applied to modern fields of study. The TM model includes static and
dynamic specifications. The dynamic level involves time-based events that can
be mapped to reality. The problem concerns the nature of a time-less static
description, which provides regions where the actions in events take place;
without them, the dynamic description collapses. The Stoics came up with a
brilliant move: the assumed reality to be a broader category than being.
Reality is made of things that exist and things that subsist. In this case, the
dynamic TM description is in existence, whereas the static, mapped portion of
the dynamic description is in subsistence. We apply such ontology to a contract
workflow example. The result seems to open a new avenue of CM that may enhance
the theoretical foundation for software and system development."
14835,"Nonetheless, our results also highlight limitations
                                        of the experimented approaches that call for further research in this ﬁeld.","Our
                                        quantitative and qualitative analyses show the potential of such techniques that, under
                                        speciﬁc conditions, can provide valuable recommendations and are ready to be inte-
                                        grated in rename refactoring tools.","Keywords Code comprehension, · Empirical Study · Machine Learning on Code

                                        Antonio Mastropaolo
                                        SEART @ Software Institute, Università della Svizzera italiana, Switzerland.",2022-12-12 07:36:27+00:00,Automated Variable Renaming: Are We There Yet?,cs.SE,['cs.SE'],"[arxiv.Result.Author('Antonio Mastropaolo'), arxiv.Result.Author('Emad Aghajani'), arxiv.Result.Author('Luca Pascarella'), arxiv.Result.Author('Gabriele Bavota')]","Identifiers, such as method and variable names, form a large portion of
source code. Therefore, low-quality identifiers can substantially hinder code
comprehension. To support developers in using meaningful identifiers, several
(semi-)automatic techniques have been proposed, mostly being data-driven (e.g.
statistical language models, deep learning models) or relying on static code
analysis. Still, limited empirical investigations have been performed on the
effectiveness of such techniques for recommending developers with meaningful
identifiers, possibly resulting in rename refactoring operations. We present a
large-scale study investigating the potential of data-driven approaches to
support automated variable renaming. We experiment with three state-of-the-art
techniques: a statistical language model and two DL-based models. The three
approaches have been trained and tested on three datasets we built with the
goal of evaluating their ability to recommend meaningful variable identifiers.
Our quantitative and qualitative analyses show the potential of such techniques
that, under specific conditions, can provide valuable recommendations and are
ready to be integrated in rename refactoring tools. Nonetheless, our results
also highlight limitations of the experimented approaches that call for further
research in this field."
14837,"The latter approach has been
tried with promising results [5] and may be worth pursuing in further research.","Potential approaches
include teaching the practice in university curricula or to prepare entrepreneur-
ship training programs with CE-related material.",14  V. Mäntylä et al.,2022-12-12 08:01:28+00:00,The Viability of Continuous Experimentation in Early-Stage Software Startups: A Descriptive Multiple-Case Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Vihtori Mäntylä'), arxiv.Result.Author('Bettina Lehtelä'), arxiv.Result.Author('Fabian Fagerholm')]","Background: Continuous experimentation (CE) has been proposed as a
data-driven approach to software product development. Several challenges with
this approach have been described in large organisations, but its application
in smaller companies with early-stage products remains largely unexplored.
Aims: The goal of this study is to understand what factors could affect the
adoption of CE in early-stage software startups. Method: We present a
descriptive multiple-case study of five startups in Finland which differ in
their utilisation of experimentation. Results: We find that practices often
mentioned as prerequisites for CE, such as iterative development and continuous
integration and delivery, were used in the case companies. CE was not widely
recognised or used as described in the literature. Only one company performed
experiments and used experimental data systematically. Conclusions: Our study
indicates that small companies may be unlikely to adopt CE unless 1) at least
some company employees have prior experience with the practice, 2) the
company's limited available resources are not exceeded by its adoption, and 3)
the practice solves a problem currently experienced by the company, or the
company perceives almost immediate benefit of adopting it. We discuss
implications for advancing CE in early-stage startups and outline directions
for future research on the approach."
14850,"For further research, different properties of the approach             Eng., vol.",Sci.,"10, no.",2022-12-09 13:44:52+00:00,Industrially Applicable System Regression Test Prioritization in Production Automation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sebastian Ulewicz'), arxiv.Result.Author('Birgit Vogel-Heuser')]","When changes are performed on an automated production system (aPS), new
faults can be accidentally introduced in the system, which are called
regressions. A common method for finding these faults is regression testing. In
most cases, this regression testing process is performed under high time
pressure and on-site in a very uncomfortable environment. Until now, there is
no automated support for finding and prioritizing system test cases regarding
the fully integrated aPS that are suitable for finding regressions. Thus, the
testing technician has to rely on personal intuition and experience, possibly
choosing an inappropriate order of test cases, finding regressions at a very
late stage of the test run. Using a suitable prioritization, this iterative
process of finding and fixing regressions can be streamlined and a lot of time
can be saved by executing test cases likely to identify new regressions
earlier. Thus, an approach is presented in this paper that uses previously
acquired runtime data from past test executions and performs a change
identification and impact analysis to prioritize test cases that have a high
probability to unveil regressions caused by side effects of a system change.
The approach was developed in cooperation with reputable industrial partners
active in the field of aPS engineering, ensuring a development in line with
industrial requirements. An industrial case study and an expert evaluation were
performed, showing promising results."
14851,"The change              Technologies and Factory Automation (ETFA), 2012.
impact analysis and prioritization algorithms should also be a
focus of further research, improving the safety of the choice of    [10] E. Estevez and M. Marcos, “Model-Based Validation of Industrial
test cases to allow for the omission of low-priority test cases           Control Systems,” IEEE Trans.","[9] H. Prähofer, F. Angerer, R. Ramler, H. Lacheiner, and F. Grillenberger,
Furthermore, the support of all IEC 61131-3 programming                   “Opportunities and challenges of static code analysis of IEC 61131-3
languages is of interest in making the approach applicable for            programs,” in IEEE International Conference on Emerging
all control programs programmed in this standard.",Ind.,2022-12-09 13:44:52+00:00,Industrially Applicable System Regression Test Prioritization in Production Automation,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sebastian Ulewicz'), arxiv.Result.Author('Birgit Vogel-Heuser')]","When changes are performed on an automated production system (aPS), new
faults can be accidentally introduced in the system, which are called
regressions. A common method for finding these faults is regression testing. In
most cases, this regression testing process is performed under high time
pressure and on-site in a very uncomfortable environment. Until now, there is
no automated support for finding and prioritizing system test cases regarding
the fully integrated aPS that are suitable for finding regressions. Thus, the
testing technician has to rely on personal intuition and experience, possibly
choosing an inappropriate order of test cases, finding regressions at a very
late stage of the test run. Using a suitable prioritization, this iterative
process of finding and fixing regressions can be streamlined and a lot of time
can be saved by executing test cases likely to identify new regressions
earlier. Thus, an approach is presented in this paper that uses previously
acquired runtime data from past test executions and performs a change
identification and impact analysis to prioritize test cases that have a high
probability to unveil regressions caused by side effects of a system change.
The approach was developed in cooperation with reputable industrial partners
active in the field of aPS engineering, ensuring a development in line with
industrial requirements. An industrial case study and an expert evaluation were
performed, showing promising results."
14973,with important contributions for further research.,"nication and interaction make it possible to react adequately to
                                        Objective: Thus, our objective is to tackle this field and come up               changing requirements and individual customer and user needs.","To this end, we               This enables companies to be competitive even in highly volatile
                                        want to identify challenges that arise from the interplay between                market environments and to be able to face the VUCA world [3].",2022-12-14 13:40:54+00:00,Key Challenges with Agile Culture -- A Survey among Practitioners,cs.SE,['cs.SE'],"[arxiv.Result.Author('Thorben Kuchel'), arxiv.Result.Author('Michael Neumann'), arxiv.Result.Author('Philipp Diebold'), arxiv.Result.Author('Eva-Maria Schön')]","Context: Within agile transformations, there are a lot of different
challenges coming up. One very important but less considered and treated in
research are cultural challenges. Although research shows that cultural clashes
and general organizational resistance to change are part of the most
significant agile adoption barriers. Objective: Thus, our objective is to
tackle this field and come up with important contributions for further
research. To this end, we want to identify challenges that arise from the
interplay between agility and organizational culture. Method: This is done
based on an iterative research approach. On the one hand, we gathered
qualitative data among our network of agile practitioners. Then, we derived in
sum 15 challenges with agile culture. On the other hand, we gathered
quantitative data by means of a questionnaire study with 92 participants.
Results: We identified 7 key challenges out of the 15 challenges with agile
culture. The results that are presented in a conceptual model show a focus on
human aspects that we need to deal with more in future. Conclusion: Based on
our results, we started deriving future work aspects to do more detailed
research on the topic of cultural challenges while transitioning or using agile
methods in software development and beyond."
14974,"Challenges and success factors
show the need for further research in the field of cultural influences         for large-scale agile transformations: A systematic literature review.",2016.,"Journal of
on agile methods and practices.",2022-12-14 13:40:54+00:00,Key Challenges with Agile Culture -- A Survey among Practitioners,cs.SE,['cs.SE'],"[arxiv.Result.Author('Thorben Kuchel'), arxiv.Result.Author('Michael Neumann'), arxiv.Result.Author('Philipp Diebold'), arxiv.Result.Author('Eva-Maria Schön')]","Context: Within agile transformations, there are a lot of different
challenges coming up. One very important but less considered and treated in
research are cultural challenges. Although research shows that cultural clashes
and general organizational resistance to change are part of the most
significant agile adoption barriers. Objective: Thus, our objective is to
tackle this field and come up with important contributions for further
research. To this end, we want to identify challenges that arise from the
interplay between agility and organizational culture. Method: This is done
based on an iterative research approach. On the one hand, we gathered
qualitative data among our network of agile practitioners. Then, we derived in
sum 15 challenges with agile culture. On the other hand, we gathered
quantitative data by means of a questionnaire study with 92 participants.
Results: We identified 7 key challenges out of the 15 challenges with agile
culture. The results that are presented in a conceptual model show a focus on
human aspects that we need to deal with more in future. Conclusion: Based on
our results, we started deriving future work aspects to do more detailed
research on the topic of cultural challenges while transitioning or using agile
methods in software development and beyond."
14996,"These results call for further research to
evaluate the role of task-speciﬁc experience in programmers’ mental models
and their acquisition.","Task-speciﬁc experience
was measured only in 10 studies.","The most common measures used to assess experience either measured
length of experience in diﬀerent ways or used self-reporting.",2022-12-15 12:28:37+00:00,"Synthesizing Research on Programmers' Mental Models of Programs, Tasks and Concepts -- a Systematic Literature Review",cs.SE,"['cs.SE', 'cs.HC', 'F.3.2; F.3.3']","[arxiv.Result.Author('Ava Heinonen'), arxiv.Result.Author('Bettina Lehtelä'), arxiv.Result.Author('Arto Hellas'), arxiv.Result.Author('Fabian Fagerholm')]","Programmers' mental models represent their knowledge and understanding of
programs, programming concepts, and programming in general. They guide
programmers' work and influence their task performance. Understanding mental
models is important for designing work systems and practices that support
programmers. Although the importance of programmers' mental models is widely
acknowledged, research on mental models has decreased over the years. The
results are scattered and do not take into account recent developments in
software engineering. We analyze the state of research into programmers' mental
models and provide an overview of existing research. We connect results on
mental models from different strands of research to form a more unified
knowledge base on the topic. We conducted a systematic literature review on
programmers' mental models. We analyzed literature addressing mental models in
different contexts, including mental models of programs, programming tasks, and
programming concepts. Using nine search engines, we found 3678 articles
(excluding duplicates). 84 were selected for further analysis. Using the
snowballing technique, we obtained a final result set containing 187 articles.
We show that the literature shares a kernel of shared understanding of mental
models. By collating and connecting results on mental models from different
fields of research, we uncovered some well-researched aspects, which we argue
are fundamental characteristics of programmers' mental models. This work
provides a basis for future work on mental models. The research field on
programmers' mental models still faces many challenges rising from a lack of a
shared knowledge base and poorly defined constructs. We created a unified
knowledge base on the topic. We also point to directions for future studies. In
particular, we call for studies that examine programmers working with modern
practices and tools."
14997,"We argue that these aspects can be used to build a shared understanding of
mental models to use as a basis for further research.","We have also detailed fundamental aspects
of mental models that are prominent in results from across the research ﬁeld.","Declaration of Competing Interest
    The authors declare that they have no known competing ﬁnancial inter-

ests or personal relationships that could have appeared to inﬂuence the work
reported in this article.",2022-12-15 12:28:37+00:00,"Synthesizing Research on Programmers' Mental Models of Programs, Tasks and Concepts -- a Systematic Literature Review",cs.SE,"['cs.SE', 'cs.HC', 'F.3.2; F.3.3']","[arxiv.Result.Author('Ava Heinonen'), arxiv.Result.Author('Bettina Lehtelä'), arxiv.Result.Author('Arto Hellas'), arxiv.Result.Author('Fabian Fagerholm')]","Programmers' mental models represent their knowledge and understanding of
programs, programming concepts, and programming in general. They guide
programmers' work and influence their task performance. Understanding mental
models is important for designing work systems and practices that support
programmers. Although the importance of programmers' mental models is widely
acknowledged, research on mental models has decreased over the years. The
results are scattered and do not take into account recent developments in
software engineering. We analyze the state of research into programmers' mental
models and provide an overview of existing research. We connect results on
mental models from different strands of research to form a more unified
knowledge base on the topic. We conducted a systematic literature review on
programmers' mental models. We analyzed literature addressing mental models in
different contexts, including mental models of programs, programming tasks, and
programming concepts. Using nine search engines, we found 3678 articles
(excluding duplicates). 84 were selected for further analysis. Using the
snowballing technique, we obtained a final result set containing 187 articles.
We show that the literature shares a kernel of shared understanding of mental
models. By collating and connecting results on mental models from different
fields of research, we uncovered some well-researched aspects, which we argue
are fundamental characteristics of programmers' mental models. This work
provides a basis for future work on mental models. The research field on
programmers' mental models still faces many challenges rising from a lack of a
shared knowledge base and poorly defined constructs. We created a unified
knowledge base on the topic. We also point to directions for future studies. In
particular, we call for studies that examine programmers working with modern
practices and tools."
15024,"We believe that this    GNNExplainer gives a score for each edge in the graph, and
motivates further research into causal detection of bugs for     LIT gives a score for each token in the program.",language features or naming conventions.,"To compare
generalization.",2022-12-15 19:49:34+00:00,An Empirical Study of Deep Learning Models for Vulnerability Detection,cs.SE,"['cs.SE', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Benjamin Steenhoek'), arxiv.Result.Author('Md Mahbubur Rahman'), arxiv.Result.Author('Richard Jiles'), arxiv.Result.Author('Wei Le')]","Deep learning (DL) models of code have recently reported great progress for
vulnerability detection. In some cases, DL-based models have outperformed
static analysis tools. Although many great models have been proposed, we do not
yet have a good understanding of these models. This limits the further
advancement of model robustness, debugging, and deployment for the
vulnerability detection. In this paper, we surveyed and reproduced 9
state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability
detection datasets: Devign and MSR. We investigated 6 research questions in
three areas, namely model capabilities, training data, and model
interpretation. We experimentally demonstrated the variability between
different runs of a model and the low agreement among different models'
outputs. We investigated models trained for specific types of vulnerabilities
compared to a model that is trained on all the vulnerabilities at once. We
explored the types of programs DL may consider ""hard"" to handle. We
investigated the relations of training data sizes and training data composition
with model performance. Finally, we studied model interpretations and analyzed
important features that the models used to make predictions. We believe that
our findings can help better understand model results, provide guidance on
preparing training data, and improve the robustness of the models. All of our
datasets, code, and results are available at
https://figshare.com/s/284abfba67dba448fdc2."
15025,"We believe that this    GNNExplainer gives a score for each edge in the graph, and
motivates further research into causal detection of bugs for     LIT gives a score for each token in the program.",language features or naming conventions.,"To compare
generalization.",2022-12-15 19:49:34+00:00,An Empirical Study of Deep Learning Models for Vulnerability Detection,cs.SE,"['cs.SE', 'cs.CR', 'cs.LG']","[arxiv.Result.Author('Benjamin Steenhoek'), arxiv.Result.Author('Md Mahbubur Rahman'), arxiv.Result.Author('Richard Jiles'), arxiv.Result.Author('Wei Le')]","Deep learning (DL) models of code have recently reported great progress for
vulnerability detection. In some cases, DL-based models have outperformed
static analysis tools. Although many great models have been proposed, we do not
yet have a good understanding of these models. This limits the further
advancement of model robustness, debugging, and deployment for the
vulnerability detection. In this paper, we surveyed and reproduced 9
state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability
detection datasets: Devign and MSR. We investigated 6 research questions in
three areas, namely model capabilities, training data, and model
interpretation. We experimentally demonstrated the variability between
different runs of a model and the low agreement among different models'
outputs. We investigated models trained for specific types of vulnerabilities
compared to a model that is trained on all the vulnerabilities at once. We
explored the types of programs DL may consider ""hard"" to handle. We
investigated the relations of training data sizes and training data composition
with model performance. Finally, we studied model interpretations and analyzed
important features that the models used to make predictions. We believe that
our findings can help better understand model results, provide guidance on
preparing training data, and improve the robustness of the models. All of our
datasets, code, and results are available at
https://figshare.com/s/284abfba67dba448fdc2."
15027,"The results        practice, developers can iteratively infer FQNs for multiple
of such exploratory interactions are not counted in our             simple names and provide feedback during the process,
RQs, but they inspire ways to help CoPilot better serve             for example by conﬁrming the correctly-inferred FQNs or
the SE tasks and call for further research in human-CoPilot         correcting the wrongly-inferred FQNs, which may help
interaction.","In
teractions with CoPilot inspired by its outputs.",subsequent inferences.,2022-12-16 01:18:49+00:00,SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval,cs.SE,['cs.SE'],"[arxiv.Result.Author('Qing Huang'), arxiv.Result.Author('Dianshu Liao'), arxiv.Result.Author('Zhenchang Xing'), arxiv.Result.Author('Zhiqiang Yuan'), arxiv.Result.Author('Qinghua Lu'), arxiv.Result.Author('Xiwei Xu'), arxiv.Result.Author('Jiaxing Lu')]","Pre-trained giant code models (PCMs) start coming into the developers' daily
practices. Understanding what types of and how much software knowledge is
packed into PCMs is the foundation for incorporating PCMs into software
engineering (SE) tasks and fully releasing their potential. In this work, we
conduct the first systematic study on the SE factual knowledge in the
state-of-the-art PCM CoPilot, focusing on APIs' Fully Qualified Names (FQNs),
the fundamental knowledge for effective code analysis, search and reuse. Driven
by FQNs' data distribution properties, we design a novel lightweight in-context
learning on Copilot for FQN inference, which does not require code compilation
as traditional methods or gradient update by recent FQN prompt-tuning. We
systematically experiment with five in-context-learning design factors to
identify the best in-context learning configuration that developers can adopt
in practice. With this best configuration, we investigate the effects of amount
of example prompts and FQN data properties on Copilot's FQN inference
capability. Our results confirm that Copilot stores diverse FQN knowledge and
can be applied for the FQN inference due to its high inference accuracy and
non-reliance on code analysis. Based on our experience interacting with
Copilot, we discuss various opportunities to improve human-CoPilot interaction
in the FQN inference task."
15091,"The use of context information while modeling
source code is still nascent, therefore, further research on how to eﬀectively
build and use project-wide contexts is an open problem.","This example is representative of one of the ways in which how larger contexts
may be constructed Other techniques might consider including context from
sibling methods in the same class, or from methods based on data-ﬂow, control-
ﬂow, or dependency graphs.","JEMMA: An Extensible Java Dataset for ML4Code Applications                     51

Table A1 Method call completion (by size) without vs. with context, and % improvement.",2022-12-18 17:04:14+00:00,JEMMA: An Extensible Java Dataset for ML4Code Applications,cs.SE,"['cs.SE', 'cs.LG']","[arxiv.Result.Author('Anjan Karmakar'), arxiv.Result.Author('Miltiadis Allamanis'), arxiv.Result.Author('Romain Robbes')]","Machine Learning for Source Code (ML4Code) is an active research field in
which extensive experimentation is needed to discover how to best use source
code's richly structured information. With this in mind, we introduce JEMMA, an
Extensible Java Dataset for ML4Code Applications, which is a large-scale,
diverse, and high-quality dataset targeted at ML4Code. Our goal with JEMMA is
to lower the barrier to entry in ML4Code by providing the building blocks to
experiment with source code models and tasks. JEMMA comes with a considerable
amount of pre-processed information such as metadata, representations (e.g.,
code tokens, ASTs, graphs), and several properties (e.g., metrics, static
analysis results) for 50,000 Java projects from the 50KC dataset, with over 1.2
million classes and over 8 million methods. JEMMA is also extensible allowing
users to add new properties and representations to the dataset, and evaluate
tasks on them. Thus, JEMMA becomes a workbench that researchers can use to
experiment with novel representations and tasks operating on source code. To
demonstrate the utility of the dataset, we also report results from two
empirical studies on our data, ultimately showing that significant work lies
ahead in the design of context-aware source code models that can reason over a
broader network of source code entities in a software project, the very task
that JEMMA is designed to help with."
15182,"In
this context, S15 also identiﬁed a connection to the social dimension once there is a need for further study of
enterprise architecture and delivery mechanisms that enables software ecosystems.","From the viewpoint of governance, S15 linked the keystone actor to the processes of managing
the evolution of the enterprise architecture and the interactions among all actors within the ecosystem.","According to S15, keystones
must understand developers’ motivations and expectations to adopt appropriate governance mechanisms.",2022-12-20 17:18:44+00:00,Software Ecosystems: A Tertiary Study and a Thematic Model,cs.SE,['cs.SE'],"[arxiv.Result.Author('Paulo Malcher'), arxiv.Result.Author('Olavo Barbosa'), arxiv.Result.Author('Davi Viana'), arxiv.Result.Author('Rodrigo Santos')]","A software ecosystem (SECO) is an interaction, communication, cooperation,
and synergy among a set of players. Depending on the actors type of interaction
with others, each one can play a different role. These interactions provide a
set of positive relationships (symbiosis) between actors who work together
around a common technology platform or a service. SECO has been explored in
several studies, some related to their general characteristics and others
focusing on a specific topic (e.g., requirements, governance, open-source,
mobile). There are many literature reviews of different natures (e.g.,
systematic literature reviews and systematic mapping studies). This study
presents the status of the SECO field motivated by analyzing several secondary
studies published over the years. To do so, we conducted a tertiary study. From
an initial set of 518 studies on the subject, we selected 22 studies. We
identified the theoretical foundations used by researchers and their influences
and relationships with other ecosystems. We performed a thematic synthesis and
identified one high-order theme, 5 themes, 10 subthemes, and 206 categories. As
a result, we proposed a thematic model for SECO containing five themes, namely:
social, technical, business, management, and an evaluation theme named Software
Ecosystems Assessment Models (SEAM). Our main conclusion is that relationships
between SECO themes should not be seen in isolation, and it must be interpreted
in a holistic approach, given the number of implications to other themes mainly
related to the distinction of governance and management activities in the SECO
interactions. Finally, this work provides an overview of the field and points
out areas for future research, such as the need of SECO community to further
investigate the results from other ecosystems, mainly from the Digital
Ecosystem and Digital Business Ecosystem communities."
15183,"In addition, we
can analyze the themes identiﬁed in this research in case studies and conduct further research on the SEAM
dimension.","relationships mapped in the thematic synthesis in terms of themes, subthemes and categories.","Acknowledgements This study was ﬁnanced in part by the Coordena¸ca˜o de Aperfei¸coamento de Pessoal de N´ıvel Superior –
Brasil (CAPES) – Finance Code 001.",2022-12-20 17:18:44+00:00,Software Ecosystems: A Tertiary Study and a Thematic Model,cs.SE,['cs.SE'],"[arxiv.Result.Author('Paulo Malcher'), arxiv.Result.Author('Olavo Barbosa'), arxiv.Result.Author('Davi Viana'), arxiv.Result.Author('Rodrigo Santos')]","A software ecosystem (SECO) is an interaction, communication, cooperation,
and synergy among a set of players. Depending on the actors type of interaction
with others, each one can play a different role. These interactions provide a
set of positive relationships (symbiosis) between actors who work together
around a common technology platform or a service. SECO has been explored in
several studies, some related to their general characteristics and others
focusing on a specific topic (e.g., requirements, governance, open-source,
mobile). There are many literature reviews of different natures (e.g.,
systematic literature reviews and systematic mapping studies). This study
presents the status of the SECO field motivated by analyzing several secondary
studies published over the years. To do so, we conducted a tertiary study. From
an initial set of 518 studies on the subject, we selected 22 studies. We
identified the theoretical foundations used by researchers and their influences
and relationships with other ecosystems. We performed a thematic synthesis and
identified one high-order theme, 5 themes, 10 subthemes, and 206 categories. As
a result, we proposed a thematic model for SECO containing five themes, namely:
social, technical, business, management, and an evaluation theme named Software
Ecosystems Assessment Models (SEAM). Our main conclusion is that relationships
between SECO themes should not be seen in isolation, and it must be interpreted
in a holistic approach, given the number of implications to other themes mainly
related to the distinction of governance and management activities in the SECO
interactions. Finally, this work provides an overview of the field and points
out areas for future research, such as the need of SECO community to further
investigate the results from other ecosystems, mainly from the Digital
Ecosystem and Digital Business Ecosystem communities."
15203,"[Conclusion] Our results showed that most of the
                                                                  empirical studies on RE4AI focused on autonomous, self-driving vehicles and managing data
                                                                  requirements, and areas such as ethics, trust, and explainability need further research.","The ﬁndings highlighted that current RE applications
                                                                  were not adequately adaptable for building AI systems and emphasised the need to provide
                                                                  new techniques and tools to support RE4AI.",1.,2022-12-20 23:52:52+00:00,Requirements Engineering for Artificial Intelligence Systems: A Systematic Mapping Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Khlood Ahmad'), arxiv.Result.Author('Mohamed Abdelrazek'), arxiv.Result.Author('Chetan Arora'), arxiv.Result.Author('Muneera Bano'), arxiv.Result.Author('John Grundy')]","[Context] In traditional software systems, Requirements Engineering (RE)
activities are well-established and researched. However, building Artificial
Intelligence (AI) based software with limited or no insight into the system's
inner workings poses significant new challenges to RE. Existing literature has
focused on using AI to manage RE activities, with limited research on RE for AI
(RE4AI). [Objective] This paper investigates current approaches for specifying
requirements for AI systems, identifies available frameworks, methodologies,
tools, and techniques used to model requirements, and finds existing challenges
and limitations. [Method] We performed a systematic mapping study to find
papers on current RE4AI approaches. We identified 43 primary studies and
analysed the existing methodologies, models, tools, and techniques used to
specify and model requirements in real-world scenarios. [Results] We found
several challenges and limitations of existing RE4AI practices. The findings
highlighted that current RE applications were not adequately adaptable for
building AI systems and emphasised the need to provide new techniques and tools
to support RE4AI. [Conclusion] Our results showed that most of the empirical
studies on RE4AI focused on autonomous, self-driving vehicles and managing data
requirements, and areas such as ethics, trust, and explainability need further
research."
15204,": Preprint submitted to Elsevier                                                                          Page 21 of 45
Requirements Engineering for AI Systems

Table 11                                                    Outcomes / Aims                             Data Collection
Papers with Experiments Research                            Data collected is to be used in a future    Interviews + manually
                                                            case study to train a robot to perform      collecting force to insert
         Study Experiment                                   a procedure on removing cancerous cells     and remove a needle
         [72] Requirements for a surgical robot to ﬁnd      from a kidney
                                                            Using YOLO is not robust against            Image from Berkeley
                  the force required when inserting and     changes and could have severe conse-        DeepDrive [124] and
                  removing the needle into cancerous cells  quences in safety-critical systems          Gaussian noise
                                                            Capturing unacceptable behaviors and        Data gathered from 3
         [30] Adding noise to a vision detection system     identify the test cases that violate the    sensors: CCD camera, li-
                  (YOLO) to identify images for a self-     requirements                                dar, and radar
                  driving car                               They could not identify all conﬂicts and    Results taken from pre-
                                                            requires further research to create reﬁned  vious studies
         [77] Requirements are used to build a virtual      consistency notations
                  environment to test functionalities of a  Algorithms such as classiﬁcation and        Data on energy con-
                  self-driving car                          regression performed badly when the         sumption for 3 appli-
                                                            sampling rate was low                       ances and 58 houses
         [78] Prototype tool to maintain requirements       The method could be eﬀective in the         Software simulator that
                  consistency for autonomous driving and    assessment of real-time traﬃc situations    generated traﬃc scenar-
                  analyze a traﬃc situation                                                             ios
                                                            The outcomes showed improved results        Existing requirements
         [79] Implementing diﬀerent algorithms on a         when compared to other methods used         speciﬁcations
                  dataset to ﬁnd the eﬀects of sampling     to prioritize requirements
                  rate and house numbers                    The study gathers requirements and          Literature and Existing
                                                            models them using use cases and activity    requirements
         [80] Capabilities are presented in a goal          diagrams
                  model and tested in a traﬃc simulation
                  to demonstrate its eﬃciency

         [89] Satisfaction of NFR - the threshold is
                  calculated based on the trade-oﬀ on the
                  impact of a NFR on the rest

         [94] Requirement driven approach is used
                  to create a conﬁguration system for a
                  product line

application that would provide better quality of life for people with dementia.","[84] conducts workshops to identify soft goals for an AI

Ahmad et al.","In [97] the study carried out interviews to
identify requirements for a system that ﬂags users with the potential of ﬁling a lawsuit against a given power company.",2022-12-20 23:52:52+00:00,Requirements Engineering for Artificial Intelligence Systems: A Systematic Mapping Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Khlood Ahmad'), arxiv.Result.Author('Mohamed Abdelrazek'), arxiv.Result.Author('Chetan Arora'), arxiv.Result.Author('Muneera Bano'), arxiv.Result.Author('John Grundy')]","[Context] In traditional software systems, Requirements Engineering (RE)
activities are well-established and researched. However, building Artificial
Intelligence (AI) based software with limited or no insight into the system's
inner workings poses significant new challenges to RE. Existing literature has
focused on using AI to manage RE activities, with limited research on RE for AI
(RE4AI). [Objective] This paper investigates current approaches for specifying
requirements for AI systems, identifies available frameworks, methodologies,
tools, and techniques used to model requirements, and finds existing challenges
and limitations. [Method] We performed a systematic mapping study to find
papers on current RE4AI approaches. We identified 43 primary studies and
analysed the existing methodologies, models, tools, and techniques used to
specify and model requirements in real-world scenarios. [Results] We found
several challenges and limitations of existing RE4AI practices. The findings
highlighted that current RE applications were not adequately adaptable for
building AI systems and emphasised the need to provide new techniques and tools
to support RE4AI. [Conclusion] Our results showed that most of the empirical
studies on RE4AI focused on autonomous, self-driving vehicles and managing data
requirements, and areas such as ethics, trust, and explainability need further
research."
15205,": Preprint submitted to Elsevier  Page 30 of 45
                                           Requirements Engineering for AI Systems

Table 13
Mapping of the recommendations to the issues presented in literature

Recommendation                             Issue presented in the mapping study      Suggestions for further research
Identifying the need for AI                Overconﬁdence in Using AI                 Maintain a check point to list all required
                                                                                     elements needed to create an AI-based
Specifying requirements for AI systems     Deﬁning requirements - Non-functional     software system.","Also, the emergence of new requirements such as

Ahmad et al.","requirements - Emergence of new re-       Construct a reference map that would
Using existing RE tools to build AI soft-  quirements - Issues with data require-    capture the key components and at-
ware                                       ments                                     tributes needed when specifying AI sys-
                                           Nature of Machine Learning systems vs.    tem requirements
How do we decide on what modeling          traditional approach of RE - The emer-    Create a taxonomy that gathers all the
language to use?",2022-12-20 23:52:52+00:00,Requirements Engineering for Artificial Intelligence Systems: A Systematic Mapping Study,cs.SE,['cs.SE'],"[arxiv.Result.Author('Khlood Ahmad'), arxiv.Result.Author('Mohamed Abdelrazek'), arxiv.Result.Author('Chetan Arora'), arxiv.Result.Author('Muneera Bano'), arxiv.Result.Author('John Grundy')]","[Context] In traditional software systems, Requirements Engineering (RE)
activities are well-established and researched. However, building Artificial
Intelligence (AI) based software with limited or no insight into the system's
inner workings poses significant new challenges to RE. Existing literature has
focused on using AI to manage RE activities, with limited research on RE for AI
(RE4AI). [Objective] This paper investigates current approaches for specifying
requirements for AI systems, identifies available frameworks, methodologies,
tools, and techniques used to model requirements, and finds existing challenges
and limitations. [Method] We performed a systematic mapping study to find
papers on current RE4AI approaches. We identified 43 primary studies and
analysed the existing methodologies, models, tools, and techniques used to
specify and model requirements in real-world scenarios. [Results] We found
several challenges and limitations of existing RE4AI practices. The findings
highlighted that current RE applications were not adequately adaptable for
building AI systems and emphasised the need to provide new techniques and tools
to support RE4AI. [Conclusion] Our results showed that most of the empirical
studies on RE4AI focused on autonomous, self-driving vehicles and managing data
requirements, and areas such as ethics, trust, and explainability need further
research."
15281,"Our results indicate that the research       1) The introduction of long-standing mutants, as an impor-
                                        community should avoid the re-computation of mutant suites                tant category warranting further study.","We also demonstrate
                                        how consistent-by-construction long-standing mutant suites can        Speciﬁcally, this paper’s primary contributions are:
                                        be identiﬁed with a 10x improvement in mutant relevance over
                                        an arbitrary test suite.","and focus, instead, on long-standing mutants, thereby improving
                                        the consistency and relevance of mutation testing.",2022-12-22 14:53:45+00:00,Keeping Mutation Test Suites Consistent and Relevant with Long-Standing Mutants,cs.SE,['cs.SE'],"[arxiv.Result.Author('Milos Ojdanic'), arxiv.Result.Author('Mike Papadakis'), arxiv.Result.Author('Mark Harman')]","Mutation testing has been demonstrated to be one of the most powerful
fault-revealing tools in the tester's tool kit. Much previous work implicitly
assumed it to be sufficient to re-compute mutant suites per release. Sadly,
this makes mutation results inconsistent; mutant scores from each release
cannot be directly compared, making it harder to measure test improvement.
Furthermore, regular code change means that a mutant suite's relevance will
naturally degrade over time. We measure this degradation in relevance for
143,500 mutants in 4 non-trivial systems finding that, on overage, 52% degrade.
We introduce a mutant brittleness measure and use it to audit software systems
and their mutation suites. We also demonstrate how consistent-by-construction
long-standing mutant suites can be identified with a 10x improvement in mutant
relevance over an arbitrary test suite. Our results indicate that the research
community should avoid the re-computation of mutant suites and focus, instead,
on long-standing mutants, thereby improving the consistency and relevance of
mutation testing."
15282,"This suggests
   We believe that long-standing mutants are an interesting        future work on identifying mutants that have high ‘transitive
category in their own right, and worthy of further research.","If there are transitive dependencies between
                                                                   A on B then we can expect high degrees of mutant coupling,
                         V. FUTURE PLANS                           and even subsumption between the two regions.","dependence reach’ through their transitive dependencies, using
They have implications not only for mutation testing, but also     techniques such as slicing [27] and chopping [28].",2022-12-22 14:53:45+00:00,Keeping Mutation Test Suites Consistent and Relevant with Long-Standing Mutants,cs.SE,['cs.SE'],"[arxiv.Result.Author('Milos Ojdanic'), arxiv.Result.Author('Mike Papadakis'), arxiv.Result.Author('Mark Harman')]","Mutation testing has been demonstrated to be one of the most powerful
fault-revealing tools in the tester's tool kit. Much previous work implicitly
assumed it to be sufficient to re-compute mutant suites per release. Sadly,
this makes mutation results inconsistent; mutant scores from each release
cannot be directly compared, making it harder to measure test improvement.
Furthermore, regular code change means that a mutant suite's relevance will
naturally degrade over time. We measure this degradation in relevance for
143,500 mutants in 4 non-trivial systems finding that, on overage, 52% degrade.
We introduce a mutant brittleness measure and use it to audit software systems
and their mutation suites. We also demonstrate how consistent-by-construction
long-standing mutant suites can be identified with a 10x improvement in mutant
relevance over an arbitrary test suite. Our results indicate that the research
community should avoid the re-computation of mutant suites and focus, instead,
on long-standing mutants, thereby improving the consistency and relevance of
mutation testing."
15395,"They highlighted that, com-              Besides the additional steps towards validating the pro-
pared to previously used frameworks and processes, the               posed theoretical contribution to compositional architec-
compositional architecture framework helped their devel-             ture frameworks, further research can be conducted in how
opment in keeping a more structured and better overview              to apply category theory to system and software architec-
of the architecture, in ﬁnding trade-oﬀs easier between              tures.","Further research
case owners who applied the architectural framework in
their projects was favourable.","Category theory is a very broad ﬁeld of mathemat-
quality aspects, in establishing traceability of design de-          ics, which is why it is diﬃcult to ﬁnd a starting point to

                                                                 19
                Table 7: Additional suggestions for validation, including hypotheses and actions.",2022-12-27 18:05:02+00:00,A Compositional Approach to Creating Architecture Frameworks with an Application to Distributed AI Systems,cs.SE,"['cs.SE', 'cs.AI', 'D.2.1; D.2.11']","[arxiv.Result.Author('Hans-Martin Heyn'), arxiv.Result.Author('Eric Knauss'), arxiv.Result.Author('Patrizio Pelliccione')]","Artificial intelligence (AI) in its various forms finds more and more its way
into complex distributed systems. For instance, it is used locally, as part of
a sensor system, on the edge for low-latency high-performance inference, or in
the cloud, e.g. for data mining. Modern complex systems, such as connected
vehicles, are often part of an Internet of Things (IoT). To manage complexity,
architectures are described with architecture frameworks, which are composed of
a number of architectural views connected through correspondence rules. Despite
some attempts, the definition of a mathematical foundation for architecture
frameworks that are suitable for the development of distributed AI systems
still requires investigation and study. In this paper, we propose to extend the
state of the art on architecture framework by providing a mathematical model
for system architectures, which is scalable and supports co-evolution of
different aspects for example of an AI system. Based on Design Science
Research, this study starts by identifying the challenges with architectural
frameworks. Then, we derive from the identified challenges four rules and we
formulate them by exploiting concepts from category theory. We show how
compositional thinking can provide rules for the creation and management of
architectural frameworks for complex systems, for example distributed systems
with AI. The aim of the paper is not to provide viewpoints or architecture
models specific to AI systems, but instead to provide guidelines based on a
mathematical formulation on how a consistent framework can be built up with
existing, or newly created, viewpoints. To put in practice and test the
approach, the identified and formulated rules are applied to derive an
architectural framework for the EU Horizon 2020 project ``Very efficient deep
learning in the IoT"" (VEDLIoT) in the form of a case study."
15405,"Further, we present limitations of our theory   netic algorithms to create patches, other ways to generate patches
on which we hope to perform further research.","As with FL, there are multiple
debugging (specifically, instead of performing FL first, identifying      approaches: while the first APR technique, GenProg [3], used ge-
the type of patch first).","subsequently emerged, such as using templates as with PAR [21],
                                                                          generating constraints that patches should meet then solving those
   Overall, our contributions are:                                        constraints with SMT solvers, as with Angelix [11], or by using
                                                                          deep neural networks [22].",2022-12-28 10:34:44+00:00,A Bayesian Framework for Automated Debugging,cs.SE,['cs.SE'],"[arxiv.Result.Author('Sungmin Kang'), arxiv.Result.Author('Wonkeun Choi'), arxiv.Result.Author('Shin Yoo')]","Debugging takes up a significant portion of developer time. As a result,
automated debugging techniques including Fault Localization (FL) and Automated
Program Repair (APR) have garnered significant attention due to their potential
to aid developers in debugging tasks. Despite intensive research on these
subjects, we are unaware of a theoretic framework that highlights the
principles behind automated debugging and allows abstract analysis of
techniques. Such a framework would heighten our understanding of the endeavor
and provide a way to formally analyze techniques and approaches. To this end,
we first propose a Bayesian framework of understanding automated repair and
find that in conjunction with a concrete statement of the objective of
automated debugging, we can recover maximal fault localization formulae from
prior work, as well as analyze existing APR techniques and their underlying
assumptions.
  As a means of empirically demonstrating our framework, we further propose
BAPP, a Bayesian Patch Prioritization technique that incorporates intermediate
program values to analyze likely patch locations and repair actions, with its
core equations being derived by our Bayesian framework. We find that
incorporating program values allows BAPP to identify correct patches more
precisely: when applied to the patches generated by kPAR, the rankings produced
by BAPP reduce the number of required patch validation by 68% and consequently
reduce the repair time by 34 minutes on average. Further, BAPP improves the
precision of FL, increasing acc@5 on the studied bugs from 8 to 11. These
results highlight the potential of value-cognizant automated debugging
techniques, and further validates our theoretical framework. Finally, future
directions that the framework suggests are provided."
