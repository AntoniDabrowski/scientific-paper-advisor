,further research line,further research prefix,further research suffix,publication date,title,primary category,categories,authors,abstract
0,"This issue has motivated researchers

to conduct further research related to hospital resource optimization.","There are many challenges associated with growth in the healthcare sector, including

increased pressure on the limited resources of hospitals.","Since hospitalization

constitutes a significant cost of patient care, many researchers have been investigating the

problem of patient Length of Stay (LOS) prediction.",2021-12-30 03:48:41+00:00,A Literature Review on Length of Stay Prediction for Stroke Patients using Machine Learning and Statistical Approaches,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ola Alkhatib'), arxiv.Result.Author('Ayman Alahmar')]","Hospital length of stay (LOS) is one of the most essential healthcare metrics
that reflects the hospital quality of service and helps improve hospital
scheduling and management. LOS prediction helps in cost management because
patients who remain in hospitals usually do so in hospital units where
resources are severely limited. In this study, we reviewed papers on LOS
prediction using machine learning and statistical approaches. Our literature
review considers research studies that focus on LOS prediction for stroke
patients. Some of the surveyed studies revealed that authors reached
contradicting conclusions. For example, the age of the patient was considered
an important predictor of LOS for stroke patients in some studies, while other
studies concluded that age was not a significant factor. Therefore, additional
research is required in this domain to further understand the predictors of LOS
for stroke patients."
1,"We have
avoided Softmax for similar reason, and a further study can be conducted to characterize the scores with Softmax or other
modiﬁcations.","Furthermore, recall that we illustrated using the 2D
example the reason we avoid sigmoid function: suppressed change in the CO score due to its asymptotic part.","From here, the ideal vision is to develop a model that scales with CO score in not only a computationally
relevant way, but also in a human relevant way: we want a model that increases predictive probability when the heatmap
highlights exactly the correct localization of the objects or highly relevant features related to the objects.",2021-12-30 12:46:23+00:00,Augmentative eXplanation and the Distributional Gap of Confidence Optimization Score,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Erico Tjoa'), arxiv.Result.Author('Hong Jing Khok'), arxiv.Result.Author('Tushar Chouhan'), arxiv.Result.Author('Guan Cuntai')]","This paper introduces the Confidence Optimization (CO) score to directly
measure the contribution of heatmaps/saliency maps to the classification
performance of a model. Common heatmap generation methods used in the
eXplainable Artificial Intelligence (XAI) community are tested through a
process we call the Augmentative eXplanation (AX). We find a surprising
\textit{gap} in CO scores distribution on these heatmap methods. The gap
potentially serves as a novel indicator for the correctness of deep neural
network (DNN) prediction. We further introduces Generative AX (GAX) method to
generate saliency maps capable of attaining high CO scores. Using GAX, we also
qualitatively demonstrate the unintuitiveness of DNN architectures."
2,"We have
avoided Softmax for similar reason, and a further study can be conducted to characterize the scores with Softmax or other
modiﬁcations.","Furthermore, recall that we illustrated using the 2D
example the reason we avoid sigmoid function: suppressed change in the CO score due to its asymptotic part.","From here, the ideal vision is to develop a model that scales with CO score in not only a computationally
relevant way, but also in a human relevant way: we want a model that increases predictive probability when the heatmap
highlights exactly the correct localization of the objects or highly relevant features related to the objects.",2021-12-30 12:46:23+00:00,Improving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Erico Tjoa'), arxiv.Result.Author('Hong Jing Khok'), arxiv.Result.Author('Tushar Chouhan'), arxiv.Result.Author('Guan Cuntai')]","This paper quantifies the quality of heatmap-based eXplainable AI methods
w.r.t image classification problem. Here, a heatmap is considered desirable if
it improves the probability of predicting the correct classes. Different XAI
heatmap-based methods are empirically shown to improve classification
confidence to different extents depending on the datasets, e.g. Saliency works
best on ImageNet and Deconvolution on ChestX-Ray Pneumonia dataset. The novelty
includes a new gap distribution that shows a stark difference between correct
and wrong predictions. Finally, the generative augmentative explanation is
introduced, a method to generate heatmaps maps capable of improving predictive
confidence to a high level."
3,"We suppose further research in AIRL will be necessary to
ences over objectives, which is not always feasible when combining     prevent overfitting of the reward network.","However, MO-MPO requires explicit prefer-           input.","Similarly to Gleave and
learned rewards that are inherently difficult to compare.",2021-12-30 19:21:03+00:00,MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Markus Peschl'), arxiv.Result.Author('Arkady Zgonnikov'), arxiv.Result.Author('Frans A. Oliehoek'), arxiv.Result.Author('Luciano C. Siebert')]","Inferring reward functions from demonstrations and pairwise preferences are
auspicious approaches for aligning Reinforcement Learning (RL) agents with
human intentions. However, state-of-the art methods typically focus on learning
a single reward model, thus rendering it difficult to trade off different
reward functions from multiple experts. We propose Multi-Objective Reinforced
Active Learning (MORAL), a novel method for combining diverse demonstrations of
social norms into a Pareto-optimal policy. Through maintaining a distribution
over scalarization weights, our approach is able to interactively tune a deep
RL agent towards a variety of preferences, while eliminating the need for
computing multiple policies. We empirically demonstrate the effectiveness of
MORAL in two scenarios, which model a delivery and an emergency task that
require an agent to act in the presence of normative conflicts. Overall, we
consider our research a step towards multi-objective RL with learned rewards,
bridging the gap between current reward learning and machine ethics literature."
6,"RTD correlates strikingly well with the disagreement of models’ predictions; this is an intriguing
topic for further research.","We used the RTD to gain insights on neural networks representations in

                             9
computer vision and NLP domains for various problems: training dynamics analysis, data distribu-
tion shift, transfer learning, ensemble learning, disentanglement assessment.","Finally, R-Cross-Barcode and RTD are general tools and which are not
limited only to representations comparison.",2021-12-31 21:08:56+00:00,Representation Topology Divergence: A Method for Comparing Neural Network Representations,cs.LG,"['cs.LG', 'cs.AI', 'math.AT']","[arxiv.Result.Author('Serguei Barannikov'), arxiv.Result.Author('Ilya Trofimov'), arxiv.Result.Author('Nikita Balabin'), arxiv.Result.Author('Evgeny Burnaev')]","Comparison of data representations is a complex multi-aspect problem that has
not enjoyed a complete solution yet. We propose a method for comparing two data
representations. We introduce the Representation Topology Divergence (RTD),
measuring the dissimilarity in multi-scale topology between two point clouds of
equal size with a one-to-one correspondence between points. The data point
clouds are allowed to lie in different ambient spaces. The RTD is one of the
few TDA-based practical methods applicable to real machine learning datasets.
Experiments show that the proposed RTD agrees with the intuitive assessment of
data representation similarity and is sensitive to its topological structure.
We apply RTD to gain insights on neural networks representations in computer
vision and NLP domains for various problems: training dynamics analysis, data
distribution shift, transfer learning, ensemble learning, disentanglement
assessment."
15,"6 Future work

A fruitful direction for further research would be to investigate the effects of combining our current solutions
with some of the recent breakthroughs in regularization, feature engineering, etc.","Dataset                            Gain over           Gain over    Gain over
                                        MLP     TabTransformer         TabNet
bank_marketing
1995_income                                1.3                 1.0          3.1
blastchar                                  0.9                 0.7          2.5
                                           0.4                 0.5          1.6

Figure 3: AUROC gain charts for the 3 datasets - comparison between baseline TabTransformer (blue) and
the proposed GatedTabTransformer (orange).","6
Radostin Cholakov and Todor Kolev  The GatedTabTransformer.",2022-01-01 14:52:04+00:00,The GatedTabTransformer. An enhanced deep learning architecture for tabular modeling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Radostin Cholakov'), arxiv.Result.Author('Todor Kolev')]","There is an increasing interest in the application of deep learning
architectures to tabular data. One of the state-of-the-art solutions is
TabTransformer which incorporates an attention mechanism to better track
relationships between categorical features and then makes use of a standard MLP
to output its final logits. In this paper we propose multiple modifications to
the original TabTransformer performing better on binary classification tasks
for three separate datasets with more than 1% AUROC gains. Inspired by gated
MLP, linear projections are implemented in the MLP block and multiple
activation functions are tested. We also evaluate the importance of specific
hyper parameters during training."
36,"The relationship between
the post facto optimal solution and the ﬁnal prediction result could also oﬀer appealing topics for
further study.","This section oﬀers some thoughts on
possible improvements for the weighting model and the Autoencoder.","Firstly, one can do considerable optimization of the weighting model, DONUT.",2022-01-02 22:19:26+00:00,The DONUT Approach to EnsembleCombination Forecasting,cs.LG,"['cs.LG', 'econ.EM']","[arxiv.Result.Author('Lars Lien Ankile'), arxiv.Result.Author('Kjartan Krange')]","This paper presents an ensemble forecasting method that shows strong results
on the M4Competition dataset by decreasing feature and model selection
assumptions, termed DONUT(DO Not UTilize human assumptions). Our assumption
reductions, consisting mainly of auto-generated features and a more diverse
model pool for the ensemble, significantly outperforms the
statistical-feature-based ensemble method FFORMA by Montero-Manso et al.
(2020). Furthermore, we investigate feature extraction with a Long short-term
memory Network(LSTM) Autoencoder and find that such features contain crucial
information not captured by traditional statistical feature approaches. The
ensemble weighting model uses both LSTM features and statistical features to
combine the models accurately. Analysis of feature importance and interaction
show a slight superiority for LSTM features over the statistical ones alone.
Clustering analysis shows that different essential LSTM features are different
from most statistical features and each other. We also find that increasing the
solution space of the weighting model by augmenting the ensemble with new
models is something the weighting model learns to use, explaining part of the
accuracy gains. Lastly, we present a formal ex-post-facto analysis of optimal
combination and selection for ensembles, quantifying differences through linear
optimization on the M4 dataset. We also include a short proof that model
combination is superior to model selection, a posteriori."
37,"The relationship between the post-facto optimal solution and the ﬁnal
prediction result could also offer appealing topics for further study.","This section offers some thoughts on possible improvements
for the weighting model and the Autoencoder.","Firstly, one can do considerable optimization of the weighting model, DONUT.",2022-01-02 22:19:26+00:00,Deep Learning and Linear Programming for Automated Ensemble Forecasting and Interpretation,cs.LG,"['cs.LG', 'econ.EM']","[arxiv.Result.Author('Lars Lien Ankile'), arxiv.Result.Author('Kjartan Krange')]","This paper presents an ensemble forecasting method that shows strong results
on the M4 Competition dataset by decreasing feature and model selection
assumptions, termed DONUT (DO Not UTilize human beliefs). Our assumption
reductions, primarily consisting of auto-generated features and a more diverse
model pool for the ensemble, significantly outperform the statistical,
feature-based ensemble method FFORMA by Montero-Manso et al. (2020). We also
investigate feature extraction with a Long Short-term Memory Network (LSTM)
Autoencoder and find that such features contain crucial information not
captured by standard statistical feature approaches. The ensemble weighting
model uses LSTM and statistical features to combine the models accurately. The
analysis of feature importance and interaction shows a slight superiority for
LSTM features over the statistical ones alone. Clustering analysis shows that
essential LSTM features differ from most statistical features and each other.
We also find that increasing the solution space of the weighting model by
augmenting the ensemble with new models is something the weighting model learns
to use, thus explaining part of the accuracy gains. Moreover, we present a
formal ex-post-facto analysis of an optimal combination and selection for
ensembles, quantifying differences through linear optimization on the M4
dataset. Our findings indicate that classical statistical time series features,
such as trend and seasonality, alone do not capture all relevant information
for forecasting a time series. On the contrary, our novel LSTM features contain
significantly more predictive power than the statistical ones alone, but
combining the two feature sets proved the best in practice."
44,"RETA: A Schema-Aware, End-to-End Solution for Instance Completion in
   These positive results encourage us to explore further research                              Knowledge Graphs.",2021.,"In Proceedings of the WWW ’21: The Web Conference 2021,
                                                                                                Virtual Event / Ljubljana, Slovenia, April 19-23, 2021.",2022-01-03 10:25:10+00:00,Swift and Sure: Hardness-aware Contrastive Learning for Low-dimensional Knowledge Graph Embeddings,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kai Wang'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Quan Z. Sheng')]","Knowledge graph embedding (KGE) has drawn great attention due to its
potential in automatic knowledge graph (KG) completion and knowledge-driven
tasks. However, recent KGE models suffer from high training cost and large
storage space, thus limiting their practicality in real-world applications. To
address this challenge, based on the latest findings in the field of
Contrastive Learning, we propose a novel KGE training framework called
Hardness-aware Low-dimensional Embedding (HaLE). Instead of the traditional
Negative Sampling, we design a new loss function based on query sampling that
can balance two important training targets, Alignment and Uniformity.
Furthermore, we analyze the hardness-aware ability of recent low-dimensional
hyperbolic models and propose a lightweight hardness-aware activation
mechanism, which can help the KGE models focus on hard instances and speed up
convergence. The experimental results show that in the limited training time,
HaLE can effectively improve the performance and training speed of KGE models
on five commonly-used datasets. The HaLE-trained models can obtain a high
prediction accuracy after training few minutes and are competitive compared to
the state-of-the-art models in both low- and high-dimensional conditions."
45,"pre-trained high-dimensional KGE model generates soft labels
for each training sample and accelerates the convergence of               These positive results encourage us to explore further research
the small student model.",A       faster convergence speed in limited training time.,MulDE [29] is a multi-teacher knowl-          activities in the future.,2022-01-03 10:25:10+00:00,Swift and Sure: Hardness-aware Contrastive Learning for Low-dimensional Knowledge Graph Embeddings,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kai Wang'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Quan Z. Sheng')]","Knowledge graph embedding (KGE) has shown great potential in automatic
knowledge graph (KG) completion and knowledge-driven tasks. However, recent KGE
models suffer from high training cost and large storage space, thus limiting
their practicality in real-world applications. To address this challenge, based
on the latest findings in the field of Contrastive Learning, we propose a novel
KGE training framework called Hardness-aware Low-dimensional Embedding (HaLE).
Instead of the traditional Negative Sampling, we design a new loss function
based on query sampling that can balance two important training targets,
Alignment and Uniformity. Furthermore, we analyze the hardness-aware ability of
recent low-dimensional hyperbolic models and propose a lightweight
hardness-aware activation mechanism. The experimental results show that in the
limited training time, HaLE can effectively improve the performance and
training speed of KGE models on five commonly-used datasets. After training
just a few minutes, the HaLE-trained models are competitive compared to the
state-of-the-art models in both low- and high-dimensional conditions."
52,"The book provides a carefully curated bibliography to guide further study, whether
for interview preparation or simply as a matter of interest or job-relevant research.",A note about Bibliography                                                                            ACKNOWLEDGEMENTS.,"A                    The thanks and acknowledgements of the publisher are due to the following:
comprehensive bibliography would be far too long to include here, and would be of                      My dear son, Amir Ivry, Matthew Isaac Harvey, Sandy Noymer, Steve foot and Velimir
little immediate use, so the selections have been made with deliberate attention to the                Gayevskiy.",2021-12-30 13:28:27+00:00,Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI,cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Shlomo Kashani'), arxiv.Result.Author('Amir Ivry')]","The second edition of Deep Learning Interviews is home to hundreds of
fully-solved problems, from a wide range of key topics in AI. It is designed to
both rehearse interview or exam specific topics and provide machine learning M.
Sc./Ph. D. students, and those awaiting an interview a well-organized overview
of the field. The problems it poses are tough enough to cut your teeth on and
to dramatically improve your skills-but they're framed within thought-provoking
questions and engaging stories. That is what makes the volume so specifically
valuable to students and job seekers: it provides them with the ability to
speak confidently and quickly on any relevant topic, to answer technical
questions clearly and correctly, and to fully understand the purpose and
meaning of interview questions and answers. Those are powerful, indispensable
advantages to have when walking into the interview room. The book's contents is
a large inventory of numerous topics relevant to DL job interviews and graduate
level exams. That places this work at the forefront of the growing trend in
science to teach a core set of practical mathematical and computational skills.
It is widely accepted that the training of every computer scientist must
include the fundamental theorems of ML, and AI appears in the curriculum of
nearly every university. This volume is designed as an excellent reference for
graduates of such programs."
53,"A note about Bibliography
The book provides a carefully curated bibliography to guide further study, whether
for interview preparation or simply as a matter of interest or job-relevant research.","It will
       inevitably prove more successful at handling some of them than others, but it
       has at least made a sincere and devoted effort.","A
comprehensive bibliography would be far too long to include here, and would be of
little immediate use, so the selections have been made with deliberate attention to the
value of each included text.",2021-12-30 13:28:27+00:00,Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI,cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Shlomo Kashani'), arxiv.Result.Author('Amir Ivry')]","The second edition of Deep Learning Interviews is home to hundreds of
fully-solved problems, from a wide range of key topics in AI. It is designed to
both rehearse interview or exam specific topics and provide machine learning
MSc / PhD. students, and those awaiting an interview a well-organized overview
of the field. The problems it poses are tough enough to cut your teeth on and
to dramatically improve your skills-but they're framed within thought-provoking
questions and engaging stories. That is what makes the volume so specifically
valuable to students and job seekers: it provides them with the ability to
speak confidently and quickly on any relevant topic, to answer technical
questions clearly and correctly, and to fully understand the purpose and
meaning of interview questions and answers. Those are powerful, indispensable
advantages to have when walking into the interview room. The book's contents is
a large inventory of numerous topics relevant to DL job interviews and graduate
level exams. That places this work at the forefront of the growing trend in
science to teach a core set of practical mathematical and computational skills.
It is widely accepted that the training of every computer scientist must
include the fundamental theorems of ML, and AI appears in the curriculum of
nearly every university. This volume is designed as an excellent reference for
graduates of such programs."
55,"Although there has been numerous        6) We motivate further research in this realm by presenting
articles on deep learning for other RF signal intelligence               open research challenges and future directions.",less protocol classiﬁcation.,"approaches (modulation and wireless protocol classiﬁcation)
[4]–[14], a comprehensive presentation spanning conventional          We emphasize here that unlike existing surveys, our article
principled approaches as well as supervised deep learning for      is comprehensive in presenting all aspects of RF ﬁnger-
RF ﬁngerprinting is lacking.",2022-01-03 14:42:53+00:00,"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Anu Jagannath'), arxiv.Result.Author('Jithin Jagannath'), arxiv.Result.Author('Prem Sagar Pattanshetty Vasanth Kumar')]","Fifth generation (5G) networks and beyond envisions massive Internet of
Things (IoT) rollout to support disruptive applications such as extended
reality (XR), augmented/virtual reality (AR/VR), industrial automation,
autonomous driving, and smart everything which brings together massive and
diverse IoT devices occupying the radio frequency (RF) spectrum. Along with
spectrum crunch and throughput challenges, such a massive scale of wireless
devices exposes unprecedented threat surfaces. RF fingerprinting is heralded as
a candidate technology that can be combined with cryptographic and zero-trust
security measures to ensure data privacy, confidentiality, and integrity in
wireless networks. Motivated by the relevance of this subject in the future
communication networks, in this work, we present a comprehensive survey of RF
fingerprinting approaches ranging from a traditional view to the most recent
deep learning (DL) based algorithms. Existing surveys have mostly focused on a
constrained presentation of the wireless fingerprinting approaches, however,
many aspects remain untold. In this work, however, we mitigate this by
addressing every aspect - background on signal intelligence (SIGINT),
applications, relevant DL algorithms, systematic literature review of RF
fingerprinting techniques spanning the past two decades, discussion on
datasets, and potential research avenues - necessary to elucidate this topic to
the reader in an encyclopedic manner."
56,"Although there has been numerous        6) We motivate further research in this realm by presenting
articles on deep learning for other RF signal intelligence               open research challenges and future directions.",less protocol classiﬁcation.,"approaches (modulation and wireless protocol classiﬁcation)
[4]–[14], a comprehensive presentation spanning conventional          We emphasize here that unlike existing surveys, our article
principled approaches as well as supervised deep learning for      is comprehensive in presenting all aspects of RF ﬁnger-
RF ﬁngerprinting is lacking.",2022-01-03 14:42:53+00:00,"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Anu Jagannath'), arxiv.Result.Author('Jithin Jagannath'), arxiv.Result.Author('Prem Sagar Pattanshetty Vasanth Kumar')]","Fifth generation (5G) networks and beyond envisions massive Internet of
Things (IoT) rollout to support disruptive applications such as extended
reality (XR), augmented/virtual reality (AR/VR), industrial automation,
autonomous driving, and smart everything which brings together massive and
diverse IoT devices occupying the radio frequency (RF) spectrum. Along with
spectrum crunch and throughput challenges, such a massive scale of wireless
devices exposes unprecedented threat surfaces. RF fingerprinting is heralded as
a candidate technology that can be combined with cryptographic and zero-trust
security measures to ensure data privacy, confidentiality, and integrity in
wireless networks. Motivated by the relevance of this subject in the future
communication networks, in this work, we present a comprehensive survey of RF
fingerprinting approaches ranging from a traditional view to the most recent
deep learning (DL) based algorithms. Existing surveys have mostly focused on a
constrained presentation of the wireless fingerprinting approaches, however,
many aspects remain untold. In this work, however, we mitigate this by
addressing every aspect - background on signal intelligence (SIGINT),
applications, relevant DL algorithms, systematic literature review of RF
fingerprinting techniques spanning the past two decades, discussion on
datasets, and potential research avenues - necessary to elucidate this topic to
the reader in an encyclopedic manner."
57,"Although there has been numerous        6) We motivate further research in this realm by presenting
articles on deep learning for other RF signal intelligence               open research challenges and future directions.",less protocol classiﬁcation.,"approaches (modulation and wireless protocol classiﬁcation)
[4]–[14], a comprehensive presentation spanning conventional          We emphasize here that unlike existing surveys, our article
principled approaches as well as supervised deep learning for      is comprehensive in presenting all aspects of RF ﬁnger-
RF ﬁngerprinting is lacking.",2022-01-03 14:42:53+00:00,"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting: Traditional Approaches, Deep Learning, and Open Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Anu Jagannath'), arxiv.Result.Author('Jithin Jagannath'), arxiv.Result.Author('Prem Sagar Pattanshetty Vasanth Kumar')]","Fifth generation (5G) network and beyond envision massive Internet of Things
(IoT) rollout to support disruptive applications such as extended reality (XR),
augmented/virtual reality (AR/VR), industrial automation, autonomous driving,
and smart everything which brings together massive and diverse IoT devices
occupying the radio frequency (RF) spectrum. Along with the spectrum crunch and
throughput challenges, such a massive scale of wireless devices exposes
unprecedented threat surfaces. RF fingerprinting is heralded as a candidate
technology that can be combined with cryptographic and zero-trust security
measures to ensure data privacy, confidentiality, and integrity in wireless
networks. Motivated by the relevance of this subject in the future
communication networks, in this work, we present a comprehensive survey of RF
fingerprinting approaches ranging from a traditional view to the most recent
deep learning (DL)-based algorithms. Existing surveys have mostly focused on a
constrained presentation of the wireless fingerprinting approaches, however,
many aspects remain untold. In this work, however, we mitigate this by
addressing every aspect - background on signal intelligence (SIGINT),
applications, relevant DL algorithms, systematic literature review of RF
fingerprinting techniques spanning the past two decades, discussion on
datasets, and potential research avenues - necessary to elucidate this topic to
the reader in an encyclopedic manner."
72,"A possible way to solve this issue is to include
homomorphic encryption in the protocol to anonymize the models submitted on the
smart contracts;

   Furthermore, many of the protocols or marketplaces so far do not consider
transaction privacy, and so the further study may be necessary for conjunction with
[15].","That is to say, any user can access the model weights
submitted on the DanKu contracts.","4.3 Cost Control

The organizers of the protocol and marketplace also need to take into account the
control of the cost e.g.",2022-01-04 04:47:45+00:00,Survey on the Convergence of Machine Learning and Blockchain,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Shengwen Ding'), arxiv.Result.Author('Chenhui Hu')]","Machine learning (ML) has been pervasively researched nowadays and it has
been applied in many aspects of real life. Nevertheless, issues of model and
data still accompany the development of ML. For instance, training of
traditional ML models is limited to the access of data sets, which are
generally proprietary; published ML models may soon be out of date without an
update of new data and continuous training; malicious data contributors may
upload wrongly labeled data that leads to undesirable training results; and the
abuse of private data and data leakage also exit. With the utilization of
blockchain, an emerging and swiftly developing technology, these problems can
be efficiently solved. In this paper, we survey the convergence of
collaborative ML and blockchain. Different ways of the combination of these two
technologies are investigated and their fields of application are examined.
Discussion on the limitations of current research and their future directions
are also included."
121,"Since in FL, each attack and fault can appear independently in each
client, we further study the effect of the proportion of affected clients on the attack’s
success.","To better understand the current state-of-the-art (SOTA) FL techniques against
common attacks and quality issues, in our study, we perform a set of experiments on
four FL attacks (Label Flip (Fang et al., 2020), Sign Flip (Blanchard et al., 2017),
Random Update (Blanchard et al., 2017), and Backdoor attack (Sun et al., 2019)) and
four DL mutation operators (Noise, Overlap, Delete and Unbalance) (Humbatova
et al., 2021).","Additionally, we run the experiments on both generic and real-world federated
medical image datasets.",2022-01-05 02:06:39+00:00,Towards Understanding Quality Challenges of the Federated Learning: A First Look from the Lens of Robustness,cs.LG,['cs.LG'],"[arxiv.Result.Author('Amin Eslami Abyane'), arxiv.Result.Author('Derui Zhu'), arxiv.Result.Author('Roberto Medeiros de Souza'), arxiv.Result.Author('Lei Ma'), arxiv.Result.Author('Hadi Hemmati')]","Federated learning (FL) is a widely adopted distributed learning paradigm in
practice, which intends to preserve users' data privacy while leveraging the
entire dataset of all participants for training. In FL, multiple models are
trained independently on the users and aggregated centrally to update a global
model in an iterative process. Although this approach is excellent at
preserving privacy by design, FL still tends to suffer from quality issues such
as attacks or byzantine faults. Some recent attempts have been made to address
such quality challenges on the robust aggregation techniques for FL. However,
the effectiveness of state-of-the-art (SOTA) robust FL techniques is still
unclear and lacks a comprehensive study. Therefore, to better understand the
current quality status and challenges of these SOTA FL techniques in the
presence of attacks and faults, in this paper, we perform a large-scale
empirical study to investigate the SOTA FL's quality from multiple angles of
attacks, simulated faults (via mutation operators), and aggregation (defense)
methods. In particular, we perform our study on two generic image datasets and
one real-world federated medical image dataset. We also systematically
investigate the effect of the distribution of attacks/faults over users and the
independent and identically distributed (IID) factors, per dataset, on the
robustness results. After a large-scale analysis with 496 configurations, we
find that most mutators on each individual user have a negligible effect on the
final model. Moreover, choosing the most robust FL aggregator depends on the
attacks and datasets. Finally, we illustrate that it is possible to achieve a
generic solution that works almost as well or even better than any single
aggregator on all attacks and configurations with a simple ensemble model of
aggregators."
122,"It also provides guidance on FL robustness areas that require
further research.",software in practice.,"2 Background

In this section, we brieﬂy introduce basic concepts needed for understanding this
paper, including FL and its attacks and defense mechanisms and mutation testing for
deep learning.",2022-01-05 02:06:39+00:00,Towards Understanding Quality Challenges of the Federated Learning: A First Look from the Lens of Robustness,cs.LG,['cs.LG'],"[arxiv.Result.Author('Amin Eslami Abyane'), arxiv.Result.Author('Derui Zhu'), arxiv.Result.Author('Roberto Medeiros de Souza'), arxiv.Result.Author('Lei Ma'), arxiv.Result.Author('Hadi Hemmati')]","Federated learning (FL) is a widely adopted distributed learning paradigm in
practice, which intends to preserve users' data privacy while leveraging the
entire dataset of all participants for training. In FL, multiple models are
trained independently on the users and aggregated centrally to update a global
model in an iterative process. Although this approach is excellent at
preserving privacy by design, FL still tends to suffer from quality issues such
as attacks or byzantine faults. Some recent attempts have been made to address
such quality challenges on the robust aggregation techniques for FL. However,
the effectiveness of state-of-the-art (SOTA) robust FL techniques is still
unclear and lacks a comprehensive study. Therefore, to better understand the
current quality status and challenges of these SOTA FL techniques in the
presence of attacks and faults, in this paper, we perform a large-scale
empirical study to investigate the SOTA FL's quality from multiple angles of
attacks, simulated faults (via mutation operators), and aggregation (defense)
methods. In particular, we perform our study on two generic image datasets and
one real-world federated medical image dataset. We also systematically
investigate the effect of the distribution of attacks/faults over users and the
independent and identically distributed (IID) factors, per dataset, on the
robustness results. After a large-scale analysis with 496 configurations, we
find that most mutators on each individual user have a negligible effect on the
final model. Moreover, choosing the most robust FL aggregator depends on the
attacks and datasets. Finally, we illustrate that it is possible to achieve a
generic solution that works almost as well or even better than any single
aggregator on all attacks and configurations with a simple ensemble model of
aggregators."
159,"In a further study it would be interesting to study the
between assets.","Stablecoins USDC, USDT and BUSD stand
                                                                    out from the others on ﬁgure 8, it is however not surprising
   Figures 2, 3 and tables I and II demonstrate that a signiﬁ-      because they are not extensively impacted by large market
cant number of synchronous and asynchronous relations exist         moves.","The number of asynchronous relations rapidly        relationship between the market return and the discrepancy of
decreases to 0 when the lag becomes large.",2022-01-05 14:40:32+00:00,Deep Fusion of Lead-lag Graphs:Application to Cryptocurrencies,cs.LG,['cs.LG'],"[arxiv.Result.Author('Hugo Schnoering'), arxiv.Result.Author('Hugo Inzirillo')]","The study of time series has motivated many researchers, particularly on the
area of multivariate-analysis. The study of co-movements and dependency between
random variables leads us to develop metrics to describe existing connection
between assets. The most commonly used are correlation and causality. Despite
the growing literature, some connections remained still undetected. The
objective of this paper is to propose a new representation learning algorithm
capable to integrate synchronous and asynchronous relationships."
160,"In a further study it would be interesting to study the
between assets.","Stablecoins USDC, USDT and BUSD stand
                                                                    out from the others on ﬁgure 8, it is however not surprising
   Figures 2, 3 and tables I and II demonstrate that a signiﬁ-      because they are not extensively impacted by large market
cant number of synchronous and asynchronous relations exist         moves.","The number of asynchronous relations rapidly        relationship between the market return and the discrepancy of
decreases to 0 when the lag becomes large.",2022-01-05 14:40:32+00:00,Deep Fusion of Lead-lag Graphs: Application to Cryptocurrencies,cs.LG,['cs.LG'],"[arxiv.Result.Author('Hugo Schnoering'), arxiv.Result.Author('Hugo Inzirillo')]","The study of time series has motivated many researchers, particularly on the
area of multivariate-analysis. The study of co-movements and dependency between
random variables leads us to develop metrics to describe existing connection
between assets. The most commonly used are correlation and causality. Despite
the growing literature, some connections remained still undetected. The
objective of this paper is to propose a new representation learning algorithm
capable to integrate synchronous and asynchronous relationships."
170,"(λb = 0.7)
further research question: In the battle of the beneﬁts and draw-   Noise      R@20 N@20             R@20 N@20
backs of multi-cause confounders, which one will prevail?",(λb = 0.3)  High Conf.,"The       0.1        0.4071 0.3974         0.4098 0.4039
answer, which the authors believe, is that no matter the results,   0.5        0.4034 0.3963         0.4036 0.3982
a deconfounded recommender should always be preferred over          0.9        0.4010 0.3928         0.3993 0.3934
a non-causality-based one if unobserved confounders indeed          N.F.",2022-01-06 15:00:01+00:00,Deep Causal Reasoning for Recommendations,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Yaochen Zhu'), arxiv.Result.Author('Jing Yi'), arxiv.Result.Author('Jiayi Xie'), arxiv.Result.Author('Zhenzhong Chen')]","Traditional recommender systems aim to estimate a user's rating to an item
based on observed ratings from the population. As with all observational
studies, hidden confounders, which are factors that affect both item exposures
and user ratings, lead to a systematic bias in the estimation. Consequently, a
new trend in recommender system research is to negate the influence of
confounders from a causal perspective. Observing that confounders in
recommendations are usually shared among items and are therefore multi-cause
confounders, we model the recommendation as a multi-cause multi-outcome (MCMO)
inference problem. Specifically, to remedy confounding bias, we estimate
user-specific latent variables that render the item exposures independent
Bernoulli trials. The generative distribution is parameterized by a DNN with
factorized logistic likelihood and the intractable posteriors are estimated by
variational inference. Controlling these factors as substitute confounders,
under mild assumptions, can eliminate the bias incurred by multi-cause
confounders. Furthermore, we show that MCMO modeling may lead to high variance
due to scarce observations associated with the high-dimensional causal space.
Fortunately, we theoretically demonstrate that introducing user features as
pre-treatment variables can substantially improve sample efficiency and
alleviate overfitting. Empirical studies on simulated and real-world datasets
show that the proposed deep causal recommender shows more robustness to
unobserved confounders than state-of-the-art causal recommenders. Codes and
datasets are released at https://github.com/yaochenzhu/deep-deconf."
171,"(λb = 0.7)
further research question: In the battle of the beneﬁts and draw-   Noise      R@20 N@20             R@20 N@20
backs of multi-cause confounders, which one will prevail?",(λb = 0.3)  High Conf.,"The       0.1        0.4071 0.3974         0.4098 0.4039
answer, which the authors believe, is that no matter the results,   0.5        0.4034 0.3963         0.4036 0.3982
a deconfounded recommender should always be preferred over          0.9        0.4010 0.3928         0.3993 0.3934
a non-causality-based one if unobserved confounders indeed          N.F.",2022-01-06 15:00:01+00:00,Deep Causal Reasoning for Recommendations,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Yaochen Zhu'), arxiv.Result.Author('Jing Yi'), arxiv.Result.Author('Jiayi Xie'), arxiv.Result.Author('Zhenzhong Chen')]","Traditional recommender systems aim to estimate a user's rating to an item
based on observed ratings from the population. As with all observational
studies, hidden confounders, which are factors that affect both item exposures
and user ratings, lead to a systematic bias in the estimation. Consequently, a
new trend in recommender system research is to negate the influence of
confounders from a causal perspective. Observing that confounders in
recommendations are usually shared among items and are therefore multi-cause
confounders, we model the recommendation as a multi-cause multi-outcome (MCMO)
inference problem. Specifically, to remedy confounding bias, we estimate
user-specific latent variables that render the item exposures independent
Bernoulli trials. The generative distribution is parameterized by a DNN with
factorized logistic likelihood and the intractable posteriors are estimated by
variational inference. Controlling these factors as substitute confounders,
under mild assumptions, can eliminate the bias incurred by multi-cause
confounders. Furthermore, we show that MCMO modeling may lead to high variance
due to scarce observations associated with the high-dimensional causal space.
Fortunately, we theoretically demonstrate that introducing user features as
pre-treatment variables can substantially improve sample efficiency and
alleviate overfitting. Empirical studies on simulated and real-world datasets
show that the proposed deep causal recommender shows more robustness to
unobserved confounders than state-of-the-art causal recommenders. Codes and
datasets are released at https://github.com/yaochenzhu/deep-deconf."
217,"The creation of effective negative samples
and robust generalisability remain open research questions in need of further study.","In the context of our work, the need
for diverse and effective negative samples is pertinent, given the number of false positive
predictions despite relatively high confidence.","Human curation augmentation for PTM‑PPI extraction
The prediction quality can be improved by providing more training samples [34, 35],
which requires manual curation.",2022-01-06 19:59:14+00:00,Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Aparna Elangovan'), arxiv.Result.Author('Yuan Li'), arxiv.Result.Author('Douglas E. V. Pires'), arxiv.Result.Author('Melissa J. Davis'), arxiv.Result.Author('Karin Verspoor')]","Protein-protein interactions (PPIs) are critical to normal cellular function
and are related to many disease pathways. However, only 4% of PPIs are
annotated with PTMs in biological knowledge databases such as IntAct, mainly
performed through manual curation, which is neither time nor cost-effective. We
use the IntAct PPI database to create a distant supervised dataset annotated
with interacting protein pairs, their corresponding PTM type, and associated
abstracts from the PubMed database. We train an ensemble of BioBERT models -
dubbed PPI-BioBERT-x10 to improve confidence calibration. We extend the use of
ensemble average confidence approach with confidence variation to counteract
the effects of class imbalance to extract high confidence predictions. The
PPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro
41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low
variation to identify high quality predictions, tuning the predictions for
precision, we retained 19% of the test predictions with 100% precision. We
evaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6
million (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ~ 5700
(4584 unique) high confidence predictions. Of the 5700, human evaluation on a
small randomly sampled subset shows that the precision drops to 33.7% despite
confidence calibration and highlights the challenges of generalisability beyond
the test set even with confidence calibration. We circumvent the problem by
only including predictions associated with multiple papers, improving the
precision to 58.8%. In this work, we highlight the benefits and challenges of
deep learning-based text mining in practice, and the need for increased
emphasis on confidence calibration to facilitate human curation efforts."
225,"Lastly, we conclude with a summary and an outlook for further research
in Section 6.","The neural architecture and our intrinsic model evaluation are then presented in Section 4,
followed by the numerical results in Section 5.","2 General framework

Figure 2 illustrates two common examples in life insurance, where a policyholder evolves over time and can change
or maintain its state of being active, invalid or dead.",2022-01-07 11:03:46+00:00,Neural calibration of hidden inhomogeneous Markov chains -- Information decompression in life insurance,cs.LG,"['cs.LG', 'q-fin.ST']","[arxiv.Result.Author('Mark Kiermayer'), arxiv.Result.Author('Christian Weiß')]","Markov chains play a key role in a vast number of areas, including life
insurance mathematics. Standard actuarial quantities as the premium value can
be interpreted as compressed, lossy information about the underlying Markov
process. We introduce a method to reconstruct the underlying Markov chain given
collective information of a portfolio of contracts. Our neural architecture
explainably characterizes the process by explicitly providing one-step
transition probabilities. Further, we provide an intrinsic, economic model
validation to inspect the quality of the information decompression. Lastly, our
methodology is successfully tested for a realistic data set of German term life
insurance contracts."
263,These are directions for further research.,It is also interesting to extend the approach on deep forests [34].,"Another interesting direction for research is to investigate various
functions instead of the softmax which is used in the proposed models.",2022-01-08 19:35:57+00:00,Attention-based Random Forest and Contamination Model,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","A new approach called ABRF (the attention-based random forest) and its
modifications for applying the attention mechanism to the random forest (RF)
for regression and classification are proposed. The main idea behind the
proposed ABRF models is to assign attention weights with trainable parameters
to decision trees in a specific way. The weights depend on the distance between
an instance, which falls into a corresponding leaf of a tree, and instances,
which fall in the same leaf. This idea stems from representation of the
Nadaraya-Watson kernel regression in the form of a RF. Three modifications of
the general approach are proposed. The first one is based on applying the
Huber's contamination model and on computing the attention weights by solving
quadratic or linear optimization problems. The second and the third
modifications use the gradient-based algorithms for computing trainable
parameters. Numerical experiments with various regression and classification
datasets illustrate the proposed method."
264,"Choice of the kernel functions, which lead to simple computations and outperforming
prediction results, is a direction for further research.","It is obvious that kernel functions
should be used.","In models ABRF-1 and ABRF-3, objective functions
have been optimized over the whole set of weights, i.e., over the unit simplex.",2022-01-08 19:35:57+00:00,Attention-based Random Forest and Contamination Model,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","A new approach called ABRF (the attention-based random forest) and its
modifications for applying the attention mechanism to the random forest (RF)
for regression and classification are proposed. The main idea behind the
proposed ABRF models is to assign attention weights with trainable parameters
to decision trees in a specific way. The weights depend on the distance between
an instance, which falls into a corresponding leaf of a tree, and instances,
which fall in the same leaf. This idea stems from representation of the
Nadaraya-Watson kernel regression in the form of a RF. Three modifications of
the general approach are proposed. The first one is based on applying the
Huber's contamination model and on computing the attention weights by solving
quadratic or linear optimization problems. The second and the third
modifications use the gradient-based algorithms for computing trainable
parameters. Numerical experiments with various regression and classification
datasets illustrate the proposed method."
265,"These
ideas can be also viewed as directions for further research.","Moreover, there
are other statistical contamination models [36] which could be incorporated into the ABRF models.","Finally, we have considered the distance between
vectors xs and Ak(xs) inside one leaf where the instance xs falls into.",2022-01-08 19:35:57+00:00,Attention-based Random Forest and Contamination Model,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","A new approach called ABRF (the attention-based random forest) and its
modifications for applying the attention mechanism to the random forest (RF)
for regression and classification are proposed. The main idea behind the
proposed ABRF models is to assign attention weights with trainable parameters
to decision trees in a specific way. The weights depend on the distance between
an instance, which falls into a corresponding leaf of a tree, and instances,
which fall in the same leaf. This idea stems from representation of the
Nadaraya-Watson kernel regression in the form of a RF. Three modifications of
the general approach are proposed. The first one is based on applying the
Huber's contamination model and on computing the attention weights by solving
quadratic or linear optimization problems. The second and the third
modifications use the gradient-based algorithms for computing trainable
parameters. Numerical experiments with various regression and classification
datasets illustrate the proposed method."
266,"This is also an interesting direction
for further research.","Moreover, there are diﬀerent deﬁnitions of vector Ak(xs)
itself, for example, we can use the median instead of the mean value.","21
Acknowledgement

This work is supported by the Russian Science Foundation under grant 21-11-00116.",2022-01-08 19:35:57+00:00,Attention-based Random Forest and Contamination Model,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","A new approach called ABRF (the attention-based random forest) and its
modifications for applying the attention mechanism to the random forest (RF)
for regression and classification are proposed. The main idea behind the
proposed ABRF models is to assign attention weights with trainable parameters
to decision trees in a specific way. The weights depend on the distance between
an instance, which falls into a corresponding leaf of a tree, and instances,
which fall in the same leaf. This idea stems from representation of the
Nadaraya-Watson kernel regression in the form of a RF. Three modifications of
the general approach are proposed. The first one is based on applying the
Huber's contamination model and on computing the attention weights by solving
quadratic or linear optimization problems. The second and the third
modifications use the gradient-based algorithms for computing trainable
parameters. Numerical experiments with various regression and classification
datasets illustrate the proposed method."
272,"To be sure, this particular application to healthcare opens interesting avenues of further research
to the expanding scope of open-set recognition.","Whether it is reconstruction or not,
latent representations must encapsulate structural information of the data to be more eﬀective.","Likewise, this study hopefully represents a stride
towards these techniques beneﬁting actual patients’ treatments in the future.",2022-01-09 04:35:55+00:00,Open-Set Recognition of Breast Cancer Treatments,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexander Cao'), arxiv.Result.Author('Diego Klabjan'), arxiv.Result.Author('Yuan Luo')]","Open-set recognition generalizes a classification task by classifying test
samples as one of the known classes from training or ""unknown."" As novel cancer
drug cocktails with improved treatment are continually discovered, predicting
cancer treatments can naturally be formulated in terms of an open-set
recognition problem. Drawbacks, due to modeling unknown samples during
training, arise from straightforward implementations of prior work in
healthcare open-set learning. Accordingly, we reframe the problem methodology
and apply a recent existing Gaussian mixture variational autoencoder model,
which achieves state-of-the-art results for image datasets, to breast cancer
patient data. Not only do we obtain more accurate and robust classification
results, with a 24.5% average F1 increase compared to a recent method, but we
also reexamine open-set recognition in terms of deployability to a clinical
setting."
318,"These results are promising for further research on end-to-end
automatic scoring of descriptive answers.","As QWK is over 0.8, it represents
acceptable similarity of scoring between the automatic scoring system and the
human examiners.","Keywords— handwritten language answers, handwriting recognition, automatic
scoring, ensemble recognition, deep neural networks

This is the preprint revision.",2022-01-10 08:47:52+00:00,Fully automatic scoring of handwritten descriptive answers in Japanese language tests,cs.LG,"['cs.LG', 'cs.CL', 'cs.CV']","[arxiv.Result.Author('Hung Tuan Nguyen'), arxiv.Result.Author('Cuong Tuan Nguyen'), arxiv.Result.Author('Haruki Oka'), arxiv.Result.Author('Tsunenori Ishioka'), arxiv.Result.Author('Masaki Nakagawa')]","This paper presents an experiment of automatically scoring handwritten
descriptive answers in the trial tests for the new Japanese university entrance
examination, which were made for about 120,000 examinees in 2017 and 2018.
There are about 400,000 answers with more than 20 million characters. Although
all answers have been scored by human examiners, handwritten characters are not
labelled. We present our attempt to adapt deep neural network-based handwriting
recognizers trained on a labelled handwriting dataset into this unlabeled
answer set. Our proposed method combines different training strategies,
ensembles multiple recognizers, and uses a language model built from a large
general corpus to avoid overfitting into specific data. In our experiment, the
proposed method records character accuracy of over 97% using about 2,000
verified labelled answers that account for less than 0.5% of the dataset. Then,
the recognized answers are fed into a pre-trained automatic scoring system
based on the BERT model without correcting misrecognized characters and
providing rubric annotations. The automatic scoring system achieves from 0.84
to 0.98 of Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents
acceptable similarity of scoring between the automatic scoring system and the
human examiners. These results are promising for further research on end-to-end
automatic scoring of descriptive answers."
351,"The answer to this question is especially crucial if we want to further study
oﬄine MARL with a large number of agents and we do not want the sample complexity scales exponentially
with the number of agents.","Another direction is to design decentralized
algorithm for oﬄine MARL.","Lastly, in this paper we only focus on the most fundamental tabular setting.",2022-01-10 18:34:32+00:00,When is Offline Two-Player Zero-Sum Markov Game Solvable?,cs.LG,"['cs.LG', 'cs.AI', 'cs.GT', 'stat.ML']","[arxiv.Result.Author('Qiwen Cui'), arxiv.Result.Author('Simon S. Du')]","We study what dataset assumption permits solving offline two-player zero-sum
Markov game. In stark contrast to the offline single-agent Markov decision
process, we show that the single strategy concentration assumption is
insufficient for learning the Nash equilibrium (NE) strategy in offline
two-player zero-sum Markov games. On the other hand, we propose a new
assumption named unilateral concentration and design a pessimism-type algorithm
that is provably efficient under this assumption. In addition, we show that the
unilateral concentration assumption is necessary for learning an NE strategy.
Furthermore, our algorithm can achieve minimax sample complexity without any
modification for two widely studied settings: dataset with uniform
concentration assumption and turn-based Markov game. Our work serves as an
important initial step towards understanding offline multi-agent reinforcement
learning."
352,"Our benchmark and baseline provides a starting point for
further research on mitigating reward hacking.","We observe that different detectors are better for different tasks, suggesting that future detectors
could do better than any of our baselines.","6 DISCUSSION

In this work, we designed a diverse set of environments and proxy rewards, uncovered several in-
stances of phase transitions, and proposed an anomaly detection task to help mitigate these transi-
tions.",2022-01-10 18:58:52+00:00,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Alexander Pan'), arxiv.Result.Author('Kush Bhatia'), arxiv.Result.Author('Jacob Steinhardt')]","Reward hacking -- where RL agents exploit gaps in misspecified reward
functions -- has been widely observed, but not yet systematically studied. To
understand how reward hacking arises, we construct four RL environments with
misspecified rewards. We investigate reward hacking as a function of agent
capabilities: model capacity, action space resolution, observation space noise,
and training time. More capable agents often exploit reward misspecifications,
achieving higher proxy reward and lower true reward than less capable agents.
Moreover, we find instances of phase transitions: capability thresholds at
which the agent's behavior qualitatively shifts, leading to a sharp decrease in
the true reward. Such phase transitions pose challenges to monitoring the
safety of ML systems. To address this, we propose an anomaly detection task for
aberrant policies and offer several baseline detectors."
353,"Our benchmark and baseline provides a starting point for
further research on mitigating reward hacking.","We observe that different detectors are better for different tasks, suggesting that future detectors
could do better than any of our baselines.","6 DISCUSSION

In this work, we designed a diverse set of environments and proxy rewards, uncovered several in-
stances of phase transitions, and proposed an anomaly detection task to help mitigate these transi-
tions.",2022-01-10 18:58:52+00:00,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Alexander Pan'), arxiv.Result.Author('Kush Bhatia'), arxiv.Result.Author('Jacob Steinhardt')]","Reward hacking -- where RL agents exploit gaps in misspecified reward
functions -- has been widely observed, but not yet systematically studied. To
understand how reward hacking arises, we construct four RL environments with
misspecified rewards. We investigate reward hacking as a function of agent
capabilities: model capacity, action space resolution, observation space noise,
and training time. More capable agents often exploit reward misspecifications,
achieving higher proxy reward and lower true reward than less capable agents.
Moreover, we find instances of phase transitions: capability thresholds at
which the agent's behavior qualitatively shifts, leading to a sharp decrease in
the true reward. Such phase transitions pose challenges to monitoring the
safety of ML systems. To address this, we propose an anomaly detection task for
aberrant policies and offer several baseline detectors."
377,"Some concluding remarks are collected
in Section 7 together with further research lines.","The latter includes also convergence
guaranties for the small step gradient like method.","Finally, Appendix A gathers technical
results used throughout the paper.",2022-01-11 07:56:33+00:00,Path differentiability of ODE flows,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Swann Marx'), arxiv.Result.Author('Edouard Pauwels')]","We consider flows of ordinary differential equations (ODEs) driven by path
differentiable vector fields. Path differentiable functions constitute a proper
subclass of Lipschitz functions which admit conservative gradients, a notion of
generalized derivative compatible with basic calculus rules. Our main result
states that such flows inherit the path differentiability property of the
driving vector field. We show indeed that forward propagation of derivatives
given by the sensitivity differential inclusions provide a conservative
Jacobian for the flow. This allows to propose a nonsmooth version of the
adjoint method, which can be applied to integral costs under an ODE constraint.
This result constitutes a theoretical ground to the application of small step
first order methods to solve a broad class of nonsmooth optimization problems
with parametrized ODE constraints. This is illustrated with the convergence of
small step first order methods based on the proposed nonsmooth adjoint."
410,We hope these observations will steer further research.,"Furthermore, GradDrop and PCGrad introduce signiﬁcant stochasticity, which
is often linked to the same effect [31, 34].",MGDA Let us denote the convex hull of a set A by Conv(A).,2022-01-11 18:44:17+00:00,In Defense of the Unitary Scalarization for Deep Multi-Task Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Vitaly Kurin'), arxiv.Result.Author('Alessandro De Palma'), arxiv.Result.Author('Ilya Kostrikov'), arxiv.Result.Author('Shimon Whiteson'), arxiv.Result.Author('M. Pawan Kumar')]","Recent multi-task learning research argues against unitary scalarization,
where training simply minimizes the sum of the task losses. Several ad-hoc
multi-task optimization algorithms have instead been proposed, inspired by
various hypotheses about what makes multi-task settings difficult. The majority
of these optimizers require per-task gradients, and introduce significant
memory, runtime, and implementation overhead. We show that unitary
scalarization, coupled with standard regularization and stabilization
techniques from single-task learning, matches or improves upon the performance
of complex multi-task optimizers in popular supervised and reinforcement
learning settings. We then present an analysis suggesting that many specialized
multi-task optimizers can be partly interpreted as forms of regularization,
potentially explaining our surprising results. We believe our results call for
a critical reevaluation of recent research in the area."
433,"To further study these clusters, we plot density plots for the clusters and variables
28                                                                    A. Bjo¨rklund et al.","We ﬁnd ﬁve clusters
with different local models.",Table 4 Comparing SLISEMAP with and without the escape heuristic.,2022-01-12 13:06:21+00:00,SLISEMAP: Supervised dimensionality reduction through local explanations,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC']","[arxiv.Result.Author('Anton Björklund'), arxiv.Result.Author('Jarmo Mäkelä'), arxiv.Result.Author('Kai Puolamäki')]","Existing methods for explaining black box learning models often focus on
building local explanations of model behaviour for a particular data item. It
is possible to create global explanations for all data items, but these
explanations generally have low fidelity for complex black box models. We
propose a new supervised manifold visualisation method, SLISEMAP, that
simultaneously finds local explanations for all data items and builds a
(typically) two-dimensional global visualisation of the black box model such
that data items with similar local explanations are projected nearby. We
provide a mathematical derivation of our problem and an open source
implementation implemented using the GPU-optimised PyTorch library. We compare
SLISEMAP to multiple popular dimensionality reduction methods and find that
SLISEMAP is able to utilise labelled data to create embeddings with consistent
local white box models. We also compare SLISEMAP to other model-agnostic local
explanation methods and show that SLISEMAP provides comparable explanations and
that the visualisations can give a broader understanding of black box
regression and classification models."
496,"On one hand, Roadside Car Data (RCD) is
                                        Above all, this work intends to stimulate further research efforts               obtained by deploying sensors at particular locations of the
                                        towards enhancing the quality of synthetic trafﬁc samples and                    trafﬁc network, such as inductive loops or trafﬁc cameras.",technique in use.,"thereby, reducing the need for sensing infrastructure.",2022-01-11 15:16:18+00:00,On the Design of Graph Embeddings for the Sensorless Estimation of Road Traffic Profiles,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Eric L. Manibardo'), arxiv.Result.Author('Ibai Laña'), arxiv.Result.Author('Esther Villar'), arxiv.Result.Author('Javier Del Ser')]","Traffic forecasting models rely on data that needs to be sensed, processed,
and stored. This requires the deployment and maintenance of traffic sensing
infrastructure, often leading to unaffordable monetary costs. The lack of
sensed locations can be complemented with synthetic data simulations that
further lower the economical investment needed for traffic monitoring. One of
the most common data generative approaches consists of producing real-like
traffic patterns, according to data distributions from analogous roads. The
process of detecting roads with similar traffic is the key point of these
systems. However, without collecting data at the target location no flow
metrics can be employed for this similarity-based search. We present a method
to discover locations among those with available traffic data by inspecting
topological features of road segments. Relevant topological features are
extracted as numerical representations (embeddings) to compare different
locations and eventually find the most similar roads based on the similarity
between their embeddings. The performance of this novel selection system is
examined and compared to simpler traffic estimation approaches. After finding a
similar source of data, a generative method is used to synthesize traffic
profiles. Depending on the resemblance of the traffic behavior at the sensed
road, the generation method can be fed with data from one road only. Several
generation approaches are analyzed in terms of the precision of the synthesized
samples. Above all, this work intends to stimulate further research efforts
towards enhancing the quality of synthetic traffic samples and thereby,
reducing the need for sensing infrastructure."
497,"Above all, this work                     These anomalies are strongly inﬂuenced by the sensing
                                        intends to stimulate further research efforts towards enhancing                 technique in use.","Several generation approaches are analyzed in terms of
                                        the precision of the synthesized samples.","On one hand, Roadside Car Data (RCD) is
                                        the quality of synthetic trafﬁc samples and thereby, reducing the               obtained by deploying sensors at particular locations of the
                                        need for sensing infrastructure.",2022-01-11 15:16:18+00:00,A Graph-based Methodology for the Sensorless Estimation of Road Traffic Profiles,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Eric L. Manibardo'), arxiv.Result.Author('Ibai Laña'), arxiv.Result.Author('Esther Villar'), arxiv.Result.Author('Javier Del Ser')]","Traffic forecasting models rely on data that needs to be sensed, processed,
and stored. This requires the deployment and maintenance of traffic sensing
infrastructure, often leading to unaffordable monetary costs. The lack of
sensed locations can be complemented with synthetic data simulations that
further lower the economical investment needed for traffic monitoring. One of
the most common data generative approaches consists of producing real-like
traffic patterns, according to data distributions from analogous roads. The
process of detecting roads with similar traffic is the key point of these
systems. However, without collecting data at the target location no flow
metrics can be employed for this similarity-based search. We present a method
to discover locations among those with available traffic data by inspecting
topological features. These features are extracted from domain-specific
knowledge as numerical representations (embeddings) to compare different
locations and eventually find roads with analogous daily traffic profiles based
on the similarity between embeddings. The performance of this novel selection
system is examined and compared to simpler traffic estimation approaches. After
finding a similar source of data, a generative method is used to synthesize
traffic profiles. Depending on the resemblance of the traffic behavior at the
sensed road, the generation method can be fed with data from one road only.
Several generation approaches are analyzed in terms of the precision of the
synthesized samples. Above all, this work intends to stimulate further research
efforts towards enhancing the quality of synthetic traffic samples and thereby,
reducing the need for sensing infrastructure."
592,"The behavior
of PINN in such a framework of Learning Theory for Deep Learning remains
to be investigated and could lead to further research questions.","Belkin et al (2019),
demonstrate the existence of a double-descent risk curve across a wide range of
models and datasets, and they oﬀer a mechanism for its genesis.","In particular,
the function class H of the hypothesis space in which PINN is optimized might
be further examined by specifying such space based on the type of diﬀerential
equations that it is solving and thus taking into account the physics informed
portion of the network.",2022-01-14 19:05:44+00:00,Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next,cs.LG,"['cs.LG', 'cs.AI', 'cs.NA', 'math.NA', 'physics.data-an']","[arxiv.Result.Author('Salvatore Cuomo'), arxiv.Result.Author('Vincenzo Schiano di Cola'), arxiv.Result.Author('Fabio Giampaolo'), arxiv.Result.Author('Gianluigi Rozza'), arxiv.Result.Author('Maziar Raissi'), arxiv.Result.Author('Francesco Piccialli')]","Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode
model equations, like Partial Differential Equations (PDE), as a component of
the neural network itself. PINNs are nowadays used to solve PDEs, fractional
equations, integral-differential equations, and stochastic PDEs. This novel
methodology has arisen as a multi-task learning framework in which a NN must
fit observed data while reducing a PDE residual. This article provides a
comprehensive review of the literature on PINNs: while the primary goal of the
study was to characterize these networks and their related advantages and
disadvantages. The review also attempts to incorporate publications on a
broader range of collocation-based physics informed neural networks, which
stars form the vanilla PINN, as well as many other variants, such as
physics-constrained neural networks (PCNN), variational hp-VPINN, and
conservative PINN (CPINN). The study indicates that most research has focused
on customizing the PINN through different activation functions, gradient
optimization techniques, neural network structures, and loss function
structures. Despite the wide range of applications for which PINNs have been
used, by demonstrating their ability to be more feasible in some contexts than
classical numerical techniques like Finite Element Method (FEM), advancements
are still possible, most notably theoretical issues that remain unresolved."
597,"However, as a linear estimation model, when the prediction
interval becomes less than 5 minutes, whether the model can maintain a strong
performance is worth further study.","Kalman Filter method is a linear
quadratic estimation algorithm, which is one of the best prediction methods with high
precision and flexibility.","Regression models measure various factors affecting
travel time.",2022-01-15 05:25:03+00:00,Big Data Application for Network Level Travel Time Prediction,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tianya T. Zhang'), arxiv.Result.Author('Ying Ye'), arxiv.Result.Author('Yu Kathy Zhang')]","Travel time is essential in advanced traveler information systems (ATIS).
This paper used the big data analytics engines Apache Spark and Apache MXNet
for data processing and modeling. The efficiency gain was evaluated by
comparing it with popular data science and deep learning frameworks. The
hierarchical feature pooling is explored for both between layer and the output
layer LSTM (Long-Short-Term-Memory). The designed hierarchical LSTM (hiLSTM)
model can consider the dependencies at a different time scale to capture the
spatial-temporal correlations from network-level corridor travel time. A
self-attention module is then used to connect temporal and spatial features to
the fully connected layers, predicting travel time for all corridors instead of
a single link/route. Seasonality and autocorrelation were performed to explore
the trend of time-varying data. The case study shows that the Hierarchical LSTM
with Attention (hiLSTMat) model gives the best result and outperforms baseline
models. The California Bay Area corridor travel time dataset covering four-year
periods was published from Caltrans Performance Measurement System (PeMS)
system."
604,"This work complements
what we have studied in this work and provides as well directions for further research.","There is recent work in which the neural networks
is studied within the kernel theory, which has shown interesting understanding about the
global optimisation of neural networks (Du et al., 2019) and the convergence of wide neural
networks through Neural Tangent Kernels (Jacot et al., 2018).","On
the other hand, this previous research could be considered to deﬁne directions for better
deﬁning approximation functions that would use well know properties of kernels and the
adaptability of neural networks.",2022-01-15 09:11:54+00:00,Hyperplane bounds for neural feature mappings,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Antonio Jimeno Yepes')],"Deep learning methods minimise the empirical risk using loss functions such
as the cross entropy loss. When minimising the empirical risk, the
generalisation of the learnt function still depends on the performance on the
training data, the Vapnik-Chervonenkis(VC)-dimension of the function and the
number of training examples. Neural networks have a large number of parameters,
which correlates with their VC-dimension that is typically large but not
infinite, and typically a large number of training instances are needed to
effectively train them.
  In this work, we explore how to optimize feature mappings using neural
network with the intention to reduce the effective VC-dimension of the
hyperplane found in the space generated by the mapping. An interpretation of
the results of this study is that it is possible to define a loss that controls
the VC-dimension of the separating hyperplane. We evaluate this approach and
observe that the performance when using this method improves when the size of
the training set is small."
642,"Therefore, there is a necessity
for further research exploring the usefulness of the improvements in time series
generation.","However, it is worth noting that many of these GAN improve-
ments have been proposed for image generation.","3.2 On-Demand Anomalies

One of the signiﬁcant advantages of using GAN over other algorithms in the
literature to synthetically augment the data is that it gives us more control
over the generation through the latent space.",2022-01-12 15:09:10+00:00,Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jaime Pérez'), arxiv.Result.Author('Patricia Arroba'), arxiv.Result.Author('José M. Moya')]","The Cloud paradigm is at a critical point in which the existing
energy-efficiency techniques are reaching a plateau, while the computing
resources demand at Data Center facilities continues to increase exponentially.
The main challenge in achieving a global energy efficiency strategy based on
Artificial Intelligence is that we need massive amounts of data to feed the
algorithms. Nowadays, any optimization strategy must begin with data. However,
companies with access to these large amounts of data decide not to share them
because it could compromise their security. This paper proposes a time-series
data augmentation methodology based on synthetic scenario forecasting within
the Data Center. For this purpose, we will implement a powerful generative
algorithm: Generative Adversarial Networks (GANs). The use of GANs will allow
us to handle multivariate data and data from different natures (e.g.,
categorical). On the other hand, adapting Data Centers' operational management
to the occurrence of sporadic anomalies is complicated due to the reduced
frequency of failures in the system. Therefore, we also propose a methodology
to increase the generated data variability by introducing on-demand anomalies.
We validated our approach using real data collected from an operating Data
Center, successfully obtaining forecasts of random scenarios with several hours
of prediction. Our research will help to optimize the energy consumed in Data
Centers, although the proposed methodology can be employed in any similar
time-series-like problem."
643,"Therefore, further research is needed to investigate their usefulness for
time series generation.","However, it should be
noted that many of these enhancements have been proposed for image gener-
ation.","To further this purpose, some improvements mentioned
above will be analyzed in Section 5, in the hyperparameter tuning phase.",2022-01-12 15:09:10+00:00,Data augmentation through multivariate scenario forecasting in Data Centers using Generative Adversarial Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jaime Pérez'), arxiv.Result.Author('Patricia Arroba'), arxiv.Result.Author('José M. Moya')]","The Cloud paradigm is at a critical point in which the existing
energy-efficiency techniques are reaching a plateau, while the computing
resources demand at Data Center facilities continues to increase exponentially.
The main challenge in achieving a global energy efficiency strategy based on
Artificial Intelligence is that we need massive amounts of data to feed the
algorithms. This paper proposes a time-series data augmentation methodology
based on synthetic scenario forecasting within the Data Center. For this
purpose, we will implement a powerful generative algorithm: Generative
Adversarial Networks (GANs). Specifically, our work combines the disciplines of
GAN-based data augmentation and scenario forecasting, filling the gap in the
generation of synthetic data in DCs. Furthermore, we propose a methodology to
increase the variability and heterogeneity of the generated data by introducing
on-demand anomalies without additional effort or expert knowledge. We also
suggest the use of Kullback-Leibler Divergence and Mean Squared Error as new
metrics in the validation of synthetic time series generation, as they provide
a better overall comparison of multivariate data distributions. We validate our
approach using real data collected in an operating Data Center, successfully
generating synthetic data helpful for prediction and optimization models. Our
research will help optimize the energy consumed in Data Centers, although the
proposed methodology can be employed in any similar time-series-like problem."
693,"The results yield a number of
insights that could be useful for further research in the ﬁeld of AI & Law.","Discussion
In this paper, we altered the quantity, training/test split, and labelling quality of data
sets to investigate the impacts on a trained NN classiﬁer.","Overall, Experiment 1 clearly shows the varying importance of larger data sets.",2022-01-17 23:05:14+00:00,Data-Centric Machine Learning in the Legal Domain,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Hannes Westermann'), arxiv.Result.Author('Jaromir Savelka'), arxiv.Result.Author('Vern R. Walker'), arxiv.Result.Author('Kevin D. Ashley'), arxiv.Result.Author('Karim Benyekhlef')]","Machine learning research typically starts with a fixed data set created
early in the process. The focus of the experiments is finding a model and
training procedure that result in the best possible performance in terms of
some selected evaluation metric. This paper explores how changes in a data set
influence the measured performance of a model. Using three publicly available
data sets from the legal domain, we investigate how changes to their size, the
train/test splits, and the human labelling accuracy impact the performance of a
trained deep learning classifier. We assess the overall performance (weighted
average) as well as the per-class performance. The observed effects are
surprisingly pronounced, especially when the per-class performance is
considered. We investigate how ""semantic homogeneity"" of a class, i.e., the
proximity of sentences in a semantic embedding space, influences the difficulty
of its classification. The presented results have far reaching implications for
efforts related to data collection and curation in the field of AI & Law. The
results also indicate that enhancements to a data set could be considered,
alongside the advancement of the ML models, as an additional path for
increasing classification performance on various tasks in AI & Law. Finally, we
discuss the need for an established methodology to assess the potential effects
of data set properties."
703,"arXiv:2201.06835v1 [cs.LG] 18 Jan 2022  RAY BASED DISTRIBUTED AUTONOMOUS VEHICLE RESEARCH
                                                                          PLATFORM

                                                                                                  TECHNICAL REPORT

                                                                                                         Derek Xu
                                                                                            SAAS Research & Publications
                                                                                          University of California, Berkeley

                                                                                               Berkeley, California, USA
                                                                                              xzrderek@berkeley.edu

                                                                                            ABSTRACT

                                                   My project tackles the question of whether Ray can be used to quickly train autonomous vehicles
                                                   using a simulator (Carla), and whether a platform robust enough for further research purposes can
                                                   be built around it.",15,"Ray is an open-source framework that enables distributed machine learning
                                                   applications.",2022-01-18 09:13:27+00:00,Ray Based Distributed Autonomous Vehicle Research Platform,cs.LG,"['cs.LG', 'cs.DC']",[arxiv.Result.Author('Derek Xu')],"My project tackles the question of whether Ray can be used to quickly train
autonomous vehicles using a simulator (Carla), and whether a platform robust
enough for further research purposes can be built around it. Ray is an
open-source framework that enables distributed machine learning applications.
Distributed computing is a technique which parallelizes computational tasks,
such as training a model, among many machines. Ray abstracts away the complex
coordination of these machines, making it rapidly scalable. Carla is a vehicle
simulator that generates data used to train a model. The bulk of the project
was writing the training logic that Ray would use to train my distributed
model. Imitation learning is the best fit for autonomous vehicles. Imitation
learning is an alternative to reinforcement learning and it works by trying to
learn the optimal policy by imitating an expert (usually a human) given a set
of demonstrations. A key deliverable for the project was showcasing my trained
agent in a few benchmark tests, such as navigating a complex turn through
traffic. Beyond that, the broader ambition was to develop a research platform
where others could quickly train and run experiments on huge amounts of Carla
vehicle data. Thus, my end product is not a single model, but a large-scale,
open-source research platform (RayCarla) for autonomous vehicle researchers to
utilize."
707,"3
arXiv Template                                                               A PREPRINT

For further research on the TFI-HSS task, the volume-to-point (VP) and the volume-to-boundary (VB) problems are
commonly used in heat conduction problems [Aslan et al., 2018, Chen et al., 2016, 2021b].","∂ k ∂T + ∂ k ∂T + φ(x, y) = 0,                                     (4)

                       ∂x ∂x       ∂y ∂y

                  T = T0 or k ∂∂Tn = 0 or k ∂∂Tn = h (T − T0) .","From the perspective
of mathematical models, the difference between VB and VP problems is the boundary condition.",2022-01-18 11:21:35+00:00,Temperature Field Inversion of Heat-Source Systems via Physics-Informed Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Wei Peng'), arxiv.Result.Author('Zhiqiang Gong'), arxiv.Result.Author('Weien Zhou'), arxiv.Result.Author('Wen Yao')]","Temperature field inversion of heat-source systems (TFI-HSS) with limited
observations is essential to monitor the system health. Although some methods
such as interpolation have been proposed to solve TFI-HSS, those existing
methods ignore correlations between data constraints and physics constraints,
causing the low precision. In this work, we develop a physics-informed neural
network-based temperature field inversion (PINN-TFI) method to solve the
TFI-HSS task and a coefficient matrix condition number based position selection
of observations (CMCN-PSO) method to select optima positions of noise
observations. For the TFI-HSS task, the PINN-TFI method encodes constrain terms
into the loss function, thus the task is transformed into an optimization
problem of minimizing the loss function. In addition, we have found that noise
observations significantly affect reconstruction performances of the PINN-TFI
method. To alleviate the effect of noise observations, the CMCN-PSO method is
proposed to find optimal positions, where the condition number of observations
is used to evaluate positions. The results demonstrate that the PINN-TFI method
can significantly improve prediction precisions and the CMCN-PSO method can
find good positions to acquire a more robust temperature field."
726,"While we further study private policy-based
algorithms with linear function approximation, Luyo et al.","In the concurrent work (Luyo et al., 2021), the authors also proposed a uniﬁed analysis for private UCRL-
VTR under both JDP and LDP constraints, which shares a similar high-level idea as ours but more detailed
results on the LDP part is provided in (Luyo et al., 2021).","(2021) further investigates private model-free
algorithms and highlights interesting and unique issues in the model-free setting.",2022-01-18 15:25:24+00:00,Differentially Private Reinforcement Learning with Linear Function Approximation,cs.LG,['cs.LG'],[arxiv.Result.Author('Xingyu Zhou')],"Motivated by the wide adoption of reinforcement learning (RL) in real-world
personalized services, where users' sensitive and private information needs to
be protected, we study regret minimization in finite-horizon Markov decision
processes (MDPs) under the constraints of differential privacy (DP). Compared
to existing private RL algorithms that work only on tabular finite-state,
finite-actions MDPs, we take the first step towards privacy-preserving learning
in MDPs with large state and action spaces. Specifically, we consider MDPs with
linear function approximation (in particular linear mixture MDPs) under the
notion of joint differential privacy (JDP), where the RL agent is responsible
for protecting users' sensitive data. We design two private RL algorithms that
are based on value iteration and policy optimization, respectively, and show
that they enjoy sub-linear regret performance while guaranteeing privacy
protection. Moreover, the regret bounds are independent of the number of
states, and scale at most logarithmically with the number of actions, making
the algorithms suitable for privacy protection in nowadays large-scale
personalized services. Our results are achieved via a general procedure for
learning in linear mixture MDPs under changing regularizers, which not only
generalizes previous results for non-private learning, but also serves as a
building block for general private reinforcement learning."
754,"Below we study the semantic retention of cell scores                                    Predictive Analysis Next, we further study the semantic reten-
through a comparative analysis between 𝑠 (𝐺) and 𝑠 (𝜈), and a pre-                                     tion of cell scores with a predictive analysis.",(“sentences”).,"Traditionally [5, 26,

dictive analysis.",2022-01-18 21:40:36+00:00,Transparent Single-Cell Set Classification with Kernel Mean Embeddings,cs.LG,"['cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Siyuan Shan'), arxiv.Result.Author('Vishal Baskaran'), arxiv.Result.Author('Haidong Yi'), arxiv.Result.Author('Jolene Ranek'), arxiv.Result.Author('Natalie Stanley'), arxiv.Result.Author('Junier Oliva')]","Modern single-cell flow and mass cytometry technologies measure the
expression of several proteins of the individual cells within a blood or tissue
sample. Each profiled biological sample is thus represented by a set of
hundreds of thousands of multidimensional cell feature vectors, which incurs a
high computational cost to predict each biological sample's associated
phenotype with machine learning models. Such a large set cardinality also
limits the interpretability of machine learning models due to the difficulty in
tracking how each individual cell influences the ultimate prediction. We
propose using Kernel Mean Embedding to encode the cellular landscape of each
profiled biological sample. Although our foremost goal is to make a more
transparent model, we find that our method achieves comparable or better
accuracies than the state-of-the-art gating-free methods through a simple
linear classifier. As a result, our model contains few parameters but still
performs similarly to deep learning models with millions of parameters. In
contrast with deep learning approaches, the linearity and sub-selection step of
our model makes it easy to interpret classification results. Analysis further
shows that our method admits rich biological interpretability for linking
cellular heterogeneity to clinical phenotype."
758,"In addition, since frequency is a quantity which theoretical study is relative easy to access in,

the F-Principle provides a theoretical direction for further study.","The study of synthetic data shows a clear guidance to examine the F-Principle in the high-dimensional

data.","2.2 Two-dimensional experiments

An image can be regarded as a mapping from two-dimensional space coordinate to pixel intensity.",2022-01-19 03:08:33+00:00,Overview frequency principle/spectral bias in deep learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhi-Qin John Xu'), arxiv.Result.Author('Yaoyu Zhang'), arxiv.Result.Author('Tao Luo')]","Understanding deep learning is increasingly emergent as it penetrates more
and more into industry and science. In recent years, a research line from
Fourier analysis sheds lights into this magical ""black box"" by showing a
Frequency Principle (F-Principle or spectral bias) of the training behavior of
deep neural networks (DNNs) -- DNNs often fit functions from low to high
frequency during the training. The F-Principle is first demonstrated by
one-dimensional synthetic data followed by the verification in high-dimensional
real datasets. A series of works subsequently enhance the validity of the
F-Principle. This low-frequency implicit bias reveals the strength of neural
network in learning low-frequency functions as well as its deficiency in
learning high-frequency functions. Such understanding inspires the design of
DNN-based algorithms in practical problems, explains experimental phenomena
emerging in various scenarios, and further advances the study of deep learning
from the frequency perspective. Although incomplete, we provide an overview of
F-Principle and propose some open problems for future research."
759,"In addition, since frequency is a quantity which theoretical study is relative easy to access in,

the F-Principle provides a theoretical direction for further study.","The study of synthetic data shows a clear guidance to examine the F-Principle in the high-dimensional

data.","2.2 Two-dimensional experiments

An image can be regarded as a mapping from two-dimensional space coordinate to pixel intensity.",2022-01-19 03:08:33+00:00,Overview frequency principle/spectral bias in deep learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhi-Qin John Xu'), arxiv.Result.Author('Yaoyu Zhang'), arxiv.Result.Author('Tao Luo')]","Understanding deep learning is increasingly emergent as it penetrates more
and more into industry and science. In recent years, a research line from
Fourier analysis sheds lights into this magical ""black box"" by showing a
Frequency Principle (F-Principle or spectral bias) of the training behavior of
deep neural networks (DNNs) -- DNNs often fit functions from low to high
frequency during the training. The F-Principle is first demonstrated by
one-dimensional synthetic data followed by the verification in high-dimensional
real datasets. A series of works subsequently enhance the validity of the
F-Principle. This low-frequency implicit bias reveals the strength of neural
network in learning low-frequency functions as well as its deficiency in
learning high-frequency functions. Such understanding inspires the design of
DNN-based algorithms in practical problems, explains experimental phenomena
emerging in various scenarios, and further advances the study of deep learning
from the frequency perspective. Although incomplete, we provide an overview of
F-Principle and propose some open problems for future research."
789,"They talk about the scope that exists for further research in the field
of Economics using remote sensing.","[17] summarise some existing work in
the domain of economics using satellite imagery.","The paper also discusses the various shortcomings of using remote sensing for
research in the economic context, such as spatial dependence, privacy concerns, dataset size and measurement error.",2022-01-19 14:10:37+00:00,ReGNL: Rapid Prediction of GDP during Disruptive Events using Nightlights,cs.LG,['cs.LG'],"[arxiv.Result.Author('Rushabh Musthyala'), arxiv.Result.Author('Rudrajit Kargupta'), arxiv.Result.Author('Hritish Jain'), arxiv.Result.Author('Dipanjan Chakraborty')]","Policy makers often make decisions based on parameters such as GDP,
unemployment rate, industrial output, etc. The primary methods to obtain or
even estimate such information are resource intensive and time consuming. In
order to make timely and well-informed decisions, it is imperative to be able
to come up with proxies for these parameters which can be sampled quickly and
efficiently, especially during disruptive events, like the COVID-19 pandemic.
Recently, there has been a lot of focus on using remote sensing data for this
purpose. The data has become cheaper to collect compared to surveys, and can be
available in real time. In this work, we present Regional GDP NightLight
(ReGNL), a neural network based model which is trained on a custom dataset of
historical nightlights and GDP data along with the geographical coordinates of
a place, and estimates the GDP of the place, given the other parameters. Taking
the case of 50 US states, we find that ReGNL is disruption-agnostic and is able
to predict the GDP for both normal years (2019) and for years with a disruptive
event (2020). ReGNL outperforms timeseries ARIMA methods for prediction, even
during the pandemic. Following from our findings, we make a case for building
infrastructures to collect and make available granular data, especially in
resource-poor geographies, so that these can be leveraged for policy making
during disruptive events."
819,"However, to further study
tions.","butions as works under the statistical-mechanics approach do,      Therefore, the preceding self-organization (risk minimization)
which is achieved by analyzing worst-case behaviors similar        is a salient “order from ﬂuctuations” phenomenon that charac-
to works in statistical learning theory, and that we do calculate  terizes self-organization, and the order emerges from disorder
many-body interaction in the system, but at the granularity of     in circuits by selective (positive, or negative) feedback signals
assemblies that characterize hierarchical many-body interac-       that breaks the circuit symmetries.","This approach could be appreciated biologically: given      such a symmetry-breaking process, we need to clarify the con-
the unknown in an environment, a strategy for an organism          cept of order in DNNs next, and a more rigorous presentation
to prevent coincidental survival risk is to hoard a reservoir of   and further details are given in supp.",2022-01-03 09:06:44+00:00,Complexity from Adaptive-Symmetries Breaking: Global Minima in the Statistical Mechanics of Deep Neural Networks,cs.LG,"['cs.LG', 'physics.bio-ph', 'physics.data-an']",[arxiv.Result.Author('Shawn W. M. Li')],"An antithetical concept, adaptive symmetry, to conservative symmetry in
physics is proposed to understand the deep neural networks (DNNs). It
characterizes the invariance of variance, where a biotic system explores
different pathways of evolution with equal probability in absence of feedback
signals, and complex functional structure emerges from quantitative
accumulation of adaptive-symmetries breaking in response to feedback signals.
Theoretically and experimentally, we characterize the optimization process of a
DNN system as an extended adaptive-symmetry-breaking process. One particular
finding is that a hierarchically large DNN would have a large reservoir of
adaptive symmetries, and when the information capacity of the reservoir exceeds
the complexity of the dataset, the system could absorb all perturbations of the
examples and self-organize into a functional structure of zero training errors
measured by a certain surrogate risk. More specifically, this process is
characterized by a statistical-mechanical model that could be appreciated as a
generalization of statistics physics to the DNN organized complex system, and
characterizes regularities in higher dimensionality. The model consists of
three constitutes that could be appreciated as the counterparts of Boltzmann
distribution, Ising model, and conservative symmetry, respectively: (1) a
stochastic definition/interpretation of DNNs that is a multilayer probabilistic
graphical model, (2) a formalism of circuits that perform biological
computation, (3) a circuit symmetry from which self-similarity between the
microscopic and the macroscopic adaptability manifests. The model is analyzed
with a method referred as the statistical assembly method that analyzes the
coarse-grained behaviors (over a symmetry group) of the heterogeneous
hierarchical many-body interaction in DNNs."
851,"Our hope is
that this research will lead to further study of this important setup.","Finally, a more robust aggregation technique
would alleviate the undesirable degradation of performance when local data is limited.","Acknowledgement: We thank the following people for useful discussions and proofreading:
Leonidas Guibas, Zan Gojcic, Francis Williams, and Cinjon Resnick.",2022-01-20 21:36:25+00:00,Federated Learning with Heterogeneous Architectures using Graph HyperNetworks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Or Litany'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('David Acuna'), arxiv.Result.Author('Jan Kautz'), arxiv.Result.Author('Gal Chechik'), arxiv.Result.Author('Sanja Fidler')]","Standard Federated Learning (FL) techniques are limited to clients with
identical network architectures. This restricts potential use-cases like
cross-platform training or inter-organizational collaboration when both data
privacy and architectural proprietary are required. We propose a new FL
framework that accommodates heterogeneous client architecture by adopting a
graph hypernetwork for parameter sharing. A property of the graph hyper network
is that it can adapt to various computational graphs, thereby allowing
meaningful parameter sharing across models. Unlike existing solutions, our
framework does not limit the clients to share the same architecture type, makes
no use of external data and does not require clients to disclose their model
architecture. Compared with distillation-based and non-graph hypernetwork
baselines, our method performs notably better on standard benchmarks. We
additionally show encouraging generalization performance to unseen
architectures."
865,"Reformulating learning as a dynamical system poses an opportunity to further study its divergent nature and formulate
stabilizing controllers to improve learning performance.","Yet, the of majority of deep Q-learning applications employ some heuristics such
as a target network [21] or random experience replay [22].",Contribution.,2022-01-21 09:47:34+00:00,Deep Q-learning: a robust control approach,cs.LG,['cs.LG'],"[arxiv.Result.Author('Balazs Varga'), arxiv.Result.Author('Balazs Kulcsar'), arxiv.Result.Author('Morteza Haghir Chehreghani')]","In this paper, we place deep Q-learning into a control-oriented perspective
and study its learning dynamics with well-established techniques from robust
control. We formulate an uncertain linear time-invariant model by means of the
neural tangent kernel to describe learning. We show the instability of learning
and analyze the agent's behavior in frequency-domain. Then, we ensure
convergence via robust controllers acting as dynamical rewards in the loss
function. We synthesize three controllers: state-feedback gain scheduling H2,
dynamic Hinf, and constant gain Hinf controllers. Setting up the learning agent
with a control-oriented tuning methodology is more transparent and has
well-established literature compared to the heuristics in reinforcement
learning. In addition, our approach does not use a target network and
randomized replay memory. The role of the target network is overtaken by the
control input, which also exploits the temporal dependency of samples (opposed
to a randomized memory buffer). Numerical simulations in different OpenAI Gym
environments suggest that the Hinf controlled learning performs slightly better
than Double deep Q-learning."
898,"We hope
      aid in deﬁning a concept of “dataset diﬁculty” for a      further research using these methods can continue to enrich
      certain model architecture.",Perhaps this notion of thinking can     tency can assist in the training and generalization.,the study in deepnets and their training paradigms.,2022-01-21 23:21:26+00:00,Nearest Class-Center Simplification through Intermediate Layers,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ido Ben-Shaul'), arxiv.Result.Author('Shai Dekel')]","Recent advances in theoretical Deep Learning have introduced geometric
properties that occur during training, past the Interpolation Threshold --
where the training error reaches zero. We inquire into the phenomena coined
Neural Collapse in the intermediate layers of the networks, and emphasize the
innerworkings of Nearest Class-Center Mismatch inside the deepnet. We further
show that these processes occur both in vision and language model
architectures. Lastly, we propose a Stochastic Variability-Simplification Loss
(SVSL) that encourages better geometrical features in intermediate layers, and
improves both train metrics and generalization."
899,"We        nomena of deep learning through the prism of interpola-
hope further research using these methods can continue to         tion.","We
also show how encouraging inner-layer class-center con-        Belkin, M. Fit without fear: remarkable mathematical phe-
sistency can assist in the training and generalization.","Acta Numerica, 30:203 – 248, 2021.
enrich the study in deepnets and their training paradigms.",2022-01-21 23:21:26+00:00,Nearest Class-Center Simplification through Intermediate Layers,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ido Ben-Shaul'), arxiv.Result.Author('Shai Dekel')]","Recent advances in theoretical Deep Learning have introduced geometric
properties that occur during training, past the Interpolation Threshold --
where the training error reaches zero. We inquire into the phenomena coined
Neural Collapse in the intermediate layers of the networks, and emphasize the
innerworkings of Nearest Class-Center Mismatch inside the deepnet. We further
show that these processes occur both in vision and language model
architectures. Lastly, we propose a Stochastic Variability-Simplification Loss
(SVSL) that encourages better geometrical features in intermediate layers, and
improves both train metrics and generalization."
902,"Applications of multi-class classiﬁcation with ambiguous labeling can beneﬁt from our
method, and we anticipate further research in PLL to extend this framework to tasks beyond image
classiﬁcation.","Theoretical analysis shows that PiCO can be interpreted from an EM-algorithm
perspective.","We hope our work will draw more attention from the community toward a broader
view of using contrastive prototypes for partial label learning.",2022-01-22 07:48:41+00:00,PiCO: Contrastive Label Disambiguation for Partial Label Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Haobo Wang'), arxiv.Result.Author('Ruixuan Xiao'), arxiv.Result.Author('Yixuan Li'), arxiv.Result.Author('Lei Feng'), arxiv.Result.Author('Gang Niu'), arxiv.Result.Author('Gang Chen'), arxiv.Result.Author('Junbo Zhao')]","Partial label learning (PLL) is an important problem that allows each
training example to be labeled with a coarse candidate set, which well suits
many real-world data annotation scenarios with label ambiguity. Despite the
promise, the performance of PLL often lags behind the supervised counterpart.
In this work, we bridge the gap by addressing two key research challenges in
PLL -- representation learning and label disambiguation -- in one coherent
framework. Specifically, our proposed framework PiCO consists of a contrastive
learning module along with a novel class prototype-based label disambiguation
algorithm. PiCO produces closely aligned representations for examples from the
same classes and facilitates label disambiguation. Theoretically, we show that
these two components are mutually beneficial, and can be rigorously justified
from an expectation-maximization (EM) algorithm perspective. Extensive
experiments demonstrate that PiCO significantly outperforms the current
state-of-the-art approaches in PLL and even achieves comparable results to
fully supervised learning. Code and data available:
https://github.com/hbzju/PiCO."
903,"Applications of multi-class classiﬁcation with ambiguous labeling can beneﬁt from our
method, and we anticipate further research in PLL to extend this framework to tasks beyond image
classiﬁcation.","Theoretical analysis shows that PiCO can be interpreted from an EM-algorithm
perspective.","We hope our work will draw more attention from the community toward a broader
view of using contrastive prototypes for partial label learning.",2022-01-22 07:48:41+00:00,PiCO: Contrastive Label Disambiguation for Partial Label Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Haobo Wang'), arxiv.Result.Author('Ruixuan Xiao'), arxiv.Result.Author('Yixuan Li'), arxiv.Result.Author('Lei Feng'), arxiv.Result.Author('Gang Niu'), arxiv.Result.Author('Gang Chen'), arxiv.Result.Author('Junbo Zhao')]","Partial label learning (PLL) is an important problem that allows each
training example to be labeled with a coarse candidate set, which well suits
many real-world data annotation scenarios with label ambiguity. Despite the
promise, the performance of PLL often lags behind the supervised counterpart.
In this work, we bridge the gap by addressing two key research challenges in
PLL -- representation learning and label disambiguation -- in one coherent
framework. Specifically, our proposed framework PiCO consists of a contrastive
learning module along with a novel class prototype-based label disambiguation
algorithm. PiCO produces closely aligned representations for examples from the
same classes and facilitates label disambiguation. Theoretically, we show that
these two components are mutually beneficial, and can be rigorously justified
from an expectation-maximization (EM) algorithm perspective. Extensive
experiments demonstrate that PiCO significantly outperforms the current
state-of-the-art approaches in PLL and even achieves comparable results to
fully supervised learning. Code and data available:
https://github.com/hbzju/PiCO."
907,"DP-SGD was show√n to
• We further study the utility of DP-SGDA                  attain the optimal excess population risk O(1/ n +
in the nonconvex-strongly-concave case in
terms of the primal population risk, i.e.,                    d log(1/δ)/n ) in Bassily et al.",DP-SGD and DP-SGDA.,"(2019, 2020); Wang
R(Aw(S)) = maxv∈V E F (Aw(S), v) .",2022-01-22 13:05:39+00:00,Differentially Private SGDA for Minimax Problems,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Zhenhuan Yang'), arxiv.Result.Author('Shu Hu'), arxiv.Result.Author('Yunwen Lei'), arxiv.Result.Author('Kush R. Varshney'), arxiv.Result.Author('Siwei Lyu'), arxiv.Result.Author('Yiming Ying')]","Stochastic gradient descent ascent (SGDA) and its variants have been the
workhorse for solving minimax problems. However, in contrast to the
well-studied stochastic gradient descent (SGD) with differential privacy (DP)
constraints, there is little work on understanding the generalization (utility)
of SGDA with DP constraints. In this paper, we use the algorithmic stability
approach to establish the generalization (utility) of DP-SGDA in different
settings. In particular, for the convex-concave setting, we prove that the
DP-SGDA can achieve an optimal utility rate in terms of the weak primal-dual
population risk in both smooth and non-smooth cases. To our best knowledge,
this is the first-ever-known result for DP-SGDA in the non-smooth case. We
further provide its utility analysis in the nonconvex-strongly-concave setting
which is the first-ever-known result in terms of the primal population risk.
The convergence and generalization results for this nonconvex setting are new
even in the non-private setting. Finally, numerical experiments are conducted
to demonstrate the effectiveness of DP-SGDA for both convex and nonconvex
cases."
908,"DP-SGD was show√n to
• We further study the utility of DP-SGDA                  attain the optimal excess population risk O(1/ n +
in the nonconvex-strongly-concave case in
terms of the primal population risk, i.e.,                    d log(1/δ)/n ) in Bassily et al.",DP-SGD and DP-SGDA.,"(2019, 2020); Wang
R(Aw(S)) = maxv∈V E F (Aw(S), v) .",2022-01-22 13:05:39+00:00,Differentially Private SGDA for Minimax Problems,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Zhenhuan Yang'), arxiv.Result.Author('Shu Hu'), arxiv.Result.Author('Yunwen Lei'), arxiv.Result.Author('Kush R. Varshney'), arxiv.Result.Author('Siwei Lyu'), arxiv.Result.Author('Yiming Ying')]","Stochastic gradient descent ascent (SGDA) and its variants have been the
workhorse for solving minimax problems. However, in contrast to the
well-studied stochastic gradient descent (SGD) with differential privacy (DP)
constraints, there is little work on understanding the generalization (utility)
of SGDA with DP constraints. In this paper, we use the algorithmic stability
approach to establish the generalization (utility) of DP-SGDA in different
settings. In particular, for the convex-concave setting, we prove that the
DP-SGDA can achieve an optimal utility rate in terms of the weak primal-dual
population risk in both smooth and non-smooth cases. To our best knowledge,
this is the first-ever-known result for DP-SGDA in the non-smooth case. We
further provide its utility analysis in the nonconvex-strongly-concave setting
which is the first-ever-known result in terms of the primal population risk.
The convergence and generalization results for this nonconvex setting are new
even in the non-private setting. Finally, numerical experiments are conducted
to demonstrate the effectiveness of DP-SGDA for both convex and nonconvex
cases."
909,"DP-SGD was show√n to
• We further study the utility of DP-SGDA                  attain the optimal excess population risk O(1/ n +
in the nonconvex-strongly-concave case in
terms of the primal population risk, i.e.,                    d log(1/δ)/n ) in Bassily et al.",DP-SGD and DP-SGDA.,"(2019, 2020); Wang
R(Aw(S)) = maxv∈V E F (Aw(S), v) .",2022-01-22 13:05:39+00:00,Differentially Private SGDA for Minimax Problems,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Zhenhuan Yang'), arxiv.Result.Author('Shu Hu'), arxiv.Result.Author('Yunwen Lei'), arxiv.Result.Author('Kush R. Varshney'), arxiv.Result.Author('Siwei Lyu'), arxiv.Result.Author('Yiming Ying')]","Stochastic gradient descent ascent (SGDA) and its variants have been the
workhorse for solving minimax problems. However, in contrast to the
well-studied stochastic gradient descent (SGD) with differential privacy (DP)
constraints, there is little work on understanding the generalization (utility)
of SGDA with DP constraints. In this paper, we use the algorithmic stability
approach to establish the generalization (utility) of DP-SGDA in different
settings. In particular, for the convex-concave setting, we prove that the
DP-SGDA can achieve an optimal utility rate in terms of the weak primal-dual
population risk in both smooth and non-smooth cases. To our best knowledge,
this is the first-ever-known result for DP-SGDA in the non-smooth case. We
further provide its utility analysis in the nonconvex-strongly-concave setting
which is the first-ever-known result in terms of the primal population risk.
The convergence and generalization results for this nonconvex setting are new
even in the non-private setting. Finally, numerical experiments are conducted
to demonstrate the effectiveness of DP-SGDA for both convex and nonconvex
cases."
910,"Such
                                                                 algorithms include nested algorithms [Raﬁque et al., 2021]
• We further study the utility of DP-SGDA in the nonconvex-      for weakly-convex-weakly-concave problems, multi-step
                                                                 GDA [Nouiehed et al., 2019] under the one-sided PL con-
strongly-concave case in terms of the primal population          dition, epoch-wise SGDA [Yan et al., 2020], and stochastic
                                                                 recursive SGDA [Luo et al., 2020] for nonconvex-strongly-
risk, i.e., R(Aw(S)) = maxv∈V E F (Aw(S), v) .",prove their local convergence for the nonconvex case.,"In par-           concave problems, to mention but a few.",2022-01-22 13:05:39+00:00,Differentially Private SGDA for Minimax Problems,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Zhenhuan Yang'), arxiv.Result.Author('Shu Hu'), arxiv.Result.Author('Yunwen Lei'), arxiv.Result.Author('Kush R. Varshney'), arxiv.Result.Author('Siwei Lyu'), arxiv.Result.Author('Yiming Ying')]","Stochastic gradient descent ascent (SGDA) and its variants have been the
workhorse for solving minimax problems. However, in contrast to the
well-studied stochastic gradient descent (SGD) with differential privacy (DP)
constraints, there is little work on understanding the generalization (utility)
of SGDA with DP constraints. In this paper, we use the algorithmic stability
approach to establish the generalization (utility) of DP-SGDA in different
settings. In particular, for the convex-concave setting, we prove that the
DP-SGDA can achieve an optimal utility rate in terms of the weak primal-dual
population risk in both smooth and non-smooth cases. To our best knowledge,
this is the first-ever-known result for DP-SGDA in the non-smooth case. We
further provide its utility analysis in the nonconvex-strongly-concave setting
which is the first-ever-known result in terms of the primal population risk.
The convergence and generalization results for this nonconvex setting are new
even in the non-private setting. Finally, numerical experiments are conducted
to demonstrate the effectiveness of DP-SGDA for both convex and nonconvex
cases."
924,"The message passing
We further study the generalizability of FeTA by incorporat-     paradigm employs neural networks for updating representa-
ing its ability to perform attention over the graph spectrum     tion of neighboring nodes by exchanging messages between
into low-pass ﬁlter GNN for empowering it to include the         them.","GCN (Kipf & Welling, 2017), GIN (Xu et al., 2018), and
                                                                 GraphSAGE (Hamilton et al., 2017).","The use of transformer-style attention to GNNs for
full graph spectrum, observing an empirical edge on base         aggregating local information within the graphs is also an
model.",2022-01-23 18:03:22+00:00,Investigating Expressiveness of Transformer in Spectral Domain for Graphs,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anson Bastos'), arxiv.Result.Author('Abhishek Nadgeri'), arxiv.Result.Author('Kuldeep Singh'), arxiv.Result.Author('Hiroki Kanezashi'), arxiv.Result.Author('Toyotaro Suzumura'), arxiv.Result.Author(""Isaiah Onando Mulang'"")]","Transformers have been proven to be inadequate for graph representation
learning. To understand this inadequacy, there is need to investigate if
spectral analysis of transformer will reveal insights on its expressive power.
Similar studies already established that spectral analysis of Graph neural
networks (GNNs) provides extra perspectives on their expressiveness. In this
work, we systematically study and prove the link between the spatial and
spectral domain in the realm of the transformer. We further provide a
theoretical analysis that the spatial attention mechanism in the transformer
cannot effectively capture the desired frequency response, thus, inherently
limiting its expressiveness in spectral space. Therefore, we propose FeTA, a
framework that aims to perform attention over the entire graph spectrum
analogous to the attention in spatial space. Empirical results suggest that
FeTA provides homogeneous performance gain against vanilla transformer across
all tasks on standard benchmarks and can easily be extended to GNN based models
with low-pass characteristics (e.g., GAT). Furthermore, replacing the vanilla
transformer model with FeTA in recently proposed position encoding schemes has
resulted in comparable or better performance than transformer and GNN
baselines."
925,"The message passing
We further study the generalizability of FeTA by incorporat-     paradigm employs neural networks for updating representa-
ing its ability to perform attention over the graph spectrum     tion of neighboring nodes by exchanging messages between
into low-pass ﬁlter GNN for empowering it to include the         them.","GCN (Kipf & Welling, 2017), GIN (Xu et al., 2018), and
                                                                 GraphSAGE (Hamilton et al., 2017).","The use of transformer-style attention to GNNs for
full graph spectrum, observing an empirical edge on base         aggregating local information within the graphs is also an
model.",2022-01-23 18:03:22+00:00,Investigating Expressiveness of Transformer in Spectral Domain for Graphs,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anson Bastos'), arxiv.Result.Author('Abhishek Nadgeri'), arxiv.Result.Author('Kuldeep Singh'), arxiv.Result.Author('Hiroki Kanezashi'), arxiv.Result.Author('Toyotaro Suzumura'), arxiv.Result.Author(""Isaiah Onando Mulang'"")]","Transformers have been proven to be inadequate for graph representation
learning. To understand this inadequacy, there is need to investigate if
spectral analysis of transformer will reveal insights on its expressive power.
Similar studies already established that spectral analysis of Graph neural
networks (GNNs) provides extra perspectives on their expressiveness. In this
work, we systematically study and prove the link between the spatial and
spectral domain in the realm of the transformer. We further provide a
theoretical analysis that the spatial attention mechanism in the transformer
cannot effectively capture the desired frequency response, thus, inherently
limiting its expressiveness in spectral space. Therefore, we propose FeTA, a
framework that aims to perform attention over the entire graph spectrum
analogous to the attention in spatial space. Empirical results suggest that
FeTA provides homogeneous performance gain against vanilla transformer across
all tasks on standard benchmarks and can easily be extended to GNN based models
with low-pass characteristics (e.g., GAT). Furthermore, replacing the vanilla
transformer model with FeTA in recently proposed position encoding schemes has
resulted in comparable or better performance than transformer and GNN
baselines."
926,"We further study the generalizability of FeTA      vey in (Chen et al., 2020b)).","Therefore, it
is able to perform attention over the entire graph signals in     GNNs and Graph Transformers: In this section, we stick
spectral space analogous to the traditional transformers in       to the work closely related to our approach (detailed sur-
spatial space.","Since the early attempts for
by incorporating its ability to perform attention over the        GNNs (Scarselli et al., 2008), many variants of the message
graph spectrum into low-pass ﬁlter GNN for empowering it          passing scheme were developed for graph structures such as
to include the full graph spectrum, observing an empirical        GCN (Kipf & Welling, 2017), GIN (Xu et al., 2018), and
edge on base model.",2022-01-23 18:03:22+00:00,Investigating Expressiveness of Transformer in Spectral Domain for Graphs,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anson Bastos'), arxiv.Result.Author('Abhishek Nadgeri'), arxiv.Result.Author('Kuldeep Singh'), arxiv.Result.Author('Hiroki Kanezashi'), arxiv.Result.Author('Toyotaro Suzumura'), arxiv.Result.Author(""Isaiah Onando Mulang'"")]","Transformers have been proven to be inadequate for graph representation
learning. To understand this inadequacy, there is need to investigate if
spectral analysis of transformer will reveal insights on its expressive power.
Similar studies already established that spectral analysis of Graph neural
networks (GNNs) provides extra perspectives on their expressiveness. In this
work, we systematically study and prove the link between the spatial and
spectral domain in the realm of the transformer. We further provide a
theoretical analysis that the spatial attention mechanism in the transformer
cannot effectively capture the desired frequency response, thus, inherently
limiting its expressiveness in spectral space. Therefore, we propose FeTA, a
framework that aims to perform attention over the entire graph spectrum
analogous to the attention in spatial space. Empirical results suggest that
FeTA provides homogeneous performance gain against vanilla transformer across
all tasks on standard benchmarks and can easily be extended to GNN based models
with low-pass characteristics (e.g., GAT). Furthermore, replacing the vanilla
transformer model with FeTA in recently proposed position encoding schemes has
resulted in comparable or better performance than transformer and GNN
baselines."
927,"We further study the generalizability of FeTA      vey in (Chen et al., 2020b)).","Therefore, it
is able to perform attention over the entire graph signals in     GNNs and Graph Transformers: In this section, we stick
spectral space analogous to the traditional transformers in       to the work closely related to our approach (detailed sur-
spatial space.","Since the early attempts for
by incorporating its ability to perform attention over the        GNNs (Scarselli et al., 2008), many variants of the message
graph spectrum into low-pass ﬁlter GNN for empowering it          passing scheme were developed for graph structures such as
to include the full graph spectrum, observing an empirical        GCN (Kipf & Welling, 2017), GIN (Xu et al., 2018), and
edge on base model.",2022-01-23 18:03:22+00:00,How Expressive are Transformers in Spectral Domain for Graphs?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anson Bastos'), arxiv.Result.Author('Abhishek Nadgeri'), arxiv.Result.Author('Kuldeep Singh'), arxiv.Result.Author('Hiroki Kanezashi'), arxiv.Result.Author('Toyotaro Suzumura'), arxiv.Result.Author(""Isaiah Onando Mulang'"")]","The recent works proposing transformer-based models for graphs have proven
the inadequacy of Vanilla Transformer for graph representation learning. To
understand this inadequacy, there is a need to investigate if spectral analysis
of the transformer will reveal insights into its expressive power. Similar
studies already established that spectral analysis of Graph neural networks
(GNNs) provides extra perspectives on their expressiveness. In this work, we
systematically study and establish the link between the spatial and spectral
domain in the realm of the transformer. We further provide a theoretical
analysis and prove that the spatial attention mechanism in the transformer
cannot effectively capture the desired frequency response, thus, inherently
limiting its expressiveness in spectral space. Therefore, we propose FeTA, a
framework that aims to perform attention over the entire graph spectrum (i.e.,
actual frequency components of the graphs) analogous to the attention in
spatial space. Empirical results suggest that FeTA provides homogeneous
performance gain against vanilla transformer across all tasks on standard
benchmarks and can easily be extended to GNN-based models with low-pass
characteristics (e.g., GAT)."
947,"Through easy
access to interpretable tools, Pearl facilitates experimentation, further research and innovation, particularly at the overlap
between RL and EC.","0 ES             Sphere                          0 ES             Matyas                                            Ackley                                          Beale
                  AdamES                                           AdamES
                                                                                           -4                        ES                                 0
        -50                                               -1                                                         AdamES

                                                                                           -6                                                           -25,000

Reward  -100                                      Reward  -2                       Reward  -8                                                   Reward  -50,000

        -150 -3 -10                                                                                                                                     -75,000

        -200                                                                               -12                                                          -100,000     ES
                                                                                                                                                                     AdamES
                                                          -4     50 100 150 200 250 -14 0
              0      50 100 150 200 250                       0                                                      50 100 150 200 250                           0  50 100 150 200 250
                          EggSHteoplder                                 HimmSteeplblau                                       RaSstteripgin
                                                          0                                                                                             0                   RoseSntebprock
                                                                                                                -20
                 ES                                                                                                                         ES
                 AdamES                                                                                                                     AdamES
        60
                                                      -200

        50                                                                                 -30                                                          -1,000

Reward                                        Reward  -400                         Reward  -40                                                  Reward

        40                                                                                 -50                                                          -2,000

                                                      -600

        30                                                         -800 EASdamES -60                                 50 100 Step 150 200 250                           ES
            0        50 100 Step 150 200 250 0 50 100 Step 150 200 250 0                                                                                               AdamES
                                                                                                                                                                  0 50 100 Step 150 200 250

                                                                           Figure 3: Learning curves

                                                                                   4
                                                                                                    Pearl                                                                                    A PREPRINT

                                Sphere                                          Matyas                                              Ackley                                                     Beale

               10 3                            ES                                               ES                                                 ES                                        100 150             ES
               10 4                            AdamES                                           AdamES                                             AdamES                                                        AdamES
               10 5                                                                                                                                                                          RoseSntebprock  200 250
               10 6 0                                      10 3                                             10 3                                               10 3                                          200 250
                                                                                                                                                                                             100 Step 150
KL Divergence                                           KL Divergence                                   KL Divergence  10 4                                  KL Divergence

                                                                       10 4                                                                                                 10 4

                                                                                                                       10 5

                                                                       10 5                                            10 6                                                 10 5

                        50 100 150             200 250                       0  50 100 150 200 250                           0      50 100 150 200 250                            0  50
                                                                                HimmSteeplblau                                      RaSstteripgin
                                EggSHteoplder

                                                                                                ES                                                 ES
                                                                                                AdamES                                             AdamES
                                                                                                                       10 3                                                 10 3
               10 3                                                    10 3

KL Divergence  10 4                                     KL Divergence  10 4                                                   10 4KL Divergence              KL Divergence  10 4
                                                                                                                              10 5
               10 5                                                    10 5                                                   10 6                                          10 5
                                                                                50 100 Step 150 200 250 0
               10 6     ES                                             10 6                                                                                                 10 6     ES
                     0  AdamES                                                                                                                                                       AdamES

                            50  100 Step 150   200 250                       0                                                      50 100 Step 150 200 250                       0  50

                                                                                Figure 4: Population divergence with log scale

5 Conclusions and Future Work

This paper has introduced Pearl, a library especially designed to allow researchers to rapidly prototype and test new
ideas in order to optimize decision making algorithms in reinforcement learning type environments.",Future work will involve new agents and features such as the Intrinsic Curiosity Module (Pathak et al.,2022-01-24 10:22:30+00:00,Pearl: Parallel Evolutionary and Reinforcement Learning Library,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Author('Rohan Tangri'), arxiv.Result.Author('Danilo P. Mandic'), arxiv.Result.Author('Anthony G. Constantinides')]","Reinforcement learning is increasingly finding success across domains where
the problem can be represented as a Markov decision process. Evolutionary
computation algorithms have also proven successful in this domain, exhibiting
similar performance to the generally more complex reinforcement learning.
Whilst there exist many open-source reinforcement learning and evolutionary
computation libraries, no publicly available library combines the two
approaches for enhanced comparison, cooperation, or visualization. To this end,
we have created Pearl (https://github.com/LondonNode/Pearl), an open source
Python library designed to allow researchers to rapidly and conveniently
perform optimized reinforcement learning, evolutionary computation and
combinations of the two. The key features within Pearl include: modular and
expandable components, opinionated module settings, Tensorboard integration,
custom callbacks and comprehensive visualizations."
962,"In all, we believe that this
work opens up interesting avenues for further research.","This demonstrates the beneﬁts of automating both algorithm
selection and hyperparameter tuning for online learning, unlike several previ-
ous studies that focus only on either one of these.","Springer Nature 2021 LATEX template

22 Online AutoML: An adaptive AutoML framework for online learning

Acknowledgments.",2022-01-24 15:37:20+00:00,Online AutoML: An adaptive AutoML framework for online learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bilge Celik'), arxiv.Result.Author('Prabhant Singh'), arxiv.Result.Author('Joaquin Vanschoren')]","Automated Machine Learning (AutoML) has been used successfully in settings
where the learning task is assumed to be static. In many real-world scenarios,
however, the data distribution will evolve over time, and it is yet to be shown
whether AutoML techniques can effectively design online pipelines in dynamic
environments. This study aims to automate pipeline design for online learning
while continuously adapting to data drift. For this purpose, we design an
adaptive Online Automated Machine Learning (OAML) system, searching the
complete pipeline configuration space of online learners, including
preprocessing algorithms and ensembling techniques. This system combines the
inherent adaptation capabilities of online learners with the fast automated
pipeline (re)optimization capabilities of AutoML. Focusing on optimization
techniques that can adapt to evolving objectives, we evaluate asynchronous
genetic programming and asynchronous successive halving to optimize these
pipelines continually. We experiment on real and artificial data streams with
varying types of concept drift to test the performance and adaptation
capabilities of the proposed system. The results confirm the utility of OAML
over popular online learning algorithms and underscore the benefits of
continuous pipeline redesign in the presence of data drift."
963,"In all, we believe that this
work opens up interesting avenues for further research.","This demonstrates the beneﬁts of automating both algorithm
selection and hyperparameter tuning for online learning, unlike several previ-
ous studies that focus only on either one of these.","Springer Nature 2021 LATEX template

22 Online AutoML: An adaptive AutoML framework for online learning

Acknowledgments.",2022-01-24 15:37:20+00:00,Online AutoML: An adaptive AutoML framework for online learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bilge Celik'), arxiv.Result.Author('Prabhant Singh'), arxiv.Result.Author('Joaquin Vanschoren')]","Automated Machine Learning (AutoML) has been used successfully in settings
where the learning task is assumed to be static. In many real-world scenarios,
however, the data distribution will evolve over time, and it is yet to be shown
whether AutoML techniques can effectively design online pipelines in dynamic
environments. This study aims to automate pipeline design for online learning
while continuously adapting to data drift. For this purpose, we design an
adaptive Online Automated Machine Learning (OAML) system, searching the
complete pipeline configuration space of online learners, including
preprocessing algorithms and ensembling techniques. This system combines the
inherent adaptation capabilities of online learners with the fast automated
pipeline (re)optimization capabilities of AutoML. Focusing on optimization
techniques that can adapt to evolving objectives, we evaluate asynchronous
genetic programming and asynchronous successive halving to optimize these
pipelines continually. We experiment on real and artificial data streams with
varying types of concept drift to test the performance and adaptation
capabilities of the proposed system. The results confirm the utility of OAML
over popular online learning algorithms and underscore the benefits of
continuous pipeline redesign in the presence of data drift."
964,"In all, we believe that this work
opens up interesting avenues for further research.","In
addition, currently available public data streams do not suﬃce to understand
the importance of preprocessing in the search space, which requires further
collection of imperfect, concept drift data.","Springer Nature 2021 LATEX template

24 Online AutoML: An adaptive AutoML framework for online learning

Acknowledgments.",2022-01-24 15:37:20+00:00,Online AutoML: An adaptive AutoML framework for online learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bilge Celik'), arxiv.Result.Author('Prabhant Singh'), arxiv.Result.Author('Joaquin Vanschoren')]","Automated Machine Learning (AutoML) has been used successfully in settings
where the learning task is assumed to be static. In many real-world scenarios,
however, the data distribution will evolve over time, and it is yet to be shown
whether AutoML techniques can effectively design online pipelines in dynamic
environments. This study aims to automate pipeline design for online learning
while continuously adapting to data drift. For this purpose, we design an
adaptive Online Automated Machine Learning (OAML) system, searching the
complete pipeline configuration space of online learners, including
preprocessing algorithms and ensembling techniques. This system combines the
inherent adaptation capabilities of online learners with the fast automated
pipeline (re)optimization capabilities of AutoML. Focusing on optimization
techniques that can adapt to evolving objectives, we evaluate asynchronous
genetic programming and asynchronous successive halving to optimize these
pipelines continually. We experiment on real and artificial data streams with
varying types of concept drift to test the performance and adaptation
capabilities of the proposed system. The results confirm the utility of OAML
over popular online learning algorithms and underscore the benefits of
continuous pipeline redesign in the presence of data drift."
969,"8 Conclusion
   We further study how the amount of logged data inﬂuences
the simple regret of IMO3.",T varies in our experiments.,"We ﬁx T = 100, and vary the                               In this work, we study the problem of multi-objective opti-
size of the logged dataset used for policy-value estimation.",2022-01-24 16:51:41+00:00,IMO^3: Interactive Multi-Objective Off-Policy Optimization,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Nan Wang'), arxiv.Result.Author('Hongning Wang'), arxiv.Result.Author('Maryam Karimzadehgan'), arxiv.Result.Author('Branislav Kveton'), arxiv.Result.Author('Craig Boutilier')]","Most real-world optimization problems have multiple objectives. A system
designer needs to find a policy that trades off these objectives to reach a
desired operating point. This problem has been studied extensively in the
setting of known objective functions. We consider a more practical but
challenging setting of unknown objective functions. In industry, this problem
is mostly approached with online A/B testing, which is often costly and
inefficient. As an alternative, we propose interactive multi-objective
off-policy optimization (IMO^3). The key idea in our approach is to interact
with a system designer using policies evaluated in an off-policy fashion to
uncover which policy maximizes her unknown utility function. We theoretically
show that IMO^3 identifies a near-optimal policy with high probability,
depending on the amount of feedback from the designer and training data for
off-policy estimation. We demonstrate its effectiveness empirically on multiple
multi-objective optimization problems."
970,"8 Conclusion
   We further study how the amount of logged data inﬂuences
the simple regret of IMO3.",T varies in our experiments.,"We ﬁx T = 100, and vary the                               In this work, we study the problem of multi-objective opti-
size of the logged dataset used for policy-value estimation.",2022-01-24 16:51:41+00:00,IMO$^3$: Interactive Multi-Objective Off-Policy Optimization,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Nan Wang'), arxiv.Result.Author('Hongning Wang'), arxiv.Result.Author('Maryam Karimzadehgan'), arxiv.Result.Author('Branislav Kveton'), arxiv.Result.Author('Craig Boutilier')]","Most real-world optimization problems have multiple objectives. A system
designer needs to find a policy that trades off these objectives to reach a
desired operating point. This problem has been studied extensively in the
setting of known objective functions. We consider a more practical but
challenging setting of unknown objective functions. In industry, this problem
is mostly approached with online A/B testing, which is often costly and
inefficient. As an alternative, we propose interactive multi-objective
off-policy optimization (IMO$^3$). The key idea in our approach is to interact
with a system designer using policies evaluated in an off-policy fashion to
uncover which policy maximizes her unknown utility function. We theoretically
show that IMO$^3$ identifies a near-optimal policy with high probability,
depending on the amount of feedback from the designer and training data for
off-policy estimation. We demonstrate its effectiveness empirically on multiple
multi-objective optimization problems."
973,"It is noteworthy that the U-net architecture using CNN is suitable for general
physics laws, which can be expressed as PDEs, so for further research, complex
aerodynamic optimization tasks will be carried out.","This optimization framework is very useful

                                                  19
for practical application when the training data is not accurate or available.",6.,2022-01-21 10:43:57+00:00,Heat Conduction Plate Layout Optimization using Physics-driven Convolutional Neural Networks,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Hao Ma'), arxiv.Result.Author('Yang Sun'), arxiv.Result.Author('Mario Chiarelli')]","The layout optimization of the heat conduction is essential during design in
engineering, especially for thermal sensible products. When the optimization
algorithm iteratively evaluates different loading cases, the traditional
numerical simulation methods used usually lead to a substantial computational
cost. To effectively reduce the computational effort, data-driven approaches
are used to train a surrogate model as a mapping between the prescribed
external loads and various geometry. However, the existing model are trained by
data-driven methods which requires intensive training samples that from
numerical simulations and not really effectively solve the problem. Choosing
the steady heat conduction problems as examples, this paper proposes a
Physics-driven Convolutional Neural Networks (PD-CNN) method to infer the
physical field solutions for random varied loading cases. After that, the
Particle Swarm Optimization (PSO) algorithm is used to optimize the sizes and
the positions of the hole masks in the prescribed design domain, and the
average temperature value of the entire heat conduction field is minimized, and
the goal of minimizing heat transfer is achieved. Compared with the existing
data-driven approaches, the proposed PD-CNN optimization framework not only
predict field solutions that are highly consistent with conventional simulation
results, but also generate the solution space with without any pre-obtained
training data."
988,"We
further study how the dimension averaged causal emergence dCE changes with the scale q
which is measured by the number of effective information channels on the well-trained NIS
model as shown in Figure 4(f).","Although there are larger and larger deviation
from the prediction and the real data, the general trends can be captured by NIS model.",dCE peaks at q = 2 which is exactly same as in the ground truth.,2022-01-25 07:55:06+00:00,Neural Information Squeezer for Causal Emergence,cs.LG,"['cs.LG', 'physics.soc-ph', '68P30', 'K.3.2']","[arxiv.Result.Author('Jiang Zhang'), arxiv.Result.Author('Kaiwei Liu')]","The classic studies of causal emergence have revealed that in some Markovian
dynamical systems, far stronger causal connections can be found on the
higher-level descriptions than the lower-level of the same systems if we
coarse-grain the system states in an appropriate way. However, identifying this
emergent causality from the data is still a hard problem that has not been
solved because the correct coarse-graining strategy can not be found easily.
This paper proposes a general machine learning framework called Neural
Information Squeezer to automatically extract the effective coarse-graining
strategy and the macro-state dynamics, as well as identify causal emergence
directly from the time series data. By decomposing a coarse-graining operation
into two processes: information conversion and information dropping out, we can
not only exactly control the width of the information channel, but also can
derive some important properties analytically including the exact expression of
the effective information of a macro-dynamics. We also show how our framework
can extract the dynamics on different levels and identify causal emergence from
the data on several exampled systems."
1012,"We

lues 𝑉 for the query 𝑄 based on the similarity between 𝑄 and the

5 https://wiki.openstreetmap.org/wiki/Map_features

                                                                                                                           4
make OSM-Reverts available as open data to facilitate reproduci-              Table 1: Dataset statistics for OSM-Reverts and OSM-Manual
bility and further research.6 Furthermore, we consider a second
smaller dataset (“OSM-Manual”) for our experiments.",Attention selects the most relevant va-                                                              allowing for training of supervised machine learning models.,"Table 1 sum-              Dataset property                        OSM-Reverts      OSM-Manual
marizes selected dataset statistics.",2022-01-25 15:52:54+00:00,Attention-Based Vandalism Detection in OpenStreetMap,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nicolas Tempelmeier'), arxiv.Result.Author('Elena Demidova')]","OpenStreetMap (OSM), a collaborative, crowdsourced Web map, is a unique
source of openly available worldwide map data, increasingly adopted in Web
applications. Vandalism detection is a critical task to support trust and
maintain OSM transparency. This task is remarkably challenging due to the large
scale of the dataset, the sheer number of contributors, various vandalism
forms, and the lack of annotated data. This paper presents Ovid - a novel
attention-based method for vandalism detection in OSM. Ovid relies on a novel
neural architecture that adopts a multi-head attention mechanism to summarize
information indicating vandalism from OSM changesets effectively. To facilitate
automated vandalism detection, we introduce a set of original features that
capture changeset, user, and edit information. Furthermore, we extract a
dataset of real-world vandalism incidents from the OSM edit history for the
first time and provide this dataset as open data. Our evaluation conducted on
real-world vandalism data demonstrates the effectiveness of Ovid."
1015,"Our evaluation suite lays the ground for further research on hard combinatorial (graph)
           problems and aims at providing a fair and comparable environment for further evaluations.","Our DGL-
           TREESEARCH is a modern re-implementation of the INTEL-TREESEARCH, implemented in
           PyTorch (Paszke et al., 2019) and the Deep Graph Library (Wang et al., 2019), with a focus
           on clean, readable code, as well as performance, and it ﬁxes various issues of the original
           code.","• Using our re-implementation of the tree search, we propose and analyze additional tech-
           niques aiming at improving the guided search.",2022-01-25 17:37:34+00:00,What's Wrong with Deep Learning in Tree Search for Combinatorial Optimization,cs.LG,"['cs.LG', 'cs.AI', 'math.OC']","[arxiv.Result.Author('Maximilian Böther'), arxiv.Result.Author('Otto Kißig'), arxiv.Result.Author('Martin Taraz'), arxiv.Result.Author('Sarel Cohen'), arxiv.Result.Author('Karen Seidel'), arxiv.Result.Author('Tobias Friedrich')]","Combinatorial optimization lies at the core of many real-world problems.
Especially since the rise of graph neural networks (GNNs), the deep learning
community has been developing solvers that derive solutions to NP-hard problems
by learning the problem-specific solution structure. However, reproducing the
results of these publications proves to be difficult. We make three
contributions. First, we present an open-source benchmark suite for the NP-hard
Maximum Independent Set problem, in both its weighted and unweighted variants.
The suite offers a unified interface to various state-of-the-art traditional
and machine learning-based solvers. Second, using our benchmark suite, we
conduct an in-depth analysis of the popular guided tree search algorithm by Li
et al. [NeurIPS 2018], testing various configurations on small and large
synthetic and real-world graphs. By re-implementing their algorithm with a
focus on code quality and extensibility, we show that the graph convolution
network used in the tree search does not learn a meaningful representation of
the solution structure, and can in fact be replaced by random values. Instead,
the tree search relies on algorithmic techniques like graph kernelization to
find good solutions. Thus, the results from the original publication are not
reproducible. Third, we extend the analysis to compare the tree search
implementations to other solvers, showing that the classical algorithmic
solvers often are faster, while providing solutions of similar quality.
Additionally, we analyze a recent solver based on reinforcement learning and
observe that for this solver, the GNN is responsible for the competitive
solution quality."
1087,"Here we would like to present these ideas that might serve as heuristics
for further research.","There are, however, still some issues that we have a handful of insights instead of rigorous
proof or empirical evidence.","B.1 Applying LP Algorithm to LV Problem Model

LP and LV’s are two distinctly different problems: the optimal prices in an LV problem is not
necessarily linear w.r.t.",2022-01-27 06:40:03+00:00,Towards Agnostic Feature-based Dynamic Pricing: Linear Policies vs Linear Valuation with Unknown Noise,cs.LG,"['cs.LG', 'econ.EM', 'stat.ML', 'I.2.6']","[arxiv.Result.Author('Jianyu Xu'), arxiv.Result.Author('Yu-Xiang Wang')]","In feature-based dynamic pricing, a seller sets appropriate prices for a
sequence of products (described by feature vectors) on the fly by learning from
the binary outcomes of previous sales sessions (""Sold"" if valuation $\geq$
price, and ""Not Sold"" otherwise). Existing works either assume noiseless linear
valuation or precisely-known noise distribution, which limits the applicability
of those algorithms in practice when these assumptions are hard to verify. In
this work, we study two more agnostic models: (a) a ""linear policy"" problem
where we aim at competing with the best linear pricing policy while making no
assumptions on the data, and (b) a ""linear noisy valuation"" problem where the
random valuation is linear plus an unknown and assumption-free noise. For the
former model, we show a $\tilde{\Theta}(d^{\frac13}T^{\frac23})$ minimax regret
up to logarithmic factors. For the latter model, we present an algorithm that
achieves an $\tilde{O}(T^{\frac34})$ regret, and improve the best-known lower
bound from $\Omega(T^{\frac35})$ to $\tilde{\Omega}(T^{\frac23})$. These
results demonstrate that no-regret learning is possible for feature-based
dynamic pricing under weak assumptions, but also reveal a disappointing fact
that the seemingly richer pricing feedback is not significantly more useful
than the bandit-feedback in regret reduction."
1088,"Here we would like to present these ideas
that might serve as heuristics for further research.","There are, however, still some issues that we have a handful of insights
instead of rigorous proof or empirical evidence.","C.1 Diﬀerences between LP and LV

As we stated in Section 1, LP models our strategy while LV modes the nature.",2022-01-27 06:40:03+00:00,Towards Agnostic Feature-based Dynamic Pricing: Linear Policies vs Linear Valuation with Unknown Noise,cs.LG,"['cs.LG', 'econ.EM', 'stat.ML', 'I.2.6']","[arxiv.Result.Author('Jianyu Xu'), arxiv.Result.Author('Yu-Xiang Wang')]","In feature-based dynamic pricing, a seller sets appropriate prices for a
sequence of products (described by feature vectors) on the fly by learning from
the binary outcomes of previous sales sessions (""Sold"" if valuation $\geq$
price, and ""Not Sold"" otherwise). Existing works either assume noiseless linear
valuation or precisely-known noise distribution, which limits the applicability
of those algorithms in practice when these assumptions are hard to verify. In
this work, we study two more agnostic models: (a) a ""linear policy"" problem
where we aim at competing with the best linear pricing policy while making no
assumptions on the data, and (b) a ""linear noisy valuation"" problem where the
random valuation is linear plus an unknown and assumption-free noise. For the
former model, we show a $\tilde{\Theta}(d^{\frac13}T^{\frac23})$ minimax regret
up to logarithmic factors. For the latter model, we present an algorithm that
achieves an $\tilde{O}(T^{\frac34})$ regret, and improve the best-known lower
bound from $\Omega(T^{\frac35})$ to $\tilde{\Omega}(T^{\frac23})$. These
results demonstrate that no-regret learning is possible for feature-based
dynamic pricing under weak assumptions, but also reveal a disappointing fact
that the seemingly richer pricing feedback is not significantly more useful
than the bandit-feedback in regret reduction."
1111,The agent invests into 8 stocks in       algorithms also deserves further study.,"How to combine the
rate is set as 5 × 10−4, and it is multiplied by a decay factor  quantile criterion with more effective policy optimization
0.7 every 500 episodes.","200 time steps and can look back at stock prices within the
past 5 steps.",2022-01-27 12:01:36+00:00,Quantile-Based Policy Optimization for Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jinyang Jiang'), arxiv.Result.Author('Jiaqiao Hu'), arxiv.Result.Author('Yijie Peng')]","Classical reinforcement learning (RL) aims to optimize the expected
cumulative rewards. In this work, we consider the RL setting where the goal is
to optimize the quantile of the cumulative rewards. We parameterize the policy
controlling actions by neural networks and propose a novel policy gradient
algorithm called Quantile-Based Policy Optimization (QPO) and its variant
Quantile-Based Proximal Policy Optimization (QPPO) to solve deep RL problems
with quantile objectives. QPO uses two coupled iterations running at different
time scales for simultaneously estimating quantiles and policy parameters and
is shown to converge to the global optimal policy under certain conditions. Our
numerical results demonstrate that the proposed algorithms outperform the
existing baseline algorithms under the quantile criterion."
1112,"The agent invests into 8 stocks in                    quantile criterion with more effective policy optimization
200 time steps and can look back at stock prices within the                   algorithms also deserves further study.","How to combine the
0.7 every 500 episodes.",past 5 steps.,2022-01-27 12:01:36+00:00,Quantile-Based Policy Optimization for Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jinyang Jiang'), arxiv.Result.Author('Jiaqiao Hu'), arxiv.Result.Author('Yijie Peng')]","Classical reinforcement learning (RL) aims to optimize the expected
cumulative rewards. In this work, we consider the RL setting where the goal is
to optimize the quantile of the cumulative rewards. We parameterize the policy
controlling actions by neural networks and propose a novel policy gradient
algorithm called Quantile-Based Policy Optimization (QPO) and its variant
Quantile-Based Proximal Policy Optimization (QPPO) to solve deep RL problems
with quantile objectives. QPO uses two coupled iterations running at different
time scales for simultaneously estimating quantiles and policy parameters and
is shown to converge to the global optimal policy under certain conditions. Our
numerical results demonstrate that the proposed algorithms outperform the
existing baseline algorithms under the quantile criterion."
1130,"This topic should
be further researched in order to be generalized.",There is no clear tendency in Adam.,"4.5 Varying the Hyper-parameters

4.5.1 Experimental details

In this section, this paper will compare test accuracy, sparsity, and selectivity of each optimizer while
changing hyper-parameters.",2022-01-25 05:40:24+00:00,Representation learnt by SGD and Adaptive learning rules -- Conditions that Vary Sparsity and Selectivity in Neural Network,cs.LG,['cs.LG'],[arxiv.Result.Author('Jinhyun Park')],"From the point of view of the human brain, continual learning can perform
various tasks without mutual interference. An effective way to reduce mutual
interference can be found in sparsity and selectivity of neurons. According to
Aljundi et al. and Hadsell et al., imposing sparsity at the representational
level is advantageous for continual learning because sparse neuronal
activations encourage less overlap between parameters, resulting in less
interference. Similarly, highly selective neural networks are likely to induce
less interference since particular response in neurons will reduce the chance
of overlap with other parameters. Considering that the human brain performs
continual learning over the lifespan, finding conditions where sparsity and
selectivity naturally arises may provide insight for understanding how the
brain functions. This paper investigates various conditions that naturally
increase sparsity and selectivity in a neural network. This paper tested
different optimizers with Hoyer's sparsity metric and CCMAS selectivity metric
in MNIST classification task. It is essential to note that investigations on
the natural occurrence of sparsity and selectivity concerning various
conditions have not been acknowledged in any sector of neuroscience nor machine
learning until this day. This paper found that particular conditions increase
sparsity and selectivity such as applying a large learning rate and lowering a
batch size. In addition to the relationship between the condition, sparsity,
and selectivity, the following will be discussed based on empirical analysis:
1. The relationship between sparsity and selectivity and 2. The relationship
between test accuracy, sparsity, and selectivity."
1136,"See Table 2 for experimental results for InceptionTime and         Based on the above analysis, further research can be done
Appendix B for RNN and Self Attention.",Discussion                                                      beneﬁt the model.,to improve or modify cutout for non spectrogram data.,2022-01-27 18:57:49+00:00,Robust Augmentation for Multivariate Time Series Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Hong Yang'), arxiv.Result.Author('Travis Desell')]","Neural networks are capable of learning powerful representations of data, but
they are susceptible to overfitting due to the number of parameters. This is
particularly challenging in the domain of time series classification, where
datasets may contain fewer than 100 training examples. In this paper, we show
that the simple methods of cutout, cutmix, mixup, and window warp improve the
robustness and overall performance in a statistically significant way for
convolutional, recurrent, and self-attention based architectures for time
series classification. We evaluate these methods on 26 datasets from the
University of East Anglia Multivariate Time Series Classification (UEA MTSC)
archive and analyze how these methods perform on different types of time series
data.. We show that the InceptionTime network with augmentation improves
accuracy by 1% to 45% in 18 different datasets compared to without
augmentation. We also show that augmentation improves accuracy for recurrent
and self attention based architectures."
1137,"The rest of the paper is organized as
                                        follows: Section 2 discusses the background and some related works; Section 3
                                        discusses our approach and gives the details of the techniques behind it; Section
                                        4 provides a critical evaluation and discussion of the empirical results; and the
                                        ﬁnal section presents the conclusion and further research.","Two application cases were presented in this paper (i) learning node embed-
                                        ding representation and then suggesting similar chorales based on similarities
                                        in the embedding space, and (ii) learning node labels from neighboring nodes
                                        using the collective classiﬁcation approach.","2  Somnuk Phon-Amnuaisuk

2 Graph Representation Learning of Chorales

Two important criteria in music representation systems: expressive completeness
and structural generality were discussed in [5].",2022-01-27 09:46:10+00:00,Exploring Graph Representation of Chorales,cs.LG,"['cs.LG', 'cs.AI', 'cs.SD', 'eess.AS']",[arxiv.Result.Author('Somnuk Phon-Amnuaisuk')],"This work explores areas overlapping music, graph theory, and machine
learning. An embedding representation of a node, in a weighted undirected graph
$\mathcal{G}$, is a representation that captures the meaning of nodes in an
embedding space. In this work, 383 Bach chorales were compiled and represented
as a graph. Two application cases were investigated in this paper (i) learning
node embedding representation using \emph{Continuous Bag of Words (CBOW),
skip-gram}, and \emph{node2vec} algorithms, and (ii) learning node labels from
neighboring nodes based on a collective classification approach. The results of
this exploratory study ascertains many salient features of the graph-based
representation approach applicable to music applications."
1140,"However, in the case of         we performed lay the roadwork to further research for the ef-
Meta-Dataset, where the new set of tasks might be of en-        fect of task diversity domain in meta-learning and lay some
tirely different datasets or domains, this approach tends to    groundwork and rules for task sampling in meta-learning.","We believe that the experiments
lead to high-performing models.",do more harm to the model rather than aid.,2022-01-27 19:39:07+00:00,The Effect of Diversity in Meta-Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ramnath Kumar'), arxiv.Result.Author('Tristan Deleu'), arxiv.Result.Author('Yoshua Bengio')]","Few-shot learning aims to learn representations that can tackle novel tasks
given a small number of examples. Recent studies show that task distribution
plays a vital role in the model's performance. Conventional wisdom is that task
diversity should improve the performance of meta-learning. In this work, we
find evidence to the contrary; we study different task distributions on a
myriad of models and datasets to evaluate the effect of task diversity on
meta-learning algorithms. For this experiment, we train on multiple datasets,
and with three broad classes of meta-learning models - Metric-based (i.e.,
Protonet, Matching Networks), Optimization-based (i.e., MAML, Reptile, and
MetaOptNet), and Bayesian meta-learning models (i.e., CNAPs). Our experiments
demonstrate that the effect of task diversity on all these algorithms follows a
similar trend, and task diversity does not seem to offer any benefits to the
learning of the model. Furthermore, we also demonstrate that even a handful of
tasks, repeated over multiple batches, would be sufficient to achieve a
performance similar to uniform sampling and draws into question the need for
additional tasks to create better models."
1141,"We believe that the experiments and task
diversity deﬁnition we performed and deﬁned lay the roadwork to further research for the effect
of task diversity domain in meta-learning and lay some groundwork and rules for task sampling in
meta-learning.","This is a crucial ﬁnding since this questions the need to increase
the support set pool to improve the models’ performance.","9
Reproducability Statement

In this paper, we work with four different datasets - Omniglot, miniImageNet, tieredImageNet
and Meta-Dataset.",2022-01-27 19:39:07+00:00,The Effect of Diversity in Meta-Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ramnath Kumar'), arxiv.Result.Author('Tristan Deleu'), arxiv.Result.Author('Yoshua Bengio')]","Few-shot learning aims to learn representations that can tackle novel tasks
given a small number of examples. Recent studies show that task distribution
plays a vital role in the model's performance. Conventional wisdom is that task
diversity should improve the performance of meta-learning. In this work, we
find evidence to the contrary; we study different task distributions on a
myriad of models and datasets to evaluate the effect of task diversity on
meta-learning algorithms. For this experiment, we train on multiple datasets,
and with three broad classes of meta-learning models - Metric-based (i.e.,
Protonet, Matching Networks), Optimization-based (i.e., MAML, Reptile, and
MetaOptNet), and Bayesian meta-learning models (i.e., CNAPs). Our experiments
demonstrate that the effect of task diversity on all these algorithms follows a
similar trend, and task diversity does not seem to offer any benefits to the
learning of the model. Furthermore, we also demonstrate that even a handful of
tasks, repeated over multiple batches, would be sufficient to achieve a
performance similar to uniform sampling and draws into question the need for
additional tasks to create better models."
1142,"We believe that the experiments and task diversity deﬁnition we performed and deﬁned lay the
roadwork to further research on the effect of task diversity domain in meta-learning and encourage
more in-depth studies into the efﬁcacy of our meta-learning methods.","In contradiction, we notice that increasing task diversity using the d-DPP
Sampler hampers the performance of the meta-learning model.","Reproducibility Statement

In this paper, we work with four different datasets - Omniglot, miniImageNet, tieredImageNet
and Meta-Dataset.",2022-01-27 19:39:07+00:00,The Effect of Diversity in Meta-Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ramnath Kumar'), arxiv.Result.Author('Tristan Deleu'), arxiv.Result.Author('Yoshua Bengio')]","Recent studies show that task distribution plays a vital role in the
meta-learner's performance. Conventional wisdom is that task diversity should
improve the performance of meta-learning. In this work, we find evidence to the
contrary; (i) our experiments draw into question the efficacy of our learned
models: similar manifolds can be learned with a subset of the data (lower task
diversity). This finding questions the advantage of providing more data to the
model, and (ii) adding diversity to the task distribution (higher task
diversity) sometimes hinders the model and does not lead to a significant
improvement in performance as previously believed. To strengthen our findings,
we provide both empirical and theoretical evidence."
1149,"Thus, the DAE     proved when translating them to the latent space setting,
can be a signiﬁcant source of noise even for an otherwise        demonstrating the potential of further research at the inter-
deterministic objective function.","Furthermore, we show that
objective values are in the range [0, 1], a standard deviation   high-dimensional BO methods can be signiﬁcantly im-
of 0.1 represents signiﬁcant additional noise.",section of these areas.,2022-01-28 00:55:58+00:00,Local Latent Space Bayesian Optimization over Structured Inputs,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Natalie Maus'), arxiv.Result.Author('Haydn T. Jones'), arxiv.Result.Author('Juston S. Moore'), arxiv.Result.Author('Matt J. Kusner'), arxiv.Result.Author('John Bradshaw'), arxiv.Result.Author('Jacob R. Gardner')]","Bayesian optimization over the latent spaces of deep autoencoder models
(DAEs) has recently emerged as a promising new approach for optimizing
challenging black-box functions over structured, discrete, hard-to-enumerate
search spaces (e.g., molecules). Here the DAE dramatically simplifies the
search space by mapping inputs into a continuous latent space where familiar
Bayesian optimization tools can be more readily applied. Despite this
simplification, the latent space typically remains high-dimensional. Thus, even
with a well-suited latent space, these approaches do not necessarily provide a
complete solution, but may rather shift the structured optimization problem to
a high-dimensional one. In this paper, we propose LOL-BO, which adapts the
notion of trust regions explored in recent work on high-dimensional Bayesian
optimization to the structured setting. By reformulating the encoder to
function as both an encoder for the DAE globally and as a deep kernel for the
surrogate model within a trust region, we better align the notion of local
optimization in the latent space with local optimization in the input space.
LOL-BO achieves as much as 20 times improvement over state-of-the-art latent
space Bayesian optimization methods across six real-world benchmarks,
demonstrating that improvement in optimization strategies is as important as
developing better DAE models."
1154,"the hyperparameters used is given in Appendix C.
                                                                  Using the non-iid distributed FMNIST data we further study
6.1.",More details on          hyperparameters.,"In-Distribution Evaluation                                   how the performance is affected as the number of clients
                                                                  grows and the number of local epochs increases.",2022-01-28 08:42:43+00:00,Gradient Masked Averaging for Federated Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Irene Tenison'), arxiv.Result.Author('Sai Aravind Sreeramadas'), arxiv.Result.Author('Vaikkunth Mugunthan'), arxiv.Result.Author('Edouard Oyallon'), arxiv.Result.Author('Eugene Belilovsky'), arxiv.Result.Author('Irina Rish')]","Federated learning is an emerging paradigm that permits a large number of
clients with heterogeneous data to coordinate learning of a unified global
model without the need to share data amongst each other. Standard federated
learning algorithms involve averaging of model parameters or gradient updates
to approximate the global model at the server. However, in heterogeneous
settings averaging can result in information loss and lead to poor
generalization due to the bias induced by dominant clients. We hypothesize that
to generalize better across non-i.i.d datasets as in FL settings, the
algorithms should focus on learning the invariant mechanism that is constant
while ignoring spurious mechanisms that differ across clients. Inspired from
recent work in the Out-of-Distribution literature, we propose a gradient masked
averaging approach for federated learning as an alternative to the standard
averaging of client updates. This client update aggregation technique can be
adapted as a drop-in replacement in most existing federated algorithms. We
perform extensive experiments with gradient masked approach on multiple FL
algorithms with in-distribution, real-world, and out-of-distribution (as the
worst case scenario) test dataset and show that it provides consistent
improvements, particularly in the case of heterogeneous clients."
1159,"{ θ ∈ RD : F (θ, X˜ , Y ) < 81 ,                                        There are several natural directions of further research and
                 α0(X˜ , θ) ≥ c(δ0)d0d1βw,                                 we list some of them below.","We believe that our contribution
2                              F                                           deepens the understanding of the optimization theory of NN.","First direction is towards the
                                                                           theory of deep networks, where one could try to combine
                         L(X˜ , θ) ≤ Cd0d1d2βw2 βv2N log(N ),              Thm.",2022-01-28 11:30:06+00:00,Improved Overparametrization Bounds for Global Convergence of Stochastic Gradient Descent for Shallow Neural Networks,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Bartłomiej Polaczyk'), arxiv.Result.Author('Jacek Cyranka')]","We study the overparametrization bounds required for the global convergence
of stochastic gradient descent algorithm for a class of one hidden layer
feed-forward neural networks, considering most of the activation functions used
in practice, including ReLU. We improve the existing state-of-the-art results
in terms of the required hidden layer width. We introduce a new proof technique
combining nonlinear analysis with properties of random initializations of the
network. First, we establish the global convergence of continuous solutions of
the differential inclusion being a nonsmooth analogue of the gradient flow for
the MSE loss. Second, we provide a technical result (working also for general
approximators) relating solutions of the aforementioned differential inclusion
to the (discrete) stochastic gradient descent sequences, hence establishing
linear convergence towards zero loss for the stochastic gradient descent
iterations."
1160,There are several natural directions of further research and we list some of them below.,"We believe that our contribution deepens the understanding of the optimization theory
of NN.","First
direction is towards the theory of deep networks, where one could try to combine Theorem 5.6 with
an analysis of DI dynamics in order to obtain improved overparametrization guarantees.",2022-01-28 11:30:06+00:00,Improved Overparametrization Bounds for Global Convergence of Stochastic Gradient Descent for Shallow Neural Networks,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Bartłomiej Polaczyk'), arxiv.Result.Author('Jacek Cyranka')]","We study the overparametrization bounds required for the global convergence
of stochastic gradient descent algorithm for a class of one hidden layer
feed-forward neural networks, considering most of the activation functions used
in practice, including ReLU. We improve the existing state-of-the-art results
in terms of the required hidden layer width. We introduce a new proof technique
combining nonlinear analysis with properties of random initializations of the
network. First, we establish the global convergence of continuous solutions of
the differential inclusion being a nonsmooth analogue of the gradient flow for
the MSE loss. Second, we provide a technical result (working also for general
approximators) relating solutions of the aforementioned differential inclusion
to the (discrete) stochastic gradient descent sequences, hence establishing
linear convergence towards zero loss for the stochastic gradient descent
iterations."
1168,"effectiveness of R-LACE for selective information removal, we conclude that the connection between the ability to
predict gender from the representation, and the TPR-gap metric, is not clear cut, and requires further study.","9
                                                                                                                                              A PREPRINT

Figure 4: Application of R-LACE on the raw pixels of image data, from top to bottom we present the original images
and the same images after a rank-1 projection, for the concepts “smile"" and “glasses"".","5.3 Erasing Concepts in Image Data
Our empirical focus is concept removal in textual data.",2022-01-28 13:00:17+00:00,Linear Adversarial Concept Erasure,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Shauli Ravfogel'), arxiv.Result.Author('Michael Twiton'), arxiv.Result.Author('Yoav Goldberg'), arxiv.Result.Author('Ryan Cotterell')]","Modern neural models trained on textual data rely on pre-trained
representations that emerge without direct supervision. As these
representations are increasingly being used in real-world applications, the
inability to \emph{control} their content becomes an increasingly important
problem.
  We formulate the problem of identifying and erasing a linear subspace that
corresponds to a given concept, in order to prevent linear predictors from
recovering the concept. We model this problem as a constrained, linear minimax
game, and show that existing solutions are generally not optimal for this task.
We derive a closed-form solution for certain objectives, and propose a convex
relaxation, R-LACE, that works well for others. When evaluated in the context
of binary gender removal, the method recovers a low-dimensional subspace whose
removal mitigates bias by intrinsic and extrinsic evaluation. We show that the
method -- despite being linear -- is highly expressive, effectively mitigating
bias in deep nonlinear classifiers while maintaining tractability and
interpretability."
1169,"We
                                                     763.5                    hope our proposed method can inspire further research for
4          81.6K                766.2                                         vision-based RL from the perspective of improving the rep-
                                                                              resentation learning.","We conduct detailed ab-
                                                     833.0                    lation study for the proposed designs in MLR and analyze
2          40.8K                767.3                789.5                    their differences from that in NLP and CV domains.","Moreover, the concept of the masked
8          163.2K               728.3                                         latent reconstruction is also worthy of being explored and
                                                                              extended in the ﬁelds of computer vision and neural lan-
rameters).",2022-01-28 13:07:11+00:00,Mask-based Latent Reconstruction for Reinforcement Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tao Yu'), arxiv.Result.Author('Zhizheng Zhang'), arxiv.Result.Author('Cuiling Lan'), arxiv.Result.Author('Zhibo Chen'), arxiv.Result.Author('Yan Lu')]","For deep reinforcement learning (RL) from pixels, learning effective state
representations is crucial for achieving high performance. However, in
practice, limited experience and high-dimensional input prevent effective
representation learning. To address this, motivated by the success of masked
modeling in other research fields, we introduce mask-based reconstruction to
promote state representation learning in RL. Specifically, we propose a simple
yet effective self-supervised method, Mask-based Latent Reconstruction (MLR),
to predict the complete state representations in the latent space from the
observations with spatially and temporally masked pixels. MLR enables the
better use of context information when learning state representations to make
them more informative, which facilitates RL agent training. Extensive
experiments show that our MLR significantly improves the sample efficiency in
RL and outperforms the state-of-the-art sample-efficient RL methods on multiple
continuous benchmark environments."
1170,"We hope our proposed method can inspire further research for vision-
based RL from the perspective of improving representation learning.","We conduct a
detailed ablation study for the proposed designs in MLR and analyze their differences from that
in NLP and CV domains.","Moreover, the masked latent
reconstruction concept is also worthy of being explored and extended in computer vision and neural

                               9
language processing.",2022-01-28 13:07:11+00:00,Mask-based Latent Reconstruction for Reinforcement Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tao Yu'), arxiv.Result.Author('Zhizheng Zhang'), arxiv.Result.Author('Cuiling Lan'), arxiv.Result.Author('Yan Lu'), arxiv.Result.Author('Zhibo Chen')]","For deep reinforcement learning (RL) from pixels, learning effective state
representations is crucial for achieving high performance. However, in
practice, limited experience and high-dimensional input prevent effective
representation learning. To address this, motivated by the success of masked
modeling in other research fields, we introduce mask-based reconstruction to
promote state representation learning in RL. Specifically, we propose a simple
yet effective self-supervised method, Mask-based Latent Reconstruction (MLR),
to predict the complete state representations in the latent space from the
observations with spatially and temporally masked pixels. MLR enables the
better use of context information when learning state representations to make
them more informative, which facilitates RL agent training. Extensive
experiments show that our MLR significantly improves the sample efficiency in
RL and outperforms the state-of-the-art sample-efficient RL methods on multiple
continuous and discrete control benchmarks. The code will be released soon."
1174,"L cannot be characterized by any data-independent
      function G : RM × RM → R.                                         Overall, we make complete deﬁnitions of regularization and
                                                                        provide insightful overlapping recipes and examples for the
 (b) Following Section 5.2, for any k ∈ N we can ﬁnd a                  further study of the implicit regularization for L subject to
      σ ∈ Ck such that if the implicit regularization for L             neural networks.","The generality of our recipes suggests that
                                                                        implicit regularization that can hardly or even cannot be
 (a) For any k ∈ N, we can construct a σ ∈ Ck following                 characterized by data-independent functions in certain sense
      Section 5.1, such that the implicit regularization for            is common.","By our recipes, we believe that we can
      is characterized by a data-independent function G ∈               only obtain partial information in general about an implicit
      C1(RM × RM ) in the weak sense, then G(·, θ0) is                  regularization by looking at the value of a data-independent
      constant on an open set of RM for some θ0 ∈ RM .",2022-01-28 15:53:30+00:00,Limitation of characterizing implicit regularization by data-independent functions,cs.LG,['cs.LG'],"[arxiv.Result.Author('Leyang Zhang'), arxiv.Result.Author('Zhi-Qin John Xu'), arxiv.Result.Author('Tao Luo'), arxiv.Result.Author('Yaoyu Zhang')]","In recent years, understanding the implicit regularization of neural networks
(NNs) has become a central task of deep learning theory. However, implicit
regularization is in itself not completely defined and well understood. In this
work, we make an attempt to mathematically define and study the implicit
regularization. Importantly, we explore the limitation of a common approach of
characterizing the implicit regularization by data-independent functions. We
propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping
mechanisms, based on which we provide two recipes for producing classes of
one-hidden-neuron NNs that provably cannot be fully characterized by a type of
or all data-independent functions. Our results signify the profound
data-dependency of implicit regularization in general, inspiring us to study in
detail the data-dependency of NN implicit regularization in the future."
1175,"We recommend further study into multi-agent            clustering, 2018.
      backdoor defenses.","We are cautioned that the effectiveness of existing
      (single-agent) backdoor defenses drop when the num-        Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Ed-
      ber of attackers increase, thus they may not be prepared     wards, B., Lee, T., Molloy, I., and Srivastava, B. Detecting
      to robustify models against multi-agent backdoor at-          backdoor attacks on deep neural networks by activation
      tacks.","Chen, H., Fu, C., Zhao, J., and Koushanfar, F. Deepin-
Henceforth, we recommend using the multi-agent setting as           spect: A black-box trojan detection and mitigation frame-
a baseline for practical backdoor attack/defense work.",2022-01-28 16:11:40+00:00,Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire,cs.LG,"['cs.LG', 'cs.CR', 'cs.MA']","[arxiv.Result.Author('Siddhartha Datta'), arxiv.Result.Author('Nigel Shadbolt')]","Malicious agents in collaborative learning and outsourced data collection
threaten the training of clean models. Backdoor attacks, where an attacker
poisons a model during training to successfully achieve targeted
misclassification, are a major concern to train-time robustness. In this paper,
we investigate a multi-agent backdoor attack scenario, where multiple attackers
attempt to backdoor a victim model simultaneously. A consistent backfiring
phenomenon is observed across a wide range of games, where agents suffer from a
low collective attack success rate. We examine different modes of backdoor
attack configurations, non-cooperation / cooperation, joint distribution
shifts, and game setups to return an equilibrium attack success rate at the
lower bound. The results motivate the re-evaluation of backdoor defense
research for practical environments."
1176,"Studying
Neural Optimal Transport

saddle points of (17) and arg inf sets (19) is an important     Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and
challenge to address in the further research.","Our networks converge in 1-3 days on a Tesla V100                                               costs, the issue leads to the conditional collapse.","Courville, A. C. Improved training of Wasserstein GANs.",2022-01-28 16:24:13+00:00,Neural Optimal Transport,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexander Korotin'), arxiv.Result.Author('Daniil Selikhanovych'), arxiv.Result.Author('Evgeny Burnaev')]","We present a novel neural-networks-based algorithm to compute optimal
transport maps and plans for strong and weak transport costs. To justify the
usage of neural networks, we prove that they are universal approximators of
transport plans between probability distributions. We evaluate the performance
of our optimal transport algorithm on toy examples and on the unpaired
image-to-image style translation task."
1185,"To further study whether the obtained scores by GStarX are sparse,
we follow [11] to evaluate an entropy-based sparsity measure on model output scores.",Explanation sparsity study.,"We show the
average GStarX entropy-based sparsity on all datasets, and compare them with three reference score
distributions on all n nodes in a graph.",2022-01-28 19:19:39+00:00,Explaining Graph Neural Networks with Structure-Aware Cooperative Games,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shichang Zhang'), arxiv.Result.Author('Yozen Liu'), arxiv.Result.Author('Neil Shah'), arxiv.Result.Author('Yizhou Sun')]","Explaining machine learning models is an important and increasingly popular
area of research interest. The Shapley value from game theory has been proposed
as a prime approach to compute feature importance towards model predictions on
images, text, tabular data, and recently graph neural networks (GNNs) on
graphs. In this work, we revisit the appropriateness of the Shapley value for
GNN explanation, where the task is to identify the most important subgraph and
constituent nodes for GNN predictions. We claim that the Shapley value is a
non-ideal choice for graph data because it is by definition not
structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method
to leverage the critical graph structure information to improve the
explanation. Specifically, we define a scoring function based on a new
structure-aware value from the cooperative game theory proposed by Hamiache and
Navarro (HN). When used to score node importance, the HN value utilizes graph
structures to attribute cooperation surplus between neighbor nodes, resembling
message passing in GNNs, so that node importance scores reflect not only the
node feature importance, but also the node structural roles. We demonstrate
that GStarX produces qualitatively more intuitive explanations, and
quantitatively improves explanation fidelity over strong baselines on chemical
graph property prediction and text graph sentiment classification."
1198,"4.1 Open Foundational Interfaces

Flashlight is built on top of three open foundational APIs, each addressing design and implementation challenges faced
by machine and deep learning tools: a Tensor interface, a memory management subsystem, and a distributed computing

                                                            4
                                                                                      A PREPRINT - FEBRUARY 1, 2022

Computation
    Model

Eager        Op call          Execute            Op call           Execute   Op call  Execute  Ready

Deferred     Op call Op call Op call   Data Request                Compile     Execute         Ready
  Hybrid                                                           & Launch       Execute      Ready
   Static                                                                                      Ready
     Time                              Data Request Op call                  Execute

             Op call Op call           Compile            Execute
                                       & Launch

              Add op to    Add op to    Add op to            Launch
             computation  computation  computation        Computation

Figure 2: Flashlight’s Tensor API supports backend implementations with any of the above computation modes (or
entirely new modes that may result from further research).","This approach promotes type
safety, foregoes the runtime overheads associated with interpreters, and, unlike eager-based approaches, enables global
optimizations where possible.",interface.,2022-01-29 01:03:29+00:00,Flashlight: Enabling Innovation in Tools for Machine Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Jacob Kahn'), arxiv.Result.Author('Vineel Pratap'), arxiv.Result.Author('Tatiana Likhomanenko'), arxiv.Result.Author('Qiantong Xu'), arxiv.Result.Author('Awni Hannun'), arxiv.Result.Author('Jeff Cai'), arxiv.Result.Author('Paden Tomasello'), arxiv.Result.Author('Ann Lee'), arxiv.Result.Author('Edouard Grave'), arxiv.Result.Author('Gilad Avidov'), arxiv.Result.Author('Benoit Steiner'), arxiv.Result.Author('Vitaliy Liptchinsky'), arxiv.Result.Author('Gabriel Synnaeve'), arxiv.Result.Author('Ronan Collobert')]","As the computational requirements for machine learning systems and the size
and complexity of machine learning frameworks increases, essential framework
innovation has become challenging. While computational needs have driven recent
compiler, networking, and hardware advancements, utilization of those
advancements by machine learning tools is occurring at a slower pace. This is
in part due to the difficulties involved in prototyping new computational
paradigms with existing frameworks. Large frameworks prioritize machine
learning researchers and practitioners as end users and pay comparatively
little attention to systems researchers who can push frameworks forward -- we
argue that both are equally important stakeholders. We introduce Flashlight, an
open-source library built to spur innovation in machine learning tools and
systems by prioritizing open, modular, customizable internals and
state-of-the-art, research-ready models and training setups across a variety of
domains. Flashlight allows systems researchers to rapidly prototype and
experiment with novel ideas in machine learning computation and has low
overhead, competing with and often outperforming other popular machine learning
frameworks. We see Flashlight as a tool enabling research that can benefit
widely used libraries downstream and bring machine learning and systems
researchers closer together."
1199,"4
                                                                                      A PREPRINT - JUNE 24, 2022

Computation
    Model

Eager        Op call          Execute            Op call           Execute   Op call  Execute  Ready

Deferred     Op call Op call Op call   Data Request                Compile     Execute         Ready
  Hybrid                                                           & Launch       Execute      Ready
   Static                                                                                      Ready
     Time                              Data Request Op call                  Execute

             Op call Op call           Compile            Execute
                                       & Launch

              Add op to    Add op to    Add op to            Launch
             computation  computation  computation        Computation

Figure 2: Flashlight’s Tensor API supports backend implementations with any of the above computation modes (or
entirely new modes that may result from further research).","This approach promotes type
safety, foregoes the runtime overheads associated with interpreters, and, unlike eager-based approaches, enables global
optimizations where possible.","4.1 Open Foundational Interfaces

Flashlight is built on top of three open foundational APIs, each addressing design and implementation challenges faced
by machine and deep learning tools: a Tensor interface, a memory management subsystem, and a distributed computing
interface.",2022-01-29 01:03:29+00:00,Flashlight: Enabling Innovation in Tools for Machine Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Jacob Kahn'), arxiv.Result.Author('Vineel Pratap'), arxiv.Result.Author('Tatiana Likhomanenko'), arxiv.Result.Author('Qiantong Xu'), arxiv.Result.Author('Awni Hannun'), arxiv.Result.Author('Jeff Cai'), arxiv.Result.Author('Paden Tomasello'), arxiv.Result.Author('Ann Lee'), arxiv.Result.Author('Edouard Grave'), arxiv.Result.Author('Gilad Avidov'), arxiv.Result.Author('Benoit Steiner'), arxiv.Result.Author('Vitaliy Liptchinsky'), arxiv.Result.Author('Gabriel Synnaeve'), arxiv.Result.Author('Ronan Collobert')]","As the computational requirements for machine learning systems and the size
and complexity of machine learning frameworks increases, essential framework
innovation has become challenging. While computational needs have driven recent
compiler, networking, and hardware advancements, utilization of those
advancements by machine learning tools is occurring at a slower pace. This is
in part due to the difficulties involved in prototyping new computational
paradigms with existing frameworks. Large frameworks prioritize machine
learning researchers and practitioners as end users and pay comparatively
little attention to systems researchers who can push frameworks forward -- we
argue that both are equally important stakeholders. We introduce Flashlight, an
open-source library built to spur innovation in machine learning tools and
systems by prioritizing open, modular, customizable internals and
state-of-the-art, research-ready models and training setups across a variety of
domains. Flashlight allows systems researchers to rapidly prototype and
experiment with novel ideas in machine learning computation and has low
overhead, competing with and often outperforming other popular machine learning
frameworks. We see Flashlight as a tool enabling research that can benefit
widely used libraries downstream and bring machine learning and systems
researchers closer together. Flashlight is available at
https://github.com/flashlight/flashlight ."
1211,"Our strong empirical
results motivate further study into mimicking the complementary learning system in the brain more
faithfully to enable optimal continual learning in DNNs.","We further showed that CLS-ER converges to ﬂatter minima, mitigates the bias
towards recent tasks, and provides a well-calibrated high-performance model.","9
Published as a conference paper at ICLR 2022

REFERENCES

Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio.",2022-01-29 15:15:23+00:00,"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Elahe Arani'), arxiv.Result.Author('Fahad Sarfraz'), arxiv.Result.Author('Bahram Zonooz')]","Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for ""general continual learning"". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings."
1212,"Our strong empirical
results motivate further study into mimicking the complementary learning system in the brain more
faithfully to enable optimal continual learning in DNNs.","We further showed that CLS-ER converges to ﬂatter minima, mitigates the bias
towards recent tasks, and provides a well-calibrated high-performance model.","9
Published as a conference paper at ICLR 2022

REFERENCES

Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio.",2022-01-29 15:15:23+00:00,"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Elahe Arani'), arxiv.Result.Author('Fahad Sarfraz'), arxiv.Result.Author('Bahram Zonooz')]","Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for ""general continual learning"". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings."
1221,"Another interesting
                                                                direction would be to further study and quantify the two-phase
                                                                convergence phenomenon of Local SGD when training large-
                                                                scale neural networks, as we discussed in Section IV-B.","As future work, one interesting direction would be to
                                                                generalize our results to the partial node participation setting,
                                                                which is practical in federated learning.","This
                                                                may need combining the interpolation assumption with the
                                                                special architectures of neural networks (see, e.g., [1], [8]).",2022-01-30 04:05:56+00:00,Faster Convergence of Local SGD for Over-Parameterized Models,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Tiancheng Qin'), arxiv.Result.Author('S. Rasoul Etesami'), arxiv.Result.Author('César A. Uribe')]","Modern machine learning architectures are often highly expressive. They are
usually over-parameterized and can interpolate the data by driving the
empirical loss close to zero. We analyze the convergence of Local SGD (or
FedAvg) for such over-parameterized models in the heterogeneous data setting
and improve upon the existing literature by establishing the following
convergence rates. We show an error bound of $\O(\exp(-T))$ for strongly-convex
loss functions, where $T$ is the total number of iterations. For general convex
loss functions, we establish an error bound of $\O(1/T)$ under a mild data
similarity assumption and an error bound of $\O(K/T)$ otherwise, where $K$ is
the number of local steps. We also extend our results for non-convex loss
functions by proving an error bound of $\O(K/T)$. Before our work, the
best-known convergence rate for strongly-convex loss functions was
$\O(\exp(-T/K))$, and none existed for general convex or non-convex loss
functions under the overparameterized setting. We complete our results by
providing problem instances in which such convergence rates are tight to a
constant factor under a reasonably small stepsize scheme. Finally, we validate
our theoretical results using numerical experiments on real and synthetic data."
1240,"Our model, operating
tion and further progress is possible using advanced multi-                                                  in the univariate regime and accepting only the predicted
scale processing approaches in the context of time-series                                                    time-series’ history, signiﬁcantly outperforms all previous
forecasting, motivating further research.","Notwithstanding our current success,                                                    tically improved, interpretable and computationally efﬁcient
we believe we barely scratched the surface in the right direc-                                               long-horizon time-series predictions.","Transformer-based multi-variate models using an order of
                                                                                                             magnitude less computation.",2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting long-horizon
forecasting are the volatility of the predictions and their computational
complexity. In this paper, we introduce N-HiTS, a model which addresses both
challenges by incorporating novel hierarchical interpolation and multi-rate
data sampling techniques. These techniques enable the proposed method to
assemble its predictions sequentially, selectively emphasizing components with
different frequencies and scales, while decomposing the input signal and
synthesizing the forecast. We conduct an extensive empirical evaluation
demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon
forecasting methods. On an array of multivariate forecasting tasks, the
proposed method provides an average accuracy improvement of 25% over the latest
Transformer architectures while reducing the computation time by an order of
magnitude. Our code is available at
\href{https://github.com/cchallu/n-hits}{this repository}."
1241,"motivates further research to ﬁnd models that are able to
Fig.","This sets a new baseline for
N-HiTS outperforms SoTA baselines while simultane-                                                           all ensuing multi-variate work on six popular datasets and
ously providing an interpretable non-linear decomposition.",1 showcases N-HiTS perfectly reconstructing latent                                                      effectively use information from multiple variables.,2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting long-horizon
forecasting are the volatility of the predictions and their computational
complexity. In this paper, we introduce N-HiTS, a model which addresses both
challenges by incorporating novel hierarchical interpolation and multi-rate
data sampling techniques. These techniques enable the proposed method to
assemble its predictions sequentially, selectively emphasizing components with
different frequencies and scales, while decomposing the input signal and
synthesizing the forecast. We conduct an extensive empirical evaluation
demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon
forecasting methods. On an array of multivariate forecasting tasks, the
proposed method provides an average accuracy improvement of 25% over the latest
Transformer architectures while reducing the computation time by an order of
magnitude. Our code is available at
\href{https://github.com/cchallu/n-hits}{this repository}."
1242,"Our model, operating
tion and further progress is possible using advanced multi-                                                  in the univariate regime and accepting only the predicted
scale processing approaches in the context of time-series                                                    time-series’ history, signiﬁcantly outperforms all previous
forecasting, motivating further research.","Notwithstanding our current success,                                                    tically improved, interpretable and computationally efﬁcient
we believe we barely scratched the surface in the right direc-                                               long-horizon time-series predictions.","Transformer-based multi-variate models using an order of
                                                                                                             magnitude less computation.",2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting long-horizon
forecasting are the volatility of the predictions and their computational
complexity. In this paper, we introduce N-HiTS, a model which addresses both
challenges by incorporating novel hierarchical interpolation and multi-rate
data sampling techniques. These techniques enable the proposed method to
assemble its predictions sequentially, selectively emphasizing components with
different frequencies and scales, while decomposing the input signal and
synthesizing the forecast. We conduct an extensive empirical evaluation
demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon
forecasting methods. On an array of multivariate forecasting tasks, the
proposed method provides an average accuracy improvement of 25% over the latest
Transformer architectures while reducing the computation time by an order of
magnitude. Our code is available at https://github.com/cchallu/n-hits."
1243,"motivates further research to ﬁnd models that are able to
Fig.","This sets a new baseline for
N-HiTS outperforms SoTA baselines while simultane-                                                           all ensuing multi-variate work on six popular datasets and
ously providing an interpretable non-linear decomposition.",1 showcases N-HiTS perfectly reconstructing latent                                                      effectively use information from multiple variables.,2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting long-horizon
forecasting are the volatility of the predictions and their computational
complexity. In this paper, we introduce N-HiTS, a model which addresses both
challenges by incorporating novel hierarchical interpolation and multi-rate
data sampling techniques. These techniques enable the proposed method to
assemble its predictions sequentially, selectively emphasizing components with
different frequencies and scales, while decomposing the input signal and
synthesizing the forecast. We conduct an extensive empirical evaluation
demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon
forecasting methods. On an array of multivariate forecasting tasks, the
proposed method provides an average accuracy improvement of 25% over the latest
Transformer architectures while reducing the computation time by an order of
magnitude. Our code is available at https://github.com/cchallu/n-hits."
1244,"Our model, operating
tion and further progress is possible using advanced multi-                                                  in the univariate regime and accepting only the predicted
scale processing approaches in the context of time-series                                                    time-series’ history, signiﬁcantly outperforms all previous
forecasting, motivating further research.","Notwithstanding our current success,                                                    tically improved, interpretable and computationally efﬁcient
we believe we barely scratched the surface in the right direc-                                               long-horizon time-series predictions.","Transformer-based multi-variate models using an order of
                                                                                                             magnitude less computation.",2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler-Canseco'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting long-horizon
forecasting are the volatility of the predictions and their computational
complexity. In this paper, we introduce N-HiTS, a model which addresses both
challenges by incorporating novel hierarchical interpolation and multi-rate
data sampling techniques. These techniques enable the proposed method to
assemble its predictions sequentially, selectively emphasizing components with
different frequencies and scales, while decomposing the input signal and
synthesizing the forecast. We conduct an extensive empirical evaluation
demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon
forecasting methods. On an array of multivariate forecasting tasks, the
proposed method provides an average accuracy improvement of 25% over the latest
Transformer architectures while reducing the computation time by an order of
magnitude. Our code is available at https://bit.ly/3JLIBp8."
1245,"motivates further research to ﬁnd models that are able to
Fig.","This sets a new baseline for
N-HiTS outperforms SoTA baselines while simultane-                                                           all ensuing multi-variate work on six popular datasets and
ously providing an interpretable non-linear decomposition.",1 showcases N-HiTS perfectly reconstructing latent                                                      effectively use information from multiple variables.,2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler-Canseco'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting long-horizon
forecasting are the volatility of the predictions and their computational
complexity. In this paper, we introduce N-HiTS, a model which addresses both
challenges by incorporating novel hierarchical interpolation and multi-rate
data sampling techniques. These techniques enable the proposed method to
assemble its predictions sequentially, selectively emphasizing components with
different frequencies and scales, while decomposing the input signal and
synthesizing the forecast. We conduct an extensive empirical evaluation
demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon
forecasting methods. On an array of multivariate forecasting tasks, the
proposed method provides an average accuracy improvement of 25% over the latest
Transformer architectures while reducing the computation time by an order of
magnitude. Our code is available at https://bit.ly/3JLIBp8."
1246,"Notwithstanding our current success, we believe we barely scratched the
surface in the right direction and further progress is possible using advanced multi-scale processing
approaches in the context of time-series forecasting, motivating further research.","The latter obviously providing a detrimental inductive bias
for long-horizon forecasting.","N-HiTS outperforms SoTA baselines while simultaneously providing an interpretable non-linear
decomposition.",2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler-Canseco'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting the task are
the volatility of the predictions and their computational complexity. We
introduce N-HiTS, a model which addresses both challenges by incorporating
novel hierarchical interpolation and multi-rate data sampling techniques. These
techniques enable the proposed method to assemble its predictions sequentially,
emphasizing components with different frequencies and scales while decomposing
the input signal and synthesizing the forecast. We prove that the hierarchical
interpolation technique can efficiently approximate arbitrarily long horizons
in the presence of smoothness. Additionally, we conduct extensive large-scale
dataset experiments from the long-horizon forecast literature, demonstrating
the advantages of our method over the state-of-the-art methods, where N-HiTS
provides an average accuracy improvement of 25% over the latest Transformer
architectures while reducing the computation time by an order of magnitude (50
times)."
1247,"This sets a new baseline for all ensuing multi-variate work on six popular datasets and
motivates further research to effectively use information from multiple variables.","Our model, oper-
ating in the univariate regime and accepting only the predicted time-series’ history, signiﬁcantly
outperforms all previous Transformer-based multi-variate models using an order of magnitude less
computation.","10
References

Ahmed M. Alaa and Mihaela van der Schaar.",2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler-Canseco'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting the task are
the volatility of the predictions and their computational complexity. We
introduce N-HiTS, a model which addresses both challenges by incorporating
novel hierarchical interpolation and multi-rate data sampling techniques. These
techniques enable the proposed method to assemble its predictions sequentially,
emphasizing components with different frequencies and scales while decomposing
the input signal and synthesizing the forecast. We prove that the hierarchical
interpolation technique can efficiently approximate arbitrarily long horizons
in the presence of smoothness. Additionally, we conduct extensive large-scale
dataset experiments from the long-horizon forecast literature, demonstrating
the advantages of our method over the state-of-the-art methods, where N-HiTS
provides an average accuracy improvement of 25% over the latest Transformer
architectures while reducing the computation time by an order of magnitude (50
times)."
1248,"The best conﬁguration is to have the low-frequency/large-
scale components synthesized and removed from analy-
motivates further research to effectively use information from
multiple variables.","Finally and most im-
portantly, in Appendix G we show that the order in which hi-
erarchical interpolation is implemented matters signiﬁcantly.","References                                  Churpek, M. M.; Adhikari, R.; and Edelson, D. P. 2016.",2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler-Canseco'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting the task are
the volatility of the predictions and their computational complexity. We
introduce N-HiTS, a model which addresses both challenges by incorporating
novel hierarchical interpolation and multi-rate data sampling techniques. These
techniques enable the proposed method to assemble its predictions sequentially,
emphasizing components with different frequencies and scales while decomposing
the input signal and synthesizing the forecast. We prove that the hierarchical
interpolation technique can efficiently approximate arbitrarily long horizons
in the presence of smoothness. Additionally, we conduct extensive large-scale
dataset experiments from the long-horizon forecasting literature, demonstrating
the advantages of our method over the state-of-the-art methods, where N-HiTS
provides an average accuracy improvement of 16% over the latest Transformer
architectures while reducing the computation time by an order of magnitude (50
times). Our code is available at https://bit.ly/3JLIBp8."
1249,"This sets a new baseline for
   Additional ablation studies are reported in Appendix           all ensuing multi-variate work on six popular datasets and
G. The MaxPool multi-rate sampling wins over Average-             motivates further research to effectively use information from
Pool.","Our model, operating
        336 0.338                                                 in the univariate regime and accepting only the predicted
                                                                  time-series’ history, signiﬁcantly outperforms all previous
        720 0.439                                                 Transformer-based multi-variate models using an order of
                                                                  magnitude less computation.",Linear interpolation wins over nearest neighbor and         multiple variables.,2022-01-30 17:52:19+00:00,N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cristian Challu'), arxiv.Result.Author('Kin G. Olivares'), arxiv.Result.Author('Boris N. Oreshkin'), arxiv.Result.Author('Federico Garza'), arxiv.Result.Author('Max Mergenthaler-Canseco'), arxiv.Result.Author('Artur Dubrawski')]","Recent progress in neural forecasting accelerated improvements in the
performance of large-scale forecasting systems. Yet, long-horizon forecasting
remains a very difficult task. Two common challenges afflicting the task are
the volatility of the predictions and their computational complexity. We
introduce N-HiTS, a model which addresses both challenges by incorporating
novel hierarchical interpolation and multi-rate data sampling techniques. These
techniques enable the proposed method to assemble its predictions sequentially,
emphasizing components with different frequencies and scales while decomposing
the input signal and synthesizing the forecast. We prove that the hierarchical
interpolation technique can efficiently approximate arbitrarily long horizons
in the presence of smoothness. Additionally, we conduct extensive large-scale
dataset experiments from the long-horizon forecasting literature, demonstrating
the advantages of our method over the state-of-the-art methods, where N-HiTS
provides an average accuracy improvement of almost 20% over the latest
Transformer architectures while reducing the computation time by an order of
magnitude (50 times). Our code is available at bit.ly/3VA5DoT"
1255,"This will enable us to further study the relationship between
diversity and classiﬁcation accuracy and to investigate trade-oﬀs between the
two.","In future work, we will extend the current method by implementing a
local competition (LC) variant, so that it is possible to include objectives of accu-
racy into the NS.","We will also propose more deﬁnitions of diversity with new and improved
metrics, which will provide more insight into what makes a good diversity metric
that ﬁts the task of constructing a diverse high-performing ensemble.",2022-01-30 19:13:32+00:00,Augmenting Novelty Search with a Surrogate Model to Engineer Meta-Diversity in Ensembles of Classifiers,cs.LG,"['cs.LG', 'cs.CV', 'cs.NE']","[arxiv.Result.Author('Rui P. Cardoso'), arxiv.Result.Author('Emma Hart'), arxiv.Result.Author('David Burth Kurka'), arxiv.Result.Author('Jeremy V. Pitt')]","Using Neuroevolution combined with Novelty Search to promote behavioural
diversity is capable of constructing high-performing ensembles for
classification. However, using gradient descent to train evolved architectures
during the search can be computationally prohibitive. Here we propose a method
to overcome this limitation by using a surrogate model which estimates the
behavioural distance between two neural network architectures required to
calculate the sparseness term in Novelty Search. We demonstrate a speedup of 10
times over previous work and significantly improve on previous reported results
on three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and
SVHN. This results from the expanded architecture search space facilitated by
using a surrogate. Our method represents an improved paradigm for implementing
horizontal scaling of learning algorithms by making an explicit search for
diversity considerably more tractable for the same bounded resources."
1256,"This will enable us to further study the relationship between
diversity and classiﬁcation accuracy and to investigate trade-oﬀs between the
two.","In future work, we will extend the current method by implementing a
local competition (LC) variant, so that it is possible to include objectives of accu-
racy into the NS.","We will also propose more deﬁnitions of diversity with new and improved
metrics, which will provide more insight into what makes a good diversity metric
that ﬁts the task of constructing a diverse high-performing ensemble.",2022-01-30 19:13:32+00:00,Augmenting Novelty Search with a Surrogate Model to Engineer Meta-Diversity in Ensembles of Classifiers,cs.LG,"['cs.LG', 'cs.CV', 'cs.NE']","[arxiv.Result.Author('Rui P. Cardoso'), arxiv.Result.Author('Emma Hart'), arxiv.Result.Author('David Burth Kurka'), arxiv.Result.Author('Jeremy V. Pitt')]","Using Neuroevolution combined with Novelty Search to promote behavioural
diversity is capable of constructing high-performing ensembles for
classification. However, using gradient descent to train evolved architectures
during the search can be computationally prohibitive. Here we propose a method
to overcome this limitation by using a surrogate model which estimates the
behavioural distance between two neural network architectures required to
calculate the sparseness term in Novelty Search. We demonstrate a speedup of 10
times over previous work and significantly improve on previous reported results
on three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and
SVHN. This results from the expanded architecture search space facilitated by
using a surrogate. Our method represents an improved paradigm for implementing
horizontal scaling of learning algorithms by making an explicit search for
diversity considerably more tractable for the same bounded resources."
1257,"This will enable us to further study the relationship between
diversity and classiﬁcation accuracy and to investigate trade-oﬀs between the
two.","In future work, we will extend the current method by implementing a
local competition (LC) variant, so that it is possible to include objectives of accu-
racy into the NS.","We will also propose more deﬁnitions of diversity with new and improved
metrics, which will provide more insight into what makes a good diversity metric
that ﬁts the task of constructing a diverse high-performing ensemble.",2022-01-30 19:13:32+00:00,Augmenting Novelty Search with a Surrogate Model to Engineer Meta-Diversity in Ensembles of Classifiers,cs.LG,"['cs.LG', 'cs.CV', 'cs.NE']","[arxiv.Result.Author('Rui P. Cardoso'), arxiv.Result.Author('Emma Hart'), arxiv.Result.Author('David Burth Kurka'), arxiv.Result.Author('Jeremy V. Pitt')]","Using Neuroevolution combined with Novelty Search to promote behavioural
diversity is capable of constructing high-performing ensembles for
classification. However, using gradient descent to train evolved architectures
during the search can be computationally prohibitive. Here we propose a method
to overcome this limitation by using a surrogate model which estimates the
behavioural distance between two neural network architectures required to
calculate the sparseness term in Novelty Search. We demonstrate a speedup of 10
times over previous work and significantly improve on previous reported results
on three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and
SVHN. This results from the expanded architecture search space facilitated by
using a surrogate. Our method represents an improved paradigm for implementing
horizontal scaling of learning algorithms by making an explicit search for
diversity considerably more tractable for the same bounded resources."
1280,"This suggests further research into
different types of multiplicative transformations is needed.","(2020b) further demonstrate the benefit of using a relation-specific rotation
for the multiplicative component of the score function, compared to the simple stretch
through the diagonal matrix R used by MuRP.","Lastly, both MuRP and MuRE are outperformed by TuckER (Balaˇzevi´c et al., 2019b)
on FB15k-237 (Toutanova et al., 2015) — a predominantly non-hierarchical dataset
with a large number of relations — primarily due to multi-task learning across rela-
tions.",2022-01-31 09:24:43+00:00,Learning Representations of Entities and Relations,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Ivana Balažević')],"Encoding facts as representations of entities and binary relationships
between them, as learned by knowledge graph representation models, is useful
for various tasks, including predicting new facts, question answering, fact
checking and information retrieval. The focus of this thesis is on (i)
improving knowledge graph representation with the aim of tackling the link
prediction task; and (ii) devising a theory on how semantics can be captured in
the geometry of relation representations. Most knowledge graphs are very
incomplete and manually adding new information is costly, which drives the
development of methods which can automatically infer missing facts. The first
contribution of this thesis is HypER, a convolutional model which simplifies
and improves upon the link prediction performance of the existing convolutional
state-of-the-art model ConvE and can be mathematically explained in terms of
constrained tensor factorisation. The second contribution is TuckER, a
relatively straightforward linear model, which, at the time of its
introduction, obtained state-of-the-art link prediction performance across
standard datasets. The third contribution is MuRP, first multi-relational graph
representation model embedded in hyperbolic space. MuRP outperforms all
existing models and its Euclidean counterpart MuRE in link prediction on
hierarchical knowledge graph relations whilst requiring far fewer dimensions.
Despite the development of a large number of knowledge graph representation
models with gradually increasing predictive performance, relatively little is
known of the latent structure they learn. We generalise recent theoretical
understanding of how semantic relations of similarity, paraphrase and analogy
are encoded in the geometric interactions of word embeddings to how more
general relations, as found in knowledge graphs, can be encoded in their
representations."
1282,"Logistic method can be considered as a fallback, yet further research
                                                                         is needed to put it on par with other options.","proves performance – even more when the training model is less
regularized.","Figure 7: Ablation studies (Click Task) - Access to exact test
samples also explains performance                                           Limitations.",2022-01-31 11:09:59+00:00,Lessons from the AdKDD'21 Privacy-Preserving ML Challenge,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Eustache Diemert'), arxiv.Result.Author('Romain Fabre'), arxiv.Result.Author('Alexandre Gilotte'), arxiv.Result.Author('Fei Jia'), arxiv.Result.Author('Basile Leparmentier'), arxiv.Result.Author('Jérémie Mary'), arxiv.Result.Author('Zhonghua Qu'), arxiv.Result.Author('Ugo Tanielian'), arxiv.Result.Author('Hui Yang')]","Designing data sharing mechanisms providing performance and strong privacy
guarantees is a hot topic for the Online Advertising industry. Namely, a
prominent proposal discussed under the Improving Web Advertising Business Group
at W3C only allows sharing advertising signals through aggregated,
differentially private reports of past displays. To study this proposal
extensively, an open Privacy-Preserving Machine Learning Challenge took place
at AdKDD'21, a premier workshop on Advertising Science with data provided by
advertising company Criteo. In this paper, we describe the challenge tasks, the
structure of the available datasets, report the challenge results, and enable
its full reproducibility. A key finding is that learning models on large,
aggregated data in the presence of a small set of unaggregated data points can
be surprisingly efficient and cheap. We also run additional experiments to
observe the sensitivity of winning methods to different parameters such as
privacy budget or quantity of available privileged side information. We
conclude that the industry needs either alternate designs for private data
sharing or a breakthrough in learning with aggregated data only to keep ad
relevance at a reasonable level."
1314,"Through our work, it can be seen that there are many opportunities for further research.","We also demonstrated that even when the supervised data are corrupted with noise, the
quality of PINNs prediction still remains accurate as expected.","We observed that in some
cases, PINNs are able to give a satisfying performance visually, but however still give a very high relative error, which
is evaluated either on a ﬁnite element mesh or on a random mesh, compared to the reference solution.",2022-01-31 17:54:44+00:00,Physics-informed neural networks for non-Newtonian fluid thermo-mechanical problems: an application to rubber calendering process,cs.LG,['cs.LG'],"[arxiv.Result.Author('Thi Nguyen Khoa Nguyen'), arxiv.Result.Author('Thibault Dairay'), arxiv.Result.Author('Raphaël Meunier'), arxiv.Result.Author('Mathilde Mougeot')]","Physics-Informed Neural Networks (PINNs) have gained much attention in
various fields of engineering thanks to their capability of incorporating
physical laws into the models. However, the assessment of PINNs in industrial
applications involving coupling between mechanical and thermal fields is still
an active research topic. In this work, we present an application of PINNs to a
non-Newtonian fluid thermo-mechanical problem which is often considered in the
rubber calendering process. We demonstrate the effectiveness of PINNs when
dealing with inverse and ill-posed problems, which are impractical to be solved
by classical numerical discretization methods. We study the impact of the
placement of the sensors and the distribution of unsupervised points on the
performance of PINNs in a problem of inferring hidden physical fields from some
partial data. We also investigate the capability of PINNs to identify unknown
physical parameters from the measurements captured by sensors. The effect of
noisy measurements is also considered throughout this work. The results of this
paper demonstrate that in the problem of identification, PINNs can successfully
estimate the unknown parameters using only the measurements on the sensors. In
ill-posed problems where boundary conditions are not completely defined, even
though the placement of the sensors and the distribution of unsupervised points
have a great impact on PINNs performance, we show that the algorithm is able to
infer the hidden physics from local measurements."
1315,"Through our work, it can be seen that there are many challenges and opportunities for further research.","We also demonstrated that even when the supervised
data are corrupted with noise, the quality of PINNs prediction still remains accurate as expected.","We see that, in
the cost function, we used pre-speciﬁed weight coefﬁcients to achieve a good performance in PINNs.",2022-01-31 17:54:44+00:00,Physics-informed neural networks for non-Newtonian fluid thermo-mechanical problems: an application to rubber calendering process,cs.LG,['cs.LG'],"[arxiv.Result.Author('Thi Nguyen Khoa Nguyen'), arxiv.Result.Author('Thibault Dairay'), arxiv.Result.Author('Raphaël Meunier'), arxiv.Result.Author('Mathilde Mougeot')]","Physics-Informed Neural Networks (PINNs) have gained much attention in
various fields of engineering thanks to their capability of incorporating
physical laws into the models. However, the assessment of PINNs in industrial
applications involving coupling between mechanical and thermal fields is still
an active research topic. In this work, we present an application of PINNs to a
non-Newtonian fluid thermo-mechanical problem which is often considered in the
rubber calendering process. We demonstrate the effectiveness of PINNs when
dealing with inverse and ill-posed problems, which are impractical to be solved
by classical numerical discretization methods. We study the impact of the
placement of the sensors and the distribution of unsupervised points on the
performance of PINNs in a problem of inferring hidden physical fields from some
partial data. We also investigate the capability of PINNs to identify unknown
physical parameters from the measurements captured by sensors. The effect of
noisy measurements is also considered throughout this work. The results of this
paper demonstrate that in the problem of identification, PINNs can successfully
estimate the unknown parameters using only the measurements on the sensors. In
ill-posed problems where boundary conditions are not completely defined, even
though the placement of the sensors and the distribution of unsupervised points
have a great impact on PINNs performance, we show that the algorithm is able to
infer the hidden physics from local measurements."
1319,"stream’s nature and recommend further research for such
However, our experiments conﬁrm that ADWIN, EDDM,                cases.","However, we were unable to
positive feedback loop generated from both of these factors      ﬁnd any detectors that could detect a change in the data
causes the detectors to produce worse results than ND.","HDDM A, and RDDM are suitable for drift detection on             Figure 6 plots the time scale example-based accuracy
multi-label data streams.",2022-01-31 20:16:47+00:00,Implicit Concept Drift Detection for Multi-label Data Streams,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Ege Berkay Gulcan'), arxiv.Result.Author('Fazli Can')]","Many real-world applications adopt multi-label data streams as the need for
algorithms to deal with rapidly changing data increases. Changes in data
distribution, also known as concept drift, cause the existing classification
models to rapidly lose their effectiveness. To assist the classifiers, we
propose a novel algorithm called Label Dependency Drift Detector (LD3), an
implicit (unsupervised) concept drift detector using label dependencies within
the data for multi-label data streams. Our study exploits the dynamic temporal
dependencies between labels using a label influence ranking method, which
leverages a data fusion algorithm and uses the produced ranking to detect
concept drift. LD3 is the first unsupervised concept drift detection algorithm
in the multi-label classification problem area. In this study, we perform an
extensive evaluation of LD3 by comparing it with 14 prevalent supervised
concept drift detection algorithms that we adapt to the problem area using 12
datasets and a baseline classifier. The results show that LD3 provides between
19.8\% and 68.6\% better predictive performance than comparable detectors on
both real-world and synthetic data streams."
1322,"Nevertheless, this experiments sufﬁces to prove the concept that InImNets may be used to solve a
different genre of time-series problems, with further research demanded on the implementation of
their nested derivatives.","As such,
the InImNet performs poorly on this task in lieu of sufﬁcient p-resolution, and hence training data.","We implement this experiment in a Google Colab notebook which is provided as a supplement to
this article.",2022-01-31 22:00:41+00:00,Imbedding Deep Neural Networks,cs.LG,"['cs.LG', 'math.OC', '68T07']","[arxiv.Result.Author('Andrew Corbett'), arxiv.Result.Author('Dmitry Kangin')]","Continuous depth neural networks, such as Neural ODEs, have refashioned the
understanding of residual neural networks in terms of non-linear vector-valued
optimal control problems. The common solution is to use the adjoint sensitivity
method to replicate a forward-backward pass optimisation problem. We propose a
new approach which explicates the network's `depth' as a fundamental variable,
thus reducing the problem to a system of forward-facing initial value problems.
This new method is based on the principle of `Invariant Imbedding' for which we
prove a general solution, applicable to all non-linear, vector-valued optimal
control problems with both running and terminal loss. Our new architectures
provide a tangible tool for inspecting the theoretical--and to a great extent
unexplained--properties of network depth. They also constitute a resource of
discrete implementations of Neural ODEs comparable to classes of imbedded
residual neural networks. Through a series of experiments, we show the
competitive performance of the proposed architectures for supervised learning
and time series prediction."
1323,"Nevertheless, this experiments sufﬁces to prove the concept that InImNets may be used to solve a
different genre of time-series problems, with further research demanded on the implementation of
their nested derivatives.","As such,
the InImNet performs poorly on this task in lieu of sufﬁcient p-resolution, and hence training data.","We implement this experiment in a Google Colab notebook which is provided as a supplement to
this article.",2022-01-31 22:00:41+00:00,Imbedding Deep Neural Networks,cs.LG,"['cs.LG', 'math.OC', '68T07']","[arxiv.Result.Author('Andrew Corbett'), arxiv.Result.Author('Dmitry Kangin')]","Continuous-depth neural networks, such as Neural ODEs, have refashioned the
understanding of residual neural networks in terms of non-linear vector-valued
optimal control problems. The common solution is to use the adjoint sensitivity
method to replicate a forward-backward pass optimisation problem. We propose a
new approach which explicates the network's `depth' as a fundamental variable,
thus reducing the problem to a system of forward-facing initial value problems.
This new method is based on the principle of `Invariant Imbedding' for which we
prove a general solution, applicable to all non-linear, vector-valued optimal
control problems with both running and terminal loss. Our new architectures
provide a tangible tool for inspecting the theoretical--and to a great extent
unexplained--properties of network depth. They also constitute a resource of
discrete implementations of Neural ODEs comparable to classes of imbedded
residual neural networks. Through a series of experiments, we show the
competitive performance of the proposed architectures for supervised learning
and time series prediction."
1332,"We hope that this encourages further research in developing
                                                                     RL agents capable of generalization.",based approach to achieve leading performance on URLB.,"CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery

9.",2022-02-01 00:36:29+00:00,CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Michael Laskin'), arxiv.Result.Author('Hao Liu'), arxiv.Result.Author('Xue Bin Peng'), arxiv.Result.Author('Denis Yarats'), arxiv.Result.Author('Aravind Rajeswaran'), arxiv.Result.Author('Pieter Abbeel')]","We introduce Contrastive Intrinsic Control (CIC), an algorithm for
unsupervised skill discovery that maximizes the mutual information between
skills and state transitions. In contrast to most prior approaches, CIC uses a
decomposition of the mutual information that explicitly incentivizes diverse
behaviors by maximizing state entropy. We derive a novel lower bound estimate
for the mutual information which combines a particle estimator for state
entropy to generate diverse behaviors and contrastive learning to distill these
behaviors into distinct skills. We evaluate our algorithm on the Unsupervised
Reinforcement Learning Benchmark, which consists of a long reward-free
pre-training phase followed by a short adaptation phase to downstream tasks
with extrinsic rewards. We find that CIC substantially improves over prior
unsupervised skill discovery methods and outperforms the next leading overall
exploration algorithm in terms of downstream task performance."
1333,"400  Ablation 2: CIC rep. learning but no particle entropy    We hope that this encourages further research in developing
                                                                      RL agents capable of generalization.","We showed that CIC is the ﬁrst competence-
             Ablation 1: Particle entropy but no CIC rep. learning    based approach to achieve leading performance on URLB.","Reward  300
                                                                      9.",2022-02-01 00:36:29+00:00,CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Michael Laskin'), arxiv.Result.Author('Hao Liu'), arxiv.Result.Author('Xue Bin Peng'), arxiv.Result.Author('Denis Yarats'), arxiv.Result.Author('Aravind Rajeswaran'), arxiv.Result.Author('Pieter Abbeel')]","We introduce Contrastive Intrinsic Control (CIC), an algorithm for
unsupervised skill discovery that maximizes the mutual information between
state-transitions and latent skill vectors. CIC utilizes contrastive learning
between state-transitions and skills to learn behavior embeddings and maximizes
the entropy of these embeddings as an intrinsic reward to encourage behavioral
diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning
Benchmark, which consists of a long reward-free pre-training phase followed by
a short adaptation phase to downstream tasks with extrinsic rewards. CIC
substantially improves over prior methods in terms of adaptation efficiency,
outperforming prior unsupervised skill discovery methods by 1.79x and the next
leading overall exploration algorithm by 1.18x."
1334,"400  Ablation 2: CIC rep. learning but no particle entropy    We hope that this encourages further research in developing
                                                                      RL agents capable of generalization.","We showed that CIC is the ﬁrst competence-
             Ablation 1: Particle entropy but no CIC rep. learning    based approach to achieve leading performance on URLB.","Reward  300
                                                                      9.",2022-02-01 00:36:29+00:00,CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Michael Laskin'), arxiv.Result.Author('Hao Liu'), arxiv.Result.Author('Xue Bin Peng'), arxiv.Result.Author('Denis Yarats'), arxiv.Result.Author('Aravind Rajeswaran'), arxiv.Result.Author('Pieter Abbeel')]","We introduce Contrastive Intrinsic Control (CIC), an algorithm for
unsupervised skill discovery that maximizes the mutual information between
state-transitions and latent skill vectors. CIC utilizes contrastive learning
between state-transitions and skills to learn behavior embeddings and maximizes
the entropy of these embeddings as an intrinsic reward to encourage behavioral
diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning
Benchmark, which consists of a long reward-free pre-training phase followed by
a short adaptation phase to downstream tasks with extrinsic rewards. CIC
substantially improves over prior methods in terms of adaptation efficiency,
outperforming prior unsupervised skill discovery methods by 1.79x and the next
leading overall exploration algorithm by 1.18x."
1345,"Finally, we note that the
secondary aim of this work is to be a stepping-stone that encourages further research on the architecture
side of continual learning by focusing on the breadth rather than depth of some topics in this work.","In fact, one can
enjoy the improvements on both sides, as we will discuss in Appendix B.","We
believe our work provides many interesting directions that require deeper analysis beyond the scope of
this paper but can signiﬁcantly improve our understanding of continual learning.",2022-02-01 08:32:22+00:00,Architecture Matters in Continual Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Seyed Iman Mirzadeh'), arxiv.Result.Author('Arslan Chaudhry'), arxiv.Result.Author('Dong Yin'), arxiv.Result.Author('Timothy Nguyen'), arxiv.Result.Author('Razvan Pascanu'), arxiv.Result.Author('Dilan Gorur'), arxiv.Result.Author('Mehrdad Farajtabar')]","A large body of research in continual learning is devoted to overcoming the
catastrophic forgetting of neural networks by designing new algorithms that are
robust to the distribution shifts. However, the majority of these works are
strictly focused on the ""algorithmic"" part of continual learning for a ""fixed
neural network architecture"", and the implications of using different
architectures are mostly neglected. Even the few existing continual learning
methods that modify the model assume a fixed architecture and aim to develop an
algorithm that efficiently uses the model throughout the learning experience.
However, in this work, we show that the choice of architecture can
significantly impact the continual learning performance, and different
architectures lead to different trade-offs between the ability to remember
previous tasks and learning new ones. Moreover, we study the impact of various
architectural decisions, and our findings entail best practices and
recommendations that can improve the continual learning performance."
1346,"It, however, remains an interesting
future direction to further study the gains brought by max pooling in both the standard and continual
learning setups.","This could have
transferred over to the continual learning setup that we considered.","3.5 Global Pooling Layers

Global average pooling (GAP) layers are typically used in convolutional networks just before the ﬁnal
classiﬁcation layer to reduce the number of parameters in the classiﬁer.",2022-02-01 08:32:22+00:00,Architecture Matters in Continual Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Seyed Iman Mirzadeh'), arxiv.Result.Author('Arslan Chaudhry'), arxiv.Result.Author('Dong Yin'), arxiv.Result.Author('Timothy Nguyen'), arxiv.Result.Author('Razvan Pascanu'), arxiv.Result.Author('Dilan Gorur'), arxiv.Result.Author('Mehrdad Farajtabar')]","A large body of research in continual learning is devoted to overcoming the
catastrophic forgetting of neural networks by designing new algorithms that are
robust to the distribution shifts. However, the majority of these works are
strictly focused on the ""algorithmic"" part of continual learning for a ""fixed
neural network architecture"", and the implications of using different
architectures are mostly neglected. Even the few existing continual learning
methods that modify the model assume a fixed architecture and aim to develop an
algorithm that efficiently uses the model throughout the learning experience.
However, in this work, we show that the choice of architecture can
significantly impact the continual learning performance, and different
architectures lead to different trade-offs between the ability to remember
previous tasks and learning new ones. Moreover, we study the impact of various
architectural decisions, and our findings entail best practices and
recommendations that can improve the continual learning performance."
1352,"Overcoming these assumptions and making the proposed methodology more
broadly applicable is subject of further research.","We assume that there is no external force, e.g., gravity, that
inﬂuences the system.","The developed methodology will help to make a step forward in developing a ﬂexible and robust
tool for the discovery of physical laws in material mechanics.",2022-02-01 09:59:53+00:00,Learning Physics-Consistent Particle Interactions,cs.LG,"['cs.LG', 'physics.comp-ph']","[arxiv.Result.Author('Zhichao Han'), arxiv.Result.Author('David S. Kammer'), arxiv.Result.Author('Olga Fink')]","Interacting particle systems play a key role in science and engineering.
Access to the governing particle interaction law is fundamental for a complete
understanding of such systems. However, the inherent system complexity keeps
the particle interaction hidden in many cases. Machine learning methods have
the potential to learn the behavior of interacting particle systems by
combining experiments with data analysis methods. However, most existing
algorithms focus on learning the kinetics at the particle level. Learning
pairwise interaction, e.g., pairwise force or pairwise potential energy,
remains an open challenge. Here, we propose an algorithm that adapts the Graph
Networks framework, which contains an edge part to learn the pairwise
interaction and a node part to model the dynamics at particle level. Different
from existing approaches that use neural networks in both parts, we design a
deterministic operator in the node part. The designed physics operator on the
nodes restricts the output space of the edge neural network to be exactly the
pairwise interaction. We test the proposed methodology on multiple datasets and
demonstrate that it achieves considerably better performance in inferring
correctly the pairwise interactions while also being consistent with the
underlying physics on all the datasets than existing purely data-driven models.
The developed methodology can support a better understanding and discovery of
the underlying particle interaction laws, and hence guide the design of
materials with targeted properties."
1353,"Overcoming           Mtij ∈ Rd, e.g.,
all of these assumptions and making the proposed methodology
more broadly applicable is subject of further research.","We denote the corresponding output as
aggregated incoming messages in the node part.","Mtij  GˆE  (  concat(η  t  ,  ηjt  );  θ  E  )             [4]
                                                                                                i
    The developed methodology will help to make a step for-
ward in developing a ﬂexible and robust tool for the discovery          According to Newton’s Second law, the net acceleration of
of physical laws in material mechanics.",2022-02-01 09:59:53+00:00,Learning Physics-Consistent Particle Interactions,cs.LG,"['cs.LG', 'physics.comp-ph']","[arxiv.Result.Author('Zhichao Han'), arxiv.Result.Author('David S. Kammer'), arxiv.Result.Author('Olga Fink')]","Interacting particle systems play a key role in science and engineering.
Access to the governing particle interaction law is fundamental for a complete
understanding of such systems. However, the inherent system complexity keeps
the particle interaction hidden in many cases. Machine learning methods have
the potential to learn the behavior of interacting particle systems by
combining experiments with data analysis methods. However, most existing
algorithms focus on learning the kinetics at the particle level. Learning
pairwise interaction, e.g., pairwise force or pairwise potential energy,
remains an open challenge. Here, we propose an algorithm that adapts the Graph
Networks framework, which contains an edge part to learn the pairwise
interaction and a node part to model the dynamics at particle level. Different
from existing approaches that use neural networks in both parts, we design a
deterministic operator in the node part that allows to precisely infer the
pairwise interactions that are consistent with underlying physical laws by only
being trained to predict the particle acceleration. We test the proposed
methodology on multiple datasets and demonstrate that it achieves superior
performance in inferring correctly the pairwise interactions while also being
consistent with the underlying physics on all the datasets. The proposed
framework is scalable to larger systems and transferable to any type of
particle interactions. The developed methodology can support a better
understanding and discovery of the underlying particle interaction laws, and
hence guide the design of materials with targeted properties."
1358,"We conclude with a discussion of avenues of possible further research in the
ﬁnal Section.","Section 5 compares the statistical mechanics of optimal learning machines to that of
physical systems.","The rest of the introduction i) provides further motivation for the introduction of
the notion of relevance, on the basis of general arguments as well as of some examples, ii) it
clariﬁes its relation with the entropy as a measure of information content and how this notion
ﬁts into the growing literature on statistical learning, and iii) it summarises the results that are
discussed in the rest of the review.",2022-02-01 11:16:04+00:00,Quantifying Relevance in Learning and Inference,cs.LG,"['cs.LG', 'cond-mat.dis-nn', 'physics.data-an', 'stat.ML']","[arxiv.Result.Author('Matteo Marsili'), arxiv.Result.Author('Yasser Roudi')]","Learning is a distinctive feature of intelligent behaviour. High-throughput
experimental data and Big Data promise to open new windows on complex systems
such as cells, the brain or our societies. Yet, the puzzling success of
Artificial Intelligence and Machine Learning shows that we still have a poor
conceptual understanding of learning. These applications push statistical
inference into uncharted territories where data is high-dimensional and scarce,
and prior information on ""true"" models is scant if not totally absent. Here we
review recent progress on understanding learning, based on the notion of
""relevance"". The relevance, as we define it here, quantifies the amount of
information that a dataset or the internal representation of a learning machine
contains on the generative model of the data. This allows us to define
maximally informative samples, on one hand, and optimal learning machines on
the other. These are ideal limits of samples and of machines, that contain the
maximal amount of information about the unknown generative process, at a given
resolution (or level of compression). Both ideal limits exhibit critical
features in the statistical sense: Maximally informative samples are
characterised by a power-law frequency distribution (statistical criticality)
and optimal learning machines by an anomalously large susceptibility. The
trade-off between resolution (i.e. compression) and relevance distinguishes the
regime of noisy representations from that of lossy compression. These are
separated by a special point characterised by Zipf's law statistics. This
identifies samples obeying Zipf's law as the most compressed loss-less
representations that are optimal in the sense of maximal relevance. Criticality
in optimal learning machines manifests in an exponential degeneracy of energy
levels, that leads to unusual thermodynamic properties."
1395,"Arbel and Mairal
(2021) further study fully-amortized warm-started solvers that arise in bi-level optimization
problems for hyper-parameter optimization and use the theoretical framework from singularly
perturbed systems (Habets, 2010) to analyze properties of the approximate solutions.","(2018, Table 2) compare semi-amortized models (SA-VAE) to warm-starting and ﬁne-tuning
(VAE+SVI) and demonstrate that the end-to-end learning signal is helpful.","3.2.4 On second-order derivatives of the objective

Second-order derivatives of the objective often arise when optimizing semi-amortized models
and may be a computational bottleneck.",2022-02-01 18:58:33+00:00,Tutorial on amortized optimization for learning to optimize over continuous domains,cs.LG,"['cs.LG', 'cs.AI', 'math.OC']",[arxiv.Result.Author('Brandon Amos')],"Optimization is a ubiquitous modeling tool that is often deployed in settings
that repeatedly solve similar instances of the same problem. Amortized
optimization methods use learning to predict the solutions to problems in these
settings. This leverages the shared structure between similar problem
instances. In this tutorial, we will discuss the key design choices behind
amortized optimization, roughly categorizing 1) models into fully-amortized and
semi-amortized approaches, and 2) learning methods into regression-based and
objective-based. We then view existing applications through these foundations
to draw connections between them, including for manifold optimization,
variational inference, sparse coding, meta-learning, control, reinforcement
learning, convex optimization, and deep equilibrium networks. This framing
enables us easily see, for example, that the amortized inference in variational
autoencoders is conceptually identical to value gradients in control and
reinforcement learning as they both use fully-amortized models with a
objective-based loss. The source code for this tutorial is available at
https://www.github.com/facebookresearch/amortized-optimization-tutorial"
1396,"Arbel and Mairal
(2021) further study fully-amortized warm-started solvers that arise in bi-level optimization
problems for hyper-parameter optimization and use the theoretical framework from singularly
perturbed systems (Habets, 2010) to analyze properties of the approximate solutions.","(2018, Table 2) compare semi-amortized models (SA-VAE) to warm-starting and ﬁne-tuning
(VAE+SVI) and demonstrate that the end-to-end learning signal is helpful.","3.2.4 On second-order derivatives of the objective

Second-order derivatives of the objective often arise when optimizing semi-amortized models
and may be a computational bottleneck.",2022-02-01 18:58:33+00:00,Tutorial on amortized optimization for learning to optimize over continuous domains,cs.LG,"['cs.LG', 'cs.AI', 'math.OC']",[arxiv.Result.Author('Brandon Amos')],"Optimization is a ubiquitous modeling tool and is often deployed in settings
which repeatedly solve similar instances of the same problem. Amortized
optimization methods use learning to predict the solutions to problems in these
settings. This leverages the shared structure between similar problem
instances. In this tutorial, we will discuss the key design choices behind
amortized optimization, roughly categorizing 1) models into fully-amortized and
semi-amortized approaches, and 2) learning methods into regression-based and
objective-based. We then view existing applications through these foundations
to draw connections between them, including for manifold optimization,
variational inference, sparse coding, meta-learning, control, reinforcement
learning, convex optimization, and deep equilibrium networks. This framing
enables us easily see, for example, that the amortized inference in variational
autoencoders is conceptually identical to value gradients in control and
reinforcement learning as they both use fully-amortized models with an
objective-based loss. The source code for this tutorial is available at
https://www.github.com/facebookresearch/amortized-optimization-tutorial"
1409,The limits of communication rounds needed           and robustness in the FL setting deserve further research.,"Thus,
ing on many factors such as the function class, regularization,     fundamental connections between personalization, fairness,
and loss types.",for rate-optimality in FL (if it exists) have yet to be studied.,2022-02-01 23:32:21+00:00,Federated Learning Challenges and Opportunities: An Outlook,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Jie Ding'), arxiv.Result.Author('Eric Tramel'), arxiv.Result.Author('Anit Kumar Sahu'), arxiv.Result.Author('Shuang Wu'), arxiv.Result.Author('Salman Avestimehr'), arxiv.Result.Author('Tao Zhang')]","Federated learning (FL) has been developed as a promising framework to
leverage the resources of edge devices, enhance customers' privacy, comply with
regulations, and reduce development costs. Although many methods and
applications have been developed for FL, several critical challenges for
practical FL systems remain unaddressed. This paper provides an outlook on FL
development, categorized into five emerging directions of FL, namely algorithm
foundation, personalization, hardware and security constraints, lifelong
learning, and nonstandard data. Our unique perspectives are backed by practical
observations from large-scale federated systems for edge devices."
1419,"As a consequence,
in future research, it would be interesting to further study these models through the
lens of the multi-task learning framework and thus compare them to other graph-based
multi-task learning methods [98, 99].","Nonetheless, while our scope is limited to autoencoders, we acknowledge that Modularity-
Aware GAE and VGAE could also be seen as multi-task models, as we train them to
perform community detection and link prediction simultaneously.","In addition, there are also interesting parallels between our proposed method and
pre-training methods for self-supervised learning (SSL) [100, 101, 102], that might deserve
further investigations in future research.",2022-02-02 11:07:11+00:00,Modularity-Aware Graph Autoencoders for Joint Community Detection and Link Prediction,cs.LG,"['cs.LG', 'cs.SI', 'stat.ML']","[arxiv.Result.Author('Guillaume Salha-Galvan'), arxiv.Result.Author('Johannes F. Lutzeyer'), arxiv.Result.Author('George Dasoulas'), arxiv.Result.Author('Romain Hennequin'), arxiv.Result.Author('Michalis Vazirgiannis')]","Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as
powerful methods for link prediction. Their performances are less impressive on
community detection problems where, according to recent and concurring
experimental evaluations, they are often outperformed by simpler alternatives
such as the Louvain method. It is currently still unclear to which extent one
can improve community detection with GAE and VGAE, especially in the absence of
node features. It is moreover uncertain whether one could do so while
simultaneously preserving good performances on link prediction. In this paper,
we show that jointly addressing these two tasks with high accuracy is possible.
For this purpose, we introduce and theoretically study a community-preserving
message passing scheme, doping our GAE and VGAE encoders by considering both
the initial graph structure and modularity-based prior communities when
computing embedding spaces. We also propose novel training and optimization
strategies, including the introduction of a modularity-inspired regularizer
complementing the existing reconstruction losses for joint link prediction and
community detection. We demonstrate the empirical effectiveness of our
approach, referred to as Modularity-Aware GAE and VGAE, through in-depth
experimental validation on various real-world graphs."
1449,"Advanced Scheme: MSL and Two-Stage
Performance Gains  +0.6  CropDisease              Performance Gains  +0.0
                                                                                                    In this section, we further study SL and SSL in a more
                         EuroSAT                                                                    advanced scheme, which can be explained from the domain
                                                                                                    similarity and few-shot difﬁculty perspective, in line with
                   +0.4 ICShIeCstX                                   -1.0  Places                   previous observations.","(a) Small Similarity (k=1)                                           (b) Large Similarity (k=1)
                                                                                                    6.","We ﬁrst investigate whether SL and
                                                                                                    SSL can synergize by studying MSL.",2022-02-01 12:35:25+00:00,Understanding Cross-Domain Few-Shot Learning: An Experimental Study,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jaehoon Oh'), arxiv.Result.Author('Sungnyun Kim'), arxiv.Result.Author('Namgyu Ho'), arxiv.Result.Author('Jin-Hwa Kim'), arxiv.Result.Author('Hwanjun Song'), arxiv.Result.Author('Se-Young Yun')]","Cross-domain few-shot learning has drawn increasing attention for handling
large differences between the source and target domains--an important concern
in real-world scenarios. To overcome these large differences, recent works have
considered exploiting small-scale unlabeled data from the target domain during
the pre-training stage. This data enables self-supervised pre-training on the
target domain, in addition to supervised pre-training on the source domain. In
this paper, we empirically investigate scenarios under which it is advantageous
to use each pre-training scheme, based on domain similarity and few-shot
difficulty: performance gain of self-supervised pre-training over supervised
pre-training increases when domain similarity is smaller or few-shot difficulty
is lower. We further design two pre-training schemes, mixed-supervised and
two-stage learning, that improve performance. In this light, we present seven
findings for CD-FSL which are supported by extensive experiments and analyses
on three source and eight target benchmark datasets with varying levels of
domain similarity and few-shot difficulty. Our code is available at
https://anonymous.4open.science/r/understandingCDFSL."
1450,"Advanced Scheme: MSL and Two-Stage
Performance Gains  +0.6  CropDisease              Performance Gains  +0.0
                                                                                                    In this section, we further study SL and SSL in a more
                         EuroSAT                                                                    advanced scheme, which can be explained from the domain
                                                                                                    similarity and few-shot difﬁculty perspective, in line with
                   +0.4 ICShIeCstX                                   -1.0  Places                   previous observations.","(a) Small Similarity (k=1)                                           (b) Large Similarity (k=1)
                                                                                                    6.","We ﬁrst investigate whether SL and
                                                                                                    SSL can synergize by studying MSL.",2022-02-01 12:35:25+00:00,Understanding Cross-Domain Few-Shot Learning: An Experimental Study,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jaehoon Oh'), arxiv.Result.Author('Sungnyun Kim'), arxiv.Result.Author('Namgyu Ho'), arxiv.Result.Author('Jin-Hwa Kim'), arxiv.Result.Author('Hwanjun Song'), arxiv.Result.Author('Se-Young Yun')]","Cross-domain few-shot learning has drawn increasing attention for handling
large differences between the source and target domains--an important concern
in real-world scenarios. To overcome these large differences, recent works have
considered exploiting small-scale unlabeled data from the target domain during
the pre-training stage. This data enables self-supervised pre-training on the
target domain, in addition to supervised pre-training on the source domain. In
this paper, we empirically investigate scenarios under which it is advantageous
to use each pre-training scheme, based on domain similarity and few-shot
difficulty: performance gain of self-supervised pre-training over supervised
pre-training increases when domain similarity is smaller or few-shot difficulty
is lower. We further design two pre-training schemes, mixed-supervised and
two-stage learning, that improve performance. In this light, we present seven
findings for CD-FSL which are supported by extensive experiments and analyses
on three source and eight target benchmark datasets with varying levels of
domain similarity and few-shot difficulty. Our code is available at
https://anonymous.4open.science/r/understandingCDFSL."
1451,"6 Advanced Scheme: MSL and Two-Stage

In this section, we further study SL and SSL in a more advanced scheme from the domain similarity
and few-shot difﬁculty perspective, in line with previous observations.","The same trend is
observed when tieredImageNet is used as the source dataset (Appendix I).","We ﬁrst investigate whether
SL and SSL can synergize by studying MSL.",2022-02-01 12:35:25+00:00,Understanding Cross-Domain Few-Shot Learning Based on Domain Similarity and Few-Shot Difficulty,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jaehoon Oh'), arxiv.Result.Author('Sungnyun Kim'), arxiv.Result.Author('Namgyu Ho'), arxiv.Result.Author('Jin-Hwa Kim'), arxiv.Result.Author('Hwanjun Song'), arxiv.Result.Author('Se-Young Yun')]","Cross-domain few-shot learning (CD-FSL) has drawn increasing attention for
handling large differences between the source and target domains--an important
concern in real-world scenarios. To overcome these large differences, recent
works have considered exploiting small-scale unlabeled data from the target
domain during the pre-training stage. This data enables self-supervised
pre-training on the target domain, in addition to supervised pre-training on
the source domain. In this paper, we empirically investigate which pre-training
is preferred based on domain similarity and few-shot difficulty of the target
domain. We discover that the performance gain of self-supervised pre-training
over supervised pre-training becomes large when the target domain is dissimilar
to the source domain, or the target domain itself has low few-shot difficulty.
We further design two pre-training schemes, mixed-supervised and two-stage
learning, that improve performance. In this light, we present six findings for
CD-FSL, which are supported by extensive experiments and analyses on three
source and eight target benchmark datasets with varying levels of domain
similarity and few-shot difficulty. Our code is available at
https://github.com/sungnyun/understanding-cdfsl."
1452,"To further study the performance of our model, we conducted a Pearson correlation coefﬁcient analysis between installed
solar capacity and the predicted total solar installation area for the state of Karnataka in India.",These predictions allow the Pearson correlation analysis between Karnataka solar install capacity and model predictions.,"To do this, we run inference for
the different median composite Sentinel 2 imagery after histogram matching.",2022-01-31 23:53:19+00:00,An Artificial Intelligence Dataset for Solar Energy Locations in India,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anthony Ortiz'), arxiv.Result.Author('Dhaval Negandhi'), arxiv.Result.Author('Sagar R Mysorekar'), arxiv.Result.Author('Joseph Kiesecker'), arxiv.Result.Author('Shivaprakash K Nagaraju'), arxiv.Result.Author('Caleb Robinson'), arxiv.Result.Author('Priyal Bhatia'), arxiv.Result.Author('Aditi Khurana'), arxiv.Result.Author('Jane Wang'), arxiv.Result.Author('Felipe Oviedo'), arxiv.Result.Author('Juan Lavista Ferres')]","Rapid development of renewable energy sources, particularly solar
photovoltaics, is critical to mitigate climate change. As a result, India has
set ambitious goals to install 300 gigawatts of solar energy capacity by 2030.
Given the large footprint projected to meet these renewable energy targets the
potential for land use conflicts over environmental and social values is high.
To expedite development of solar energy, land use planners will need access to
up-to-date and accurate geo-spatial information of PV infrastructure. The
majority of recent studies use either predictions of resource suitability or
databases that are either developed thru crowdsourcing that often have
significant sampling biases or have time lags between when projects are
permitted and when location data becomes available. Here, we address this
shortcoming by developing a spatially explicit machine learning model to map
utility-scale solar projects across India. Using these outputs, we provide a
cumulative measure of the solar footprint across India and quantified the
degree of land modification associated with land cover types that may cause
conflicts. Our analysis indicates that over 74\% of solar development In India
was built on landcover types that have natural ecosystem preservation, and
agricultural values. Thus, with a mean accuracy of 92\% this method permits the
identification of the factors driving land suitability for solar projects and
will be of widespread interest for studies seeking to assess trade-offs
associated with the global decarbonization of green-energy systems. In the same
way, our model increases the feasibility of remote sensing and long-term
monitoring of renewable energy deployment targets."
1453,"To further study the performance of our model, we conducted a Pearson correlation coefﬁcient analysis between installed
solar capacity and the predicted total solar installation area for the state of Karnataka in India.","To alleviate covariate shift we perform tile-wise histogram
matching29 from Top of Atmosphere (ToA) Sentinel 2 median composites to the 2020 tiles surface reﬂectance Sentinel 2
median composites.","To do this, we run inference for
the different median composite Sentinel 2 imagery after histogram matching.",2022-01-31 23:53:19+00:00,An Artificial Intelligence Dataset for Solar Energy Locations in India,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anthony Ortiz'), arxiv.Result.Author('Dhaval Negandhi'), arxiv.Result.Author('Sagar R Mysorekar'), arxiv.Result.Author('Joseph Kiesecker'), arxiv.Result.Author('Shivaprakash K Nagaraju'), arxiv.Result.Author('Caleb Robinson'), arxiv.Result.Author('Priyal Bhatia'), arxiv.Result.Author('Aditi Khurana'), arxiv.Result.Author('Jane Wang'), arxiv.Result.Author('Felipe Oviedo'), arxiv.Result.Author('Juan Lavista Ferres')]","Rapid development of renewable energy sources, particularly solar
photovoltaics (PV), is critical to mitigate climate change. As a result, India
has set ambitious goals to install 500 gigawatts of solar energy capacity by
2030. Given the large footprint projected to meet renewables energy targets,
the potential for land use conflicts over environmental values is high. To
expedite development of solar energy, land use planners will need access to
up-to-date and accurate geo-spatial information of PV infrastructure. In this
work, we developed a spatially explicit machine learning model to map
utility-scale solar projects across India using freely available satellite
imagery with a mean accuracy of 92%. Our model predictions were validated by
human experts to obtain a dataset of 1363 solar PV farms. Using this dataset,
we measure the solar footprint across India and quantified the degree of
landcover modification associated with the development of PV infrastructure.
Our analysis indicates that over 74% of solar development In India was built on
landcover types that have natural ecosystem preservation, or agricultural
value."
1456,"We consider this
third potential source of uncertainty a particularly interesting avenue for further study.","14 indicate that a subset of the columns in this dataset could be behaving a
way that is “mechanically distinct” where the buckled column deforms in a manner that is more dissimilar from typical
ﬁrst mode buckling of a symmetrically loaded ﬁxed-ﬁxed column without geometric heterogeneity.","18
                                                                        ABC Dataset and Metamodel  A PREPRINT

                                                         Sub-dataset 1  Sub-dataset 2              Sub-dataset 3

Low Confidence and Correct Low Confidence and Incorrect

High Confidence and Correct

                                                       X-Displacement (% of beam width)
                     -15.0 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 15.0
Figure 13: Examples of deformed columns for high and low conﬁdence predictions.",2022-02-03 02:46:16+00:00,Learning Mechanically Driven Emergent Behavior with Message Passing Neural Networks,cs.LG,"['cs.LG', 'physics.data-an', '74G60, 74B20, 74A40', 'J.2; I.6.3; I.6.5']","[arxiv.Result.Author('Peerasait Prachaseree'), arxiv.Result.Author('Emma Lejeune')]","From designing architected materials to connecting mechanical behavior across
scales, computational modeling is a critical tool in solid mechanics. Recently,
there has been a growing interest in using machine learning to reduce the
computational cost of physics-based simulations. Notably, while machine
learning approaches that rely on Graph Neural Networks (GNNs) have shown
success in learning mechanics, the performance of GNNs has yet to be
investigated on a myriad of solid mechanics problems. In this work, we examine
the ability of GNNs to predict a fundamental aspect of mechanically driven
emergent behavior: the connection between a column's geometric structure and
the direction that it buckles. To accomplish this, we introduce the Asymmetric
Buckling Columns (ABC) dataset, a dataset comprised of three sub-datasets of
asymmetric and heterogeneous column geometries where the goal is to classify
the direction of symmetry breaking (left or right) under compression after the
onset of instability. Because of complex local geometry, the ""image-like"" data
representations required for implementing standard convolutional neural network
based metamodels are not ideal, thus motivating the use of GNNs. In addition to
investigating GNN model architecture, we study the effect of different input
data representation approaches, data augmentation, and combining multiple
models as an ensemble. While we were able to obtain good results, we also
showed that predicting solid mechanics based emergent behavior is non-trivial.
Because both our model implementation and dataset are distributed under
open-source licenses, we hope that future researchers can build on our work to
create enhanced mechanics-specific machine learning pipelines for capturing the
behavior of complex geometric structures."
1457,"We consider this
third potential source of uncertainty a particularly interesting avenue for further study.","14 indicate that a subset of the columns in this dataset could be behaving a
way that is “mechanically distinct” where the buckled column deforms in a manner that is more dissimilar from typical
ﬁrst mode buckling of a symmetrically loaded ﬁxed-ﬁxed column without geometric heterogeneity.","18
                                                                        ABC Dataset and Metamodel  A PREPRINT

                                                         Sub-Dataset 1  Sub-Dataset 2              Sub-Dataset 3

Low Confidence and Correct Low Confidence and Incorrect

High Confidence and Correct

                                                       x-Displacement (% of beam width)
                     -15.0 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 15.0
Figure 13: Examples of deformed columns for high and low conﬁdence predictions.",2022-02-03 02:46:16+00:00,Learning Mechanically Driven Emergent Behavior with Message Passing Neural Networks,cs.LG,"['cs.LG', 'physics.data-an', '74G60, 74B20, 74A40', 'J.2; I.6.3; I.6.5']","[arxiv.Result.Author('Peerasait Prachaseree'), arxiv.Result.Author('Emma Lejeune')]","From designing architected materials to connecting mechanical behavior across
scales, computational modeling is a critical tool in solid mechanics. Recently,
there has been a growing interest in using machine learning to reduce the
computational cost of physics-based simulations. Notably, while machine
learning approaches that rely on Graph Neural Networks (GNNs) have shown
success in learning mechanics, the performance of GNNs has yet to be
investigated on a myriad of solid mechanics problems. In this work, we examine
the ability of GNNs to predict a fundamental aspect of mechanically driven
emergent behavior: the connection between a column's geometric structure and
the direction that it buckles. To accomplish this, we introduce the Asymmetric
Buckling Columns (ABC) dataset, a dataset comprised of three sub-datasets of
asymmetric and heterogeneous column geometries where the goal is to classify
the direction of symmetry breaking (left or right) under compression after the
onset of instability. Because of complex local geometry, the ""image-like"" data
representations required for implementing standard convolutional neural network
based metamodels are not ideal, thus motivating the use of GNNs. In addition to
investigating GNN model architecture, we study the effect of different input
data representation approaches, data augmentation, and combining multiple
models as an ensemble. While we were able to obtain good results, we also
showed that predicting solid mechanics based emergent behavior is non-trivial.
Because both our model implementation and dataset are distributed under
open-source licenses, we hope that future researchers can build on our work to
create enhanced mechanics-specific machine learning pipelines for capturing the
behavior of complex geometric structures."
1489,"We believe that the
proportional fairness guarantee of PropFair is a welcome addition to existing FL systems, and we plan to
further study its applicability in training generative models.","Compared to other fair FL algorithms, PropFair achieves better worst-case performance with no sacriﬁce
(in fact often improvement) on the overall performance, and thus yields better equity.","13
References

Audet, C., Savard, G., and Zghal, W. Multiobjective optimization through a series of single-objective
   formulations.",2022-02-03 16:28:04+00:00,Equality Is Not Equity: Proportional Fairness in Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Guojun Zhang'), arxiv.Result.Author('Saber Malekmohammadi'), arxiv.Result.Author('Xi Chen'), arxiv.Result.Author('Yaoliang Yu')]","Ensuring fairness of machine learning (ML) algorithms is becoming an
increasingly important mission for ML service providers. This is even more
critical and challenging in the federated learning (FL) scenario, given a large
number of diverse participating clients. Simply mandating equality across
clients could lead to many undesirable consequences, potentially discouraging
high-performing clients and resulting in sub-optimal overall performance. In
order to achieve better equity rather than equality, in this work, we introduce
and study proportional fairness (PF) in FL, which has a deep connection with
game theory. By viewing FL from a cooperative game perspective, where the
players (clients) collaboratively learn a good model, we formulate PF as Nash
bargaining solutions. Based on this concept, we propose PropFair, a novel and
easy-to-implement algorithm for effectively finding PF solutions, and we prove
its convergence properties. We illustrate through experiments that PropFair
consistently improves the worst-case and the overall performances
simultaneously over state-of-the-art fair FL algorithms for a wide array of
vision and language datasets, thus achieving better equity."
1501,"B.2 D4RL Hopper Diagnostic Study on Varying Unlabeled Dataset Size

Following the discussion in Section 5.2, we further study the setting where relabeled data has higher quality than the labeled
data in the single-task hopper task by varying the amount of unlabeled data within the range of (10k, 100k, 1M) transitions.","The fact that UDS is unable to
learn on medium-replay datasets also suggests that data sharing without rewards is less useful in settings where the coverage
of the labeled ofﬂine data is already quite broad.",We pick the case where labeled data is random and unlabeled data is expert for such ablation study.,2022-02-03 18:04:54+00:00,How to Leverage Unlabeled Data in Offline Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Tianhe Yu'), arxiv.Result.Author('Aviral Kumar'), arxiv.Result.Author('Yevgen Chebotar'), arxiv.Result.Author('Karol Hausman'), arxiv.Result.Author('Chelsea Finn'), arxiv.Result.Author('Sergey Levine')]","Offline reinforcement learning (RL) can learn control policies from static
datasets but, like standard RL methods, it requires reward annotations for
every transition. In many cases, labeling large datasets with rewards may be
costly, especially if those rewards must be provided by human labelers, while
collecting diverse unlabeled data might be comparatively inexpensive. How can
we best leverage such unlabeled data in offline RL? One natural solution is to
learn a reward function from the labeled data and use it to label the unlabeled
data. In this paper, we find that, perhaps surprisingly, a much simpler method
that simply applies zero rewards to unlabeled data leads to effective data
sharing both in theory and in practice, without learning any reward model at
all. While this approach might seem strange (and incorrect) at first, we
provide extensive theoretical and empirical analysis that illustrates how it
trades off reward bias, sample complexity and distributional shift, often
leading to good results. We characterize conditions under which this simple
strategy is effective, and further show that extending it with a simple
reweighting approach can further alleviate the bias introduced by using
incorrect reward labels. Our empirical evaluation confirms these findings in
simulated robotic locomotion, navigation, and manipulation settings."
1502,"B.2 D4RL Hopper Diagnostic Study on Varying Unlabeled Dataset Size

Following the discussion in Section 5.2, we further study the setting where relabeled data has higher quality than the labeled
data in the single-task hopper task by varying the amount of unlabeled data within the range of (10k, 100k, 1M) transitions.","The fact that UDS is unable to
learn on medium-replay datasets also suggests that data sharing without rewards is less useful in settings where the coverage
of the labeled ofﬂine data is already quite broad.",We pick the case where labeled data is random and unlabeled data is expert for such ablation study.,2022-02-03 18:04:54+00:00,How to Leverage Unlabeled Data in Offline Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Tianhe Yu'), arxiv.Result.Author('Aviral Kumar'), arxiv.Result.Author('Yevgen Chebotar'), arxiv.Result.Author('Karol Hausman'), arxiv.Result.Author('Chelsea Finn'), arxiv.Result.Author('Sergey Levine')]","Offline reinforcement learning (RL) can learn control policies from static
datasets but, like standard RL methods, it requires reward annotations for
every transition. In many cases, labeling large datasets with rewards may be
costly, especially if those rewards must be provided by human labelers, while
collecting diverse unlabeled data might be comparatively inexpensive. How can
we best leverage such unlabeled data in offline RL? One natural solution is to
learn a reward function from the labeled data and use it to label the unlabeled
data. In this paper, we find that, perhaps surprisingly, a much simpler method
that simply applies zero rewards to unlabeled data leads to effective data
sharing both in theory and in practice, without learning any reward model at
all. While this approach might seem strange (and incorrect) at first, we
provide extensive theoretical and empirical analysis that illustrates how it
trades off reward bias, sample complexity and distributional shift, often
leading to good results. We characterize conditions under which this simple
strategy is effective, and further show that extending it with a simple
reweighting approach can further alleviate the bias introduced by using
incorrect reward labels. Our empirical evaluation confirms these findings in
simulated robotic locomotion, navigation, and manipulation settings."
1503,"B.2 D4RL Hopper Diagnostic Study on Varying Unlabeled Dataset Size

Following the discussion in Section 5.2, we further study the setting where relabeled data has higher quality than the labeled
data in the single-task hopper task by varying the amount of unlabeled data within the range of (10k, 100k, 1M) transitions.","The fact that UDS is unable to
learn on medium-replay datasets also suggests that data sharing without rewards is less useful in settings where the coverage
of the labeled ofﬂine data is already quite broad.",We pick the case where labeled data is random and unlabeled data is expert for such ablation study.,2022-02-03 18:04:54+00:00,How to Leverage Unlabeled Data in Offline Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Tianhe Yu'), arxiv.Result.Author('Aviral Kumar'), arxiv.Result.Author('Yevgen Chebotar'), arxiv.Result.Author('Karol Hausman'), arxiv.Result.Author('Chelsea Finn'), arxiv.Result.Author('Sergey Levine')]","Offline reinforcement learning (RL) can learn control policies from static
datasets but, like standard RL methods, it requires reward annotations for
every transition. In many cases, labeling large datasets with rewards may be
costly, especially if those rewards must be provided by human labelers, while
collecting diverse unlabeled data might be comparatively inexpensive. How can
we best leverage such unlabeled data in offline RL? One natural solution is to
learn a reward function from the labeled data and use it to label the unlabeled
data. In this paper, we find that, perhaps surprisingly, a much simpler method
that simply applies zero rewards to unlabeled data leads to effective data
sharing both in theory and in practice, without learning any reward model at
all. While this approach might seem strange (and incorrect) at first, we
provide extensive theoretical and empirical analysis that illustrates how it
trades off reward bias, sample complexity and distributional shift, often
leading to good results. We characterize conditions under which this simple
strategy is effective, and further show that extending it with a simple
reweighting approach can further alleviate the bias introduced by using
incorrect reward labels. Our empirical evaluation confirms these findings in
simulated robotic locomotion, navigation, and manipulation settings."
1504,"B.2 D4RL Hopper Diagnostic Study on Varying Unlabeled Dataset Size

Following the discussion in Section 5.2, we further study the setting where relabeled data has higher quality than the labeled
data in the single-task hopper task by varying the amount of unlabeled data within the range of (10k, 100k, 1M) transitions.","The fact that UDS is unable to
learn on medium-replay datasets also suggests that data sharing without rewards is less useful in settings where the coverage
of the labeled ofﬂine data is already quite broad.",We pick the case where labeled data is random and unlabeled data is expert for such ablation study.,2022-02-03 18:04:54+00:00,How to Leverage Unlabeled Data in Offline Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Tianhe Yu'), arxiv.Result.Author('Aviral Kumar'), arxiv.Result.Author('Yevgen Chebotar'), arxiv.Result.Author('Karol Hausman'), arxiv.Result.Author('Chelsea Finn'), arxiv.Result.Author('Sergey Levine')]","Offline reinforcement learning (RL) can learn control policies from static
datasets but, like standard RL methods, it requires reward annotations for
every transition. In many cases, labeling large datasets with rewards may be
costly, especially if those rewards must be provided by human labelers, while
collecting diverse unlabeled data might be comparatively inexpensive. How can
we best leverage such unlabeled data in offline RL? One natural solution is to
learn a reward function from the labeled data and use it to label the unlabeled
data. In this paper, we find that, perhaps surprisingly, a much simpler method
that simply applies zero rewards to unlabeled data leads to effective data
sharing both in theory and in practice, without learning any reward model at
all. While this approach might seem strange (and incorrect) at first, we
provide extensive theoretical and empirical analysis that illustrates how it
trades off reward bias, sample complexity and distributional shift, often
leading to good results. We characterize conditions under which this simple
strategy is effective, and further show that extending it with a simple
reweighting approach can further alleviate the bias introduced by using
incorrect reward labels. Our empirical evaluation confirms these findings in
simulated robotic locomotion, navigation, and manipulation settings."
1508,"More generally, a potential disadvantage of the proposed approach is that biases
of the pre-trained LMs may inﬂuence its behavior, and further study of LID-based models’ bias is
required before they may be deployed in sensitive downstream applications.","One drawback of the active data gathering is that it relies on hand-designed rules
for task relabeling.","Nevertheless, our results
demonstrate that LID enables effective combinatorial generalization across different environments,
and highlight the promise of LM pre-training for more general decision-making problems.",2022-02-03 18:55:52+00:00,Pre-Trained Language Models for Interactive Decision-Making,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Shuang Li'), arxiv.Result.Author('Xavier Puig'), arxiv.Result.Author('Chris Paxton'), arxiv.Result.Author('Yilun Du'), arxiv.Result.Author('Clinton Wang'), arxiv.Result.Author('Linxi Fan'), arxiv.Result.Author('Tao Chen'), arxiv.Result.Author('De-An Huang'), arxiv.Result.Author('Ekin Akyürek'), arxiv.Result.Author('Anima Anandkumar'), arxiv.Result.Author('Jacob Andreas'), arxiv.Result.Author('Igor Mordatch'), arxiv.Result.Author('Antonio Torralba'), arxiv.Result.Author('Yuke Zhu')]","Language model (LM) pre-training is useful in many language processing tasks.
But can pre-trained LMs be further leveraged for more general machine learning
problems? We propose an approach for using LMs to scaffold learning and
generalization in general sequential decision-making problems. In this
approach, goals and observations are represented as a sequence of embeddings,
and a policy network initialized with a pre-trained LM predicts the next
action. We demonstrate that this framework enables effective combinatorial
generalization across different environments and supervisory modalities. We
begin by assuming access to a set of expert demonstrations, and show that
initializing policies with LMs and fine-tuning them via behavior cloning
improves task completion rates by 43.6% in the VirtualHome environment. We then
examine how our framework may be used in environments without pre-collected
expert data. To do this, we integrate an active data gathering procedure into
pre-trained LMs. The agent iteratively learns by interacting with the
environment, relabeling the language goal of past 'failed' experiences, and
updating the policy in a self-supervised loop. The active data gathering
procedure also enables effective combinatorial generalization, outperforming
the best baseline by 25.1%. Finally, we explain these results by investigating
three possible factors underlying the effectiveness of the LM-based policy. We
find that sequential input representations (vs. fixed-dimensional feature
vectors) and favorable weight initialization are both important for
generalization. Surprisingly, however, the format of the policy inputs encoding
(e.g. as a natural language string vs. an arbitrary sequential encoding) has
little influence. Together, these results suggest that language modeling
induces representations that are useful for modeling not just language, but
also goals and plans."
1509,"More generally, a potential disadvantage of the proposed approach is that biases
of the pre-trained LMs may inﬂuence its behavior, and further study of LID-based models’ bias is
required before they may be deployed in sensitive downstream applications.","One drawback of the active data gathering is that it relies on hand-designed rules
for task relabeling.","Nevertheless, our results
demonstrate that LID enables effective combinatorial generalization across different environments,
and highlight the promise of LM pre-training for more general decision-making problems.",2022-02-03 18:55:52+00:00,Pre-Trained Language Models for Interactive Decision-Making,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Shuang Li'), arxiv.Result.Author('Xavier Puig'), arxiv.Result.Author('Chris Paxton'), arxiv.Result.Author('Yilun Du'), arxiv.Result.Author('Clinton Wang'), arxiv.Result.Author('Linxi Fan'), arxiv.Result.Author('Tao Chen'), arxiv.Result.Author('De-An Huang'), arxiv.Result.Author('Ekin Akyürek'), arxiv.Result.Author('Anima Anandkumar'), arxiv.Result.Author('Jacob Andreas'), arxiv.Result.Author('Igor Mordatch'), arxiv.Result.Author('Antonio Torralba'), arxiv.Result.Author('Yuke Zhu')]","Language model (LM) pre-training is useful in many language processing tasks.
But can pre-trained LMs be further leveraged for more general machine learning
problems? We propose an approach for using LMs to scaffold learning and
generalization in general sequential decision-making problems. In this
approach, goals and observations are represented as a sequence of embeddings,
and a policy network initialized with a pre-trained LM predicts the next
action. We demonstrate that this framework enables effective combinatorial
generalization across different environments and supervisory modalities. We
begin by assuming access to a set of expert demonstrations, and show that
initializing policies with LMs and fine-tuning them via behavior cloning
improves task completion rates by 43.6% in the VirtualHome environment. Next,
we integrate an active data gathering procedure in which agents iteratively
interact with the environment, relabel past ""failed"" experiences with new
goals, and update their policies in a self-supervised loop. Active data
gathering further improves combinatorial generalization, outperforming the best
baseline by 25.1%. Finally, we explain these results by investigating three
possible factors underlying the effectiveness of the LM-based policy. We find
that sequential input representations (vs. fixed-dimensional feature vectors)
and LM-based weight initialization are both important for generalization.
Surprisingly, however, the format of the policy inputs encoding (e.g. as a
natural language string vs. an arbitrary sequential encoding) has little
influence. Together, these results suggest that language modeling induces
representations that are useful for modeling not just language, but also goals
and plans; these representations can aid learning and generalization even
outside of language processing."
1519,"As a starting place for further research in this
direction, we suggest a concrete design for an explainability dialogue system.","Based on these principles, we suggest natural-language explainability dialogues as a
promising solution to enabling interactive explanations.","3.1 Principles of Interactive Explanations

Given the need for explanations to enable better interactions with models through custom queries,
additional follow-up questions, and proper contextualization, there are exciting research opportunities
for developing interactive explanations for machine learning models.",2022-02-03 22:17:21+00:00,Rethinking Explainability as a Dialogue: A Practitioner's Perspective,cs.LG,['cs.LG'],"[arxiv.Result.Author('Himabindu Lakkaraju'), arxiv.Result.Author('Dylan Slack'), arxiv.Result.Author('Yuxin Chen'), arxiv.Result.Author('Chenhao Tan'), arxiv.Result.Author('Sameer Singh')]","As practitioners increasingly deploy machine learning models in critical
domains such as health care, finance, and policy, it becomes vital to ensure
that domain experts function effectively alongside these models. Explainability
is one way to bridge the gap between human decision-makers and machine learning
models. However, most of the existing work on explainability focuses on
one-off, static explanations like feature importances or rule lists. These
sorts of explanations may not be sufficient for many use cases that require
dynamic, continuous discovery from stakeholders. In the literature, few works
ask decision-makers about the utility of existing explanations and other
desiderata they would like to see in an explanation going forward. In this
work, we address this gap and carry out a study where we interview doctors,
healthcare professionals, and policymakers about their needs and desires for
explanations. Our study indicates that decision-makers would strongly prefer
interactive explanations in the form of natural language dialogues. Domain
experts wish to treat machine learning models as ""another colleague"", i.e., one
who can be held accountable by asking why they made a particular decision
through expressive and accessible natural language interactions. Considering
these needs, we outline a set of five principles researchers should follow when
designing interactive explanations as a starting place for future work.
Further, we show why natural language dialogues satisfy these principles and
are a desirable way to build interactive explanations. Next, we provide a
design of a dialogue system for explainability and discuss the risks,
trade-offs, and research opportunities of building these systems. Overall, we
hope our work serves as a starting place for researchers and engineers to
design interactive explainability systems."
1543,"One
direction for further study is further investigation of criteria for source split-
ting.","Both methods can also serve as diagnostics and
provide quantitative and qualitative illustrations of dataset heterogeneity.","For example, the choice of data representations and number of source
subsets would beneﬁt from further analysis.",2022-02-04 14:37:31+00:00,Source data selection for out-of-domain generalization,cs.LG,"['cs.LG', 'stat.AP']","[arxiv.Result.Author('Xinran Miao'), arxiv.Result.Author('Kris Sankaran')]","Models that perform out-of-domain generalization borrow knowledge from
heterogeneous source data and apply it to a related but distinct target task.
Transfer learning has proven effective for accomplishing this generalization in
many applications. However, poor selection of a source dataset can lead to poor
performance on the target, a phenomenon called negative transfer. In order to
take full advantage of available source data, this work studies source data
selection with respect to a target task. We propose two source selection
methods that are based on the multi-bandit theory and random search,
respectively. We conduct a thorough empirical evaluation on both simulated and
real data. Our proposals can be also viewed as diagnostics for the existence of
a reweighted source subsamples that perform better than the random selection of
available samples."
1562,"However, our inference is exploratory in nature and
                                                                        thus further research should be conducted to understand advanced
                                                                        feature interaction removal methods may help improve the agree-
                                                                        ment across different CS methods.","Therefore, we suggest
                                                                        that researchers and practitioners should be cautious when using
                                                                        different CS methods interchangeably even after removing feature
                                                                        interactions.","Result 11) After removing the feature interactions, SHAP and                                                                                                                    12
Permutation yield a strong agreement on all datasets.",2022-02-04 21:00:59+00:00,The impact of feature importance methods on the interpretation of defect classifiers,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Gopi Krishnan Rajbahadur'), arxiv.Result.Author('Shaowei Wang'), arxiv.Result.Author('Yasutaka Kamei'), arxiv.Result.Author('Ahmed E. Hassan')]","Classifier specific (CS) and classifier agnostic (CA) feature importance
methods are widely used (often interchangeably) by prior studies to derive
feature importance ranks from a defect classifier. However, different feature
importance methods are likely to compute different feature importance ranks
even for the same dataset and classifier. Hence such interchangeable use of
feature importance methods can lead to conclusion instabilities unless there is
a strong agreement among different methods. Therefore, in this paper, we
evaluate the agreement between the feature importance ranks associated with the
studied classifiers through a case study of 18 software projects and six
commonly used classifiers. We find that: 1) The computed feature importance
ranks by CA and CS methods do not always strongly agree with each other. 2) The
computed feature importance ranks by the studied CA methods exhibit a strong
agreement including the features reported at top-1 and top-3 ranks for a given
dataset and classifier, while even the commonly used CS methods yield vastly
different feature importance ranks. Such findings raise concerns about the
stability of conclusions across replicated studies. We further observe that the
commonly used defect datasets are rife with feature interactions and these
feature interactions impact the computed feature importance ranks of the CS
methods (not the CA methods). We demonstrate that removing these feature
interactions, even with simple methods like CFS improves agreement between the
computed feature importance ranks of CA and CS methods. In light of our
findings, we provide guidelines for stakeholders and practitioners when
performing model interpretation and directions for future research, e.g.,
future research is needed to investigate the impact of advanced feature
interaction removal methods on computed feature importance ranks of different
CS methods."
1592,"However, how to optimize the threshold is
remarks and recommendations for further research.",The last section concludes with general         information loss.,still under debate.,2022-02-06 07:09:57+00:00,Robust Anomaly Detection for Time-series Data,cs.LG,['cs.LG'],"[arxiv.Result.Author('Min Hu'), arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Xiaowei Feng'), arxiv.Result.Author('Shengchen Zhou'), arxiv.Result.Author('Zhaoyu Wu'), arxiv.Result.Author('Yuan Qin')]","Time-series anomaly detection plays a vital role in monitoring complex
operation conditions. However, the detection accuracy of existing approaches is
heavily influenced by pattern distribution, existence of multiple normal
patterns, dynamical features representation, and parameter settings. For the
purpose of improving the robustness and guaranteeing the accuracy, this
research combined the strengths of negative selection, unthresholded recurrence
plots, and an extreme learning machine autoencoder and then proposed robust
anomaly detection for time-series data (RADTD), which can automatically learn
dynamical features in time series and recognize anomalies with low label
dependency and high robustness. Yahoo benchmark datasets and three tunneling
engineering simulation experiments were used to evaluate the performance of
RADTD. The experiments showed that in benchmark datasets RADTD possessed higher
accuracy and robustness than recurrence qualification analysis and extreme
learning machine autoencoder, respectively, and that RADTD accurately detected
the occurrence of tunneling settlement accidents, indicating its remarkable
performance in accuracy and robustness."
1605,"Finally, we high-
                                       light some open-ended issues of IoT in healthcare that leaves further research studies
                                       and investigation for scientists.","In this study, we present an extensive
                                       review of the state-of-the-art machine learning applications particularly in healthcare,
                                       challenging issues in IoT, and corresponding promising solutions.","Keywords: IoT, IoT Pipeline, Centralized and decentralized Big Data analysis, Online
                                       learning, Federated Learning.",2022-02-06 21:56:39+00:00,Applications of Machine Learning in Healthcare and Internet of Things (IOT): A Comprehensive Review,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Farid Ghareh Mohammadi'), arxiv.Result.Author('Farzan Shenavarmasouleh'), arxiv.Result.Author('Hamid R. Arabnia')]","In recent years, smart healthcare IoT devices have become ubiquitous, but
they work in isolated networks due to their policy. Having these devices
connected in a network enables us to perform medical distributed data analysis.
However, the presence of diverse IoT devices in terms of technology, structure,
and network policy, makes it a challenging issue while applying traditional
centralized learning algorithms on decentralized data collected from the IoT
devices. In this study, we present an extensive review of the state-of-the-art
machine learning applications particularly in healthcare, challenging issues in
IoT, and corresponding promising solutions. Finally, we highlight some
open-ended issues of IoT in healthcare that leaves further research studies and
investigation for scientists."
1624,"4) To facilitate further research and reproducibility, we publicly share the source code of our
       framework on github.com/selimﬁrat/oac-based-private-ensembles.","3) We systematically compare and discuss privacy of the introduced ensemble methods with
       OAC and show that our framework performs signiﬁcantly better than their orthogonal
       counterparts while using less resources.",II.,2022-02-07 13:16:11+00:00,Over-the-Air Ensemble Inference with Model Privacy,cs.LG,"['cs.LG', 'cs.CR', 'eess.SP']","[arxiv.Result.Author('Selim F. Yilmaz'), arxiv.Result.Author('Burak Hasircioglu'), arxiv.Result.Author('Deniz Gunduz')]","We consider distributed inference at the wireless edge, where multiple
clients with an ensemble of models, each trained independently on a local
dataset, are queried in parallel to make an accurate decision on a new sample.
In addition to maximizing inference accuracy, we also want to maximize the
privacy of local models. We exploit the superposition property of the air to
implement bandwidth-efficient ensemble inference methods. We introduce
different over-the-air ensemble methods and show that these schemes perform
significantly better than their orthogonal counterparts, while using less
resources and providing privacy guarantees. We also provide experimental
results verifying the benefits of the proposed over-the-air inference approach,
whose source code is shared publicly on Github."
1625,"4) To facilitate further research and reproducibility, we publicly share the source code of our
                                                                        4

        framework on github.com/selimﬁrat/oac-based-private-ensembles.","3) We systematically compare and discuss privacy of the introduced ensemble methods, and
       show that the proposed framework with OAC performs signiﬁcantly better than orthogonal
       counterparts while using less resources.",II.,2022-02-07 13:16:11+00:00,Over-the-Air Ensemble Inference with Model Privacy,cs.LG,"['cs.LG', 'cs.CR', 'eess.SP']","[arxiv.Result.Author('Selim F. Yilmaz'), arxiv.Result.Author('Burak Hasircioglu'), arxiv.Result.Author('Deniz Gunduz')]","We consider distributed inference at the wireless edge, where multiple
clients with an ensemble of models, each trained independently on a local
dataset, are queried in parallel to make an accurate decision on a new sample.
In addition to maximizing inference accuracy, we also want to maximize the
privacy of local models. We exploit the superposition property of the air to
implement bandwidth-efficient ensemble inference methods. We introduce
different over-the-air ensemble methods and show that these schemes perform
significantly better than their orthogonal counterparts, while using less
resources and providing privacy guarantees. We also provide experimental
results verifying the benefits of the proposed over-the-air inference approach,
whose source code is shared publicly on Github."
1646,"• Finally, in Sec√tion 5, we√further study SSP with adversarial costs and design PO algorithms that
  achieve O˜(T⋆ DK + DT⋆S2AK) regret with full information and O˜( Tm5 axS2AK) regret
  with bandit feedback, where Tmax is the maximum expected√hitting time of the opti√mal policy over
  all states.","These
bounds match th√e best existing results from (Chen and Luo, 2021) (and exhibit a S gap in the
second term DS AK compared to their lower bounds).","The best existing bounds for these settings are O˜( DT⋆S2AK) and O˜( DT⋆S3A2K)

  respectively (Chen and Luo, 2021).",2022-02-07 16:25:14+00:00,Policy Optimization for Stochastic Shortest Path,cs.LG,['cs.LG'],"[arxiv.Result.Author('Liyu Chen'), arxiv.Result.Author('Haipeng Luo'), arxiv.Result.Author('Aviv Rosenberg')]","Policy optimization is among the most popular and successful reinforcement
learning algorithms, and there is increasing interest in understanding its
theoretical guarantees. In this work, we initiate the study of policy
optimization for the stochastic shortest path (SSP) problem, a goal-oriented
reinforcement learning model that strictly generalizes the finite-horizon model
and better captures many applications. We consider a wide range of settings,
including stochastic and adversarial environments under full information or
bandit feedback, and propose a policy optimization algorithm for each setting
that makes use of novel correction terms and/or variants of dilated bonuses
(Luo et al., 2021). For most settings, our algorithm is shown to achieve a
near-optimal regret bound.
  One key technical contribution of this work is a new approximation scheme to
tackle SSP problems that we call \textit{stacked discounted approximation} and
use in all our proposed algorithms. Unlike the finite-horizon approximation
that is heavily used in recent SSP algorithms, our new approximation enables us
to learn a near-stationary policy with only logarithmic changes during an
episode and could lead to an exponential improvement in space complexity."
1647,"4.2.3 Order Ideals: Exploiting Structure in OAVI’s Output

To derive generalization bounds in Section 6, we further study the structure of OAVI’s output for 𝑋 ⊆ ℝ𝑛 and
1 > 𝜓 ≥ 𝜖 ≥ 0, that is, (G, O) = OAVI(𝑋, 𝜓, 𝜖), which, by Deﬁnition 4.2, forms an order ideal.","Thus, the computational complexity of OAVI beneﬁts from constructing fewer terms in O and generators
in G, increasing the sparsity of generators in G, and improving the computational complexity of ORACLE.",Deﬁnition 4.9 (Order ideal).,2022-02-07 16:48:49+00:00,Conditional Gradients for the Approximatel Vanishing Ideal,cs.LG,['cs.LG'],"[arxiv.Result.Author('Elias Wirth'), arxiv.Result.Author('Sebastian Pokutta')]","The vanishing ideal of a set of points $X\subseteq \mathbb{R}^n$ is the set
of polynomials that evaluate to $0$ over all points $\mathbf{x} \in X$ and
admits an efficient representation by a finite set of polynomials called
generators. To accommodate the noise in the data set, we introduce the pairwise
conditional gradients approximate vanishing ideal algorithm (PCGAVI) that
constructs a set of generators of the approximate vanishing ideal. The
constructed generators capture polynomial structures in data and give rise to a
feature map that can, for example, be used in combination with a linear
classifier for supervised learning. In PCGAVI, we construct the set of
generators by solving constrained convex optimization problems with the
pairwise conditional gradients algorithm. Thus, PCGAVI not only constructs few
but also sparse generators, making the corresponding feature transformation
robust and compact. Furthermore, we derive several learning guarantees for
PCGAVI that make the algorithm theoretically better motivated than related
generator-constructing methods."
1652,"A further study on the effect of the number of subsampling points and number of geometries
in the training dataset on the SDF generation accuracy and training time was outside the scope of this work.","However, convergence
was seen to occur at approximately 1500 epochs for the implicit approach, while the explicit approach took
approximately 7000 epochs.","Nevertheless,
it is expected that significant training speeds ups could be achieved by using fewer subsampling points and geometries
but may compromise performance accuracy.",2022-02-04 22:29:12+00:00,Development of a deep learning platform for optimising sheet stamping geometries subject to manufacturing constraints,cs.LG,['cs.LG'],"[arxiv.Result.Author('Hamid Reza Attar'), arxiv.Result.Author('Alistair Foster'), arxiv.Result.Author('Nan Li')]","The latest sheet stamping processes enable efficient manufacturing of complex
shape structural components that have high stiffness to weight ratios, but
these processes can introduce defects. To assist component design for stamping
processes, this paper presents a novel deep-learning-based platform for
optimising 3D component geometries. The platform adopts a non-parametric
modelling approach that is capable of optimising arbitrary geometries from
multiple geometric parameterisation schema. This approach features the
interaction of two neural networks: 1) a geometry generator and 2) a
manufacturing performance evaluator. The generator predicts continuous 3D
signed distance fields (SDFs) for geometries of different classes, and each SDF
is conditioned on a latent vector. The zero-level-set of each SDF implicitly
represents a generated geometry. Novel training strategies for the generator
are introduced and include a new loss function which is tailored for sheet
stamping applications. These strategies enable the differentiable generation of
high quality, large scale component geometries with tight local features for
the first time. The evaluator maps a 2D projection of these generated
geometries to their post-stamping physical (e.g., strain) distributions.
Manufacturing constraints are imposed based on these distributions and are used
to formulate a novel objective function for optimisation. A new gradient-based
optimisation technique is employed to iteratively update the latent vectors,
and therefore geometries, to minimise this objective function and thus meet the
manufacturing constraints. Case studies based on optimising box geometries
subject to a sheet thinning constraint for a hot stamping process are presented
and discussed. The results show that expressive geometric changes are
achievable, and that these changes are driven by stamping performance."
1657,"First, the model consolidates multiple static               conflicting and further research is required[11].","from the RCTs generally demonstrate that antihypertensives
   Our work has contributions to the field of EHR based deep                   have null effect on cancer, the evidence regarding CCBs is still
learning research.","Lastly, in our
and temporal data embeddings into a unified embedding                          modelling, we cannot entirely rule out residual confounding.",2022-02-07 20:05:05+00:00,Targeted-BEHRT: Deep learning for observational causal inference on longitudinal electronic health records,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shishir Rao'), arxiv.Result.Author('Mohammad Mamouei'), arxiv.Result.Author('Gholamreza Salimi-Khorshidi'), arxiv.Result.Author('Yikuan Li'), arxiv.Result.Author('Rema Ramakrishnan'), arxiv.Result.Author('Abdelaali Hassaine'), arxiv.Result.Author('Dexter Canoy'), arxiv.Result.Author('Kazem Rahimi')]","Observational causal inference is useful for decision making in medicine when
randomized clinical trials (RCT) are infeasible or non generalizable. However,
traditional approaches fail to deliver unconfounded causal conclusions in
practice. The rise of ""doubly robust"" non-parametric tools coupled with the
growth of deep learning for capturing rich representations of multimodal data,
offers a unique opportunity to develop and test such models for causal
inference on comprehensive electronic health records (EHR). In this paper, we
investigate causal modelling of an RCT-established null causal association: the
effect of antihypertensive use on incident cancer risk. We develop a dataset
for our observational study and a Transformer-based model, Targeted BEHRT
coupled with doubly robust estimation, we estimate average risk ratio (RR). We
compare our model to benchmark statistical and deep learning models for causal
inference in multiple experiments on semi-synthetic derivations of our dataset
with various types and intensities of confounding. In order to further test the
reliability of our approach, we test our model on situations of limited data.
We find that our model provides more accurate estimates of RR (least sum
absolute error from ground truth) compared to benchmarks for risk ratio
estimation on high-dimensional EHR across experiments. Finally, we apply our
model to investigate the original case study: antihypertensives' effect on
cancer and demonstrate that our model generally captures the validated null
association."
1661,"Our theoretical foundation motivates further study,
                                       implementation, and optimization of the new algorithmic framework and further investigation of its
                                       non-standard bounded style assumptions.","By using bounded style assumptions, we prove convergence to an ε-(global)
                                       minimum using O˜(1/ε3) gradient computations.","This new direction broadens our understanding of why
                                       and under what circumstances training of a DNN converges to a global minimum.",2022-02-07 21:23:16+00:00,Finite-Sum Optimization: A New Perspective for Convergence to a Global Solution,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Lam M. Nguyen'), arxiv.Result.Author('Trang H. Tran'), arxiv.Result.Author('Marten van Dijk')]","Deep neural networks (DNNs) have shown great success in many machine learning
tasks. Their training is challenging since the loss surface of the network
architecture is generally non-convex, or even non-smooth. How and under what
assumptions is guaranteed convergence to a \textit{global} minimum possible? We
propose a reformulation of the minimization problem allowing for a new
recursive algorithmic framework. By using bounded style assumptions, we prove
convergence to an $\varepsilon$-(global) minimum using
$\mathcal{\tilde{O}}(1/\varepsilon^3)$ gradient computations. Our theoretical
foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded
style assumptions. This new direction broadens our understanding of why and
under what circumstances training of a DNN converges to a global minimum."
1662,"Our theoretical foundation motivates further study, implementa-
tion, and optimization of the new algorithmic framework and further investigation of its non-standard bounded style
assumptions.","We emphasize that our focus is on developing a new theoretical foundation and that a translation to a practical imple-
mentation with empirical results is for future work.","This new direction broadens our understanding of why and under what circumstances training of a DNN
converges to a global minimum.",2022-02-07 21:23:16+00:00,Finite-Sum Optimization: A New Perspective for Convergence to a Global Solution,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Lam M. Nguyen'), arxiv.Result.Author('Trang H. Tran'), arxiv.Result.Author('Marten van Dijk')]","Deep neural networks (DNNs) have shown great success in many machine learning
tasks. Their training is challenging since the loss surface of the network
architecture is generally non-convex, or even non-smooth. How and under what
assumptions is guaranteed convergence to a \textit{global} minimum possible? We
propose a reformulation of the minimization problem allowing for a new
recursive algorithmic framework. By using bounded style assumptions, we prove
convergence to an $\varepsilon$-(global) minimum using
$\mathcal{\tilde{O}}(1/\varepsilon^3)$ gradient computations. Our theoretical
foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded
style assumptions. This new direction broadens our understanding of why and
under what circumstances training of a DNN converges to a global minimum."
1663,"Our theoretical foundation motivates further study, implementation, and optimization of the new algorithmic frame-
work and further investigation of its non-standard bounded style assumptions.",conjugate gradient descent).,"Possible research directions include
more practical algorithm designs based on our Framework 1, and different related methods to solve the regularized
problem and approximate the solution such as Stochastic Gradient Descent and its stochastic ﬁrst-order variants (e.g.",2022-02-07 21:23:16+00:00,Finite-Sum Optimization: A New Perspective for Convergence to a Global Solution,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Lam M. Nguyen'), arxiv.Result.Author('Trang H. Tran'), arxiv.Result.Author('Marten van Dijk')]","Deep neural networks (DNNs) have shown great success in many machine learning
tasks. Their training is challenging since the loss surface of the network
architecture is generally non-convex, or even non-smooth. How and under what
assumptions is guaranteed convergence to a \textit{global} minimum possible? We
propose a reformulation of the minimization problem allowing for a new
recursive algorithmic framework. By using bounded style assumptions, we prove
convergence to an $\varepsilon$-(global) minimum using
$\mathcal{\tilde{O}}(1/\varepsilon^3)$ gradient computations. Our theoretical
foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded
style assumptions. This new direction broadens our understanding of why and
under what circumstances training of a DNN converges to a global minimum."
1689,"classifier and a prototype generator, and how it can
                                             fill in missing parts of an input sequence, making           • We develop and experiment with multiple factor graph
                                             them a promising field for further research.","By varying the computation order, we                by an “unrolling lemma” (Lemma 1), can reuse current build-
                                             show how a single UNN can be used both as a                    ing blocks from feed-forward networks in a modular way.","architectures, tackling both structured and unstructured tasks,
                                                                                                            such as natural language parsing, image classification, and
                                       Factor graphs have historically been a very appealing toolbox        image prototype generation.",2022-02-08 10:06:51+00:00,Modeling Structure with Undirected Neural Networks,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Tsvetomila Mihaylova'), arxiv.Result.Author('Vlad Niculae'), arxiv.Result.Author('André F. T. Martins')]","Neural networks are powerful function estimators, leading to their status as
a paradigm of choice for modeling structured data. However, unlike other
structured representations that emphasize the modularity of the problem --
e.g., factor graphs -- neural networks are usually monolithic mappings from
inputs to outputs, with a fixed computation order. This limitation prevents
them from capturing different directions of computation and interaction between
the modeled variables.
  In this paper, we combine the representational strengths of factor graphs and
of neural networks, proposing undirected neural networks (UNNs): a flexible
framework for specifying computations that can be performed in any order. For
particular choices, our proposed models subsume and extend many existing
architectures: feed-forward, recurrent, self-attention networks, auto-encoders,
and networks with implicit layers. We demonstrate the effectiveness of
undirected neural architectures, both unstructured and structured, on a range
of tasks: tree-constrained dependency parsing, convolutional image
classification, and sequence completion with attention. By varying the
computation order, we show how a single UNN can be used both as a classifier
and a prototype generator, and how it can fill in missing parts of an input
sequence, making them a promising field for further research."
1690,for further research.,"In sum, our contributions are:
                                              can be used both as a classiﬁer and a prototype
                                              generator, and how it can ﬁll in missing parts of          • We present UNNs and show how they extend many exist-
                                              an input sequence, making them a promising ﬁeld              ing neural architectures.","• We provide a coordinate descent inference algorithm,
                                        1.",2022-02-08 10:06:51+00:00,Modeling Structure with Undirected Neural Networks,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Tsvetomila Mihaylova'), arxiv.Result.Author('Vlad Niculae'), arxiv.Result.Author('André F. T. Martins')]","Neural networks are powerful function estimators, leading to their status as
a paradigm of choice for modeling structured data. However, unlike other
structured representations that emphasize the modularity of the problem --
e.g., factor graphs -- neural networks are usually monolithic mappings from
inputs to outputs, with a fixed computation order. This limitation prevents
them from capturing different directions of computation and interaction between
the modeled variables.
  In this paper, we combine the representational strengths of factor graphs and
of neural networks, proposing undirected neural networks (UNNs): a flexible
framework for specifying computations that can be performed in any order. For
particular choices, our proposed models subsume and extend many existing
architectures: feed-forward, recurrent, self-attention networks, auto-encoders,
and networks with implicit layers. We demonstrate the effectiveness of
undirected neural architectures, both unstructured and structured, on a range
of tasks: tree-constrained dependency parsing, convolutional image
classification, and sequence completion with attention. By varying the
computation order, we show how a single UNN can be used both as a classifier
and a prototype generator, and how it can fill in missing parts of an input
sequence, making them a promising field for further research."
1703,"It is, therefore, relevant to conduct further research into discriminatory formulations of
vacancies in job advertisements.","If a preference for a male employee is expressed in the vacancy text, a male candidate will often ultimately be
selected for the job [8, 25].",Scientific studies into discriminatory formulations of job vacancies are relatively scarce.,2022-02-02 09:25:08+00:00,Context-Aware Discrimination Detection in Job Vacancies using Computational Language Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('S. Vethman'), arxiv.Result.Author('A. Adhikari'), arxiv.Result.Author('M. H. T. de Boer'), arxiv.Result.Author('J. A. G. M. van Genabeek'), arxiv.Result.Author('C. J. Veenman')]","Discriminatory job vacancies are disapproved worldwide, but remain
persistent. Discrimination in job vacancies can be explicit by directly
referring to demographic memberships of candidates. More implicit forms of
discrimination are also present that may not always be illegal but still
influence the diversity of applicants. Explicit written discrimination is still
present in numerous job vacancies, as was recently observed in the Netherlands.
Current efforts for the detection of explicit discrimination concern the
identification of job vacancies containing potentially discriminating terms
such as ""young"" or ""male"". However, automatic detection is inefficient due to
low precision: e.g. ""we are a young company"" or ""working with mostly male
patients"" are phrases that contain explicit terms, while the context shows that
these do not reflect discriminatory content.
  In this paper, we show how machine learning based computational language
models can raise precision in the detection of explicit discrimination by
identifying when the potentially discriminating terms are used in a
discriminatory context. We focus on gender discrimination, which indeed suffers
from low precision when filtering explicit terms. First, we created a data set
for gender discrimination in job vacancies. Second, we investigated a variety
of computational language models for discriminatory context detection. Third,
we evaluated the capability of these models to detect unforeseen discriminating
terms in context. The results show that machine learning based methods can
detect explicit gender discrimination with high precision and help in finding
new forms of discrimination. Accordingly, the proposed methods can
substantially increase the effectiveness of detecting job vacancies which are
highly suspected to be discriminatory. In turn, this may lower the
discrimination experienced at the start of the recruitment process."
1710,"12
Table 3: Top 5 Neurons identiﬁed by SinkDivLM on Sleep Stage dataset

Normalized Feature value  Neuron 6   Neuron 15  Neuron 13  Neuron 14  Neuron 16
                             1.00      0.8058     0.5405     0.4225      0.226

Table 4: Top 5 Neurons identiﬁed by sHSIC on Sleep Stage dataset

Normalized Feature value  Neuron 15  Neuron 28  Neuron 3   Neuron 13  Neuron 6
                              1.00      0.464     0.452       0.436     0.384

5.3 Type1 error versus projection dimension

We further study the results in 8 that relate how projection dimension of L aﬀects type 1 error.",These neurons are visualized in Figures 5(b) and 5(c).,"For this,
we conducted two samples tests between samples from two 100 dimensional Gaussian mixture models;
α “ N p0, Iq ` N p1, Σ0q and samples from β “ N p0, Iq ` N p1.5, Σ1q.",2022-02-08 17:11:40+00:00,Learning Sinkhorn divergences for supervised change point detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nauman Ahad'), arxiv.Result.Author('Eva L. Dyer'), arxiv.Result.Author('Keith B. Hengen'), arxiv.Result.Author('Yao Xie'), arxiv.Result.Author('Mark A. Davenport')]","Many modern applications require detecting change points in complex
sequential data. Most existing methods for change point detection are
unsupervised and, as a consequence, lack any information regarding what kind of
changes we want to detect or if some kinds of changes are safe to ignore. This
often results in poor change detection performance. We present a novel change
point detection framework that uses true change point instances as supervision
for learning a ground metric such that Sinkhorn divergences can be then used in
two-sample tests on sliding windows to detect change points in an online
manner. Our method can be used to learn a sparse metric which can be useful for
both feature selection and interpretation in high-dimensional change point
detection settings. Experiments on simulated as well as real world sequences
show that our proposed method can substantially improve change point detection
performance over existing unsupervised change point detection methods using
only few labeled change point instances."
1711,"Table 3: Top 5 Neurons identiﬁed by SinkDivLM on Sleep Stage dataset

Normalized Feature value  Neuron 6   Neuron 15         Neuron 13  Neuron 14  Neuron 16
                             1.00      0.8058            0.5405     0.4225      0.226

          Table 4: Top 5 Neurons identiﬁed by sHSIC on Sleep Stage dataset

Normalized Feature value  Neuron 15  Neuron 28         Neuron 3   Neuron 13  Neuron 6
                              1.00      0.464            0.452       0.436     0.384

5.3 Type1 error versus projection dimension

We further study the results in 8 that relate how projection dimension of L aﬀects type 1 error.",These neurons are visualized in Figures 5(b) and 5(c).,"For this,
we conducted two samples tests between samples from two 100 dimensional Gaussian mixture models;
α “ N p0, Iq ` N p1, Σ0q and samples from β “ N p0, Iq ` N p1.5, Σ1q.",2022-02-08 17:11:40+00:00,Learning Sinkhorn divergences for supervised change point detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nauman Ahad'), arxiv.Result.Author('Eva L. Dyer'), arxiv.Result.Author('Keith B. Hengen'), arxiv.Result.Author('Yao Xie'), arxiv.Result.Author('Mark A. Davenport')]","Many modern applications require detecting change points in complex
sequential data. Most existing methods for change point detection are
unsupervised and, as a consequence, lack any information regarding what kind of
changes we want to detect or if some kinds of changes are safe to ignore. This
often results in poor change detection performance. We present a novel change
point detection framework that uses true change point instances as supervision
for learning a ground metric such that Sinkhorn divergences can be then used in
two-sample tests on sliding windows to detect change points in an online
manner. Our method can be used to learn a sparse metric which can be useful for
both feature selection and interpretation in high-dimensional change point
detection settings. Experiments on simulated as well as real world sequences
show that our proposed method can substantially improve change point detection
performance over existing unsupervised change point detection methods using
only few labeled change point instances."
1712,"Table 3: Top 5 Neurons identiﬁed by SinkDivLM on Sleep Stage dataset

Normalized Feature value  Neuron 6   Neuron 15         Neuron 13  Neuron 14  Neuron 16
                             1.00      0.8058            0.5405     0.4225      0.226

          Table 4: Top 5 Neurons identiﬁed by sHSIC on Sleep Stage dataset

Normalized Feature value  Neuron 15  Neuron 28         Neuron 3   Neuron 13  Neuron 6
                              1.00      0.464            0.452       0.436     0.384

5.3 Type1 error versus projection dimension

We further study the results in 8 that relate how projection dimension of L aﬀects type 1 error.",This held true even when the projection dimension was 1000.,"For this,
we conducted two samples tests between samples from two 100 dimensional Gaussian mixture models;
α “ N p0, Iq ` N p1, Σ0q and samples from β “ N p0, Iq ` N p1.5, Σ1q.",2022-02-08 17:11:40+00:00,Learning Sinkhorn divergences for supervised change point detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nauman Ahad'), arxiv.Result.Author('Eva L. Dyer'), arxiv.Result.Author('Keith B. Hengen'), arxiv.Result.Author('Yao Xie'), arxiv.Result.Author('Mark A. Davenport')]","Many modern applications require detecting change points in complex
sequential data. Most existing methods for change point detection are
unsupervised and, as a consequence, lack any information regarding what kind of
changes we want to detect or if some kinds of changes are safe to ignore. This
often results in poor change detection performance. We present a novel change
point detection framework that uses true change point instances as supervision
for learning a ground metric such that Sinkhorn divergences can be then used in
two-sample tests on sliding windows to detect change points in an online
manner. Our method can be used to learn a sparse metric which can be useful for
both feature selection and interpretation in high-dimensional change point
detection settings. Experiments on simulated as well as real world sequences
show that our proposed method can substantially improve change point detection
performance over existing unsupervised change point detection methods using
only few labeled change point instances."
1721,"This indicates an area where
further research investment is needed.","The former is particularly problematic when learning PDEs, as the
solution is expected to feature some degree of smoothness.","In data constrained modeling, knowledge of the underlying physics may be used to restrict
the solution to within a given space.",2022-02-04 22:56:31+00:00,Machine Learning in Heterogeneous Porous Materials,cs.LG,"['cs.LG', 'cond-mat.mtrl-sci']","[arxiv.Result.Author(""Marta D'Elia""), arxiv.Result.Author('Hang Deng'), arxiv.Result.Author('Cedric Fraces'), arxiv.Result.Author('Krishna Garikipati'), arxiv.Result.Author('Lori Graham-Brady'), arxiv.Result.Author('Amanda Howard'), arxiv.Result.Author('George Karniadakis'), arxiv.Result.Author('Vahid Keshavarzzadeh'), arxiv.Result.Author('Robert M. Kirby'), arxiv.Result.Author('Nathan Kutz'), arxiv.Result.Author('Chunhui Li'), arxiv.Result.Author('Xing Liu'), arxiv.Result.Author('Hannah Lu'), arxiv.Result.Author('Pania Newell'), arxiv.Result.Author(""Daniel O'Malley""), arxiv.Result.Author('Masa Prodanovic'), arxiv.Result.Author('Gowri Srinivasan'), arxiv.Result.Author('Alexandre Tartakovsky'), arxiv.Result.Author('Daniel M. Tartakovsky'), arxiv.Result.Author('Hamdi Tchelepi'), arxiv.Result.Author('Bozo Vazic'), arxiv.Result.Author('Hari Viswanathan'), arxiv.Result.Author('Hongkyu Yoon'), arxiv.Result.Author('Piotr Zarzycki')]","The ""Workshop on Machine learning in heterogeneous porous materials"" brought
together international scientific communities of applied mathematics, porous
media, and material sciences with experts in the areas of heterogeneous
materials, machine learning (ML) and applied mathematics to identify how ML can
advance materials research. Within the scope of ML and materials research, the
goal of the workshop was to discuss the state-of-the-art in each community,
promote crosstalk and accelerate multi-disciplinary collaborative research, and
identify challenges and opportunities. As the end result, four topic areas were
identified: ML in predicting materials properties, and discovery and design of
novel materials, ML in porous and fractured media and time-dependent phenomena,
Multi-scale modeling in heterogeneous porous materials via ML, and Discovery of
materials constitutive laws and new governing equations. This workshop was part
of the AmeriMech Symposium series sponsored by the National Academies of
Sciences, Engineering and Medicine and the U.S. National Committee on
Theoretical and Applied Mechanics."
1735,"Interestingly, in
                                                                 these cases, we also often observe convergence of B
                  + (CAB)2 + (CB − w∗)2                          and C to a symmetric solution (see Section 7.1), and
                                                                 it will be interesting to further study the mechanisms
Any solution minimizing (i.e., bringing to zero) the             that underlie this.","In practice however, we
L(A, B, C) = (CAiB)2 + (CA2B)2 (11)                              empirically observe convergence to an extrapolating
                                                                 solution when using standard (non-symmetric) initial-
                          i=3                                    ization schemes (see Experiments).","above must satisfy:

                    CA2B = 0.",2022-02-09 06:28:37+00:00,On the Implicit Bias of Gradient Descent for Temporal Extrapolation,cs.LG,['cs.LG'],"[arxiv.Result.Author('Edo Cohen-Karlik'), arxiv.Result.Author('Avichai Ben David'), arxiv.Result.Author('Nadav Cohen'), arxiv.Result.Author('Amir Globerson')]","Common practice when using recurrent neural networks (RNNs) is to apply a
model to sequences longer than those seen in training. This ""extrapolating""
usage deviates from the traditional statistical learning setup where guarantees
are provided under the assumption that train and test distributions are
identical.
  Here we set out to understand when RNNs can extrapolate, focusing on a simple
case where the data generating distribution is memoryless. We first show that
even with infinite training data, there exist RNN models that interpolate
perfectly (i.e., they fit the training data) yet extrapolate poorly to longer
sequences. We then show that if gradient descent is used for training, learning
will converge to perfect extrapolation under certain assumption on
initialization. Our results complement recent studies on the implicit bias of
gradient descent, showing that it plays a key role in extrapolation when
learning temporal prediction models."
1736,"Therefore, it         well as designing efﬁcient layers or interactive mech-
      is signiﬁcant to further study on evaluating the importance        anisms to compensate damages caused by the splitting
      of different neurons or channels, and then achieve the             connections.","In this way, the computation and             methods while maintaining these speciﬁc structures, as
      transmission cost will be reduced obviously.",model pruning design with a performance guarantee.,2022-02-09 06:56:41+00:00,"Vertical Federated Learning: Challenges, Methodologies and Experiments",cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Kang Wei'), arxiv.Result.Author('Jun Li'), arxiv.Result.Author('Chuan Ma'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Sha Wei'), arxiv.Result.Author('Fan Wu'), arxiv.Result.Author('Guihai Chen'), arxiv.Result.Author('Thilina Ranbaduge')]","Recently, federated learning (FL) has emerged as a promising distributed
machine learning (ML) technology, owing to the advancing computational and
sensing capacities of end-user devices, however with the increasing concerns on
users' privacy. As a special architecture in FL, vertical FL (VFL) is capable
of constructing a hyper ML model by embracing sub-models from different
clients. These sub-models are trained locally by vertically partitioned data
with distinct attributes. Therefore, the design of VFL is fundamentally
different from that of conventional FL, raising new and unique research issues.
In this paper, we aim to discuss key challenges in VFL with effective
solutions, and conduct experiments on real-life datasets to shed light on these
issues. Specifically, we first propose a general framework on VFL, and
highlight the key differences between VFL and conventional FL. Then, we discuss
research challenges rooted in VFL systems under four aspects, i.e., security
and privacy risks, expensive computation and communication costs, possible
structural damage caused by model splitting, and system heterogeneity.
Afterwards, we develop solutions to addressing the aforementioned challenges,
and conduct extensive experiments to showcase the effectiveness of our proposed
solutions."
1751,"the best AUC on all the benchmarks and achieves the best MVCE
   We further study the impact of different combinations of bin and               on Porto Seguro and Avazu.",of bins and samples.,"We notice that boosting brings more
sample numbers on ECE and MVCE in terms of 𝐸ˆ in Figure 3.                        improvement of AUC on large datasets; this is probably becasue
                                                                                  larger datasets can benefit more from the enlarged model capacity
                                                                            𝑏𝑖𝑎𝑠  brought by boosting.",2022-02-09 08:59:16+00:00,MBCT: Tree-Based Feature-Aware Binning for Individual Uncertainty Calibration,cs.LG,['cs.LG'],"[arxiv.Result.Author('Siguang Huang'), arxiv.Result.Author('Yunli Wang'), arxiv.Result.Author('Lili Mou'), arxiv.Result.Author('Huayue Zhang'), arxiv.Result.Author('Han Zhu'), arxiv.Result.Author('Chuan Yu'), arxiv.Result.Author('Bo Zheng')]","Most machine learning classifiers only concern classification accuracy, while
certain applications (such as medical diagnosis, meteorological forecasting,
and computation advertising) require the model to predict the true probability,
known as a calibrated estimate. In previous work, researchers have developed
several calibration methods to post-process the outputs of a predictor to
obtain calibrated values, such as binning and scaling methods. Compared with
scaling, binning methods are shown to have distribution-free theoretical
guarantees, which motivates us to prefer binning methods for calibration.
However, we notice that existing binning methods have several drawbacks: (a)
the binning scheme only considers the original prediction values, thus limiting
the calibration performance; and (b) the binning approach is non-individual,
mapping multiple samples in a bin to the same value, and thus is not suitable
for order-sensitive applications. In this paper, we propose a feature-aware
binning framework, called Multiple Boosting Calibration Trees (MBCT), along
with a multi-view calibration loss to tackle the above issues. Our MBCT
optimizes the binning scheme by the tree structures of features, and adopts a
linear function in a tree node to achieve individual calibration. Our MBCT is
non-monotonic, and has the potential to improve order accuracy, due to its
learnable binning scheme and the individual calibration. We conduct
comprehensive experiments on three datasets in different fields. Results show
that our method outperforms all competing models in terms of both calibration
error and order accuracy. We also conduct simulation experiments, justifying
that the proposed multi-view calibration loss is a better metric in modeling
calibration error."
1774,"We hope
gously for orange lines, except for chopper command this                          our work will motivate further research combining reinforce-
time).","We also highlighted an unexplored
(2) the nonparametric version of our model matches the per-                       connection between skill discovery and Bayesian nonpara-
formance of tuning K (dashed green lines match the peak of                        metrics, and showed that specifying K as a hyperparameter
the green lines for all environments, except space invaders),                     can be avoided by placing a GEM prior over skills, both in
and similarly for CompILE’s nonparametric version (analo-                         our model and other skill discovery frameworks.","We highlight that we did not tune λent nor its decay                       ment learning and Bayesian nonparametrics, for example
schedule and used the same settings across all experiments,                       through small-variance asymptotics (Kulis & Jordan, 2011;
so that not tuning K does not come at the cost of having                          Jiang et al., 2012; Broderick et al., 2013; Roychowdhury
to tune other parameters.",2022-02-09 19:01:01+00:00,Bayesian Nonparametrics for Offline Skill Discovery,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Valentin Villecroze'), arxiv.Result.Author('Harry J. Braviner'), arxiv.Result.Author('Panteha Naderian'), arxiv.Result.Author('Chris J. Maddison'), arxiv.Result.Author('Gabriel Loaiza-Ganem')]","Skills or low-level policies in reinforcement learning are temporally
extended actions that can speed up learning and enable complex behaviours.
Recent work in offline reinforcement learning and imitation learning has
proposed several techniques for skill discovery from a set of expert
trajectories. While these methods are promising, the number K of skills to
discover is always a fixed hyperparameter, which requires either prior
knowledge about the environment or an additional parameter search to tune it.
We first propose a method for offline learning of options (a particular skill
framework) exploiting advances in variational inference and continuous
relaxations. We then highlight an unexplored connection between Bayesian
nonparametrics and offline skill discovery, and show how to obtain a
nonparametric version of our model. This version is tractable thanks to a
carefully structured approximate posterior with a dynamically-changing number
of options, removing the need to specify K. We also show how our nonparametric
extension can be applied in other skill frameworks, and empirically demonstrate
that our method can outperform state-of-the-art offline skill learning
algorithms across a variety of environments. Our code is available at
https://github.com/layer6ai-labs/BNPO ."
1775,"We hope our work will motivate further research
combining reinforcement learning and Bayesian nonparametrics, for example through small-variance
asymptotics (Kulis and Jordan, 2011; Jiang et al., 2012; Broderick et al., 2013; Roychowdhury et al.,
2013) for hard rather than soft clustering of skills, or with hierarchical models (Teh et al., 2006).","We also highlighted
an unexplored connection between skill discovery and Bayesian nonparametrics, and showed that
specifying K as a hyperparameter can be avoided by placing a GEM prior over skills, both in our
model and other skill discovery frameworks.","References

Achiam, J., Edwards, H., Amodei, D., and Abbeel, P. (2018).",2022-02-09 19:01:01+00:00,Bayesian Nonparametrics for Offline Skill Discovery,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Valentin Villecroze'), arxiv.Result.Author('Harry J. Braviner'), arxiv.Result.Author('Panteha Naderian'), arxiv.Result.Author('Chris J. Maddison'), arxiv.Result.Author('Gabriel Loaiza-Ganem')]","Skills or low-level policies in reinforcement learning are temporally
extended actions that can speed up learning and enable complex behaviours.
Recent work in offline reinforcement learning and imitation learning has
proposed several techniques for skill discovery from a set of expert
trajectories. While these methods are promising, the number K of skills to
discover is always a fixed hyperparameter, which requires either prior
knowledge about the environment or an additional parameter search to tune it.
We first propose a method for offline learning of options (a particular skill
framework) exploiting advances in variational inference and continuous
relaxations. We then highlight an unexplored connection between Bayesian
nonparametrics and offline skill discovery, and show how to obtain a
nonparametric version of our model. This version is tractable thanks to a
carefully structured approximate posterior with a dynamically-changing number
of options, removing the need to specify K. We also show how our nonparametric
extension can be applied in other skill frameworks, and empirically demonstrate
that our method can outperform state-of-the-art offline skill learning
algorithms across a variety of environments. Our code is available at
https://github.com/layer6ai-labs/BNPO ."
1776,"We hope our work will mo-
versions of the environments correspond to the ‘K = 0’          tivate further research combining reinforcement learning
entries in the ﬁgures: the signiﬁcant improvements with         and Bayesian nonparametrics, for example through small-
K > 0 highlight the relevance of ofﬂine skill discovery.",not augmented)        skill discovery frameworks.,"variance asymptotics (Kulis & Jordan, 2011; Jiang et al.,
The crosses in Figures 3 and 4 show the recovered values of     2012; Broderick et al., 2013; Roychowdhury et al., 2013)
K (averaged across runs) for the nonparametric models, and      for hard rather than soft clustering of skills, or with hierar-
we can see that, except for a few cases, the number of recov-   chical models (Teh et al., 2006).",2022-02-09 19:01:01+00:00,Bayesian Nonparametrics for Offline Skill Discovery,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Valentin Villecroze'), arxiv.Result.Author('Harry J. Braviner'), arxiv.Result.Author('Panteha Naderian'), arxiv.Result.Author('Chris J. Maddison'), arxiv.Result.Author('Gabriel Loaiza-Ganem')]","Skills or low-level policies in reinforcement learning are temporally
extended actions that can speed up learning and enable complex behaviours.
Recent work in offline reinforcement learning and imitation learning has
proposed several techniques for skill discovery from a set of expert
trajectories. While these methods are promising, the number K of skills to
discover is always a fixed hyperparameter, which requires either prior
knowledge about the environment or an additional parameter search to tune it.
We first propose a method for offline learning of options (a particular skill
framework) exploiting advances in variational inference and continuous
relaxations. We then highlight an unexplored connection between Bayesian
nonparametrics and offline skill discovery, and show how to obtain a
nonparametric version of our model. This version is tractable thanks to a
carefully structured approximate posterior with a dynamically-changing number
of options, removing the need to specify K. We also show how our nonparametric
extension can be applied in other skill frameworks, and empirically demonstrate
that our method can outperform state-of-the-art offline skill learning
algorithms across a variety of environments. Our code is available at
https://github.com/layer6ai-labs/BNPO ."
1779,"The further study conveys the bi-directional effect between spectral
                                                  relations acquisition and spectral propagation.","Equipped with graph networks,
                                                  SPGN then integrates spectral relations with label information to make spectral
                                                  propagation.","We conduct extensive experiments
                                                  on few-shot TSC benchmarks.",2022-02-08 14:02:41+00:00,Spectral Propagation Graph Network for Few-shot Time Series Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Luxia Zhang')]","Few-shot Time Series Classification (few-shot TSC) is a challenging problem
in time series analysis. It is more difficult to classify when time series of
the same class are not completely consistent in spectral domain or time series
of different classes are partly consistent in spectral domain. To address this
problem, we propose a novel method named Spectral Propagation Graph Network
(SPGN) to explicitly model and propagate the spectrum-wise relations between
different time series with graph network. To the best of our knowledge, SPGN is
the first to utilize spectral comparisons in different intervals and involve
spectral propagation across all time series with graph networks for few-shot
TSC. SPGN first uses bandpass filter to expand time series in spectral domain
for calculating spectrum-wise relations between time series. Equipped with
graph networks, SPGN then integrates spectral relations with label information
to make spectral propagation. The further study conveys the bi-directional
effect between spectral relations acquisition and spectral propagation. We
conduct extensive experiments on few-shot TSC benchmarks. SPGN outperforms
state-of-the-art results by a large margin in $4\% \sim 13\%$. Moreover, SPGN
surpasses them by around $12\%$ and $9\%$ under cross-domain and cross-way
settings respectively."
1780,"The further study and analysis of the proposed model prove the
superiority of SPGN.","6 Conclusion and Future Work

In this paper, we presented Spectral Propagation Graph Network for few-shot time series classiﬁcation,
which is the ﬁrst to leverage spectrum-wise relations and involve label propagation with spectral
information in graph networks.","Extensive experiments show that SPGN achieve the state-of-the-art results on
few-shot TSC tasks and outperforms previous methods by a signiﬁcant margin from different aspects,
such as cross-domain and cross-way evaluation.",2022-02-08 14:02:41+00:00,Spectral Propagation Graph Network for Few-shot Time Series Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Luxia Zhang')]","Few-shot Time Series Classification (few-shot TSC) is a challenging problem
in time series analysis. It is more difficult to classify when time series of
the same class are not completely consistent in spectral domain or time series
of different classes are partly consistent in spectral domain. To address this
problem, we propose a novel method named Spectral Propagation Graph Network
(SPGN) to explicitly model and propagate the spectrum-wise relations between
different time series with graph network. To the best of our knowledge, SPGN is
the first to utilize spectral comparisons in different intervals and involve
spectral propagation across all time series with graph networks for few-shot
TSC. SPGN first uses bandpass filter to expand time series in spectral domain
for calculating spectrum-wise relations between time series. Equipped with
graph networks, SPGN then integrates spectral relations with label information
to make spectral propagation. The further study conveys the bi-directional
effect between spectral relations acquisition and spectral propagation. We
conduct extensive experiments on few-shot TSC benchmarks. SPGN outperforms
state-of-the-art results by a large margin in $4\% \sim 13\%$. Moreover, SPGN
surpasses them by around $12\%$ and $9\%$ under cross-domain and cross-way
settings respectively."
1781,"The further study conveys the bi-directional effect between spectral
                                                  relations acquisition and spectral propagation.","Equipped with graph networks,
                                                  SPGN then integrates spectral relations with label information to make spectral
                                                  propagation.","We conduct extensive experiments
                                                  on few-shot TSC benchmarks.",2022-02-08 14:02:41+00:00,Spectral Propagation Graph Network for Few-shot Time Series Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Luxia Zhang')]","Few-shot Time Series Classification (few-shot TSC) is a challenging problem
in time series analysis. It is more difficult to classify when time series of
the same class are not completely consistent in spectral domain or time series
of different classes are partly consistent in spectral domain. To address this
problem, we propose a novel method named Spectral Propagation Graph Network
(SPGN) to explicitly model and propagate the spectrum-wise relations between
different time series with graph network. To the best of our knowledge, SPGN is
the first to utilize spectral comparisons in different intervals and involve
spectral propagation across all time series with graph networks for few-shot
TSC. SPGN first uses bandpass filter to expand time series in spectral domain
for calculating spectrum-wise relations between time series. Equipped with
graph networks, SPGN then integrates spectral relations with label information
to make spectral propagation. The further study conveys the bi-directional
effect between spectral relations acquisition and spectral propagation. We
conduct extensive experiments on few-shot TSC benchmarks. SPGN outperforms
state-of-the-art results by a large margin in $4\% \sim 13\%$. Moreover, SPGN
surpasses them by around $12\%$ and $9\%$ under cross-domain and cross-way
settings respectively."
1782,"The further study and analysis of the proposed model prove the
superiority of SPGN.","6 Conclusion and Future Work

In this paper, we presented Spectral Propagation Graph Network for few-shot time series classiﬁcation,
which is the ﬁrst to leverage spectrum-wise relations and involve label propagation with spectral
information in graph networks.","Extensive experiments show that SPGN achieve the state-of-the-art results on
few-shot TSC tasks and outperforms previous methods by a signiﬁcant margin from different aspects,
such as cross-domain and cross-way evaluation.",2022-02-08 14:02:41+00:00,Spectral Propagation Graph Network for Few-shot Time Series Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Luxia Zhang')]","Few-shot Time Series Classification (few-shot TSC) is a challenging problem
in time series analysis. It is more difficult to classify when time series of
the same class are not completely consistent in spectral domain or time series
of different classes are partly consistent in spectral domain. To address this
problem, we propose a novel method named Spectral Propagation Graph Network
(SPGN) to explicitly model and propagate the spectrum-wise relations between
different time series with graph network. To the best of our knowledge, SPGN is
the first to utilize spectral comparisons in different intervals and involve
spectral propagation across all time series with graph networks for few-shot
TSC. SPGN first uses bandpass filter to expand time series in spectral domain
for calculating spectrum-wise relations between time series. Equipped with
graph networks, SPGN then integrates spectral relations with label information
to make spectral propagation. The further study conveys the bi-directional
effect between spectral relations acquisition and spectral propagation. We
conduct extensive experiments on few-shot TSC benchmarks. SPGN outperforms
state-of-the-art results by a large margin in $4\% \sim 13\%$. Moreover, SPGN
surpasses them by around $12\%$ and $9\%$ under cross-domain and cross-way
settings respectively."
1783,"The further study conveys the bi-directional effect between spectral
                                                   relations acquisition and spectral propagation.","Equipped with graph networks,
                                                   SPGN then integrates spectral relations with label information to make spectral
                                                   propagation.","We conduct extensive experiments
                                                   on few-shot TSC benchmarks.",2022-02-08 14:02:41+00:00,Spectral Propagation Graph Network for Few-shot Time Series Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Luxia Zhang')]","Few-shot Time Series Classification (few-shot TSC) is a challenging problem
in time series analysis. It is more difficult to classify when time series of
the same class are not completely consistent in spectral domain or time series
of different classes are partly consistent in spectral domain. To address this
problem, we propose a novel method named Spectral Propagation Graph Network
(SPGN) to explicitly model and propagate the spectrum-wise relations between
different time series with graph network. To the best of our knowledge, SPGN is
the first to utilize spectral comparisons in different intervals and involve
spectral propagation across all time series with graph networks for few-shot
TSC. SPGN first uses bandpass filter to expand time series in spectral domain
for calculating spectrum-wise relations between time series. Equipped with
graph networks, SPGN then integrates spectral relations with label information
to make spectral propagation. The further study conveys the bi-directional
effect between spectral relations acquisition and spectral propagation. We
conduct extensive experiments on few-shot TSC benchmarks. SPGN outperforms
state-of-the-art results by a large margin in $4\% \sim 13\%$. Moreover, SPGN
surpasses them by around $12\%$ and $9\%$ under cross-domain and cross-way
settings respectively."
1784,"The further study and analysis of the proposed model prove the
superiority of SPGN.","6 Conclusion and Future Work

In this paper, we presented Spectral Propagation Graph Network for few-shot time series classiﬁcation,
which is the ﬁrst to leverage spectrum-wise relations and involve label propagation with spectral
information in graph networks.","Extensive experiments show that SPGN achieve the state-of-the-art results on
few-shot TSC tasks and outperforms previous methods by a signiﬁcant margin from different aspects,
such as cross-domain and cross-way evaluation.",2022-02-08 14:02:41+00:00,Spectral Propagation Graph Network for Few-shot Time Series Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Luxia Zhang')]","Few-shot Time Series Classification (few-shot TSC) is a challenging problem
in time series analysis. It is more difficult to classify when time series of
the same class are not completely consistent in spectral domain or time series
of different classes are partly consistent in spectral domain. To address this
problem, we propose a novel method named Spectral Propagation Graph Network
(SPGN) to explicitly model and propagate the spectrum-wise relations between
different time series with graph network. To the best of our knowledge, SPGN is
the first to utilize spectral comparisons in different intervals and involve
spectral propagation across all time series with graph networks for few-shot
TSC. SPGN first uses bandpass filter to expand time series in spectral domain
for calculating spectrum-wise relations between time series. Equipped with
graph networks, SPGN then integrates spectral relations with label information
to make spectral propagation. The further study conveys the bi-directional
effect between spectral relations acquisition and spectral propagation. We
conduct extensive experiments on few-shot TSC benchmarks. SPGN outperforms
state-of-the-art results by a large margin in $4\% \sim 13\%$. Moreover, SPGN
surpasses them by around $12\%$ and $9\%$ under cross-domain and cross-way
settings respectively."
1791,"MolFlow (Zang & Wang,
      further research.","Moreover, we implement              tages of both autoregressive and ﬂow-based approaches to
      SiamFlow and baselines in a uniﬁed framework for               iteratively generate molecules.",The code will be released.,2022-02-10 04:31:14+00:00,Target-aware Molecular Graph Generation,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cheng Tan'), arxiv.Result.Author('Zhangyang Gao'), arxiv.Result.Author('Stan Z. Li')]","Generating molecules with desired biological activities has attracted growing
attention in drug discovery. Previous molecular generation models are designed
as chemocentric methods that hardly consider the drug-target interaction,
limiting their practical applications. In this paper, we aim to generate
molecular drugs in a target-aware manner that bridges biological activity and
molecular design. To solve this problem, we compile a benchmark dataset from
several publicly available datasets and build baselines in a unified framework.
Building on the recent advantages of flow-based molecular generation models, we
propose SiamFlow, which forces the flow to fit the distribution of target
sequence embeddings in latent space. Specifically, we employ an alignment loss
and a uniform loss to bring target sequence embeddings and drug graph
embeddings into agreements while avoiding collapse. Furthermore, we formulate
the alignment into a one-to-many problem by learning spaces of target sequence
embeddings. Experiments quantitatively show that our proposed method learns
meaningful representations in the latent space toward the target-aware
molecular graph generation and provides an alternative approach to bridge
biology and chemistry in drug discovery."
1792,"MolFlow (Zang & Wang,
      further research.","Moreover, we implement              tages of both autoregressive and ﬂow-based approaches to
      SiamFlow and baselines in a uniﬁed framework for               iteratively generate molecules.",The code will be released.,2022-02-10 04:31:14+00:00,Target-aware Molecular Graph Generation,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cheng Tan'), arxiv.Result.Author('Zhangyang Gao'), arxiv.Result.Author('Stan Z. Li')]","Generating molecules with desired biological activities has attracted growing
attention in drug discovery. Previous molecular generation models are designed
as chemocentric methods that hardly consider the drug-target interaction,
limiting their practical applications. In this paper, we aim to generate
molecular drugs in a target-aware manner that bridges biological activity and
molecular design. To solve this problem, we compile a benchmark dataset from
several publicly available datasets and build baselines in a unified framework.
Building on the recent advantages of flow-based molecular generation models, we
propose SiamFlow, which forces the flow to fit the distribution of target
sequence embeddings in latent space. Specifically, we employ an alignment loss
and a uniform loss to bring target sequence embeddings and drug graph
embeddings into agreements while avoiding collapse. Furthermore, we formulate
the alignment into a one-to-many problem by learning spaces of target sequence
embeddings. Experiments quantitatively show that our proposed method learns
meaningful representations in the latent space toward the target-aware
molecular graph generation and provides an alternative approach to bridge
biology and chemistry in drug discovery."
1856,"Considering these new
ﬁndings further research directions are presented in the discussion section.","However, during the inves-
tigation of hydrogeological data for the monitored region, it was found that
some of them are active throughout the whole year.",2.,2022-02-11 15:03:31+00:00,Predictive modeling of microbiological seawater quality classification in karst region using cascade model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ivana Lučin'), arxiv.Result.Author('Siniša Družeta'), arxiv.Result.Author('Goran Mauša'), arxiv.Result.Author('Marta Alvir'), arxiv.Result.Author('Luka Grbčić'), arxiv.Result.Author('Darija Vukić Lušić'), arxiv.Result.Author('Ante Sikirica'), arxiv.Result.Author('Lado Kranjčević')]","In this paper, an in-depth analysis of Escherichia coli seawater measurements
during the bathing season in the city of Rijeka, Croatia was conducted.
Submerged sources of groundwater were observed at several measurement locations
which could be the cause for increased E. coli values. This specificity of
karst terrain is usually not considered during the monitoring process, thus a
novel measurement methodology is proposed. A cascade machine learning model is
used to predict coastal water quality based on meteorological data, which
improves the level of accuracy due to data imbalance resulting from rare
occurrences of measurements with reduced water quality. Currently, the cascade
model is employed as a filter method, where measurements not classified as
excellent quality need to be further analyzed. However, with improvements
proposed in the paper, the cascade model could be ultimately used as a
standalone method."
1857,"However, since it does not reduce model accuracy
either, further study should be conducted where inclusion of precipitation
features at higher cascade model stages, with greater median values, could
be more beneﬁcial.","Due to having the
lowest feature importance, precipitation features were not included in the

                                               22
cascade model testing.","Table 5: Feature importance for prediction model with all measurement points with me-

dian value as classiﬁcation limit.",2022-02-11 15:03:31+00:00,Predictive modeling of microbiological seawater quality classification in karst region using cascade model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ivana Lučin'), arxiv.Result.Author('Siniša Družeta'), arxiv.Result.Author('Goran Mauša'), arxiv.Result.Author('Marta Alvir'), arxiv.Result.Author('Luka Grbčić'), arxiv.Result.Author('Darija Vukić Lušić'), arxiv.Result.Author('Ante Sikirica'), arxiv.Result.Author('Lado Kranjčević')]","In this paper, an in-depth analysis of Escherichia coli seawater measurements
during the bathing season in the city of Rijeka, Croatia was conducted.
Submerged sources of groundwater were observed at several measurement locations
which could be the cause for increased E. coli values. This specificity of
karst terrain is usually not considered during the monitoring process, thus a
novel measurement methodology is proposed. A cascade machine learning model is
used to predict coastal water quality based on meteorological data, which
improves the level of accuracy due to data imbalance resulting from rare
occurrences of measurements with reduced water quality. Currently, the cascade
model is employed as a filter method, where measurements not classified as
excellent quality need to be further analyzed. However, with improvements
proposed in the paper, the cascade model could be ultimately used as a
standalone method."
1858,"Thus, for the purpose of further study, the threshold value of 80% is adopted.","This measurement could be an outlier or ad-
ditional cascade model improvement could eliminate this wrong prediction.",Table 8: Inﬂuence of threshold value on cascade model accuracy for diﬀerent datasets.,2022-02-11 15:03:31+00:00,Predictive modeling of microbiological seawater quality classification in karst region using cascade model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ivana Lučin'), arxiv.Result.Author('Siniša Družeta'), arxiv.Result.Author('Goran Mauša'), arxiv.Result.Author('Marta Alvir'), arxiv.Result.Author('Luka Grbčić'), arxiv.Result.Author('Darija Vukić Lušić'), arxiv.Result.Author('Ante Sikirica'), arxiv.Result.Author('Lado Kranjčević')]","In this paper, an in-depth analysis of Escherichia coli seawater measurements
during the bathing season in the city of Rijeka, Croatia was conducted.
Submerged sources of groundwater were observed at several measurement locations
which could be the cause for increased E. coli values. This specificity of
karst terrain is usually not considered during the monitoring process, thus a
novel measurement methodology is proposed. A cascade machine learning model is
used to predict coastal water quality based on meteorological data, which
improves the level of accuracy due to data imbalance resulting from rare
occurrences of measurements with reduced water quality. Currently, the cascade
model is employed as a filter method, where measurements not classified as
excellent quality need to be further analyzed. However, with improvements
proposed in the paper, the cascade model could be ultimately used as a
standalone method."
1859,"Further investigation of model parameters
through stages and further study of diﬀerent combinations of proposed im-
provements should be conducted to further increase cascade model eﬃciency.","It must be noted that RF model parameters were the same for all stages
and proposed improvements for the cascade model were investigated only for
one combination of parameters.","Currently, both cascade and single prediction models are constructed with
measurements from multiple measurement points due to the small amount of
data.",2022-02-11 15:03:31+00:00,Predictive modeling of microbiological seawater quality classification in karst region using cascade model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ivana Lučin'), arxiv.Result.Author('Siniša Družeta'), arxiv.Result.Author('Goran Mauša'), arxiv.Result.Author('Marta Alvir'), arxiv.Result.Author('Luka Grbčić'), arxiv.Result.Author('Darija Vukić Lušić'), arxiv.Result.Author('Ante Sikirica'), arxiv.Result.Author('Lado Kranjčević')]","In this paper, an in-depth analysis of Escherichia coli seawater measurements
during the bathing season in the city of Rijeka, Croatia was conducted.
Submerged sources of groundwater were observed at several measurement locations
which could be the cause for increased E. coli values. This specificity of
karst terrain is usually not considered during the monitoring process, thus a
novel measurement methodology is proposed. A cascade machine learning model is
used to predict coastal water quality based on meteorological data, which
improves the level of accuracy due to data imbalance resulting from rare
occurrences of measurements with reduced water quality. Currently, the cascade
model is employed as a filter method, where measurements not classified as
excellent quality need to be further analyzed. However, with improvements
proposed in the paper, the cascade model could be ultimately used as a
standalone method."
1860,"Thus, several directions
of further research are suggested.","types of measurements were included to increase the teaching dataset which
should lead to improved model accuracy, however with these new ﬁndings,
this could in fact reduce prediction model accuracy.","32
4.3.",2022-02-11 15:03:31+00:00,Predictive modeling of microbiological seawater quality classification in karst region using cascade model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ivana Lučin'), arxiv.Result.Author('Siniša Družeta'), arxiv.Result.Author('Goran Mauša'), arxiv.Result.Author('Marta Alvir'), arxiv.Result.Author('Luka Grbčić'), arxiv.Result.Author('Darija Vukić Lušić'), arxiv.Result.Author('Ante Sikirica'), arxiv.Result.Author('Lado Kranjčević')]","In this paper, an in-depth analysis of Escherichia coli seawater measurements
during the bathing season in the city of Rijeka, Croatia was conducted.
Submerged sources of groundwater were observed at several measurement locations
which could be the cause for increased E. coli values. This specificity of
karst terrain is usually not considered during the monitoring process, thus a
novel measurement methodology is proposed. A cascade machine learning model is
used to predict coastal water quality based on meteorological data, which
improves the level of accuracy due to data imbalance resulting from rare
occurrences of measurements with reduced water quality. Currently, the cascade
model is employed as a filter method, where measurements not classified as
excellent quality need to be further analyzed. However, with improvements
proposed in the paper, the cascade model could be ultimately used as a
standalone method."
1861,"Thus, further research should include an investigation
of correlations between coastal springs’ activation and wider regional area’s
precipitation, as these distant rainfalls are expected to possibly be more in-
ﬂuential for coastal spring activity than local rainfall.","(2005) where
the tracer was injected in V. Snežnik (Slovenia) where underground water
connection with Kvarner Bay area was identiﬁed, including measurement lo-
cations considered in this study, indicating transboundary characteristics of
the investigated area.","It is also important
to mention that a boundary between two watersheds is passing through the
center of the city of Rijeka, thus bathing locations on the east side of the
city are expected to show a diﬀerent behaviour, and diﬀerent locations for
rainfall measurements should be considered.",2022-02-11 15:03:31+00:00,Predictive modeling of microbiological seawater quality classification in karst region using cascade model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ivana Lučin'), arxiv.Result.Author('Siniša Družeta'), arxiv.Result.Author('Goran Mauša'), arxiv.Result.Author('Marta Alvir'), arxiv.Result.Author('Luka Grbčić'), arxiv.Result.Author('Darija Vukić Lušić'), arxiv.Result.Author('Ante Sikirica'), arxiv.Result.Author('Lado Kranjčević')]","In this paper, an in-depth analysis of Escherichia coli seawater measurements
during the bathing season in the city of Rijeka, Croatia was conducted.
Submerged sources of groundwater were observed at several measurement locations
which could be the cause for increased E. coli values. This specificity of
karst terrain is usually not considered during the monitoring process, thus a
novel measurement methodology is proposed. A cascade machine learning model is
used to predict coastal water quality based on meteorological data, which
improves the level of accuracy due to data imbalance resulting from rare
occurrences of measurements with reduced water quality. Currently, the cascade
model is employed as a filter method, where measurements not classified as
excellent quality need to be further analyzed. However, with improvements
proposed in the paper, the cascade model could be ultimately used as a
standalone method."
1868,"Other possibilities
could be to increase the distance between renewals with constant increases or with growing
increase, but further study for the choice of I is left as future work.","When the sequence Cj reaches n − 1,
the next values of the sequence are chosen so that Cj+1 = Cj + n − 1.",6.,2022-02-11 17:52:56+00:00,A Newton-type algorithm for federated learning based on incremental Hessian eigenvector sharing,cs.LG,"['cs.LG', 'cs.AI', 'math.OC']","[arxiv.Result.Author('Nicolò Dal Fabbro'), arxiv.Result.Author('Subhrakanti Dey'), arxiv.Result.Author('Michele Rossi'), arxiv.Result.Author('Luca Schenato')]","There is a growing interest in the decentralized optimization framework that
goes under the name of Federated Learning (FL). In particular, much attention
is being turned to FL scenarios where the network is strongly heterogeneous in
terms of communication resources (e.g., bandwidth) and data distribution. In
these cases, communication between local machines (agents) and the central
server (Master) is a main consideration. In this work, we present an original
communication-constrained Newton-type (NT) algorithm designed to accelerate FL
in such heterogeneous scenarios. The algorithm is by design robust to non
i.i.d. data distributions, handles heterogeneity of agents' communication
resources (CRs), only requires sporadic Hessian computations, and achieves
super-linear convergence. This is possible thanks to an incremental strategy,
based on a singular value decomposition (SVD) of the local Hessian matrices,
which exploits (possibly) outdated second-order information. The proposed
solution is thoroughly validated on real datasets by assessing (i) the number
of communication rounds required for convergence, (ii) the overall amount of
data transmitted and (iii) the number of local Hessian computations required.
For all these metrics, the proposed approach shows superior performance against
state-of-the art techniques like GIANT and FedNL."
1869,"The
           PETRAW data set is publicly available at www.synapse.org/PETRAW to encourage further research
           in surgical workﬂow recognition.","Is it
           relevant to spend 20 to 200 times more computing time for less than 3% of improvement?","Keywords Surgical Process Model · Workﬂow recognition · Multi-modality · OR of the future

1 Introduction

In order to bring computer-assisted surgery (CAS) system inside the operating room, it is essential to have a complete
and explicit understanding of surgical procedures.",2022-02-11 18:33:11+00:00,PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?,cs.LG,"['cs.LG', 'cs.AI', 'cs.DB']","[arxiv.Result.Author('Arnaud Huaulmé'), arxiv.Result.Author('Kanako Harada'), arxiv.Result.Author('Quang-Minh Nguyen'), arxiv.Result.Author('Bogyu Park'), arxiv.Result.Author('Seungbum Hong'), arxiv.Result.Author('Min-Kook Choi'), arxiv.Result.Author('Michael Peven'), arxiv.Result.Author('Yunshuang Li'), arxiv.Result.Author('Yonghao Long'), arxiv.Result.Author('Qi Dou'), arxiv.Result.Author('Satyadwyoom Kumar'), arxiv.Result.Author('Seenivasan Lalithkumar'), arxiv.Result.Author('Ren Hongliang'), arxiv.Result.Author('Hiroki Matsuzaki'), arxiv.Result.Author('Yuto Ishikawa'), arxiv.Result.Author('Yuriko Harai'), arxiv.Result.Author('Satoshi Kondo'), arxiv.Result.Author('Mamoru Mitsuishi'), arxiv.Result.Author('Pierre Jannin')]","This paper presents the design and results of the ""PEg TRAnsfert Workflow
recognition"" (PETRAW) challenge whose objective was to develop surgical
workflow recognition methods based on one or several modalities, among video,
kinematic, and segmentation data, in order to study their added value. The
PETRAW challenge provided a data set of 150 peg transfer sequences performed on
a virtual simulator. This data set was composed of videos, kinematics, semantic
segmentation, and workflow annotations which described the sequences at three
different granularity levels: phase, step, and activity. Five tasks were
proposed to the participants: three of them were related to the recognition of
all granularities with one of the available modalities, while the others
addressed the recognition with a combination of modalities. Average
application-dependent balanced accuracy (AD-Accuracy) was used as evaluation
metric to take unbalanced classes into account and because it is more
clinically relevant than a frame-by-frame score. Seven teams participated in at
least one task and four of them in all tasks. Best results are obtained with
the use of the video and the kinematics data with an AD-Accuracy between 93%
and 90% for the four teams who participated in all tasks. The improvement
between video/kinematic-based methods and the uni-modality ones was significant
for all of the teams. However, the difference in testing execution time between
the video/kinematic-based and the kinematic-based methods has to be taken into
consideration. Is it relevant to spend 20 to 200 times more computing time for
less than 3% of improvement? The PETRAW data set is publicly available at
www.synapse.org/PETRAW to encourage further research in surgical workflow
recognition."
1870,"The PETRAW data set is publicly available at www.synapse.org/
           PETRAW to encourage further research in surgical workﬂow recognition.","Indeed, it could be asked whether it is relevant to spend 20 to 200 times more computing time to
           increase accuracy by 3% only.","Keywords Surgical Process Model · Workﬂow recognition · Multi-modality · OR of the future

1 Introduction

To bring computer-assisted surgery systems inside the operating room, a complete and explicit understanding of surgical
procedures is needed.",2022-02-11 18:33:11+00:00,PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?,cs.LG,"['cs.LG', 'cs.AI', 'cs.DB']","[arxiv.Result.Author('Arnaud Huaulmé'), arxiv.Result.Author('Kanako Harada'), arxiv.Result.Author('Quang-Minh Nguyen'), arxiv.Result.Author('Bogyu Park'), arxiv.Result.Author('Seungbum Hong'), arxiv.Result.Author('Min-Kook Choi'), arxiv.Result.Author('Michael Peven'), arxiv.Result.Author('Yunshuang Li'), arxiv.Result.Author('Yonghao Long'), arxiv.Result.Author('Qi Dou'), arxiv.Result.Author('Satyadwyoom Kumar'), arxiv.Result.Author('Seenivasan Lalithkumar'), arxiv.Result.Author('Ren Hongliang'), arxiv.Result.Author('Hiroki Matsuzaki'), arxiv.Result.Author('Yuto Ishikawa'), arxiv.Result.Author('Yuriko Harai'), arxiv.Result.Author('Satoshi Kondo'), arxiv.Result.Author('Mamoru Mitsuishi'), arxiv.Result.Author('Pierre Jannin')]","This paper presents the design and results of the ""PEg TRAnsfert Workflow
recognition"" (PETRAW) challenge whose objective was to develop surgical
workflow recognition methods based on one or several modalities, among video,
kinematic, and segmentation data, in order to study their added value. The
PETRAW challenge provided a data set of 150 peg transfer sequences performed on
a virtual simulator. This data set was composed of videos, kinematics, semantic
segmentation, and workflow annotations which described the sequences at three
different granularity levels: phase, step, and activity. Five tasks were
proposed to the participants: three of them were related to the recognition of
all granularities with one of the available modalities, while the others
addressed the recognition with a combination of modalities. Average
application-dependent balanced accuracy (AD-Accuracy) was used as evaluation
metric to take unbalanced classes into account and because it is more
clinically relevant than a frame-by-frame score. Seven teams participated in at
least one task and four of them in all tasks. Best results are obtained with
the use of the video and the kinematics data with an AD-Accuracy between 93%
and 90% for the four teams who participated in all tasks. The improvement
between video/kinematic-based methods and the uni-modality ones was significant
for all of the teams. However, the difference in testing execution time between
the video/kinematic-based and the kinematic-based methods has to be taken into
consideration. Is it relevant to spend 20 to 200 times more computing time for
less than 3% of improvement? The PETRAW data set is publicly available at
www.synapse.org/PETRAW to encourage further research in surgical workflow
recognition."
1886,"Effects of Public Data Size

We further study the effects of public data size.",C.3.,"Only a very small set of public data (even 0.04% the size of private training
data) can provide good preconditioner estimates.",2022-02-12 03:02:06+00:00,Private Adaptive Optimization with Side Information,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Tian Li'), arxiv.Result.Author('Manzil Zaheer'), arxiv.Result.Author('Sashank J. Reddi'), arxiv.Result.Author('Virginia Smith')]","Adaptive optimization methods have become the default solvers for many
machine learning tasks. Unfortunately, the benefits of adaptivity may degrade
when training with differential privacy, as the noise added to ensure privacy
reduces the effectiveness of the adaptive preconditioner. To this end, we
propose AdaDPS, a general framework that uses non-sensitive side information to
precondition the gradients, allowing the effective use of adaptive methods in
private settings. We formally show AdaDPS reduces the amount of noise needed to
achieve similar privacy guarantees, thereby improving optimization performance.
Empirically, we leverage simple and readily available side information to
explore the performance of AdaDPS in practice, comparing to strong baselines in
both centralized and federated settings. Our results show that AdaDPS improves
accuracy by 7.7% (absolute) on average -- yielding state-of-the-art
privacy-utility trade-offs on large-scale text and image benchmarks."
1889,"Clearly, further research is needed to better understand in which settings this is expected
to be the case.","We speculate that imposing a model structure (even if
both the structure and the parameters still need to be inferred from data) acts as strong
regularization helping to ensure successful control which is robust to environmental changes.","In the future, it would be interesting to consider situations where the
model inference becomes even harder, for example, because more parts of the system are
unobserved.",2022-02-12 12:37:29+00:00,"Learning by Doing: Controlling a Dynamical System using Causality, Control, and Reinforcement Learning",cs.LG,"['cs.LG', 'cs.RO', 'cs.SY', 'eess.SY', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Sebastian Weichwald'), arxiv.Result.Author('Søren Wengel Mogensen'), arxiv.Result.Author('Tabitha Edith Lee'), arxiv.Result.Author('Dominik Baumann'), arxiv.Result.Author('Oliver Kroemer'), arxiv.Result.Author('Isabelle Guyon'), arxiv.Result.Author('Sebastian Trimpe'), arxiv.Result.Author('Jonas Peters'), arxiv.Result.Author('Niklas Pfister')]","Questions in causality, control, and reinforcement learning go beyond the
classical machine learning task of prediction under i.i.d. observations.
Instead, these fields consider the problem of learning how to actively perturb
a system to achieve a certain effect on a response variable. Arguably, they
have complementary views on the problem: In control, one usually aims to first
identify the system by excitation strategies to then apply model-based design
techniques to control the system. In (non-model-based) reinforcement learning,
one directly optimizes a reward. In causality, one focus is on identifiability
of causal structure. We believe that combining the different views might create
synergies and this competition is meant as a first step toward such synergies.
The participants had access to observational and (offline) interventional data
generated by dynamical systems. Track CHEM considers an open-loop problem in
which a single impulse at the beginning of the dynamics can be set, while Track
ROBO considers a closed-loop problem in which control variables can be set at
each time step. The goal in both tracks is to infer controls that drive the
system to a desired state. Code is open-sourced (
https://github.com/LearningByDoingCompetition/learningbydoing-comp ) to
reproduce the winning solutions of the competition and to facilitate trying out
new methods on the competition tasks."
1910,"For
standing of how the choice of NΩ impacts optimization       example, in [20] and [14], the distribution of the LOS
performance and improving run time are meaningful           in the ICU is assumed to have a mean and standard
subjects of further research.",Developing a better under-          ated based on strong distributional assumptions.,All optimization problems     deviation of no more than 3.5 days.,2022-02-13 18:36:16+00:00,Surgical Scheduling via Optimization and Machine Learning with Long-Tailed Data,cs.LG,"['cs.LG', 'stat.AP']","[arxiv.Result.Author('Yuan Shi'), arxiv.Result.Author('Saied Mahdian'), arxiv.Result.Author('Jose Blanchet'), arxiv.Result.Author('Peter Glynn'), arxiv.Result.Author('Andrew Y. Shin'), arxiv.Result.Author('David Scheinker')]","Using data from cardiovascular surgery patients with long and highly variable
post-surgical lengths of stay (LOS), we develop a model to reduce recovery unit
congestion. We estimate LOS using a variety of machine learning models,
schedule procedures with a variety of online optimization models, and estimate
performance with simulation. The machine learning models achieved only modest
LOS prediction accuracy, despite access to a very rich set of patient
characteristics. Compared to the current paper-based system used in the
hospital, most optimization models failed to reduce congestion without
increasing wait times for surgery. A conservative stochastic optimization with
sufficient sampling to capture the long tail of the LOS distribution
outperformed the current manual process. These results highlight the perils of
using oversimplified distributional models of patient length of stay for
scheduling procedures and the importance of using stochastic optimization
well-suited to dealing with long-tailed behavior."
1911,"To improve practicality of robust optimization,
                                                              further research is needed to develop new formulations
    The median of changes in patient wait time com-           less sensitive to parameter choices.","In contrast, the interquartile ranges        reveals a shortcoming of using robust optimization ap-
of Conservative-RSO (blue) and RRO (red) both show            proaches that involve a large number of parameters,
more signiﬁcant reduction in the number of high ICU           and demonstrates that increasing algorithmic complex-
occupancy days compared to RDO for all values of cho-         ity does not always leads to better performance in prac-
sen β.                                                        tice.",pared to the status quo are presented in Figure 10.,2022-02-13 18:36:16+00:00,Surgical Scheduling via Optimization and Machine Learning with Long-Tailed Data,cs.LG,"['cs.LG', 'stat.AP']","[arxiv.Result.Author('Yuan Shi'), arxiv.Result.Author('Saied Mahdian'), arxiv.Result.Author('Jose Blanchet'), arxiv.Result.Author('Peter Glynn'), arxiv.Result.Author('Andrew Y. Shin'), arxiv.Result.Author('David Scheinker')]","Using data from cardiovascular surgery patients with long and highly variable
post-surgical lengths of stay (LOS), we develop a modeling framework to reduce
recovery unit congestion. We estimate the LOS and its probability distribution
using machine learning models, schedule procedures on a rolling basis using a
variety of optimization models, and estimate performance with simulation. The
machine learning models achieved only modest LOS prediction accuracy, despite
access to a very rich set of patient characteristics. Compared to the current
paper-based system used in the hospital, most optimization models failed to
reduce congestion without increasing wait times for surgery. A conservative
stochastic optimization with sufficient sampling to capture the long tail of
the LOS distribution outperformed the current manual process and other
stochastic and robust optimization approaches. These results highlight the
perils of using oversimplified distributional models of LOS for scheduling
procedures and the importance of using optimization methods well-suited to
dealing with long-tailed behavior."
1912,"First, much work
AC&CG iterations impacts optimization performance           is needed to explore and evaluate alternative designs of
is a meaningful subject of further research.","Developing a better understanding of      of LOS to improve surgical scheduling, and they provide
how the choice of NΩ and the maximum number of              meaningful directions for future work.","data-driven optimization formulations, calibrated and
                                                            evaluated using empirical data of LOS.",2022-02-13 18:36:16+00:00,Surgical Scheduling via Optimization and Machine Learning with Long-Tailed Data,cs.LG,"['cs.LG', 'stat.AP']","[arxiv.Result.Author('Yuan Shi'), arxiv.Result.Author('Saied Mahdian'), arxiv.Result.Author('Jose Blanchet'), arxiv.Result.Author('Peter Glynn'), arxiv.Result.Author('Andrew Y. Shin'), arxiv.Result.Author('David Scheinker')]","Using data from cardiovascular surgery patients with long and highly variable
post-surgical lengths of stay (LOS), we develop a modeling framework to reduce
recovery unit congestion. We estimate the LOS and its probability distribution
using machine learning models, schedule procedures on a rolling basis using a
variety of optimization models, and estimate performance with simulation. The
machine learning models achieved only modest LOS prediction accuracy, despite
access to a very rich set of patient characteristics. Compared to the current
paper-based system used in the hospital, most optimization models failed to
reduce congestion without increasing wait times for surgery. A conservative
stochastic optimization with sufficient sampling to capture the long tail of
the LOS distribution outperformed the current manual process and other
stochastic and robust optimization approaches. These results highlight the
perils of using oversimplified distributional models of LOS for scheduling
procedures and the importance of using optimization methods well-suited to
dealing with long-tailed behavior."
1937,Tackling these problems would be our further research direction.,"In addition, clustering on auxiliary information or data will also lose precision.","9
Published as a conference paper at ICLR 2022

ETHICS STATEMENT

All authors of this work have read the ICLR code of ethics and commit to adhering to it.",2022-02-14 12:57:31+00:00,Learning Weakly-Supervised Contrastive Representations,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yao-Hung Hubert Tsai'), arxiv.Result.Author('Tianqin Li'), arxiv.Result.Author('Weixin Liu'), arxiv.Result.Author('Peiyuan Liao'), arxiv.Result.Author('Ruslan Salakhutdinov'), arxiv.Result.Author('Louis-Philippe Morency')]","We argue that a form of the valuable information provided by the auxiliary
information is its implied data clustering information. For instance,
considering hashtags as auxiliary information, we can hypothesize that an
Instagram image will be semantically more similar with the same hashtags. With
this intuition, we present a two-stage weakly-supervised contrastive learning
approach. The first stage is to cluster data according to its auxiliary
information. The second stage is to learn similar representations within the
same cluster and dissimilar representations for data from different clusters.
Our empirical experiments suggest the following three contributions. First,
compared to conventional self-supervised representations, the
auxiliary-information-infused representations bring the performance closer to
the supervised representations, which use direct downstream labels as
supervision signals. Second, our approach performs the best in most cases, when
comparing our approach with other baseline representation learning methods that
also leverage auxiliary data information. Third, we show that our approach also
works well with unsupervised constructed clusters (e.g., no auxiliary
information), resulting in a strong unsupervised representation learning
approach."
1938,Tackling these problems would be our further research direction.,"In addition, clustering on auxiliary information or data will also lose precision.","9
Published as a conference paper at ICLR 2022

ETHICS STATEMENT

All authors of this work have read the ICLR code of ethics and commit to adhering to it.",2022-02-14 12:57:31+00:00,Learning Weakly-Supervised Contrastive Representations,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yao-Hung Hubert Tsai'), arxiv.Result.Author('Tianqin Li'), arxiv.Result.Author('Weixin Liu'), arxiv.Result.Author('Peiyuan Liao'), arxiv.Result.Author('Ruslan Salakhutdinov'), arxiv.Result.Author('Louis-Philippe Morency')]","We argue that a form of the valuable information provided by the auxiliary
information is its implied data clustering information. For instance,
considering hashtags as auxiliary information, we can hypothesize that an
Instagram image will be semantically more similar with the same hashtags. With
this intuition, we present a two-stage weakly-supervised contrastive learning
approach. The first stage is to cluster data according to its auxiliary
information. The second stage is to learn similar representations within the
same cluster and dissimilar representations for data from different clusters.
Our empirical experiments suggest the following three contributions. First,
compared to conventional self-supervised representations, the
auxiliary-information-infused representations bring the performance closer to
the supervised representations, which use direct downstream labels as
supervision signals. Second, our approach performs the best in most cases, when
comparing our approach with other baseline representation learning methods that
also leverage auxiliary data information. Third, we show that our approach also
works well with unsupervised constructed clusters (e.g., no auxiliary
information), resulting in a strong unsupervised representation learning
approach."
1953,"Besides, it would be interesting to further study
whether the proposed model can be applied to other spatial-temporal data mining tasks,
such as trajectory prediction.","Moreover, multi-source data, such as weather conditions, road congestion, and accidents,
could also be explored in the future.","Conflicts of Interest

The authors declare no conflict of interest.",2022-02-10 13:18:11+00:00,Graph-GAN: A spatial-temporal neural network for short-term passenger flow prediction in urban rail transit systems,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY', 'none', 'E.0']","[arxiv.Result.Author('Hua Li'), arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Jianguo Qi'), arxiv.Result.Author('Ziyou Gao')]","Short-term passenger flow prediction plays an important role in better
managing the urban rail transit (URT) systems. Emerging deep learning models
provide good insights to improve short-term prediction accuracy. However, a
large number of existing prediction models combine diverse neural network
layers to improve accuracy, making their model structures extremely complex and
difficult to be applied to the real world. Therefore, it is necessary to trade
off between the model complexity and prediction performance from the
perspective of real-world applications. To this end, we propose a deep
learning-based Graph-GAN model with a simple structure and high prediction
accuracy to predict short-term passenger flows of the URT network. The
Graph-GAN consists of two major parts: (1) a simplified and static version of
the graph convolution network (GCN) used to extract network topological
information; (2) a generative adversarial network (GAN) used to predict
passenger flows, with generators and discriminators in GAN just composed of
simple fully connected neural networks. The Graph-GAN is tested on two
large-scale real-world datasets from Beijing Subway. A comparison of the
prediction performance of Graph-GAN with those of several state-of-the-art
models illustrates its superiority and robustness. This study can provide
critical experience in conducting short-term passenger flow predictions,
especially from the perspective of real-world applications."
1954,"Besides, it would be interesting to further study whether the
                                                                             proposed model can be applied to other spatiotemporal data
                                                                             mining tasks, such as trajectory prediction.","In this study, we propose a deep learning architecture called             Moreover, multi-source data, such as weather conditions, road
STG-GAN for a more accurate short-term passenger flow                        congestion, and accidents, could also be explored in the future.","REFERENCES:

                                                                             [1] Y. Liu, Z. Liu and R. Jia, ""DeepPF: A deep learning based architecture for
                                                                                   metro passenger flow prediction,"" Transportation Research Part C:
                                                                                   Emerging Technologies, vol.",2022-02-10 13:18:11+00:00,STG-GAN: A spatiotemporal graph generative adversarial networks for short-term passenger flow prediction in urban rail transit systems,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY', 'E.0']","[arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Hua Li'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Guangyin Jin'), arxiv.Result.Author('Jianguo Qi'), arxiv.Result.Author('Ziyou Gao')]","Short-term passenger flow prediction is an important but challenging task for
better managing urban rail transit (URT) systems. Some emerging deep learning
models provide good insights to improve short-term prediction accuracy.
However, there exist many complex spatiotemporal dependencies in URT systems.
Most previous methods only consider the absolute error between ground truth and
predictions as the optimization objective, which fails to account for spatial
and temporal constraints on the predictions. Furthermore, a large number of
existing prediction models introduce complex neural network layers to improve
accuracy while ignoring their training efficiency and memory occupancy,
decreasing the chances to be applied to the real world. To overcome these
limitations, we propose a novel deep learning-based spatiotemporal graph
generative adversarial network (STG-GAN) model with higher prediction accuracy,
higher efficiency, and lower memory occupancy to predict short-term passenger
flows of the URT network. Our model consists of two major parts, which are
optimized in an adversarial learning manner: (1) a generator network including
gated temporal conventional networks (TCN) and weight sharing graph convolution
networks (GCN) to capture structural spatiotemporal dependencies and generate
predictions with a relatively small computational burden; (2) a discriminator
network including a spatial discriminator and a temporal discriminator to
enhance the spatial and temporal constraints of the predictions. The STG-GAN is
evaluated on two large-scale real-world datasets from Beijing Subway. A
comparison with those of several state-of-the-art models illustrates its
superiority and robustness. This study can provide critical experience in
conducting short-term passenger flow predictions, especially from the
perspective of real-world applications."
1956,"In the standard experimental setup of continual learn-
                                           come in further research before we apply this technique         ing for regression tasks, all novelties can be viewed as new
                                           to real-world applications.","leads to a decrease in the performance of the neural net-
                                           However, two general and related challenges should be over-     work.","Firstly, newly collected novelties  tasks for updating the model using continual learning (He
                                           from the data stream in applications could contain anoma-       and Sick 2021; He, Huang, and Sick 2021).",2022-02-14 15:00:22+00:00,Design of Explainability Module with Experts in the Loop for Visualization and Dynamic Adjustment of Continual Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yujiang He'), arxiv.Result.Author('Zhixin Huang'), arxiv.Result.Author('Bernhard Sick')]","Continual learning can enable neural networks to evolve by learning new tasks
sequentially in task-changing scenarios. However, two general and related
challenges should be overcome in further research before we apply this
technique to real-world applications. Firstly, newly collected novelties from
the data stream in applications could contain anomalies that are meaningless
for continual learning. Instead of viewing them as a new task for updating, we
have to filter out such anomalies to reduce the disturbance of extremely
high-entropy data for the progression of convergence. Secondly, fewer efforts
have been put into research regarding the explainability of continual learning,
which leads to a lack of transparency and credibility of the updated neural
networks. Elaborated explanations about the process and result of continual
learning can help experts in judgment and making decisions. Therefore, we
propose the conceptual design of an explainability module with experts in the
loop based on techniques, such as dimension reduction, visualization, and
evaluation strategies. This work aims to overcome the mentioned challenges by
sufficiently explaining and visualizing the identified anomalies and the
updated neural network. With the help of this module, experts can be more
confident in decision-making regarding anomaly filtering, dynamic adjustment of
hyperparameters, data backup, etc."
1957,"The article is concluded with a brief out-  sponsible for evaluating the potential anomalies by com-
look of our further research.","3, we present a speciﬁc use case about how
the proposed design can serve as an extension to the ex-         2.1 Evaluation Bank
isting work in (He and Sick 2021), an adaptive continual
learning framework for solving regression problems in non-       Anomaly Evaluation The anomaly evaluation block is re-
stationary contexts.","puting an anomaly score, which assists experts in decision-
                                                                 making.",2022-02-14 15:00:22+00:00,Design of Explainability Module with Experts in the Loop for Visualization and Dynamic Adjustment of Continual Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yujiang He'), arxiv.Result.Author('Zhixin Huang'), arxiv.Result.Author('Bernhard Sick')]","Continual learning can enable neural networks to evolve by learning new tasks
sequentially in task-changing scenarios. However, two general and related
challenges should be overcome in further research before we apply this
technique to real-world applications. Firstly, newly collected novelties from
the data stream in applications could contain anomalies that are meaningless
for continual learning. Instead of viewing them as a new task for updating, we
have to filter out such anomalies to reduce the disturbance of extremely
high-entropy data for the progression of convergence. Secondly, fewer efforts
have been put into research regarding the explainability of continual learning,
which leads to a lack of transparency and credibility of the updated neural
networks. Elaborated explanations about the process and result of continual
learning can help experts in judgment and making decisions. Therefore, we
propose the conceptual design of an explainability module with experts in the
loop based on techniques, such as dimension reduction, visualization, and
evaluation strategies. This work aims to overcome the mentioned challenges by
sufficiently explaining and visualizing the identified anomalies and the
updated neural network. With the help of this module, experts can be more
confident in decision-making regarding anomaly filtering, dynamic adjustment of
hyperparameters, data backup, etc."
1958,"In our further research, we plan to im-
of updating under the case where the updating result is un-      plement the design and apply it to the existing the CLeaR
acceptable.","Experts can         explainability module integrated with various deep learning-
adjust the hyperparameters by their intuitions and profes-       related techniques, such as anomaly detection, evaluation,
sional experience for the future application or a new round      and visualization.",framework as an extension.,2022-02-14 15:00:22+00:00,Design of Explainability Module with Experts in the Loop for Visualization and Dynamic Adjustment of Continual Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yujiang He'), arxiv.Result.Author('Zhixin Huang'), arxiv.Result.Author('Bernhard Sick')]","Continual learning can enable neural networks to evolve by learning new tasks
sequentially in task-changing scenarios. However, two general and related
challenges should be overcome in further research before we apply this
technique to real-world applications. Firstly, newly collected novelties from
the data stream in applications could contain anomalies that are meaningless
for continual learning. Instead of viewing them as a new task for updating, we
have to filter out such anomalies to reduce the disturbance of extremely
high-entropy data for the progression of convergence. Secondly, fewer efforts
have been put into research regarding the explainability of continual learning,
which leads to a lack of transparency and credibility of the updated neural
networks. Elaborated explanations about the process and result of continual
learning can help experts in judgment and making decisions. Therefore, we
propose the conceptual design of an explainability module with experts in the
loop based on techniques, such as dimension reduction, visualization, and
evaluation strategies. This work aims to overcome the mentioned challenges by
sufficiently explaining and visualizing the identified anomalies and the
updated neural network. With the help of this module, experts can be more
confident in decision-making regarding anomaly filtering, dynamic adjustment of
hyperparameters, data backup, etc."
1966,We further study this similarity in Fig.,"Surprisingly, we see that the
OOD and InD conditional distributions are very similar.","1
(right), which plots expected ensemble variance conditioned on average single model uncertainty:
E[ Var | E[U ] ].",2022-02-14 19:01:01+00:00,"Deep Ensembles Work, But Are They Necessary?",cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Taiga Abe'), arxiv.Result.Author('E. Kelly Buchanan'), arxiv.Result.Author('Geoff Pleiss'), arxiv.Result.Author('Richard Zemel'), arxiv.Result.Author('John P. Cunningham')]","Ensembling neural networks is an effective way to increase accuracy, and can
often match the performance of individual larger models. This observation poses
a natural question: given the choice between a deep ensemble and a single
neural network with similar accuracy, is one preferable over the other? Recent
work suggests that deep ensembles may offer distinct benefits beyond predictive
power: namely, uncertainty quantification and robustness to dataset shift. In
this work, we demonstrate limitations to these purported benefits, and show
that a single (but larger) neural network can replicate these qualities. First,
we show that ensemble diversity, by any metric, does not meaningfully
contribute to an ensemble's uncertainty quantification on out-of-distribution
(OOD) data, but is instead highly correlated with the relative improvement of a
single larger model. Second, we show that the OOD performance afforded by
ensembles is strongly determined by their in-distribution (InD) performance,
and -- in this sense -- is not indicative of any ""effective robustness"". While
deep ensembles are a practical way to achieve improvements to predictive power,
uncertainty quantification, and robustness, our results show that these
improvements can be replicated by a (larger) single model."
1971,"Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D.
framework will provide such infrastructure for these studies,                  Silver and D. Wierstra, ""Continuous control with deep reinforcement
and it will open doors for further research.","The                  [13] T. P. Lillicrap, J. J.","learning,"" arXiv preprint arXiv:1509.02971, 2015.",2022-02-14 20:34:08+00:00,QuadSim: A Quadcopter Rotational Dynamics Simulation Framework For Reinforcement Learning Algorithms,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Burak Han Demirbilek')],"This study focuses on designing and developing a mathematically based
quadcopter rotational dynamics simulation framework for testing reinforcement
learning (RL) algorithms in many flexible configurations. The design of the
simulation framework aims to simulate both linear and nonlinear representations
of a quadcopter by solving initial value problems for ordinary differential
equation (ODE) systems. In addition, the simulation environment is capable of
making the simulation deterministic/stochastic by adding random Gaussian noise
in the forms of process and measurement noises. In order to ensure that the
scope of this simulation environment is not limited only with our own RL
algorithms, the simulation environment has been expanded to be compatible with
the OpenAI Gym toolkit. The framework also supports multiprocessing
capabilities to run simulation environments simultaneously in parallel. To test
these capabilities, many state-of-the-art deep RL algorithms were trained in
this simulation framework and the results were compared in detail."
1972,"4344–4352, 2017.
for the collapse in training with small mini-batch sizes will
be subject to further research.","The reason       sion and Pattern Recognition, pp.","Jia, K., Li, S., Wen, Y., Liu, T., and Tao, D. Orthogo-
                                                                  nal deep neural networks.",2022-02-14 21:46:07+00:00,Orthogonalising gradients to speed up neural network optimisation,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mark Tuddenham'), arxiv.Result.Author('Adam Prügel-Bennett'), arxiv.Result.Author('Jonathan Hare')]","The optimisation of neural networks can be sped up by orthogonalising the
gradients before the optimisation step, ensuring the diversification of the
learned representations. We orthogonalise the gradients of the layer's
components/filters with respect to each other to separate out the intermediate
representations. Our method of orthogonalisation allows the weights to be used
more flexibly, in contrast to restricting the weights to an orthogonalised
sub-space. We tested this method on ImageNet and CIFAR-10 resulting in a large
decrease in learning time, and also obtain a speed-up on the semi-supervised
learning BarlowTwins. We obtain similar accuracy to SGD without fine-tuning and
better accuracy for na\""ively chosen hyper-parameters."
1976,"We hope this survey will ignite further research interests
in time series Transformers.","the recent advances of Transformers for modeling time series      2.3 Multi-head Attention
data.","With Query-Key-Value (QKV) model, the scaled dot-product
                                                                  attention used by Transformer is given by
2 Preliminaries of the Transformer
                                                                                                                      QKT
2.1 Vanilla Transformer                                                    Attention(Q, K, V) = sof tmax( √ )V (2)
The vanilla Transformer [Vaswani et al., 2017] follows most
competitive neural sequence models with an encoder-decoder                                                               Dk
structure.",2022-02-15 01:43:27+00:00,Transformers in Time Series: A Survey,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']","[arxiv.Result.Author('Qingsong Wen'), arxiv.Result.Author('Tian Zhou'), arxiv.Result.Author('Chaoli Zhang'), arxiv.Result.Author('Weiqi Chen'), arxiv.Result.Author('Ziqing Ma'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Liang Sun')]","Transformers have achieved superior performances in many tasks in natural
language processing and computer vision, which also intrigues great interests
in the time series community. Among multiple advantages of transformers, the
ability to capture long-range dependencies and interactions is especially
attractive for time series modeling, leading to exciting progress in various
time series applications. In this paper, we systematically review transformer
schemes for time series modeling by highlighting their strengths as well as
limitations through a new taxonomy to summarize existing time series
transformers in two perspectives. From the perspective of network
modifications, we summarize the adaptations of module level and architecture
level of the time series transformers. From the perspective of applications, we
categorize time series transformers based on common tasks including
forecasting, anomaly detection, and classification. Empirically, we perform
robust analysis, model size analysis, and seasonal-trend decomposition analysis
to study how Transformers perform in time series. Finally, we discuss and
suggest future directions to provide useful research guidance. To the best of
our knowledge, this paper is the first work to comprehensively and
systematically summarize the recent advances of Transformers for modeling time
series data. We hope this survey will ignite further research interests in time
series Transformers."
1977,"Therefore, it
Seasonal-Trend Decomposition Analysis                            is worthy further researching to exploit the strong modeling
In the latest studies, researchers [Wu et al., 2021; Zhou et     abilities of both Transformers and GNNs for time series data.","2021], but also in-depth understanding the dynamic spatio-
                                                                 temporal characteristics and latent casuality.","al., 2022; Lin et al., 2021; Liu et al., 2022] begin to real-
ize that the seasonal-trend decomposition is a crucial part for  7.3 Pre-trained Transformers for Time Series
Transformer’s performance in time series forecasting.",2022-02-15 01:43:27+00:00,Transformers in Time Series: A Survey,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']","[arxiv.Result.Author('Qingsong Wen'), arxiv.Result.Author('Tian Zhou'), arxiv.Result.Author('Chaoli Zhang'), arxiv.Result.Author('Weiqi Chen'), arxiv.Result.Author('Ziqing Ma'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Liang Sun')]","Transformers have achieved superior performances in many tasks in natural
language processing and computer vision, which also intrigues great interests
in the time series community. Among multiple advantages of transformers, the
ability to capture long-range dependencies and interactions is especially
attractive for time series modeling, leading to exciting progress in various
time series applications. In this paper, we systematically review transformer
schemes for time series modeling by highlighting their strengths as well as
limitations through a new taxonomy to summarize existing time series
transformers in two perspectives. From the perspective of network
modifications, we summarize the adaptations of module level and architecture
level of the time series transformers. From the perspective of applications, we
categorize time series transformers based on common tasks including
forecasting, anomaly detection, and classification. Empirically, we perform
robust analysis, model size analysis, and seasonal-trend decomposition analysis
to study how Transformers perform in time series. Finally, we discuss and
suggest future directions to provide useful research guidance. To the best of
our knowledge, this paper is the first work to comprehensively and
systematically summarize the recent advances of Transformers for modeling time
series data. We hope this survey will ignite further research interests in time
series Transformers."
1978,"We hope this sur-     Encoding     Module            Level                  Detection
vey will ignite further research interests in time series Trans-
formers.","To                    Modifications                              Domains
the best of our knowledge, this is the ﬁrst work to compre-
hensively and systematically review the key developments of       Positional  Attention      Architecture  Forecasting  Anomaly          Classification
Transformers for modeling time series data.","Vanilla    Learnable      Timestamp     Time Series  Spatio-Temporal     Event
                                                                  Encoding    Encoding        Encoding     Forecasting     Forecasting   Forecasting
2 Preliminaries of the Transformer
                                                                  Figure 1: Taxonomy of Transformers for time series modeling from
2.1 Vanilla Transformer                                           the perspectives of network modiﬁcations and application domains.",2022-02-15 01:43:27+00:00,Transformers in Time Series: A Survey,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']","[arxiv.Result.Author('Qingsong Wen'), arxiv.Result.Author('Tian Zhou'), arxiv.Result.Author('Chaoli Zhang'), arxiv.Result.Author('Weiqi Chen'), arxiv.Result.Author('Ziqing Ma'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Liang Sun')]","Transformers have achieved superior performances in many tasks in natural
language processing and computer vision, which also intrigues great interests
in the time series community. Among multiple advantages of transformers, the
ability to capture long-range dependencies and interactions is especially
attractive for time series modeling, leading to exciting progress in various
time series applications. In this paper, we systematically review transformer
schemes for time series modeling by highlighting their strengths as well as
limitations through a new taxonomy to summarize existing time series
transformers in two perspectives. From the perspective of network
modifications, we summarize the adaptations of module level and architecture
level of the time series transformers. From the perspective of applications, we
categorize time series transformers based on common tasks including
forecasting, anomaly detection, and classification. Empirically, we perform
robust analysis, model size analysis, and seasonal-trend decomposition analysis
to study how Transformers perform in time series. Finally, we discuss and
suggest future directions to provide useful research guidance. A corresponding
resource list that will be continuously updated can be found in the GitHub
repository. To the best of our knowledge, this paper is the first work to
comprehensively and systematically summarize the recent advances of
Transformers for modeling time series data. We hope this survey will ignite
further research interests in time series Transformers."
1979,"We hope this sur-     Encoding     Module            Level                  Detection
vey will ignite further research interests in time series Trans-
formers.","To                    Modifications                              Domains
the best of our knowledge, this is the ﬁrst work to compre-
hensively and systematically review the key developments of       Positional  Attention      Architecture  Forecasting  Anomaly          Classification
Transformers for modeling time series data.","Vanilla    Learnable      Timestamp     Time Series  Spatio-Temporal     Event
                                                                  Encoding    Encoding        Encoding     Forecasting     Forecasting   Forecasting
2 Preliminaries of the Transformer
                                                                  Figure 1: Taxonomy of Transformers for time series modeling from
2.1 Vanilla Transformer                                           the perspectives of network modiﬁcations and application domains.",2022-02-15 01:43:27+00:00,Transformers in Time Series: A Survey,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']","[arxiv.Result.Author('Qingsong Wen'), arxiv.Result.Author('Tian Zhou'), arxiv.Result.Author('Chaoli Zhang'), arxiv.Result.Author('Weiqi Chen'), arxiv.Result.Author('Ziqing Ma'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Liang Sun')]","Transformers have achieved superior performances in many tasks in natural
language processing and computer vision, which also intrigues great interests
in the time series community. Among multiple advantages of transformers, the
ability to capture long-range dependencies and interactions is especially
attractive for time series modeling, leading to exciting progress in various
time series applications. In this paper, we systematically review transformer
schemes for time series modeling by highlighting their strengths as well as
limitations through a new taxonomy to summarize existing time series
transformers in two perspectives. From the perspective of network
modifications, we summarize the adaptations of module level and architecture
level of the time series transformers. From the perspective of applications, we
categorize time series transformers based on common tasks including
forecasting, anomaly detection, and classification. Empirically, we perform
robust analysis, model size analysis, and seasonal-trend decomposition analysis
to study how Transformers perform in time series. Finally, we discuss and
suggest future directions to provide useful research guidance. A corresponding
resource list that will be continuously updated can be found in the GitHub
repository. To the best of our knowledge, this paper is the first work to
comprehensively and systematically summarize the recent advances of
Transformers for modeling time series data. We hope this survey will ignite
further research interests in time series Transformers."
1980,Applications of PINNs in ASO warrant further research.,"PINNs obey the underlying physical lows and thus potentially have the extrapolation
       ability.","Coupling physical mechanisms
       and ML has shown eﬀectiveness in modeling oﬀ-design aerodynamic constraints such as buf-
       feting onset.",2022-02-15 02:23:21+00:00,Machine Learning in Aerodynamic Shape Optimization,cs.LG,"['cs.LG', 'math.OC', 'physics.flu-dyn']","[arxiv.Result.Author('Jichao Li'), arxiv.Result.Author('Xiaosong Du'), arxiv.Result.Author('Joaquim R. R. A. Martins')]","Machine learning (ML) has been increasingly used to aid aerodynamic shape
optimization (ASO), thanks to the availability of aerodynamic data and
continued developments in deep learning. We review the applications of ML in
ASO to date and provide a perspective on the state-of-the-art and future
directions. We first introduce conventional ASO and current challenges. Next,
we introduce ML fundamentals and detail ML algorithms that have been successful
in ASO. Then, we review ML applications to ASO addressing three aspects:
compact geometric design space, fast aerodynamic analysis, and efficient
optimization architecture. In addition to providing a comprehensive summary of
the research, we comment on the practicality and effectiveness of the developed
methods. We show how cutting-edge ML approaches can benefit ASO and address
challenging demands, such as interactive design optimization. Practical
large-scale design optimizations remain a challenge because of the high cost of
ML training. Further research on coupling ML model construction with prior
experience and knowledge, such as physics-informed ML, is recommended to solve
large-scale ASO problems."
1981,"To address the industrial demands in ASO, we recommend further research on the following topics:

    • Extrapolatory prediction models.","Future ML studies in ASO should address an unsolved issue and end with a comparison
with state-of-the-art methods.","Most prediction models used in ASO inherently interpolate
       the data and are thus generally incapable of extrapolation.",2022-02-15 02:23:21+00:00,Machine Learning in Aerodynamic Shape Optimization,cs.LG,"['cs.LG', 'math.OC', 'physics.flu-dyn']","[arxiv.Result.Author('Jichao Li'), arxiv.Result.Author('Xiaosong Du'), arxiv.Result.Author('Joaquim R. R. A. Martins')]","Machine learning (ML) has been increasingly used to aid aerodynamic shape
optimization (ASO), thanks to the availability of aerodynamic data and
continued developments in deep learning. We review the applications of ML in
ASO to date and provide a perspective on the state-of-the-art and future
directions. We first introduce conventional ASO and current challenges. Next,
we introduce ML fundamentals and detail ML algorithms that have been successful
in ASO. Then, we review ML applications to ASO addressing three aspects:
compact geometric design space, fast aerodynamic analysis, and efficient
optimization architecture. In addition to providing a comprehensive summary of
the research, we comment on the practicality and effectiveness of the developed
methods. We show how cutting-edge ML approaches can benefit ASO and address
challenging demands, such as interactive design optimization. Practical
large-scale design optimizations remain a challenge because of the high cost of
ML training. Further research on coupling ML model construction with prior
experience and knowledge, such as physics-informed ML, is recommended to solve
large-scale ASO problems."
2015,"Excitable Neurons And Adversarial Examples
    We further study the relationship between adversarial examples and testing examples generated by

DeepSensor.","20
6.3.1.","The overlap of excitable neurons between DeepSensor and adversarial examples is shown
in Fig.",2022-02-12 16:44:15+00:00,Excitement Surfeited Turns to Errors: Deep Learning Testing Framework Based on Excitable Neurons,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Haibo Jin'), arxiv.Result.Author('Ruoxi Chen'), arxiv.Result.Author('Haibin Zheng'), arxiv.Result.Author('Jinyin Chen'), arxiv.Result.Author('Yao Cheng'), arxiv.Result.Author('Yue Yu'), arxiv.Result.Author('Xianglong Liu')]","Despite impressive capabilities and outstanding performance, deep neural
networks (DNNs) have captured increasing public concern about their security
problems, due to their frequently occurred erroneous behaviors. Therefore, it
is necessary to conduct a systematical testing for DNNs before they are
deployed to real-world applications. Existing testing methods have provided
fine-grained metrics based on neuron coverage and proposed various approaches
to improve such metrics. However, it has been gradually realized that a higher
neuron coverage does \textit{not} necessarily represent better capabilities in
identifying defects that lead to errors. Besides, coverage-guided methods
cannot hunt errors due to faulty training procedure. So the robustness
improvement of DNNs via retraining by these testing examples are
unsatisfactory. To address this challenge, we introduce the concept of
excitable neurons based on Shapley value and design a novel white-box testing
framework for DNNs, namely DeepSensor. It is motivated by our observation that
neurons with larger responsibility towards model loss changes due to small
perturbations are more likely related to incorrect corner cases due to
potential defects. By maximizing the number of excitable neurons concerning
various wrong behaviors of models, DeepSensor can generate testing examples
that effectively trigger more errors due to adversarial inputs, polluted data
and incomplete training. Extensive experiments implemented on both image
classification models and speaker recognition models have demonstrated the
superiority of DeepSensor."
2041,"• Domain Knowledge: While incorporating proper do-
7 Datasets for Evaluation                                                main knowledge can beneﬁt predictive performance,
                                                                         there is still a lack of deep understanding on how to inte-
To promote the further research of graph OOD generalization,             grate domain knowledge to help graph OOD generaliza-
we summarize the existing popular graph datasets for evalua-             tion, e.g., biochemical graphs [Han et al., 2021], knowl-
tion in Table 2.",generalization on graphs.,"There are two groups of datasets, where one             edge graph [Sinha et al., 2020; Li et al., 2022], etc.",2022-02-16 10:59:06+00:00,Out-Of-Distribution Generalization on Graphs: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Ziwei Zhang'), arxiv.Result.Author('Wenwu Zhu')]","Graph machine learning has been extensively studied in both academia and
industry. Although booming with a vast number of emerging methods and
techniques, most of the literature is built on the I.I.D. hypothesis, i.e.,
testing and training graph data are independent and identically distributed.
However, this I.I.D. hypothesis can hardly be satisfied in many real-world
graph scenarios where the model performance substantially degrades when there
exist distribution shifts between testing and training graph data. To solve
this critical problem, out-of-distribution (OOD) generalization on graphs,
which goes beyond the I.I.D. hypothesis, has made great progress and attracted
ever-increasing attention from the research community. In this paper, we
comprehensively survey OOD generalization on graphs and present a detailed
review of recent advances in this area. First, we provide a formal problem
definition of OOD generalization on graphs. Second, we categorize existing
methods into three classes from conceptually different perspectives, i.e.,
data, model, and learning strategy, based on their positions in the graph
machine learning pipeline, followed by detailed discussions for each category.
We also review the theories related to OOD generalization on graphs and
introduce the commonly used graph datasets for thorough evaluations. Last but
not least, we share our insights on future research directions. This paper is
the first systematic and comprehensive review of OOD generalization on graphs,
to the best of our knowledge."
2049,"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
                                        Task                                                    Task                                We further study the impact of buffer sizes on the perfor-
                                                                                                                                 mance of BN Tricks.","The low accuracy of iCaRL on latest
                                                                                                                                 tasks makes it seem to forget less when calculating BWT.","We conduct experiments of BN Tricks
Figure 6.",2022-02-16 12:38:43+00:00,Diagnosing Batch Normalization in Class Incremental Learning,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Minghao Zhou'), arxiv.Result.Author('Quanziang Wang'), arxiv.Result.Author('Jun Shu'), arxiv.Result.Author('Qian Zhao'), arxiv.Result.Author('Deyu Meng')]","Extensive researches have applied deep neural networks (DNNs) in class
incremental learning (Class-IL). As building blocks of DNNs, batch
normalization (BN) standardizes intermediate feature maps and has been widely
validated to improve training stability and convergence. However, we claim that
the direct use of standard BN in Class-IL models is harmful to both the
representation learning and the classifier training, thus exacerbating
catastrophic forgetting. In this paper we investigate the influence of BN on
Class-IL models by illustrating such BN dilemma. We further propose BN Tricks
to address the issue by training a better feature extractor while eliminating
classification bias. Without inviting extra hyperparameters, we apply BN Tricks
to three baseline rehearsal-based methods, ER, DER++ and iCaRL. Through
comprehensive experiments conducted on benchmark datasets of Seq-CIFAR-10,
Seq-CIFAR-100 and Seq-Tiny-ImageNet, we show that BN Tricks can bring
significant performance gains to all adopted baselines, revealing its potential
generality along this line of research."
2053,"Moreover, the trade-off effects

                                                          14
Published as a conference paper at ICLR 2022

also reﬂect the importance of homophily to the performance of node classiﬁcations and the utility
of homophily unnoticeability, where we believe future theoretical works can further study this phe-
nomenon and reveal the underlying causality for node classiﬁcation or even more other downstream
tasks.","Thus, it calls for more tailored optimization methods
to solve for better injection matrix and node features in the future.","Thus, we can develop more robust and trustworthy neural graph models that do not depend
on spurious correlations to perform the task.",2022-02-16 13:41:39+00:00,Understanding and Improving Graph Injection Attack by Promoting Unnoticeability,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Yongqiang Chen'), arxiv.Result.Author('Han Yang'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Kaili Ma'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('James Cheng')]","Recently Graph Injection Attack (GIA) emerges as a practical attack scenario
on Graph Neural Networks (GNNs), where the adversary can merely inject few
malicious nodes instead of modifying existing nodes or edges, i.e., Graph
Modification Attack (GMA). Although GIA has achieved promising results, little
is known about why it is successful and whether there is any pitfall behind the
success. To understand the power of GIA, we compare it with GMA and find that
GIA can be provably more harmful than GMA due to its relatively high
flexibility. However, the high flexibility will also lead to great damage to
the homophily distribution of the original graph, i.e., similarity among
neighbors. Consequently, the threats of GIA can be easily alleviated or even
prevented by homophily-based defenses designed to recover the original
homophily. To mitigate the issue, we introduce a novel constraint -- homophily
unnoticeability that enforces GIA to preserve the homophily, and propose
Harmonious Adversarial Objective (HAO) to instantiate it. Extensive experiments
verify that GIA with HAO can break homophily-based defenses and outperform
previous GIA attacks by a significant margin. We believe our methods can serve
for a more reliable evaluation of the robustness of GNNs."
2054,"Moreover, the trade-off effects

                                                          14
Published as a conference paper at ICLR 2022

also reﬂect the importance of homophily to the performance of node classiﬁcations and the utility
of homophily unnoticeability, where we believe future theoretical works can further study this phe-
nomenon and reveal the underlying causality for node classiﬁcation or even more other downstream
tasks.","Thus, it calls for more tailored optimization methods
to solve for better injection matrix and node features in the future.","Thus, we can develop more robust and trustworthy neural graph models that do not depend
on spurious correlations to perform the task.",2022-02-16 13:41:39+00:00,Understanding and Improving Graph Injection Attack by Promoting Unnoticeability,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Yongqiang Chen'), arxiv.Result.Author('Han Yang'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Kaili Ma'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('James Cheng')]","Recently Graph Injection Attack (GIA) emerges as a practical attack scenario
on Graph Neural Networks (GNNs), where the adversary can merely inject few
malicious nodes instead of modifying existing nodes or edges, i.e., Graph
Modification Attack (GMA). Although GIA has achieved promising results, little
is known about why it is successful and whether there is any pitfall behind the
success. To understand the power of GIA, we compare it with GMA and find that
GIA can be provably more harmful than GMA due to its relatively high
flexibility. However, the high flexibility will also lead to great damage to
the homophily distribution of the original graph, i.e., similarity among
neighbors. Consequently, the threats of GIA can be easily alleviated or even
prevented by homophily-based defenses designed to recover the original
homophily. To mitigate the issue, we introduce a novel constraint -- homophily
unnoticeability that enforces GIA to preserve the homophily, and propose
Harmonious Adversarial Objective (HAO) to instantiate it. Extensive experiments
verify that GIA with HAO can break homophily-based defenses and outperform
previous GIA attacks by a significant margin. We believe our methods can serve
for a more reliable evaluation of the robustness of GNNs."
2067,"Alongside the proposed solution, further discussion on such worse result in cross-test experiment pointed
out the possible pitfalls & provided suggestions that need to be taken care of in further research.","This indicates
that proﬁling of mutual interaction features from untrained subject-pairs protrudes more complicated channel metrics
diversity.","To develop an overall
software framework & visually depict the classiﬁcations, the study eventually developed a graphical user interface
(GUI) executable (not standalone) software designed using PyQt5 [37] GUI app builder Python module.",2022-02-16 15:40:52+00:00,A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']","[arxiv.Result.Author('Md. Mohi Uddin Khan'), arxiv.Result.Author('Abdullah Bin Shams'), arxiv.Result.Author('Md. Mohsin Sarker Raihan')]","Recent advances in 5G wireless technology and socioeconomic transformation
have brought a paradigm shift in sensor applications. Wi-Fi signal demonstrates
a strong correlation between its temporal variation and body movements, which
can be leveraged to recognize human activity. In this article, we demonstrate
the cognitive ability of device free mutual human-to-human interaction
recognition method based on the time scale Wi-Fi channel state information. The
mutual activities examined are steady-state, approaching, departing,
handshaking, high-five, hugging, kicking (left-leg), kicking (right-leg),
pointing (left-hand), pointing (right-hand), punching(left-hand), punching
(right-hand), and pushing. We explore and propose a Self-Attention furnished
Bidirectional Gated Recurrent Neural Network model to classify the 13
interaction types from the time-series data through automated temporal feature
extraction. Our proposed model can recognize a two subject pair mutual
interaction with a maximum benchmark accuracy of 94%. This has been expanded
for ten subject pairs, which secured a benchmark accuracy of 88% with improved
classification around the interaction-transition region. Also, an executable
graphical user interface (GUI) is developed, using the PyQt5 python module, to
subsequently display the overall mutual human-interaction recognition procedure
in real-time. Finally, we conclude with a brief discourse regarding the
possible solutions to the handicaps that resulted in curtailments observed
during the study. Such, Wi-Fi channel perturbation pattern analysis is believed
to be an efficient, economical & privacy-friendly approach to be potentially
utilized in mutual human-interaction recognition for indoor activity
monitoring, surveillance system, smart health monitoring systems & independent
assisted living."
2068,"Alongside the proposed solution, further discussion on such worse result in cross-test experiment pointed out the
     possible pitfalls and provided suggestions that need to be taken care of in further research.",vi.,vii.,2022-02-16 15:40:52+00:00,A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP', 'stat.ML']","[arxiv.Result.Author('Md. Mohi Uddin Khan'), arxiv.Result.Author('Abdullah Bin Shams'), arxiv.Result.Author('Md. Mohsin Sarker Raihan')]","Recent advances in 5G wireless technology and socioeconomic transformation
have brought a paradigm shift in sensor applications. Wi-Fi signal demonstrates
a strong correlation between its temporal variation and body movements, which
can be leveraged to recognize human activity. In this article, we demonstrate
the cognitive ability of device free mutual human-to-human interaction
recognition method based on the time scale Wi-Fi channel state information. The
mutual activities examined are steady-state, approaching, departing,
handshaking, high-five, hugging, kicking (left-leg), kicking (right-leg),
pointing (left-hand), pointing (right-hand), punching(left-hand), punching
(right-hand), and pushing. We explore and propose a Self-Attention furnished
Bidirectional Gated Recurrent Neural Network model to classify 13
human-to-human mutual interaction types from the time-series data. Our proposed
model can recognize a two subject pair mutual interaction with a maximum
benchmark accuracy of 94%. This has been expanded for ten subject pairs, which
secured a benchmark accuracy of 88% with improved classification around the
interaction-transition region. Also, an executable graphical user interface
(GUI) is developed, using the PyQt5 python module, to subsequently display the
overall mutual human-interaction recognition procedure in real-time. Finally,
we conclude with a brief discourse regarding the possible solutions to the
handicaps that resulted in curtailments observed during the study. Such, Wi-Fi
channel perturbation pattern analysis is believed to be an efficient,
economical and privacy-friendly approach to be potentially utilized in mutual
human-interaction recognition for indoor activity monitoring, surveillance
system, smart health monitoring systems and independent assisted living."
2070,"This paper is organized as follows: Section II describes the research method, Section III examines the ﬁndings, and
Section IV presents our ﬁnal considerations and suggestions for further research.",This study aims to examine existing knowledge on bias and unfairness in machine learning (ML) models.,"2 Method

A Systematic Literature Review (SLR) aims to consolidate research by bringing together elements for understanding
it [4].",2022-02-16 16:27:00+00:00,Bias and unfairness in machine learning models: a systematic literature review,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tiago Palma Pagano'), arxiv.Result.Author('Rafael Bessa Loureiro'), arxiv.Result.Author('Maira Matos Araujo'), arxiv.Result.Author('Fernanda Vitoria Nascimento Lisboa'), arxiv.Result.Author('Rodrigo Matos Peixoto'), arxiv.Result.Author('Guilherme Aragao de Sousa Guimaraes'), arxiv.Result.Author('Lucas Lisboa dos Santos'), arxiv.Result.Author('Gustavo Oliveira Ramos Cruz'), arxiv.Result.Author('Ewerton Lopes Silva de Oliveira'), arxiv.Result.Author('Marco Cruz'), arxiv.Result.Author('Ingrid Winkler'), arxiv.Result.Author('Erick Giovani Sperandio Nascimento')]","One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context."
2071,"As opportunities for future work, we conclude that further research is required to identify the techniques and metrics
that should be employed in each particular case in order to standardize and assure fairness in machine learning models.","The need for transparency
and explainability of ML algorithms, as well as the deﬁning and preservation of sensitive attributes was also emphasized,
with the selected datasets acting as a basis for research addressing the identiﬁcation and mitigation of bias and unfairness.","For a deﬁnition on which metric should be used for each use case, more speciﬁc studies should be conducted under
different architectures and sensitive attributes.",2022-02-16 16:27:00+00:00,Bias and unfairness in machine learning models: a systematic literature review,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tiago Palma Pagano'), arxiv.Result.Author('Rafael Bessa Loureiro'), arxiv.Result.Author('Maira Matos Araujo'), arxiv.Result.Author('Fernanda Vitoria Nascimento Lisboa'), arxiv.Result.Author('Rodrigo Matos Peixoto'), arxiv.Result.Author('Guilherme Aragao de Sousa Guimaraes'), arxiv.Result.Author('Lucas Lisboa dos Santos'), arxiv.Result.Author('Gustavo Oliveira Ramos Cruz'), arxiv.Result.Author('Ewerton Lopes Silva de Oliveira'), arxiv.Result.Author('Marco Cruz'), arxiv.Result.Author('Ingrid Winkler'), arxiv.Result.Author('Erick Giovani Sperandio Nascimento')]","One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context."
2072,"This paper is organized as follows: Section II describes the research method, Section III examines the ﬁndings, and
Section IV presents our ﬁnal considerations and suggestions for further research.",This study aims to examine existing knowledge on bias and unfairness in machine learning (ML) models.,"2 Method

A Systematic Literature Review (SLR) aims to consolidate research by bringing together elements for understanding
it [4].",2022-02-16 16:27:00+00:00,Bias and unfairness in machine learning models: a systematic literature review,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tiago Palma Pagano'), arxiv.Result.Author('Rafael Bessa Loureiro'), arxiv.Result.Author('Maira Matos Araujo'), arxiv.Result.Author('Fernanda Vitoria Nascimento Lisboa'), arxiv.Result.Author('Rodrigo Matos Peixoto'), arxiv.Result.Author('Guilherme Aragao de Sousa Guimaraes'), arxiv.Result.Author('Lucas Lisboa dos Santos'), arxiv.Result.Author('Gustavo Oliveira Ramos Cruz'), arxiv.Result.Author('Ewerton Lopes Silva de Oliveira'), arxiv.Result.Author('Marco Cruz'), arxiv.Result.Author('Ingrid Winkler'), arxiv.Result.Author('Erick Giovani Sperandio Nascimento')]","One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context."
2073,"As opportunities for future work, we conclude that further research is required to identify the techniques and metrics
that should be employed in each particular case in order to standardize and assure fairness in machine learning models.","The need for transparency
and explainability of ML algorithms, as well as the deﬁning and preservation of sensitive attributes was also emphasized,
with the selected datasets acting as a basis for research addressing the identiﬁcation and mitigation of bias and unfairness.","For a deﬁnition on which metric should be used for each use case, more speciﬁc studies should be conducted under
different architectures and sensitive attributes.",2022-02-16 16:27:00+00:00,Bias and unfairness in machine learning models: a systematic literature review,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tiago Palma Pagano'), arxiv.Result.Author('Rafael Bessa Loureiro'), arxiv.Result.Author('Maira Matos Araujo'), arxiv.Result.Author('Fernanda Vitoria Nascimento Lisboa'), arxiv.Result.Author('Rodrigo Matos Peixoto'), arxiv.Result.Author('Guilherme Aragao de Sousa Guimaraes'), arxiv.Result.Author('Lucas Lisboa dos Santos'), arxiv.Result.Author('Gustavo Oliveira Ramos Cruz'), arxiv.Result.Author('Ewerton Lopes Silva de Oliveira'), arxiv.Result.Author('Marco Cruz'), arxiv.Result.Author('Ingrid Winkler'), arxiv.Result.Author('Erick Giovani Sperandio Nascimento')]","One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context."
2074,"Section IV presents our ﬁnal considerations and suggestions
for further research.","This paper is organized as follows: Section II describes the research method and the advantages of using RSL, Section
III examines the results and addresses elements such as the Types of Bias, the Identiﬁed Datasets with the main
problems for identifying and reducing bias and unfairness, the Justice Metrics for measuring the models bias and
unfairness in different ways, and from the identiﬁcation of bias it is possible to approach the Techniques and models for
bias and unfairness Mitigation, either by manipulating the data (pre-processing), the model itself (in-processing) or
the prediction (post-processing), some techniques mainly of in-processing promote the identiﬁcation of the sensitive
attribute, important to train models independent of them.","2 Method

A Systematic Literature Review (SLR) aims to consolidate research by bringing together elements for understanding it
[4].",2022-02-16 16:27:00+00:00,Bias and unfairness in machine learning models: a systematic literature review,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tiago Palma Pagano'), arxiv.Result.Author('Rafael Bessa Loureiro'), arxiv.Result.Author('Fernanda Vitoria Nascimento Lisboa'), arxiv.Result.Author('Gustavo Oliveira Ramos Cruz'), arxiv.Result.Author('Rodrigo Matos Peixoto'), arxiv.Result.Author('Guilherme Aragao de Sousa Guimaraes'), arxiv.Result.Author('Ewerton Lopes Silva de Oliveira'), arxiv.Result.Author('Ingrid Winkler'), arxiv.Result.Author('Erick Giovani Sperandio Nascimento')]","One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context."
2075,"Section IV presents our ﬁnal considerations and suggestions
for further research.","This paper is organized as follows: Section II describes the research method and the advantages of using RSL, Section
III examines the results and addresses elements such as the Types of Bias, the Identiﬁed Datasets with the main
problems for identifying and reducing bias and unfairness, the Justice Metrics for measuring the models bias and
unfairness in different ways, and from the identiﬁcation of bias it is possible to approach the Techniques and models for
bias and unfairness Mitigation, either by manipulating the data (pre-processing), the model itself (in-processing) or
the prediction (post-processing), some techniques mainly of in-processing promote the identiﬁcation of the sensitive
attribute, important to train models independent of them.","2 Method

A Systematic Literature Review (SLR) aims to consolidate research by bringing together elements for understanding it
[4].",2022-02-16 16:27:00+00:00,Bias and unfairness in machine learning models: a systematic literature review,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tiago Palma Pagano'), arxiv.Result.Author('Rafael Bessa Loureiro'), arxiv.Result.Author('Fernanda Vitória Nascimento Lisboa'), arxiv.Result.Author('Gustavo Oliveira Ramos Cruz'), arxiv.Result.Author('Rodrigo Matos Peixoto'), arxiv.Result.Author('Guilherme Aragão de Sousa Guimarães'), arxiv.Result.Author('Lucas Lisboa dos Santos'), arxiv.Result.Author('Maira Matos Araujo'), arxiv.Result.Author('Marco Cruz'), arxiv.Result.Author('Ewerton Lopes Silva de Oliveira'), arxiv.Result.Author('Ingrid Winkler'), arxiv.Result.Author('Erick Giovani Sperandio Nascimento')]","One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context."
2080,"Observations made in the context of this study raise potential opportunities
for further research.","ALE is mostly unstable and
     aﬀected by its inability to accurately analyse eﬀects of changes in categorical
     attributes on predictions generated.","Setting criteria for evaluating XAI methods is a demanding
need to ensure acceptance and trustworthiness of XAI methods themselves.",2022-02-16 15:31:59+00:00,XAI in the context of Predictive Process Monitoring: Too much to Reveal,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ghada Elkhawaga'), arxiv.Result.Author('Mervat Abuelkheir'), arxiv.Result.Author('Manfred Reichert')]","Predictive Process Monitoring (PPM) has been integrated into process mining
tools as a value-adding task. PPM provides useful predictions on the further
execution of the running business processes. To this end, machine
learning-based techniques are widely employed in the context of PPM. In order
to gain stakeholders trust and advocacy of PPM predictions, eXplainable
Artificial Intelligence (XAI) methods are employed in order to compensate for
the lack of transparency of most efficient predictive models. Even when
employed under the same settings regarding data, preprocessing techniques, and
ML models, explanations generated by multiple XAI methods differ profoundly. A
comparison is missing to distinguish XAI characteristics or underlying
conditions that are deterministic to an explanation. To address this gap, we
provide a framework to enable studying the effect of different PPM-related
settings and ML model-related choices on characteristics and expressiveness of
resulting explanations. In addition, we compare how different explainability
methods characteristics can shape resulting explanations and enable reflecting
underlying model reasoning process"
2105,"This result advances the understanding of FL and may
                                                                                                                      be useful to guide further research in this area.",puting.,1.,2022-02-17 02:01:37+00:00,Federated Stochastic Gradient Descent Begets Self-Induced Momentum,cs.LG,['cs.LG'],"[arxiv.Result.Author('Howard H. Yang'), arxiv.Result.Author('Zuozhu Liu'), arxiv.Result.Author('Yaru Fu'), arxiv.Result.Author('Tony Q. S. Quek'), arxiv.Result.Author('H. Vincent Poor')]","Federated learning (FL) is an emerging machine learning method that can be
applied in mobile edge systems, in which a server and a host of clients
collaboratively train a statistical model utilizing the data and computation
resources of the clients without directly exposing their privacy-sensitive
data. We show that running stochastic gradient descent (SGD) in such a setting
can be viewed as adding a momentum-like term to the global aggregation process.
Based on this finding, we further analyze the convergence rate of a federated
learning system by accounting for the effects of parameter staleness and
communication resources. These results advance the understanding of the
Federated SGD algorithm, and also forges a link between staleness analysis and
federated computing systems, which can be useful for systems designers."
2106,"The developed framework reveals a link
    By taking (17) and (18) back into (16) and telescoping t     between staleness analysis and FL convergence rate, and may
from 0 to T − 1, we have                                         be useful for further research in this area.","(18)  function and hence is applicable to even the setting of deep
                                                                 learning systems.","T −1            1A few simulation examples that corroborate these observations
                                                                 are available in: https://person.zju.edu.cn/person/attachments/2022-01/01-
E[f (wT −1)] − E[f (w0)] ≤ L η2E ∇f (wt) 2                       1641711371-850767.pdf

                        t=0

T −1                    T −1

+ Lη2σ2 − 1−(1−µ)β η E ∇f (wt) 2 .",2022-02-17 02:01:37+00:00,Federated Stochastic Gradient Descent Begets Self-Induced Momentum,cs.LG,['cs.LG'],"[arxiv.Result.Author('Howard H. Yang'), arxiv.Result.Author('Zuozhu Liu'), arxiv.Result.Author('Yaru Fu'), arxiv.Result.Author('Tony Q. S. Quek'), arxiv.Result.Author('H. Vincent Poor')]","Federated learning (FL) is an emerging machine learning method that can be
applied in mobile edge systems, in which a server and a host of clients
collaboratively train a statistical model utilizing the data and computation
resources of the clients without directly exposing their privacy-sensitive
data. We show that running stochastic gradient descent (SGD) in such a setting
can be viewed as adding a momentum-like term to the global aggregation process.
Based on this finding, we further analyze the convergence rate of a federated
learning system by accounting for the effects of parameter staleness and
communication resources. These results advance the understanding of the
Federated SGD algorithm, and also forges a link between staleness analysis and
federated computing systems, which can be useful for systems designers."
2108,"To further study these datasets, we plot in
Figure 7 the correlation matrix of all k benchmark methods’ nCRPS ranks across the datasets (using only the

    6XGBoost does not provide a working version of listwise ranking.","Figure 1 showed that for 14 out of the 44 benchmark datasets, the performance of classical methods is in-
distinguishable from the performance of deep learning methods.","24
Algorithm 1: Overview of PARETOSELECT

Data: Time series models X , oﬄine evaluations D, number of default models n
Result: A set {x1, .",2022-02-17 07:40:15+00:00,Multi-Objective Model Selection for Time Series Forecasting,cs.LG,['cs.LG'],"[arxiv.Result.Author('Oliver Borchert'), arxiv.Result.Author('David Salinas'), arxiv.Result.Author('Valentin Flunkert'), arxiv.Result.Author('Tim Januschowski'), arxiv.Result.Author('Stephan Günnemann')]","Research on time series forecasting has predominantly focused on developing
methods that improve accuracy. However, other criteria such as training time or
latency are critical in many real-world applications. We therefore address the
question of how to choose an appropriate forecasting model for a given dataset
among the plethora of available forecasting methods when accuracy is only one
of many criteria. For this, our contributions are two-fold. First, we present a
comprehensive benchmark, evaluating 7 classical and 6 deep learning forecasting
methods on 44 heterogeneous, publicly available datasets. The benchmark code is
open-sourced along with evaluations and forecasts for all methods. These
evaluations enable us to answer open questions such as the amount of data
required for deep learning models to outperform classical ones. Second, we
leverage the benchmark evaluations to learn good defaults that consider
multiple objectives such as accuracy and latency. By learning a mapping from
forecasting models to performance metrics, we show that our method PARETOSELECT
is able to accurately select models from the Pareto front -- alleviating the
need to train or evaluate many forecasting models for model selection. To the
best of our knowledge, PARETOSELECT constitutes the first method to learn
default models in a multi-objective setting."
2127,"We believe that our work helps understand the vulner-
abilities of such algorithms, and will motivate further research in the ethics and security of machine
learning.","Such algorithms must not be regarded as ethical, even if
they were designed with the best intentions.","References

[AMW+21]  Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Gio-
          vanni Vigna.",2022-02-17 10:53:52+00:00,An Equivalence Between Data Poisoning and Byzantine Gradient Attacks,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Sadegh Farhadkhani'), arxiv.Result.Author('Rachid Guerraoui'), arxiv.Result.Author('Lê-Nguyên Hoang'), arxiv.Result.Author('Oscar Villemaud')]","To study the resilience of distributed learning, the ""Byzantine"" literature
considers a strong threat model where workers can report arbitrary gradients to
the parameter server. Whereas this model helped obtain several fundamental
results, it has sometimes been considered unrealistic, when the workers are
mostly trustworthy machines. In this paper, we show a surprising equivalence
between this model and data poisoning, a threat considered much more realistic.
More specifically, we prove that every gradient attack can be reduced to data
poisoning, in any personalized federated learning system with PAC guarantees
(which we show are both desirable and realistic). This equivalence makes it
possible to obtain new impossibility results on the resilience to data
poisoning as corollaries of existing impossibility theorems on Byzantine
machine learning. Moreover, using our equivalence, we derive a practical attack
that we show (theoretically and empirically) can be very effective against
classical personalized federated learning models."
2133,"We provide a profiling library, PRESTO, that helps
and configure a preprocessing pipeline, we want to highlight some         with detecting bottlenecks and automatically decide which prepro-
settings which could benefit from further research.","8 CONCLUSIONS

7 DISCUSSION                                                              This paper presents an analysis of seven concrete DL pipelines based
                                                                          on their typical preprocessing steps from CV, NLP, NILM, and the
While our analysis provides some key insights about how to profile        Audio domain.",cessing strategy is the most efficient based on an objective function.,2022-02-17 14:31:58+00:00,Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines,cs.LG,"['cs.LG', 'cs.DC', 'cs.PF', 'I.4.0; I.4.2; I.2.0; B.4.4; C.4; D.2.8']","[arxiv.Result.Author('Alexander Isenko'), arxiv.Result.Author('Ruben Mayer'), arxiv.Result.Author('Jeffrey Jedele'), arxiv.Result.Author('Hans-Arno Jacobsen')]","Preprocessing pipelines in deep learning aim to provide sufficient data
throughput to keep the training processes busy. Maximizing resource utilization
is becoming more challenging as the throughput of training processes increases
with hardware innovations (e.g., faster GPUs, TPUs, and inter-connects) and
advanced parallelization techniques that yield better scalability. At the same
time, the amount of training data needed in order to train increasingly complex
models is growing. As a consequence of this development, data preprocessing and
provisioning are becoming a severe bottleneck in end-to-end deep learning
pipelines.
  In this paper, we provide an in-depth analysis of data preprocessing
pipelines from four different machine learning domains. We introduce a new
perspective on efficiently preparing datasets for end-to-end deep learning
pipelines and extract individual trade-offs to optimize throughput,
preprocessing time, and storage consumption. Additionally, we provide an
open-source profiling library that can automatically decide on a suitable
preprocessing strategy to maximize throughput. By applying our generated
insights to real-world use-cases, we obtain an increased throughput of 3x to
13x compared to an untuned system while keeping the pipeline functionally
identical. These findings show the enormous potential of data pipeline tuning."
2134,"We provide a profiling library, PRESTO, that helps
and configure a preprocessing pipeline, we want to highlight some         with detecting bottlenecks and automatically decide which prepro-
settings which could benefit from further research.","8 CONCLUSIONS

7 DISCUSSION                                                              This paper presents an analysis of seven concrete DL pipelines based
                                                                          on their typical preprocessing steps from CV, NLP, NILM, and the
While our analysis provides some key insights about how to profile        Audio domain.",cessing strategy is the most efficient based on an objective function.,2022-02-17 14:31:58+00:00,Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines,cs.LG,"['cs.LG', 'cs.DC', 'cs.PF', 'I.4.0; I.4.2; I.2.0; B.4.4; C.4; D.2.8']","[arxiv.Result.Author('Alexander Isenko'), arxiv.Result.Author('Ruben Mayer'), arxiv.Result.Author('Jeffrey Jedele'), arxiv.Result.Author('Hans-Arno Jacobsen')]","Preprocessing pipelines in deep learning aim to provide sufficient data
throughput to keep the training processes busy. Maximizing resource utilization
is becoming more challenging as the throughput of training processes increases
with hardware innovations (e.g., faster GPUs, TPUs, and inter-connects) and
advanced parallelization techniques that yield better scalability. At the same
time, the amount of training data needed in order to train increasingly complex
models is growing. As a consequence of this development, data preprocessing and
provisioning are becoming a severe bottleneck in end-to-end deep learning
pipelines.
  In this paper, we provide an in-depth analysis of data preprocessing
pipelines from four different machine learning domains. We introduce a new
perspective on efficiently preparing datasets for end-to-end deep learning
pipelines and extract individual trade-offs to optimize throughput,
preprocessing time, and storage consumption. Additionally, we provide an
open-source profiling library that can automatically decide on a suitable
preprocessing strategy to maximize throughput. By applying our generated
insights to real-world use-cases, we obtain an increased throughput of 3x to
13x compared to an untuned system while keeping the pipeline functionally
identical. These findings show the enormous potential of data pipeline tuning."
2135,"We provide a profiling library, PRESTO, that helps
and configure a preprocessing pipeline, we want to highlight some          with detecting bottlenecks and automatically decide which prepro-
settings which could benefit from further research.","8 CONCLUSIONS

7 DISCUSSION                                                               This paper presents an analysis of seven concrete DL pipelines based
                                                                           on their typical preprocessing steps from CV, NLP, NILM, and the
While our analysis provides some key insights about how to profile         Audio domain.",cessing strategy is the most efficient based on an objective function.,2022-02-17 14:31:58+00:00,Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines,cs.LG,"['cs.LG', 'cs.DC', 'cs.PF', 'I.4.0; I.4.2; I.2.0; B.4.4; C.4; D.2.8']","[arxiv.Result.Author('Alexander Isenko'), arxiv.Result.Author('Ruben Mayer'), arxiv.Result.Author('Jeffrey Jedele'), arxiv.Result.Author('Hans-Arno Jacobsen')]","Preprocessing pipelines in deep learning aim to provide sufficient data
throughput to keep the training processes busy. Maximizing resource utilization
is becoming more challenging as the throughput of training processes increases
with hardware innovations (e.g., faster GPUs, TPUs, and inter-connects) and
advanced parallelization techniques that yield better scalability. At the same
time, the amount of training data needed in order to train increasingly complex
models is growing. As a consequence of this development, data preprocessing and
provisioning are becoming a severe bottleneck in end-to-end deep learning
pipelines.
  In this paper, we provide an in-depth analysis of data preprocessing
pipelines from four different machine learning domains. We introduce a new
perspective on efficiently preparing datasets for end-to-end deep learning
pipelines and extract individual trade-offs to optimize throughput,
preprocessing time, and storage consumption. Additionally, we provide an
open-source profiling library that can automatically decide on a suitable
preprocessing strategy to maximize throughput. By applying our generated
insights to real-world use-cases, we obtain an increased throughput of 3x to
13x compared to an untuned system while keeping the pipeline functionally
identical. These findings show the enormous potential of data pipeline tuning."
2153,"Beyond Image-to-Image Translation: In this paper, we                 We hope that this paper paves the way for further research
employ Image2Image translation methods to design an OOD              that prioritizes OOD generalization in building trustworthy
generalization method that serves as a defense against adver-        ML models for high-stakes applications.",produce IID adversarial inputs.,sarial examples in the image classiﬁcation domain.,2022-02-18 00:17:23+00:00,Rethinking Machine Learning Robustness via its Link with the Out-of-Distribution Problem,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Abderrahmen Amich'), arxiv.Result.Author('Birhanu Eshete')]","Despite multiple efforts made towards robust machine learning (ML) models,
their vulnerability to adversarial examples remains a challenging problem that
calls for rethinking the defense strategy. In this paper, we take a step back
and investigate the causes behind ML models' susceptibility to adversarial
examples. In particular, we focus on exploring the cause-effect link between
adversarial examples and the out-of-distribution (OOD) problem. To that end, we
propose an OOD generalization method that stands against both adversary-induced
and natural distribution shifts. Through an OOD to in-distribution mapping
intuition, our approach translates OOD inputs to the data distribution used to
train and test the model. Through extensive experiments on three benchmark
image datasets of different scales (MNIST, CIFAR10, and ImageNet) and by
leveraging image-to-image translation methods, we confirm that the adversarial
examples problem is a special case of the wider OOD generalization problem.
Across all datasets, we show that our translation-based approach consistently
improves robustness to OOD adversarial inputs and outperforms state-of-the-art
defenses by a significant margin, while preserving the exact accuracy on benign
(in-distribution) data. Furthermore, our method generalizes on naturally OOD
inputs such as darker or sharper images"
2159,"We hope that this work will spark further research into the generalization bounds discussed in this
paper.","Furthermore, we characterize and empirically demonstrate that
when sufﬁciently deep networks are trained, they converge to the same effective depth, implying
that our bound does not worsen as the depth increases.","It would be interesting to see if these bounds could be improved by replacing the effective
depth as deﬁned in this paper with a different notion of complexity.",2022-02-18 05:21:28+00:00,On the Implicit Bias Towards Minimal Depth of Deep Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tomer Galanti'), arxiv.Result.Author('Liane Galanti'), arxiv.Result.Author('Ido Ben-Shaul')]","Recent results in the literature suggest that the penultimate
(second-to-last) layer representations of neural networks that are trained for
classification exhibit a clustering property called neural collapse (NC). We
study the implicit bias of stochastic gradient descent (SGD) in favor of
low-depth solutions when training deep neural networks. We characterize a
notion of effective depth that measures the first layer for which sample
embeddings are separable using the nearest-class center classifier.
Furthermore, we hypothesize and empirically show that SGD implicitly selects
neural networks of small effective depths.
  Secondly, while neural collapse emerges even when generalization should be
impossible - we argue that the \emph{degree of separability} in the
intermediate layers is related to generalization. We derive a generalization
bound based on comparing the effective depth of the network with the minimal
depth required to fit the same dataset with partially corrupted labels.
Remarkably, this bound provides non-trivial estimations of the test
performance. Finally, we empirically show that the effective depth of a trained
neural network monotonically increases when increasing the number of random
labels in data."
2160,"For others, substantial further research
is required.","In some cases, we know how to complete the extensions that
are mentioned, but doing so would lengthen or complicate the paper.",Policy learning.,2022-02-18 06:09:04+00:00,Adaptivity and Confounding in Multi-Armed Bandit Experiments,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Chao Qin'), arxiv.Result.Author('Daniel Russo')]","We explore a new model of bandit experiments where a potentially
nonstationary sequence of contexts influences arms' performance.
Context-unaware algorithms risk confounding while those that perform correct
inference face information delays. Our main insight is that an algorithm we
call deconfounted Thompson sampling strikes a delicate balance between
adaptivity and robustness. Its adaptivity leads to optimal efficiency
properties in easy stationary instances, but it displays surprising resilience
in hard nonstationary ones which cause other adaptive algorithms to fail."
2181,"In this sec-
transformer neural network [Vaswani et al., 2017] together       tion, we suggest three future directions for further research.","As for gen-       cess in the automation of molecule design, challenges still
erating the sequence of decisions, it utilizes a tree-based      exist due to the complexity of molecular structure.","with relative positional encoding for tree generation, and a
attention-based predictor for residual edge prediction.",2022-02-18 14:26:23+00:00,Molecule Generation for Drug Design: a Graph Learning Perspective,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Nianzu Yang'), arxiv.Result.Author('Huaijin Wu'), arxiv.Result.Author('Junchi Yan'), arxiv.Result.Author('Xiaoyong Pan'), arxiv.Result.Author('Ye Yuan'), arxiv.Result.Author('Le Song')]","Machine learning has revolutionized many fields, and graph learning is
recently receiving increasing attention. From the application perspective, one
of the emerging and attractive areas is aiding the design and discovery of
molecules, especially in drug industry. In this survey, we provide an overview
of the state-of-the-art molecule (and mostly for de novo drug) design and
discovery aiding methods whose methodology involves (deep) graph learning.
Specifically, we propose to categorize these methods into three groups: i) all
at once, ii) fragment-based and iii) node-by-node. We further present some
representative public datasets and summarize commonly utilized evaluation
metrics for generation and optimization, respectively. Finally, we discuss
challenges and directions for future research, from the drug design
perspective."
2185,"We hope this work inspires
further research, as many tantalizing open questions remain.","By studying various prediction tasks on HMM and a
conditionally-Gaussian variant (G-HMM), we showed that parameter recovery can be very sensitive to the
interaction between the data model, prediction task, and concomitant predictors.","For instance, how does changing the number of
either predicted or conditioned-on tokens affect identiﬁability?",2022-02-18 17:09:32+00:00,Masked prediction tasks: a parameter identifiability view,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Bingbin Liu'), arxiv.Result.Author('Daniel Hsu'), arxiv.Result.Author('Pradeep Ravikumar'), arxiv.Result.Author('Andrej Risteski')]","The vast majority of work in self-supervised learning, both theoretical and
empirical (though mostly the latter), have largely focused on recovering good
features for downstream tasks, with the definition of ""good"" often being
intricately tied to the downstream task itself. This lens is undoubtedly very
interesting, but suffers from the problem that there isn't a ""canonical"" set of
downstream tasks to focus on -- in practice, this problem is usually resolved
by competing on the benchmark dataset du jour.
  In this paper, we present an alternative lens: one of parameter
identifiability. More precisely, we consider data coming from a parametric
probabilistic model, and train a self-supervised learning predictor with a
suitably chosen parametric form. Then, we ask whether we can read off the
ground truth parameters of the probabilistic model from the optimal predictor.
We focus on the widely used self-supervised learning method of predicting
masked tokens, which is popular for both natural languages and visual data.
  While incarnations of this approach have already been successfully used for
simpler probabilistic models (e.g. learning fully-observed undirected graphical
models), we focus instead on latent-variable models capturing sequential
structures -- namely Hidden Markov Models with both discrete and conditionally
Gaussian observations. We show that there is a rich landscape of possibilities,
out of which some prediction tasks yield identifiability, while others do not.
Our results, borne of a theoretical grounding of self-supervised learning,
could thus potentially beneficially inform practice. Moreover, we uncover close
connections with uniqueness of tensor rank decompositions -- a widely used tool
in studying identifiability through the lens of the method of moments."
2219,"We believe our work can be a stepping-stone for several future works, such as further studying the
interactions between multiple models in continual learning or exploiting the subspace’s region property to
improve the continual learning performance.","Motivated by the limits of the subspaces, we
have developed Subspace-Connectivity that outperforms state-of-the-art algorithms in various benchmarks

                                                                   12
yet enjoys being as computationally cheap as a single model training.","Acknowledgements

We would like to thank (alphabetical order) Mehdi Bennani, Lucas Caccia, Ludovic Denoyer, Timoth´ee
Lesort, Yee Why Teh, Maxime Wabartha, Dong Yin for useful discussions and feedbacks throughout the
work.",2022-02-20 14:30:39+00:00,Efficient Continual Learning Ensembles in Neural Network Subspaces,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thang Doan'), arxiv.Result.Author('Seyed Iman Mirzadeh'), arxiv.Result.Author('Joelle Pineau'), arxiv.Result.Author('Mehrdad Farajtabar')]","A growing body of research in continual learning focuses on the catastrophic
forgetting problem. While many attempts have been made to alleviate this
problem, the majority of the methods assume a single model in the continual
learning setup. In this work, we question this assumption and show that
employing ensemble models can be a simple yet effective method to improve
continual performance. However, the training and inference cost of ensembles
can increase linearly with the number of models. Motivated by this limitation,
we leverage the recent advances in the deep learning optimization literature,
such as mode connectivity and neural network subspaces, to derive a new method
that is both computationally advantageous and can outperform the
state-of-the-art continual learning algorithms."
2220,"In addition, these methods rely on the task identiﬁers     such as further studying the interactions between multiple
for selecting the appropriate module for prediction and often     models in continual learning and designing more efﬁcient
cannot operate without this information.","We believe
ory and compute requirement grows as the number of tasks          our work can be a stepping-stone for several future works,
grows.",ensembling techniques.,2022-02-20 14:30:39+00:00,Continual Learning Beyond a Single Model,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thang Doan'), arxiv.Result.Author('Seyed Iman Mirzadeh'), arxiv.Result.Author('Mehrdad Farajtabar')]","A growing body of research in continual learning focuses on the catastrophic
forgetting problem. While many attempts have been made to alleviate this
problem, the majority of the methods assume a single model in the continual
learning setup. In this work, we question this assumption and show that
employing ensemble models can be a simple yet effective method to improve
continual performance. However, ensembles' training and inference costs can
increase significantly as the number of models grows. Motivated by this
limitation, we study different ensemble models to understand their benefits and
drawbacks in continual learning scenarios. Finally, to overcome the high
compute cost of ensembles, we leverage recent advances in neural network
subspace to propose a computationally cheap algorithm with similar runtime to a
single model yet enjoying the performance benefits of ensembles."
2234,"We then conclude the paper in Section V with
directions for further research.",our ﬁndings.,"Probabilistic generative models based on AE are achieved
                                                                     by replacing the conventional encoder and decoder with
probabilistic variants of them [Kingma and Welling(2013)],                proposed model purely focuses on the reconstruction loss while
[Rezende and Mohamed(2015)], [Higgins et al.",2022-02-20 22:59:13+00:00,Disentangling Autoencoders (DAE),cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jaehoon Cha'), arxiv.Result.Author('Jeyan Thiyagalingam')]","Noting the importance of factorizing or disentangling the latent space, we
propose a novel framework for autoencoders based on the principles of symmetry
transformations in group-theory, which is a non-probabilistic disentangling
autoencoder model. To the best of our knowledge, this is the first model that
is aiming to achieve disentanglement based on autoencoders without
regularizers. The proposed model is compared to seven state-of-the-art
generative models based on autoencoders and evaluated based on reconstruction
loss and five metrics quantifying disentanglement losses. The experiment
results show that the proposed model can have better disentanglement when
variances of each features are different. We believe that this model leads a
new field for disentanglement learning based on autoencoders without
regularizers."
2235,"We then conclude the paper in Section V         in the latent space by minimizing the reconstruction loss,
with directions for further research.","In Section IV, we       maps an observation space to a lower-dimensional latent
evaluate the proposed method against a number of relevant            space, and a decoder Dθ that re-maps the latent space to the
models with a toy example and three benchmark datasets, and          observation space, effectively learn meaningful representations
discuss our ﬁndings.",Lrecon (cross-entropy or L2).,2022-02-20 22:59:13+00:00,Disentangling Autoencoders (DAE),cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jaehoon Cha'), arxiv.Result.Author('Jeyan Thiyagalingam')]","Noting the importance of factorizing (or disentangling) the latent space, we
propose a novel, non-probabilistic disentangling framework for autoencoders,
based on the principles of symmetry transformations in group-theory. To the
best of our knowledge, this is the first deterministic model that is aiming to
achieve disentanglement based on autoencoders without regularizers. The
proposed model is compared to seven state-of-the-art generative models based on
autoencoders and evaluated based on five supervised disentanglement metrics.
The experimental results show that the proposed model can have better
disentanglement when variances of each features are different. We believe that
this model leads to a new field for disentanglement learning based on
autoencoders without regularizers."
2254,"We further study the effective-
boundaries are not available.","In [4], MNIST-360 dataset is proposed to
validate general continual learning setting, where the task       network sparsity with VBS.","We compare our method with          ness of VBS on different layers by operating only on each
ER, MER, GSS, and A-GEM-R. A-GEM-R is a variant of                layer of the network.",2022-02-21 13:25:03+00:00,Learning Bayesian Sparse Networks with Full Experience Replay for Continual Learning,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Dong Gong'), arxiv.Result.Author('Qingsen Yan'), arxiv.Result.Author('Yuhang Liu'), arxiv.Result.Author('Anton van den Hengel'), arxiv.Result.Author('Javen Qinfeng Shi')]","Continual Learning (CL) methods aim to enable machine learning models to
learn new tasks without catastrophic forgetting of those that have been
previously mastered. Existing CL approaches often keep a buffer of
previously-seen samples, perform knowledge distillation, or use regularization
techniques towards this goal. Despite their performance, they still suffer from
interference across tasks which leads to catastrophic forgetting. To ameliorate
this problem, we propose to only activate and select sparse neurons for
learning current and past tasks at any stage. More parameters space and model
capacity can thus be reserved for the future tasks. This minimizes the
interference between parameters for different tasks. To do so, we propose a
Sparse neural Network for Continual Learning (SNCL), which employs variational
Bayesian sparsity priors on the activations of the neurons in all layers. Full
Experience Replay (FER) provides effective supervision in learning the sparse
activations of the neurons in different layers. A loss-aware reservoir-sampling
strategy is developed to maintain the memory buffer. The proposed method is
agnostic as to the network structures and the task boundaries. Experiments on
different datasets show that our approach achieves state-of-the-art performance
for mitigating forgetting."
2264,"For further research, considering
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS                           12

speciﬁc Spatio-temporal information, we will construct differ-                    [18] G. Shen, Z. Zhao, and X. Kong, “Gcn2cdd: A commercial district
ent personal mobility prediction architectures.","When sufﬁcient multi-source sensors data is available, we
                                                                          will attempt to provide ﬁne-grained analysis and service based
                                                                          on human mobility patterns.","discovery framework via embedding space clustering on graph convolu-
                                                                                        tion networks,” IEEE Transactions on Industrial Informatics, 2021, doi:
                               APPENDIX                                                 10.1109/TII.2021.3051934.",2022-02-17 06:17:23+00:00,Exploring Human Mobility for Multi-Pattern Passenger Prediction: A Graph Learning Framework,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Xiangjie Kong'), arxiv.Result.Author('Kailai Wang'), arxiv.Result.Author('Mingliang Hou'), arxiv.Result.Author('Feng Xia'), arxiv.Result.Author('Gour Karmakar'), arxiv.Result.Author('Jianxin Li')]","Traffic flow prediction is an integral part of an intelligent transportation
system and thus fundamental for various traffic-related applications. Buses are
an indispensable way of moving for urban residents with fixed routes and
schedules, which leads to latent travel regularity. However, human mobility
patterns, specifically the complex relationships between bus passengers, are
deeply hidden in this fixed mobility mode. Although many models exist to
predict traffic flow, human mobility patterns have not been well explored in
this regard. To reduce this research gap and learn human mobility knowledge
from this fixed travel behaviors, we propose a multi-pattern passenger flow
prediction framework, MPGCN, based on Graph Convolutional Network (GCN).
Firstly, we construct a novel sharing-stop network to model relationships
between passengers based on bus record data. Then, we employ GCN to extract
features from the graph by learning useful topology information and introduce a
deep clustering method to recognize mobility patterns hidden in bus passengers.
Furthermore, to fully utilize Spatio-temporal information, we propose GCN2Flow
to predict passenger flow based on various mobility patterns. To the best of
our knowledge, this paper is the first work to adopt a multipattern approach to
predict the bus passenger flow from graph learning. We design a case study for
optimizing routes. Extensive experiments upon a real-world bus dataset
demonstrate that MPGCN has potential efficacy in passenger flow prediction and
route optimization."
2281,"Our approach shows that online learning can
be deployed effectively for orchestrating DL inference in end-edge-cloud systems, and opens the
door for further research in online learning for this important and growing area.","Using this method, we observed
up to 35% speedup for average response time while sacrificing less than %0.9 accuracy on a real
end-edge-cloud system when compared to prior art.","REFERENCES

 [1] Md Golam Rabiul Alam, Mohammad Mehedi Hassan, Md ZIa Uddin, Ahmad Almogren, and Giancarlo Fortino.",2022-02-21 21:41:29+00:00,Online Learning for Orchestration of Inference in Multi-User End-Edge-Cloud Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Sina Shahhosseini'), arxiv.Result.Author('Dongjoo Seo'), arxiv.Result.Author('Anil Kanduri'), arxiv.Result.Author('Tianyi Hu'), arxiv.Result.Author('Sung-soo Lim'), arxiv.Result.Author('Bryan Donyanavard'), arxiv.Result.Author('Amir M. Rahmani'), arxiv.Result.Author('Nikil Dutt')]","Deep-learning-based intelligent services have become prevalent in
cyber-physical applications including smart cities and health-care. Deploying
deep-learning-based intelligence near the end-user enhances privacy protection,
responsiveness, and reliability. Resource-constrained end-devices must be
carefully managed in order to meet the latency and energy requirements of
computationally-intensive deep learning services. Collaborative end-edge-cloud
computing for deep learning provides a range of performance and efficiency that
can address application requirements through computation offloading. The
decision to offload computation is a communication-computation co-optimization
problem that varies with both system parameters (e.g., network condition) and
workload characteristics (e.g., inputs). On the other hand, deep learning model
optimization provides another source of tradeoff between latency and model
accuracy. An end-to-end decision-making solution that considers such
computation-communication problem is required to synergistically find the
optimal offloading policy and model for deep learning services. To this end, we
propose a reinforcement-learning-based computation offloading solution that
learns optimal offloading policy considering deep learning model selection
techniques to minimize response time while providing sufficient accuracy. We
demonstrate the effectiveness of our solution for edge devices in an
end-edge-cloud system and evaluate with a real-setup implementation using
multiple AWS and ARM core configurations. Our solution provides 35% speedup in
the average response time compared to the state-of-the-art with less than 0.9%
accuracy reduction, demonstrating the promise of our online learning framework
for orchestrating DL inference in end-edge-cloud systems."
2289,"It is essential to be considered as
open issues and challenges to implementing graph lifelong         further research in addition to the current studies to bring fair
learning, as summarized in the following points:                  comparations and improve graph lifelong learning methods.","Nowadays, benchmarking strategies
   In addition to catastrophic forgetting, there are some other   are highly non-standard.",A.,2022-02-22 06:14:07+00:00,Graph Lifelong Learning: A Survey,cs.LG,"['cs.LG', 'cs.AI', '68T07, 68T05', 'I.2.6']","[arxiv.Result.Author('Falih Gozi Febrinanto'), arxiv.Result.Author('Feng Xia'), arxiv.Result.Author('Kristen Moore'), arxiv.Result.Author('Chandra Thapa'), arxiv.Result.Author('Charu Aggarwal')]","Graph learning is a popular approach for performing machine learning on
graph-structured data. It has revolutionized the machine learning ability to
model graph data to address downstream tasks. Its application is wide due to
the availability of graph data ranging from all types of networks to
information systems. Most graph learning methods assume that the graph is
static and its complete structure is known during training. This limits their
applicability since they cannot be applied to problems where the underlying
graph grows over time and/or new tasks emerge incrementally. Such applications
require a lifelong learning approach that can learn the graph continuously and
accommodate new information whilst retaining previously learned knowledge.
Lifelong learning methods that enable continuous learning in regular domains
like images and text cannot be directly applied to continuously evolving graph
data, due to its irregular structure. As a result, graph lifelong learning is
gaining attention from the research community. This survey paper provides a
comprehensive overview of recent advancements in graph lifelong learning,
including the categorization of existing methods, and the discussions of
potential applications and open research problems."
2301,"works, which further boosted the results obtained on ILSVRC
                                        ImageNet challenge [33], stimulating further research based             This work aims to combine advancements created over the
                                        on this architecture [36], [38], [43].","This has sparked interest
                                        residual connection [13] allowed for training much deeper net-       in recent years in continual learning [29].","Greater depth increases       years in neural network architecture development and life-long
                                        models’ capacity and allows for learning more complicated            learning.",2022-02-22 11:21:41+00:00,Increasing Depth of Neural Networks for Life-long Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jędrzej Kozal'), arxiv.Result.Author('Michał Woźniak')]","Increasing neural network depth is a well-known method for improving neural
network performance. Modern deep architectures contain multiple mechanisms that
allow hundreds or even thousands of layers to train. This work is trying to
answer if extending neural network depth may be beneficial in a life-long
learning setting. In particular, we propose a novel method based on adding new
layers on top of existing ones to enable the forward transfer of knowledge and
adapting previously learned representations for new tasks. We utilize a method
of determining the most similar tasks for selecting the best location in our
network to add new nodes with trainable parameters. This approach allows for
creating a tree-like model, where each node is a set of neural network
parameters dedicated to a specific task. The proposed method is inspired by
Progressive Neural Network (PNN) concept, therefore it is rehearsal-free and
benefits from dynamic change of network structure. However, it requires fewer
parameters per task than PNN. Experiments on Permuted MNIST and SplitCIFAR show
that the proposed algorithm is on par with other continual learning methods. We
also perform ablation studies to clarify the contributions of each system part."
2321,"Semi-supervised methods might tackle this problem to some extent,
but more careful sample selection and further study with longitudinal
data may ensure disease-speciﬁcity.","First, due to demo-
graphic variations and the existence of comorbidities, it is not guaranteed
that models cluster the data based on variations of the pathology of inter-
est.","Second, spatial diﬀerences and tem-
poral changes may simultaneously contribute to subtypes derived through
clustering methods.",2022-02-16 19:13:53+00:00,Subtyping brain diseases from imaging data,cs.LG,"['cs.LG', 'cs.CV', 'eess.IV']","[arxiv.Result.Author('Junhao Wen'), arxiv.Result.Author('Erdem Varol'), arxiv.Result.Author('Zhijian Yang'), arxiv.Result.Author('Gyujoon Hwang'), arxiv.Result.Author('Dominique Dwyer'), arxiv.Result.Author('Anahita Fathi Kazerooni'), arxiv.Result.Author('Paris Alexandros Lalousis'), arxiv.Result.Author('Christos Davatzikos')]","The imaging community has increasingly adopted machine learning (ML) methods
to provide individualized imaging signatures related to disease diagnosis,
prognosis, and response to treatment. Clinical neuroscience and cancer imaging
have been two areas in which ML has offered particular promise. However, many
neurologic and neuropsychiatric diseases, as well as cancer, are often
heterogeneous in terms of their clinical manifestations, neuroanatomical
patterns or genetic underpinnings. Therefore, in such cases, seeking a single
disease signature might be ineffectual in delivering individualized precision
diagnostics. The current chapter focuses on ML methods, especially
semi-supervised clustering, that seek disease subtypes using imaging data. Work
from Alzheimer Disease and its prodromal stages, psychosis, depression, autism,
and brain cancer are discussed. Our goal is to provide the readers with a broad
overview in terms of methodology and clinical applications."
2322,"Given the diversity of data in the medical
domain, promising achievements can be expected for further research of multi-modal subtyping and deep phenotyping
using EHR data coupled with neuroimages, neuropsychological data, and neuropathological, clinical and biochemical
biomarkers.","In addition, deep learning has achieved great success in many
applications in computer vision and NLP with multiple data modalities.","Conclusion

In this study, we performed spectral clustering of AD patients using the longitudinal medical condition information
before the AD diagnosis.",2022-02-22 15:53:59+00:00,Temporal Subtyping of Alzheimer's Disease Using Medical Conditions Preceding Alzheimer's Disease Onset in Electronic Health Records,cs.LG,"['cs.LG', 'cs.AI', 'stat.AP']","[arxiv.Result.Author('Zhe He'), arxiv.Result.Author('Shubo Tian'), arxiv.Result.Author('Arslan Erdengasileng'), arxiv.Result.Author('Neil Charness'), arxiv.Result.Author('Jiang Bian')]","Subtyping of Alzheimer's disease (AD) can facilitate diagnosis, treatment,
prognosis and disease management. It can also support the testing of new
prevention and treatment strategies through clinical trials. In this study, we
employed spectral clustering to cluster 29,922 AD patients in the OneFlorida
Data Trust using their longitudinal EHR data of diagnosis and conditions into
four subtypes. These subtypes exhibit different patterns of progression of
other conditions prior to the first AD diagnosis. In addition, according to the
results of various statistical tests, these subtypes are also significantly
different with respect to demographics, mortality, and prescription medications
after the AD diagnosis. This study could potentially facilitate early detection
and personalized treatment of AD as well as data-driven generalizability
assessment of clinical trials for AD."
2339,"This motivates further research on
ial robustness of contrastive learning, several specialized        defending against indiscriminate poisoning of CL.",To enhance adversar-      to data augmentations?,"adversarial training frameworks have been proposed: Kim
et al.",2022-02-22 22:31:57+00:00,Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Hao He'), arxiv.Result.Author('Kaiwen Zha'), arxiv.Result.Author('Dina Katabi')]","Indiscriminate data poisoning attacks are quite effective against supervised
learning. However, not much is known about their impact on unsupervised
contrastive learning (CL). This paper is the first to consider indiscriminate
data poisoning attacks on contrastive learning, demonstrating the feasibility
of such attacks, and their differences from indiscriminate poisoning of
supervised learning. We also highlight differences between contrastive learning
algorithms, and show that some algorithms (e.g., SimCLR) are more vulnerable
than others (e.g., MoCo). We differentiate between two types of data poisoning
attacks: sample-wise attacks, which add specific noise to each image, cause the
largest drop in accuracy, but do not transfer well across SimCLR, MoCo, and
BYOL. In contrast, attacks that use class-wise noise, though cause a smaller
drop in accuracy, transfer well across different CL algorithms. Finally, we
show that a new data augmentation based on matrix completion can be highly
effective in countering data poisoning attacks on unsupervised contrastive
learning."
2359,"be an interesting topic to generate such ground-truth data for
further research.",569–578.,[7] Y.-J.,2022-02-23 09:05:28+00:00,Deep Graph Learning for Anomalous Citation Detection,cs.LG,"['cs.LG', 'cs.DL', 'cs.SI']","[arxiv.Result.Author('Jiaying Liu'), arxiv.Result.Author('Feng Xia'), arxiv.Result.Author('Xu Feng'), arxiv.Result.Author('Jing Ren'), arxiv.Result.Author('Huan Liu')]","Anomaly detection is one of the most active research areas in various
critical domains, such as healthcare, fintech, and public security. However,
little attention has been paid to scholarly data, i.e., anomaly detection in a
citation network. Citation is considered as one of the most crucial metrics to
evaluate the impact of scientific research, which may be gamed in multiple
ways. Therefore, anomaly detection in citation networks is of significant
importance to identify manipulation and inflation of citations. To address this
open issue, we propose a novel deep graph learning model, namely GLAD (Graph
Learning for Anomaly Detection), to identify anomalies in citation networks.
GLAD incorporates text semantic mining to network representation learning by
adding both node attributes and link attributes via graph neural networks. It
exploits not only the relevance of citation contents but also hidden
relationships between papers. Within the GLAD framework, we propose an
algorithm called CPU (Citation PUrpose) to discover the purpose of citation
based on citation texts. The performance of GLAD is validated through a
simulated anomalous citation dataset. Experimental results demonstrate the
effectiveness of GLAD on the anomalous citation detection task."
2387,"We hope that our results can incentivize further research into developing prediction strategies
that come with theoretical guarantees.","For both
classiﬁcation and regression tasks, we present necessary and sufﬁcient conditions under which
Equalized Odds can hold true with deterministic prediction functions; we also present theoretical
guarantees that under mild assumptions, one can always ﬁnd a non-trivial stochastic predictor that
satisﬁes Equalized Odds in the large sample limit, while in general fairness is not attainable with a
deterministic predictor.","Future work would naturally consider the attainability of more
ﬁne-grained (compared to group fairness) criteria of fairness (e.g., individual fairness) as well as
fairness notions that focus on discrimination involved in the data generating process.",2022-02-24 01:30:31+00:00,Attainability and Optimality: The Equalized Odds Fairness Revisited,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Zeyu Tang'), arxiv.Result.Author('Kun Zhang')]","Fairness of machine learning algorithms has been of increasing interest. In
order to suppress or eliminate discrimination in prediction, various notions as
well as approaches have been proposed to impose fairness. Given a notion of
fairness, an essential problem is then whether or not it can always be
attained, even if with an unlimited amount of data. This issue is, however, not
well addressed yet. In this paper, focusing on the Equalized Odds notion of
fairness, we consider the attainability of this criterion and, furthermore, if
it is attainable, the optimality of the prediction performance under various
settings. In particular, for prediction performed by a deterministic function
of input features, we give conditions under which Equalized Odds can hold true;
if the stochastic prediction is acceptable, we show that under mild
assumptions, fair predictors can always be derived. For classification, we
further prove that compared to enforcing fairness by post-processing, one can
always benefit from exploiting all available features during training and get
potentially better prediction performance while remaining fair. Moreover, while
stochastic prediction can attain Equalized Odds with theoretical guarantees, we
also discuss its limitation and potential negative social impacts."
2393,"Finally, we empirically verify that     inspired a number of works to further study eﬀective
the randomizing procedures are indeed eﬀective in ren-        attack schemes (Goodfellow et al., 2015; Madry et al.,
dering robust probabilistic forecasting models through        2018; Papernot et al., 2017; Athalye et al., 2018).","This
robust base learners.","In
extensive experiments on multiple real datasets.",2022-02-24 05:46:26+00:00,Robust Probabilistic Time Series Forecasting,cs.LG,['cs.LG'],"[arxiv.Result.Author('TaeHo Yoon'), arxiv.Result.Author('Youngsuk Park'), arxiv.Result.Author('Ernest K. Ryu'), arxiv.Result.Author('Yuyang Wang')]","Probabilistic time series forecasting has played critical role in
decision-making processes due to its capability to quantify uncertainties. Deep
forecasting models, however, could be prone to input perturbations, and the
notion of such perturbations, together with that of robustness, has not even
been completely established in the regime of probabilistic forecasting. In this
work, we propose a framework for robust probabilistic time series forecasting.
First, we generalize the concept of adversarial input perturbations, based on
which we formulate the concept of robustness in terms of bounded Wasserstein
deviation. Then we extend the randomized smoothing technique to attain robust
probabilistic forecasters with theoretical robustness certificates against
certain classes of adversarial perturbations. Lastly, extensive experiments
demonstrate that our methods are empirically effective in enhancing the
forecast quality under additive adversarial attacks and forecast consistency
under supplement of noisy observations."
2413,"To further study the effect of β0,
we provide more results in Figure 3.","First, we can observe that
maintaining the moving average estimators enables our algorithm perform better.",We observe that β0 = 0.1 achieves the best performance in most cases.,2022-02-24 16:37:07+00:00,Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Zi-Hao Qiu'), arxiv.Result.Author('Quanqi Hu'), arxiv.Result.Author('Yongjian Zhong'), arxiv.Result.Author('Lijun Zhang'), arxiv.Result.Author('Tianbao Yang')]","NDCG, namely Normalized Discounted Cumulative Gain, is a widely used ranking
metric in information retrieval and machine learning. However, efficient and
provable stochastic methods for maximizing NDCG are still lacking, especially
for deep models. In this paper, we propose a principled approach to optimize
NDCG and its top-$K$ variant. First, we formulate a novel compositional
optimization problem for optimizing the NDCG surrogate, and a novel bilevel
compositional optimization problem for optimizing the top-$K$ NDCG surrogate.
Then, we develop efficient stochastic algorithms with provable convergence
guarantees for the non-convex objectives. Different from existing NDCG
optimization methods, the per-iteration complexity of our algorithms scales
with the mini-batch size instead of the number of total items. To improve the
effectiveness for deep learning, we further propose practical strategies by
using initial warm-up and stop gradient operator. Experimental results on
multiple datasets demonstrate that our methods outperform prior ranking
approaches in terms of NDCG. To the best of our knowledge, this is the first
time that stochastic algorithms are proposed to optimize NDCG with a provable
convergence guarantee."
2414,"To further study the effect of β0,
we provide more results in Figure 3.","First, we can observe that
maintaining the moving average estimators enables our algorithm perform better.",We observe that β0 = 0.1 achieves the best performance in most cases.,2022-02-24 16:37:07+00:00,Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Zi-Hao Qiu'), arxiv.Result.Author('Quanqi Hu'), arxiv.Result.Author('Yongjian Zhong'), arxiv.Result.Author('Lijun Zhang'), arxiv.Result.Author('Tianbao Yang')]","NDCG, namely Normalized Discounted Cumulative Gain, is a widely used ranking
metric in information retrieval and machine learning. However, efficient and
provable stochastic methods for maximizing NDCG are still lacking, especially
for deep models. In this paper, we propose a principled approach to optimize
NDCG and its top-$K$ variant. First, we formulate a novel compositional
optimization problem for optimizing the NDCG surrogate, and a novel bilevel
compositional optimization problem for optimizing the top-$K$ NDCG surrogate.
Then, we develop efficient stochastic algorithms with provable convergence
guarantees for the non-convex objectives. Different from existing NDCG
optimization methods, the per-iteration complexity of our algorithms scales
with the mini-batch size instead of the number of total items. To improve the
effectiveness for deep learning, we further propose practical strategies by
using initial warm-up and stop gradient operator. Experimental results on
multiple datasets demonstrate that our methods outperform prior ranking
approaches in terms of NDCG. To the best of our knowledge, this is the first
time that stochastic algorithms are proposed to optimize NDCG with a provable
convergence guarantee."
2415,"To further study

                                           9
                         MovieLens20M                                                              Netflix Prize

        0.40                                                                          0.33
        0.35
NDCG@5  0.30                                               NDCG@5                     0.30
        0.25
                                                                                      0.28
                  0
                                 SONG (γ0=1.0)                                        0.25                 SONG (γ0=1.0)
        0.40                                                                                               SONG without warm-up
        0.38                     SONG without warm-up                                 0.23                 SONG
        0.36                     SONG
        0.34
        0.32         50  100 150 200 250 300 0.20 0                                            20  40  60                           80  100 120
        0.30 0               Epochs                                                                    Epochs

                             Figure 2: Ablation study on two variants of SONG.","First, we
can observe that maintaining the moving average estimators enables our algorithm perform better.","MovieLens20M                                                                                Netflix Prize

                                                                                      0.34

                                                                                      0.32

NDCG@5                                                     NDCG@5                     0.30

                                                                                      0.28

                                     full-items                                       0.26                     full-items
                                     mini-batch γ0 = 1.0                              0.24                     mini-batch γ0 = 1.0
                                     mini-batch γ0 = 0.1                                                       mini-batch γ0 = 0.3

                     20  40  60      80  100 120                                            0  20  40  60                           80  100 120
                             Epochs                                                                    Epochs

                         Figure 3: Comparison of full-items and mini-batch training.",2022-02-24 16:37:07+00:00,Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Zi-Hao Qiu'), arxiv.Result.Author('Quanqi Hu'), arxiv.Result.Author('Yongjian Zhong'), arxiv.Result.Author('Lijun Zhang'), arxiv.Result.Author('Tianbao Yang')]","NDCG, namely Normalized Discounted Cumulative Gain, is a widely used ranking
metric in information retrieval and machine learning. However, efficient and
provable stochastic methods for maximizing NDCG are still lacking, especially
for deep models. In this paper, we propose a principled approach to optimize
NDCG and its top-$K$ variant. First, we formulate a novel compositional
optimization problem for optimizing the NDCG surrogate, and a novel bilevel
compositional optimization problem for optimizing the top-$K$ NDCG surrogate.
Then, we develop efficient stochastic algorithms with provable convergence
guarantees for the non-convex objectives. Different from existing NDCG
optimization methods, the per-iteration complexity of our algorithms scales
with the mini-batch size instead of the number of total items. To improve the
effectiveness for deep learning, we further propose practical strategies by
using initial warm-up and stop gradient operator. Experimental results on
multiple datasets demonstrate that our methods outperform prior ranking
approaches in terms of NDCG. To the best of our knowledge, this is the first
time that stochastic algorithms are proposed to optimize NDCG with a provable
convergence guarantee. Our proposed methods are implemented in the LibAUC
library at https://libauc.org/."
2416,"To further study

                                           9
                         MovieLens20M                                                              Netflix Prize

        0.40                                                                          0.33
        0.35
NDCG@5  0.30                                               NDCG@5                     0.30
        0.25
                                                                                      0.28
                  0
                                 SONG (γ0=1.0)                                        0.25                 SONG (γ0=1.0)
        0.40                                                                                               SONG without warm-up
        0.38                     SONG without warm-up                                 0.23                 SONG
        0.36                     SONG
        0.34
        0.32         50  100 150 200 250 300 0.20 0                                            20  40  60                           80  100 120
        0.30 0               Epochs                                                                    Epochs

                             Figure 2: Ablation study on two variants of SONG.","First, we
can observe that maintaining the moving average estimators enables our algorithm perform better.","MovieLens20M                                                                                Netflix Prize

                                                                                      0.34

                                                                                      0.32

NDCG@5                                                     NDCG@5                     0.30

                                                                                      0.28

                                     full-items                                       0.26                     full-items
                                     mini-batch γ0 = 1.0                              0.24                     mini-batch γ0 = 1.0
                                     mini-batch γ0 = 0.1                                                       mini-batch γ0 = 0.3

                     20  40  60      80  100 120                                            0  20  40  60                           80  100 120
                             Epochs                                                                    Epochs

                         Figure 3: Comparison of full-items and mini-batch training.",2022-02-24 16:37:07+00:00,Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning with Provable Convergence,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Zi-Hao Qiu'), arxiv.Result.Author('Quanqi Hu'), arxiv.Result.Author('Yongjian Zhong'), arxiv.Result.Author('Lijun Zhang'), arxiv.Result.Author('Tianbao Yang')]","NDCG, namely Normalized Discounted Cumulative Gain, is a widely used ranking
metric in information retrieval and machine learning. However, efficient and
provable stochastic methods for maximizing NDCG are still lacking, especially
for deep models. In this paper, we propose a principled approach to optimize
NDCG and its top-$K$ variant. First, we formulate a novel compositional
optimization problem for optimizing the NDCG surrogate, and a novel bilevel
compositional optimization problem for optimizing the top-$K$ NDCG surrogate.
Then, we develop efficient stochastic algorithms with provable convergence
guarantees for the non-convex objectives. Different from existing NDCG
optimization methods, the per-iteration complexity of our algorithms scales
with the mini-batch size instead of the number of total items. To improve the
effectiveness for deep learning, we further propose practical strategies by
using initial warm-up and stop gradient operator. Experimental results on
multiple datasets demonstrate that our methods outperform prior ranking
approaches in terms of NDCG. To the best of our knowledge, this is the first
time that stochastic algorithms are proposed to optimize NDCG with a provable
convergence guarantee. Our proposed methods are implemented in the LibAUC
library at https://libauc.org/."
2424,"We further study the interpretability of content by
phoneme information probing (implementation described in Appendix F.3), which is used to show
the information captured by the discovered content units (Chorowski et al., 2019).","They switch more frequently as they capture more
detailed residual variations within each phoneme.","We measure the
frame-level phoneme classiﬁcation accuracy with or without context and compare with the baseline
(CPC + k-means).",2022-02-24 19:00:03+00:00,Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Dacheng Yin'), arxiv.Result.Author('Xuanchi Ren'), arxiv.Result.Author('Chong Luo'), arxiv.Result.Author('Yuwang Wang'), arxiv.Result.Author('Zhiwei Xiong'), arxiv.Result.Author('Wenjun Zeng')]","This paper addresses the unsupervised learning of content-style decomposed
representation. We first give a definition of style and then model the
content-style representation as a token-level bipartite graph. An unsupervised
framework, named Retriever, is proposed to learn such representations. First, a
cross-attention module is employed to retrieve permutation invariant (P.I.)
information, defined as style, from the input data. Second, a vector
quantization (VQ) module is used, together with man-induced constraints, to
produce interpretable content tokens. Last, an innovative link attention module
serves as the decoder to reconstruct data from the decomposed content and
style, with the help of the linking keys. Being modal-agnostic, the proposed
Retriever is evaluated in both speech and image domains. The state-of-the-art
zero-shot voice conversion performance confirms the disentangling ability of
our framework. Top performance is also achieved in the part discovery task for
images, verifying the interpretability of our representation. In addition, the
vivid part-based style transfer quality demonstrates the potential of Retriever
to support various fascinating generative tasks. Project page at
https://ydcustc.github.io/retriever-demo/."
2429,"On the Eﬀects of Depth This Section is meant to answer to two further research
questions.","However, the latter has better validation performances if we consider a logistic regressor
      on NCI1, which means that the ﬂow of context is indeed more beneﬁcial than increasing
      the number of hidden states C. Results are averaged over ﬁve independent runs and

                               standard deviations are shown as colored bands.","First, we want to quantify the eﬀect of depth on the architecture when cou-
pled with a classiﬁer.",2022-02-24 20:18:41+00:00,Bayesian Deep Learning for Graphs,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']",[arxiv.Result.Author('Federico Errica')],"The adaptive processing of structured data is a long-standing research topic
in machine learning that investigates how to automatically learn a mapping from
a structured input to outputs of various nature. Recently, there has been an
increasing interest in the adaptive processing of graphs, which led to the
development of different neural network-based methodologies. In this thesis, we
take a different route and develop a Bayesian Deep Learning framework for graph
learning. The dissertation begins with a review of the principles over which
most of the methods in the field are built, followed by a study on graph
classification reproducibility issues. We then proceed to bridge the basic
ideas of deep learning for graphs with the Bayesian world, by building our deep
architectures in an incremental fashion. This framework allows us to consider
graphs with discrete and continuous edge features, producing unsupervised
embeddings rich enough to reach the state of the art on several classification
tasks. Our approach is also amenable to a Bayesian nonparametric extension that
automatizes the choice of almost all model's hyper-parameters. Two real-world
applications demonstrate the efficacy of deep learning for graphs. The first
concerns the prediction of information-theoretic quantities for molecular
simulations with supervised neural models. After that, we exploit our Bayesian
models to solve a malware-classification task while being robust to
intra-procedural code obfuscation techniques. We conclude the dissertation with
an attempt to blend the best of the neural and Bayesian worlds together. The
resulting hybrid model is able to predict multimodal distributions conditioned
on input graphs, with the consequent ability to model stochasticity and
uncertainty better than most works. Overall, we aim to provide a Bayesian
perspective into the articulated research field of deep learning for graphs."
2430,"In the future, we plan to further study the impact of GMDN on more biologically
plausible synthetic datasets and ﬁnd new application domains.","The eﬀectiveness of GMDM has also been demonstrated on real-world
chemical regression tasks and as a promising tool to address link prediction.","We believe there are
plenty of directions in which we can extend GMDN, for instance by using a recurrent
encoder in order to model dynamically varying graphs [282, 283].",2022-02-24 20:18:41+00:00,Bayesian Deep Learning for Graphs,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']",[arxiv.Result.Author('Federico Errica')],"The adaptive processing of structured data is a long-standing research topic
in machine learning that investigates how to automatically learn a mapping from
a structured input to outputs of various nature. Recently, there has been an
increasing interest in the adaptive processing of graphs, which led to the
development of different neural network-based methodologies. In this thesis, we
take a different route and develop a Bayesian Deep Learning framework for graph
learning. The dissertation begins with a review of the principles over which
most of the methods in the field are built, followed by a study on graph
classification reproducibility issues. We then proceed to bridge the basic
ideas of deep learning for graphs with the Bayesian world, by building our deep
architectures in an incremental fashion. This framework allows us to consider
graphs with discrete and continuous edge features, producing unsupervised
embeddings rich enough to reach the state of the art on several classification
tasks. Our approach is also amenable to a Bayesian nonparametric extension that
automatizes the choice of almost all model's hyper-parameters. Two real-world
applications demonstrate the efficacy of deep learning for graphs. The first
concerns the prediction of information-theoretic quantities for molecular
simulations with supervised neural models. After that, we exploit our Bayesian
models to solve a malware-classification task while being robust to
intra-procedural code obfuscation techniques. We conclude the dissertation with
an attempt to blend the best of the neural and Bayesian worlds together. The
resulting hybrid model is able to predict multimodal distributions conditioned
on input graphs, with the consequent ability to model stochasticity and
uncertainty better than most works. Overall, we aim to provide a Bayesian
perspective into the articulated research field of deep learning for graphs."
2472,(2021) further study the reward-free RL algorithms under the multi-agent setting.,(2021); Qiu et al.,"Furthermore, we would like to emphasize that directly extending the existing results on reward-
free exploration (see, e.g., Wang et al.",2022-02-25 16:17:23+00:00,Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement Learning Approach,cs.LG,"['cs.LG', 'cs.GT', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Boxiang Lyu'), arxiv.Result.Author('Qinglin Meng'), arxiv.Result.Author('Shuang Qiu'), arxiv.Result.Author('Zhaoran Wang'), arxiv.Result.Author('Zhuoran Yang'), arxiv.Result.Author('Michael I. Jordan')]","Dynamic mechanism design studies how mechanism designers should allocate
resources among agents in a time-varying environment. We consider the problem
where the agents interact with the mechanism designer according to an unknown
Markov Decision Process (MDP), where agent rewards and the mechanism designer's
state evolve according to an episodic MDP with unknown reward functions and
transition kernels. We focus on the online setting with linear function
approximation and attempt to recover the dynamic Vickrey-Clarke-Grove (VCG)
mechanism over multiple rounds of interaction. A key contribution of our work
is incorporating reward-free online Reinforcement Learning (RL) to aid
exploration over a rich policy space to estimate prices in the dynamic VCG
mechanism. We show that the regret of our proposed method is upper bounded by
$\tilde{\mathcal{O}}(T^{2/3})$ and further devise a lower bound to show that
our algorithm is efficient, incurring the same $\tilde{\mathcal{O}}(T^{2 / 3})$
regret as the lower bound, where $T$ is the total number of rounds. Our work
establishes the regret guarantee for online RL in solving dynamic mechanism
design problems without prior knowledge of the underlying model."
2484,"To further study the impact of the random initial policies to begin with the subgradient descent
algorithm, in Figure 7 we obtained the corresponding results of Figure 3, but with a ﬁxed initial
policy.",Averaged over ten random training datasets of size 1000.,"We observe that again WDM and GRDR quickly converges to the oracle estimation, while
the large bias of the direct method leads to poor policy optimization.",2022-02-25 20:25:37+00:00,Off-Policy Evaluation with Policy-Dependent Optimization Response,cs.LG,['cs.LG'],"[arxiv.Result.Author('Wenshuo Guo'), arxiv.Result.Author('Michael I. Jordan'), arxiv.Result.Author('Angela Zhou')]","The intersection of causal inference and machine learning for decision-making
is rapidly expanding, but the default decision criterion remains an
\textit{average} of individual causal outcomes across a population. In
practice, various operational restrictions ensure that a decision-maker's
utility is not realized as an \textit{average} but rather as an \textit{output}
of a downstream decision-making problem (such as matching, assignment, network
flow, minimizing predictive risk). In this work, we develop a new framework for
off-policy evaluation with a \textit{policy-dependent} linear optimization
response: causal outcomes introduce stochasticity in objective function
coefficients. In this framework, a decision-maker's utility depends on the
policy-dependent optimization, which introduces a fundamental challenge of
\textit{optimization} bias even for the case of policy evaluation. We construct
unbiased estimators for the policy-dependent estimand by a perturbation method.
We also discuss the asymptotic variance properties for a set of plug-in
regression estimators adjusted to be compatible with that perturbation method.
Lastly, attaining unbiased policy evaluation allows for policy optimization,
and we provide a general algorithm for optimizing causal interventions. We
corroborate our theoretical results with numerical simulations."
2485,"To further study the impact of the random initial policies to begin with the subgradient descent
algorithm, in Figure 7 we obtained the corresponding results of Figure 3, but with a ﬁxed initial policy.","The inner
minimization (the matching problem) is again solved by the linear_sum_assignment function of
the scipy.optimize package in Python 3.","We observe that again WDM and GRDR quickly converges to the oracle estimation, while the large
bias of the direct method leads to poor policy optimization.",2022-02-25 20:25:37+00:00,Off-Policy Evaluation with Policy-Dependent Optimization Response,cs.LG,['cs.LG'],"[arxiv.Result.Author('Wenshuo Guo'), arxiv.Result.Author('Michael I. Jordan'), arxiv.Result.Author('Angela Zhou')]","The intersection of causal inference and machine learning for decision-making
is rapidly expanding, but the default decision criterion remains an
\textit{average} of individual causal outcomes across a population. In
practice, various operational restrictions ensure that a decision-maker's
utility is not realized as an \textit{average} but rather as an \textit{output}
of a downstream decision-making problem (such as matching, assignment, network
flow, minimizing predictive risk). In this work, we develop a new framework for
off-policy evaluation with \textit{policy-dependent} linear optimization
responses: causal outcomes introduce stochasticity in objective function
coefficients. Under this framework, a decision-maker's utility depends on the
policy-dependent optimization, which introduces a fundamental challenge of
\textit{optimization} bias even for the case of policy evaluation. We construct
unbiased estimators for the policy-dependent estimand by a perturbation method,
and discuss asymptotic variance properties for a set of adjusted plug-in
estimators. Lastly, attaining unbiased policy evaluation allows for policy
optimization: we provide a general algorithm for optimizing causal
interventions. We corroborate our theoretical results with numerical
simulations."
2487,"The quality of results for ﬂow based variational families for small toy-examples closely resemble gold-standard
HMC but optimisation remains a diﬃculty and further research is required to understand how to achieve good
training performance for moderate sized datasets.","increasing the number of samples
to approximate the KL-divergence calculation, using a very small learning rate 1e-05 and long training times).","Generalised Gaussian Process Latent Variable Models (GPLVM) with Stochastic Variational Inference

E Experimental Conﬁguration

Dataset N D Z Q LR Mini-batch Train w. missing

Oilﬂow 1000 12 25 10 1e-03       100  No

qpCR  450 48 40 11 1e-03         100  No

Taxi-cab 744 3 36 2 5e-03        500  No

MNIST 15K 768 100 5 0.01         100  Yes

Brendan 1965 560 120 5 0.01      450  Yes

MOCAP 533 62 30 6 0.01           200  Yes

MovieLens 943 1682 34 15 0.005   100  Yes

Table 7: Training experimental conﬁguration where N and D denote the number of data points and data space
dimensions, Z denotes the number of inducing inputs shared across dimensions, Q denotes the dimesionality of
the latent space, LR denotes the learning rate, β denotes the scalar annealing factor for the KL latent term in
the ELBO.",2022-02-25 21:21:51+00:00,Generalised Gaussian Process Latent Variable Models (GPLVM) with Stochastic Variational Inference,cs.LG,"['cs.LG', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Vidhi Lalchand'), arxiv.Result.Author('Aditya Ravuri'), arxiv.Result.Author('Neil D. Lawrence')]","Gaussian process latent variable models (GPLVM) are a flexible and non-linear
approach to dimensionality reduction, extending classical Gaussian processes to
an unsupervised learning context. The Bayesian incarnation of the GPLVM Titsias
and Lawrence, 2010] uses a variational framework, where the posterior over
latent variables is approximated by a well-behaved variational family, a
factorized Gaussian yielding a tractable lower bound. However, the
non-factories ability of the lower bound prevents truly scalable inference. In
this work, we study the doubly stochastic formulation of the Bayesian GPLVM
model amenable with minibatch training. We show how this framework is
compatible with different latent variable formulations and perform experiments
to compare a suite of models. Further, we demonstrate how we can train in the
presence of massively missing data and obtain high-fidelity reconstructions. We
demonstrate the model's performance by benchmarking against the canonical
sparse GPLVM for high-dimensional data examples."
2488,"The quality of results for ﬂow based variational families for small toy-examples closely resemble gold-standard
HMC but optimisation remains a diﬃculty and further research is required to understand how to achieve good
training performance for moderate sized datasets.","increasing the number of samples
to approximate the KL-divergence calculation, using a very small learning rate 1e-05 and long training times).","Generalised Gaussian Process Latent Variable Models (GPLVM) with Stochastic Variational Inference

E Experimental Conﬁguration

Dataset N D Z Q LR Mini-batch Train w. missing

Oilﬂow 1000 12 25 10 1e-03       100  No

qpCR  450 48 40 11 1e-03         100  No

Taxi-cab 744 3 36 2 5e-03        500  No

MNIST 15K 768 100 5 0.01         100  Yes

Brendan 1965 560 120 5 0.01      450  Yes

MOCAP 533 62 30 6 0.01           200  Yes

MovieLens 943 1682 34 15 0.005   100  Yes

Table 7: Training experimental conﬁguration where N and D denote the number of data points and data space
dimensions, Z denotes the number of inducing inputs shared across dimensions, Q denotes the dimesionality of
the latent space, LR denotes the learning rate, β denotes the scalar annealing factor for the KL latent term in
the ELBO.",2022-02-25 21:21:51+00:00,Generalised Gaussian Process Latent Variable Models (GPLVM) with Stochastic Variational Inference,cs.LG,"['cs.LG', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Vidhi Lalchand'), arxiv.Result.Author('Aditya Ravuri'), arxiv.Result.Author('Neil D. Lawrence')]","Gaussian process latent variable models (GPLVM) are a flexible and non-linear
approach to dimensionality reduction, extending classical Gaussian processes to
an unsupervised learning context. The Bayesian incarnation of the GPLVM Titsias
and Lawrence, 2010] uses a variational framework, where the posterior over
latent variables is approximated by a well-behaved variational family, a
factorized Gaussian yielding a tractable lower bound. However, the
non-factories ability of the lower bound prevents truly scalable inference. In
this work, we study the doubly stochastic formulation of the Bayesian GPLVM
model amenable with minibatch training. We show how this framework is
compatible with different latent variable formulations and perform experiments
to compare a suite of models. Further, we demonstrate how we can train in the
presence of massively missing data and obtain high-fidelity reconstructions. We
demonstrate the model's performance by benchmarking against the canonical
sparse GPLVM for high-dimensional data examples."
2516,"posed a sharding-based strategy leveraging ensemble learn-
                                                                  ing and distributed manner, which establish a general method
   • To deal with the difﬁculties in calculating the mutual in-   and a new baseline for further research.","After that, [Bourtoule et al., 2021] pro-
      which are eventually eliminated from the model.","Later studies either
      formation required in vertical unlearning, we propose an    focus on the different algorithms or minimizing computa-
      approximation method with rigorous theoretical analysis     tional and storage complexity in this area [Huang et al., 2021;
      and negligible deviation.",2022-02-27 05:25:15+00:00,Vertical Machine Unlearning: Selectively Removing Sensitive Information From Latent Feature Space,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Tao Guo'), arxiv.Result.Author('Song Guo'), arxiv.Result.Author('Jiewei Zhang'), arxiv.Result.Author('Wenchao Xu'), arxiv.Result.Author('Junxiao Wang')]","Recently, the enactment of privacy regulations has promoted the rise of
machine unlearning paradigm. Most existing studies mainly focus on removing
unwanted data samples from a learnt model. Yet we argue that they remove
overmuch information of data samples from latent feature space, which is far
beyond the sensitive feature scope that genuinely needs to be unlearned. In
this paper, we investigate a vertical unlearning mode, aiming at removing only
sensitive information from latent feature space. First, we introduce intuitive
and formal definitions for this unlearning and show its orthogonal relationship
with existing horizontal unlearning. Secondly, given the fact of lacking
general solutions to vertical unlearning, we introduce a ground-breaking
solution based on representation detachment, where the task-related information
is encouraged to retain while the sensitive information is progressively
forgotten. Thirdly, observing that some computation results during
representation detachment are hard to obtain in practice, we propose an
approximation with an upper bound to estimate it, with rigorous theoretical
analysis. We validate our method by spanning several datasets and models with
prevailing performance. We envision this work as a necessity for future machine
unlearning system and an essential component of the latest privacy-related
legislation."
2517,"Extensive experiments
a new baseline for further research.","To expedite such loss
machine unlearning and introduces expedition strategy of retrain-         calculation, we have introduced an approximation method for the
ing by ensemble learning, which establish a general method and            loss value with rigorous theoretical analysis.",Later studies either focus on        have been conducted to verify our theory and performance.,2022-02-27 05:25:15+00:00,Efficient Attribute Unlearning: Towards Selective Removal of Input Attributes from Feature Representations,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Tao Guo'), arxiv.Result.Author('Song Guo'), arxiv.Result.Author('Jiewei Zhang'), arxiv.Result.Author('Wenchao Xu'), arxiv.Result.Author('Junxiao Wang')]","Recently, the enactment of privacy regulations has promoted the rise of the
machine unlearning paradigm. Existing studies of machine unlearning mainly
focus on sample-wise unlearning, such that a learnt model will not expose
user's privacy at the sample level. Yet we argue that such ability of selective
removal should also be presented at the attribute level, especially for the
attributes irrelevant to the main task, e.g., whether a person recognized in a
face recognition system wears glasses or the age range of that person. Through
a comprehensive literature review, it is found that existing studies on
attribute-related problems like fairness and de-biasing learning cannot address
the above concerns properly. To bridge this gap, we propose a paradigm of
selectively removing input attributes from feature representations which we
name `attribute unlearning'. In this paradigm, certain attributes will be
accurately captured and detached from the learned feature representations at
the stage of training, according to their mutual information. The particular
attributes will be progressively eliminated along with the training procedure
towards convergence, while the rest of attributes related to the main task are
preserved for achieving competitive model performance. Considering the
computational complexity during the training process, we not only give a
theoretically approximate training method, but also propose an acceleration
scheme to speed up the training process. We validate our method by spanning
several datasets and models and demonstrate that our design can preserve model
fidelity and reach prevailing unlearning efficacy with high efficiency. The
proposed unlearning paradigm builds a foundation for future machine unlearning
system and will become an essential component of the latest privacy-related
legislation."
2522,"There-
                                                                  fore, how to efﬁciently utilize the meteorological ﬁeld data
    The inherent features of TCs at a particular time are al-     and fully exploit the intrinsic time-series information of the
ways represented by a column vector or tensor, which con-         inherent features of TCs still needs further research.",adequately learn the time-series features of the data.,"tains information such as the latitude, longitude intensity of
the center of TCs at that time.",2022-02-27 10:45:18+00:00,Dual-Branched Spatio-temporal Fusion Network for Multi-horizon Tropical Cyclone Track Forecast,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Zili Liu'), arxiv.Result.Author('Kun Hao'), arxiv.Result.Author('Xiaoyi Geng'), arxiv.Result.Author('Zhenwei Shi')]","Tropical cyclone (TC) is an extreme tropical weather system and its
trajectory can be described by a variety of spatio-temporal data. Effective
mining of these data is the key to accurate TCs track forecasting. However,
existing methods face the problem that the model complexity is too high or it
is difficult to efficiently extract features from multi-modal data. In this
paper, we propose the Dual-Branched spatio-temporal Fusion Network (DBF-Net) --
a novel multi-horizon tropical cyclone track forecasting model which fuses the
multi-modal features efficiently. DBF-Net contains a TC features branch that
extracts temporal features from 1D inherent features of TCs and a pressure
field branch that extracts spatio-temporal features from reanalysis 2D pressure
field. Through the encoder-decoder-based architecture and efficient feature
fusion, DBF-Net can fully mine the information of the two types of data, and
achieve good TCs track prediction results. Extensive experiments on historical
TCs track data in the Northwest Pacific show that our DBF-Net achieves
significant improvement compared with existing statistical and deep learning
TCs track forecast methods."
2537,"Finally, our results in the classiﬁcation
task yield a further research direction towards analyzing the smoothness of decision boundaries of
Π-Nets with multiplicative interactions, especially focusing on areas wherein this effect becomes
relevant such as adversarial susceptibility or knowledge distillation.","Therefore, another important direction is
to explore whether this enhanced spectral bias of polynomials towards higher complexity functions
translates to differences in their generalization properties.","ACKNOWLEDGMENTS

We are thankful to the anonymous reviewers for their constructive feedback.",2022-02-27 23:12:43+00:00,The Spectral Bias of Polynomial Neural Networks,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Moulik Choraria'), arxiv.Result.Author('Leello Tadesse Dadi'), arxiv.Result.Author('Grigorios Chrysos'), arxiv.Result.Author('Julien Mairal'), arxiv.Result.Author('Volkan Cevher')]","Polynomial neural networks (PNNs) have been recently shown to be particularly
effective at image generation and face recognition, where high-frequency
information is critical. Previous studies have revealed that neural networks
demonstrate a $\textit{spectral bias}$ towards low-frequency functions, which
yields faster learning of low-frequency components during training. Inspired by
such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK)
of PNNs. We find that the $\Pi$-Net family, i.e., a recently proposed
parametrization of PNNs, speeds up the learning of the higher frequencies. We
verify the theoretical bias through extensive experiments. We expect our
analysis to provide novel insights into designing architectures and learning
frameworks by incorporating multiplicative interactions via polynomials."
2555,"The table also provides the cost of performing infer-
trade-off in potential improvement versus additional com-      ence in relation to performing the same inference with the
putation should be the subject of further research.","Hence, the        ﬁer.",underlying static model.,2022-02-28 12:11:40+00:00,Evaluating the Adversarial Robustness of Adaptive Test-time Defenses,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Francesco Croce'), arxiv.Result.Author('Sven Gowal'), arxiv.Result.Author('Thomas Brunner'), arxiv.Result.Author('Evan Shelhamer'), arxiv.Result.Author('Matthias Hein'), arxiv.Result.Author('Taylan Cemgil')]","Adaptive defenses that use test-time optimization promise to improve
robustness to adversarial examples. We categorize such adaptive test-time
defenses and explain their potential benefits and drawbacks. In the process, we
evaluate some of the latest proposed adaptive defenses (most of them published
at peer-reviewed conferences). Unfortunately, none significantly improve upon
static models when evaluated appropriately. Some even weaken the underlying
static model while simultaneously increasing inference cost. While these
results are disappointing, we still believe that adaptive test-time defenses
are a promising avenue of research and, as such, we provide recommendations on
evaluating such defenses. We go beyond the checklist provided by Carlini et al.
(2019) by providing concrete steps that are specific to this type of defense."
2556,"Hence, the         can inﬂuence this value (e.g., architecture of the classiﬁer,
trade-off in potential improvement versus additional com-       compute infrastructure, optimized implementation) and we
putation should be the subject of further research.","Note that many factors
ﬁcation in deployment can be extremely slow.",use the runtime observed in our evaluation.,2022-02-28 12:11:40+00:00,Evaluating the Adversarial Robustness of Adaptive Test-time Defenses,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Francesco Croce'), arxiv.Result.Author('Sven Gowal'), arxiv.Result.Author('Thomas Brunner'), arxiv.Result.Author('Evan Shelhamer'), arxiv.Result.Author('Matthias Hein'), arxiv.Result.Author('Taylan Cemgil')]","Adaptive defenses, which optimize at test time, promise to improve
adversarial robustness. We categorize such adaptive test-time defenses, explain
their potential benefits and drawbacks, and evaluate a representative variety
of the latest adaptive defenses for image classification. Unfortunately, none
significantly improve upon static defenses when subjected to our careful case
study evaluation. Some even weaken the underlying static model while
simultaneously increasing inference computation. While these results are
disappointing, we still believe that adaptive test-time defenses are a
promising avenue of research and, as such, we provide recommendations for their
thorough evaluation. We extend the checklist of Carlini et al. (2019) by
providing concrete steps specific to adaptive defenses."
2557,"From an algorithmic perspective, a major hurdle in further research into fairness
in ML models is the lack of gold standard benchmarks.","Achieving “real” fairness in
machine learning models is, of course, a complex problem, going well beyond just machine
learning.","The COMPAS dataset [Larson et al.,
2016] is often used for empirical evaluations of algorithmic approaches for fairness.",2022-02-28 12:26:47+00:00,Fast Feature Selection with Fairness Constraints,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Francesco Quinzan'), arxiv.Result.Author('Rajiv Khanna'), arxiv.Result.Author('Moshik Hershcovitch'), arxiv.Result.Author('Sarel Cohen'), arxiv.Result.Author('Daniel G. Waddington'), arxiv.Result.Author('Tobias Friedrich'), arxiv.Result.Author('Michael W. Mahoney')]","We study the fundamental problem of selecting optimal features for model
construction. This problem is computationally challenging on large datasets,
even with the use of greedy algorithm variants. To address this challenge, we
extend the adaptive query model, recently proposed for the greedy forward
selection for submodular functions, to the faster paradigm of Orthogonal
Matching Pursuit for non-submodular functions. Our extension also allows the
use of downward-closed constraints, which can be used to encode certain
fairness criteria into the feature selection process. The proposed algorithm
achieves exponentially fast parallel run time in the adaptive query model,
scaling much better than prior work. The proposed algorithm also handles
certain fairness constraints by design. We prove strong approximation
guarantees for the algorithm based on standard assumptions. These guarantees
are applicable to many parametric models, including Generalized Linear Models.
Finally, we demonstrate empirically that the proposed algorithm competes
favorably with state-of-the-art techniques for feature selection, on real-world
and synthetic datasets."
2558,"We hope the ﬂexibility of our framework,
to handle general constraints, inspires further research into fair machine learning, and its
positioning within the speciﬁc societal context.","However, we hope that our framework provides a way to think
about fairness in a more algorithmically scalable way, even when future codiﬁcations of
fairness fall outside the purview of our framework.","References

Agarwal, A., Beygelzimer, A., Dudík, M., Langford, J., and Wallach, H. M. (2018).",2022-02-28 12:26:47+00:00,Fast Feature Selection with Fairness Constraints,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Francesco Quinzan'), arxiv.Result.Author('Rajiv Khanna'), arxiv.Result.Author('Moshik Hershcovitch'), arxiv.Result.Author('Sarel Cohen'), arxiv.Result.Author('Daniel G. Waddington'), arxiv.Result.Author('Tobias Friedrich'), arxiv.Result.Author('Michael W. Mahoney')]","We study the fundamental problem of selecting optimal features for model
construction. This problem is computationally challenging on large datasets,
even with the use of greedy algorithm variants. To address this challenge, we
extend the adaptive query model, recently proposed for the greedy forward
selection for submodular functions, to the faster paradigm of Orthogonal
Matching Pursuit for non-submodular functions. Our extension also allows the
use of downward-closed constraints, which can be used to encode certain
fairness criteria into the feature selection process. The proposed algorithm
achieves exponentially fast parallel run time in the adaptive query model,
scaling much better than prior work. The proposed algorithm also handles
certain fairness constraints by design. We prove strong approximation
guarantees for the algorithm based on standard assumptions. These guarantees
are applicable to many parametric models, including Generalized Linear Models.
Finally, we demonstrate empirically that the proposed algorithm competes
favorably with state-of-the-art techniques for feature selection, on real-world
and synthetic datasets."
2586,"Actual Value                                                                         Actual Value

        250          Predicted Value 250                                                                  Predicted Value

        200                                                                                 200

Volume  150                                                                         Volume  150

        100                                                                                 100

        50                                                                                  50

           0         100 200 300 400 500 600 700 800                                           0          100 200 300 400 500 600 700 800
                  0                                                                                    0
                        Tinghong Road Station(2020)                                                          Tinghong Road Station(2021)
        300                                                                                 250
        250                                                 (a)                             200                                                      Actual Value
        200                                                                                 150                                                      Predicted Value
        150                                                        Actual Value             100
        100                                                        Predicted Value
         50                                                                                  50
Volume                                                                              Volume

        0            100 200 300 400 500 600 700 800                                        0             100 200 300 400 500 600 700 800
                0                                                                                   0
                      Guangxi University Station(2020)                                                     Guangxi University Station(2021)

                                                        (b)

                                                        18
2000                                                          Actual Value          2200                                                       Actual Value
1800                                                          Predicted Value       2000                                                       Predicted Value
1600                                                                                1800
1400            100 200 300 400 500 600 700 800                                     1600           100 200 300 400 500 600 700 800
1200                                                                                1400
1000                Nanning Rail Station (2020)                                     1200                Nanning Rail Station(2021)
 800Volume                                                                          1000
 600
 400                                                                 Volume          800
 200                                                                                 600
                                                                                     400
    0                                                                                200
             0
                                                                                        0
                                                                                                0

                                                                               (c)

Figure 8 Comparison of actual and forecast values of the three selected stations in Nanning Metro New
  Year 2020 and 2021: (a) Tinghong Road station; (b) Guangxi University station; (c) Nanning Railway
                                                                station

5.4.3 Prediction performance in different time intervals

To further study the prediction performance of our proposed model in different time
intervals of a day, we calculate the average loss at each time interval from 6:00 to 23:00 for
both Nanning Metro NewYear 2020 and 2021.","In summary, the GCN-Transformer model can achieve accurate prediction not only
for the whole subway network but also for different types of stations.","The prediction performance between our
proposed model and other benchmark models at different time intervals is described as
Figure 9.",2022-02-27 01:06:24+00:00,GCN-Transformer for short-term passenger flow prediction on holidays in urban rail transit systems,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shuxin Zhang'), arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Ziyou Gao')]","The short-term passenger flow prediction of the urban rail transit system is
of great significance for traffic operation and management. The emerging deep
learning-based models provide effective methods to improve prediction accuracy.
However, most of the existing models mainly predict the passenger flow on
general weekdays, while few studies focus on predicting the holiday passenger
flow, which can provide more significant information for operators because
congestions or accidents generally occur on holidays. To this end, we propose a
deep learning-based model named GCN-Transformer comprising graph conventional
neural network (GCN) and Transformer for short-term passenger flow prediction
on holidays. The GCN is applied to extract the spatial features of passenger
flows and the Transformer is applied to extract the temporal features of
passenger flows. Moreover, in addition to the historical passenger flow data,
social media data are also incorporated into the prediction model, which has
been proven to have a potential correlation with the fluctuation of passenger
flow. The GCN-Transformer is tested on two large-scale real-world datasets from
Nanning, China during the New Year holiday and is compared with several
conventional prediction models. Results demonstrate its better robustness and
advantages among baseline methods, which provides overwhelming support for
practical applications of short-term passenger flow prediction on holidays"
2587,"It can be seen that the passenger flow in Guangxi
                          Actual Value                                                                                              Actual Value

        250               Predicted Value                                                                                250        Predicted Value

        200                                                                                                              200

Volume  150                                                                                                      Volume  150

        100                                                                                                              100

        50                                                                                                               50

               0          100 200 300 400 500 600 700 800                                                                0          100 200 300 400 500 600 700 800
                       0                                                                                                         0
                             Tinghong Road Station(2020)                                                                               Tinghong Road Station(2021)
        300
        250                                (a)                                                                                                                                  Actual Value
                                                                                                                                                                                Predicted Value
                          Actual Value                                                                                   250

                          Predicted Value

                                                                                                                         200

Volume  200                                                                                                      Volume
                                                                                                              150

        150
                                                                                                              100

        100

                                                                                                               50
         50

        0                 100 200 300 400 500 600 700 800                                                                0          100 200 300 400 500 600 700 800
                0                                                                                                                0
                           Guangxi University Station(2020)                                                                          Guangxi University Station(2021)

                                           (b)

        2000              Actual Value                                                                                   2200                                                   Actual Value
        1800                                                                                                                                                                    Predicted Value
        1600              Predicted Value
        1400                                                                                                                        100 200 300 400 500 600 700 800
        1200                                                                                                             2000
        1000                                                                                                                             Nanning Rail Station(2021)
         800                                                                                                             1800
         600
         400                                                                                                             1600
         200
Volume                                                       Volume                                                      1400
            0
                    0                                                                                                    1200

                                                                                                                         1000

                                                                                                                         800

                                                                                                                         600

                                                                                                                         400

                                                                                                                         200

                          100 200 300 400 500 600 700 800                                                                0
                                                                                                                                 0
                              Nanning Rail Station (2020)

                                           (c)

Figure 8 Comparison of actual and predictive values of the three selected stations in Nanning Subway New Year 2020 and 2021: (a) Tinghong Road station; (b)
                                                              Guangxi University station; (c) Nanning Railway station
3) Prediction performance in different time intervals              metrics of different time intervals in a day have an obvious peak
   To further study the prediction performance of the              period, indicating that the performance metrics go up when the
                                                                   passenger flow increases sharply.","The prediction result of Guangxi University station is shown
in Figure8 (b).","The performance metrics of
GCN-Transformer in different time intervals of a day, we           the statistics-based model ARIMA, have the most significant
calculate the average loss at each time interval from 6:00 to      peak characteristics, while the evaluating metrics of our
23:00 for both two datasets.",2022-02-27 01:06:24+00:00,GCN-Transformer for short-term passenger flow prediction on holidays in urban rail transit systems,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shuxin Zhang'), arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Jiateng Yin'), arxiv.Result.Author('Ziyou Gao')]","The short-term passenger flow prediction of the urban rail transit system is
of great significance for traffic operation and management. The emerging deep
learning-based models provide effective methods to improve prediction accuracy.
However, most of the existing models mainly predict the passenger flow on
general weekdays, while few studies focus on predicting the holiday passenger
flow, which can provide more significant information for operators because
congestions or accidents generally occur on holidays. To this end, we propose a
deep learning-based model named GCN-Transformer comprising graph conventional
neural network (GCN) and Transformer for short-term passenger flow prediction
on holidays. The GCN is applied to extract the spatial features of passenger
flows and the Transformer is applied to extract the temporal features of
passenger flows. Moreover, in addition to the historical passenger flow data,
social media data are also incorporated into the prediction model, which has
been proven to have a potential correlation with the fluctuation of passenger
flow. The GCN-Transformer is tested on two large-scale real-world datasets from
Nanning, China during the New Year holiday and is compared with several
conventional prediction models. Results demonstrate its better robustness and
advantages among baseline methods, which provides overwhelming support for
practical applications of short-term passenger flow prediction on holidays"
2588,"During New Year’s                                                                                                                  150
holiday, the increase on the number of tourists from other places
                                                                           150
                                                                                                                                                                         100

                                                                           100

                                                                            50 50

                                                                           0                                        0

                                                                                0 100 200 300 400 500 600 700 800                                                             0 100 200 300 400 500 600 700 800

                                                                                Guangxi University Station(2020)                                                              Guangxi University Station(2021)

                                                                   2000                                             2200

                                                                   1800                                             2000

                                                                   1600                                             1800

                                                                   1400                                             1600

                                                                   Volume                                                                                              1400
                                                                         1200

                                                                                                                                                                       1200
                                                                         1000

                                                                                                                                                                       1000
                                                                          800

                                                                                                                                                                        800

                                                                           600                                      600

                                                                           400                                      400

                                                                           200                                      200

                                                                           0                                        0

                                                                                0 100 200 300 400 500 600 700 800                                                             0 100 200 300 400 500 600 700 800

                                                                                Nanning Rail Station (2020)                                                                   Nanning Rail Station(2021)

                                                                   Figure 8 Comparison of actual and predictive values of the three selected
                                                                   stations in Nanning Subway New Year 2020 and 2021: (a) Tinghong Road
                                                                   station; (b) Guangxi University station; (c) Nanning Railway station

                                                                   3) Prediction performance in different time intervals
                                                                      To further study the prediction performance of the STAFN

                                                                   in different time intervals of a day, we calculate the average loss
                                                                   at each time interval from 6:00 to 23:00 for both two datasets.","There is no
obvious rush hour in the morning, instead, there are two peak              200
periods in the afternoon and evening.","The prediction performance between our proposed model and
                                                                   other benchmark models (except for ARIMA and BPNN,
because their metrics are too large to display) at different time                                              4) Prediction performance of different ablation studies
intervals is described in Figure 9.",2022-02-27 01:06:24+00:00,Spatial-Temporal Attention Fusion Network for short-term passenger flow prediction on holidays in urban rail transit systems,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shuxin Zhang'), arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Jiateng Yin'), arxiv.Result.Author('Ziyou Gao')]","The short term passenger flow prediction of the urban rail transit system is
of great significance for traffic operation and management. The emerging deep
learning-based models provide effective methods to improve prediction accuracy.
However, most of the existing models mainly predict the passenger flow on
general weekdays or weekends. There are only few studies focusing on predicting
the passenger flow on holidays, which is a significantly challenging task for
traffic management because of its suddenness and irregularity. To this end, we
propose a deep learning-based model named Spatial Temporal Attention Fusion
Network comprising a novel Multi-Graph Attention Network, a Conv-Attention
Block, and Feature Fusion Block for short-term passenger flow prediction on
holidays. The multi-graph attention network is applied to extract the complex
spatial dependencies of passenger flow dynamically and the conv-attention block
is applied to extract the temporal dependencies of passenger flow from global
and local perspectives. Moreover, in addition to the historical passenger flow
data, the social media data, which has been proven that they can effectively
reflect the evolution trend of passenger flow under events, are also fused into
the feature fusion block of STAFN. The STAFN is tested on two large-scale urban
rail transit AFC datasets from China on the New Year holiday, and the
prediction performance of the model are compared with that of several
conventional prediction models. Results demonstrate its better robustness and
advantages among benchmark methods, which can provide overwhelming support for
practical applications of short term passenger flow prediction on holidays."
2627,"We further study the properties of      this issue, Zhao et al [32] proposed improved deep leak-
diﬀerent datasets and architectures when it comes to         age from gradients (iDLG) that exploits the properties
assessing the severity of our method in various collabo-     of the cross-entropy loss function allowing the adver-
rative settings.","To alleviate
only reconstruction.","sary to always obtain the correct ground-truth label for
                                                             the captured gradient.",2022-03-01 14:22:29+00:00,Beyond Gradients: Exploiting Adversarial Priors in Model Inversion Attacks,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Dmitrii Usynin'), arxiv.Result.Author('Daniel Rueckert'), arxiv.Result.Author('Georgios Kaissis')]","Collaborative machine learning settings like federated learning can be
susceptible to adversarial interference and attacks. One class of such attacks
is termed model inversion attacks, characterised by the adversary
reverse-engineering the model to extract representations and thus disclose the
training data. Prior implementations of this attack typically only rely on the
captured data (i.e. the shared gradients) and do not exploit the data the
adversary themselves control as part of the training consortium. In this work,
we propose a novel model inversion framework that builds on the foundations of
gradient-based model inversion attacks, but additionally relies on matching the
features and the style of the reconstructed image to data that is controlled by
an adversary. Our technique outperforms existing gradient-based approaches both
qualitatively and quantitatively, while still maintaining the same
honest-but-curious threat model, allowing the adversary to obtain enhanced
reconstructions while remaining concealed."
2639,"A subset of the computations in this paper
consideration of limits in which the output dimension and      were performed using the Harvard University FAS Di-
dataset size are not vanishingly small relative to hidden      vision of Science Research Computing Group’s Cannon
layer width warrants further study.","Given the large scale of contemporary          by a Google Faculty Research Award and NSF Award
regression and classiﬁcation datasets (e.g., [53]), careful    #2134157.",HPC cluster.,2022-03-01 15:51:29+00:00,Contrasting random and learned features in deep Bayesian linear regression,cs.LG,"['cs.LG', 'cond-mat.dis-nn', 'stat.ML']","[arxiv.Result.Author('Jacob A. Zavatone-Veth'), arxiv.Result.Author('William L. Tong'), arxiv.Result.Author('Cengiz Pehlevan')]","Understanding how feature learning affects generalization is among the
foremost goals of modern deep learning theory. Here, we study how the ability
to learn representations affects the generalization performance of a simple
class of models: deep Bayesian linear neural networks trained on unstructured
Gaussian data. By comparing deep random feature models to deep networks in
which all layers are trained, we provide a detailed characterization of the
interplay between width, depth, data density, and prior mismatch. We show that
both models display sample-wise double-descent behavior in the presence of
label noise. Random feature models can also display model-wise double-descent
if there are narrow bottleneck layers, while deep networks do not show these
divergences. Random feature models can have particular widths that are optimal
for generalization at a given data density, while making neural networks as
wide or as narrow as possible is always optimal. Moreover, we show that the
leading-order correction to the kernel-limit learning curve cannot distinguish
between random feature models and deep networks in which all layers are
trained. Taken together, our findings begin to elucidate how architectural
details affect generalization performance in this simple class of deep
regression models."
2640,"When prior       further study [2, 5, 10, 17, 20, 22, 24, 25, 44, 52].","Paralleling the          Disentangling the causes of non-monotonicitic generaliza-
       case of optimal width, deeper models always per-         tion performance observed in experimental settings for
       form worse when the prior variance is less than the      realistic data models remains an interesting subject for
       average target scale (Figures 3 and 7).","variance is greater than the average target scale,
       shallower models perform better.",2022-03-01 15:51:29+00:00,Contrasting random and learned features in deep Bayesian linear regression,cs.LG,"['cs.LG', 'cond-mat.dis-nn', 'stat.ML']","[arxiv.Result.Author('Jacob A. Zavatone-Veth'), arxiv.Result.Author('William L. Tong'), arxiv.Result.Author('Cengiz Pehlevan')]","Understanding how feature learning affects generalization is among the
foremost goals of modern deep learning theory. Here, we study how the ability
to learn representations affects the generalization performance of a simple
class of models: deep Bayesian linear neural networks trained on unstructured
Gaussian data. By comparing deep random feature models to deep networks in
which all layers are trained, we provide a detailed characterization of the
interplay between width, depth, data density, and prior mismatch. We show that
both models display sample-wise double-descent behavior in the presence of
label noise. Random feature models can also display model-wise double-descent
if there are narrow bottleneck layers, while deep networks do not show these
divergences. Random feature models can have particular widths that are optimal
for generalization at a given data density, while making neural networks as
wide or as narrow as possible is always optimal. Moreover, we show that the
leading-order correction to the kernel-limit learning curve cannot distinguish
between random feature models and deep networks in which all layers are
trained. Taken together, our findings begin to elucidate how architectural
details affect generalization performance in this simple class of deep
regression models."
2641,"Speciﬁcally, the RS learning           dataset size are not vanishingly small relative to hidden
curve for networks of equal hidden layer width agrees with      layer width warrants further study.","Given the large scale of contemporary
   As noted above, our results for deep linear neural net-      regression and classiﬁcation datasets (e.g., [60]), careful
works partially overlap with those obtained previously by       consideration of limits in which the output dimension and
Li and Sompolinsky [37].","the result they obtained through an alternative heuristic,
and, as a result, their criteria for when generalization           This regime has thus far proven challenging to access
improves or degrades with width and depth coincide with         perturbatively, as large deviations from the kernel limit
those obtained here.",2022-03-01 15:51:29+00:00,Contrasting random and learned features in deep Bayesian linear regression,cs.LG,"['cs.LG', 'cond-mat.dis-nn', 'stat.ML']","[arxiv.Result.Author('Jacob A. Zavatone-Veth'), arxiv.Result.Author('William L. Tong'), arxiv.Result.Author('Cengiz Pehlevan')]","Understanding how feature learning affects generalization is among the
foremost goals of modern deep learning theory. Here, we study how the ability
to learn representations affects the generalization performance of a simple
class of models: deep Bayesian linear neural networks trained on unstructured
Gaussian data. By comparing deep random feature models to deep networks in
which all layers are trained, we provide a detailed characterization of the
interplay between width, depth, data density, and prior mismatch. We show that
both models display sample-wise double-descent behavior in the presence of
label noise. Random feature models can also display model-wise double-descent
if there are narrow bottleneck layers, while deep networks do not show these
divergences. Random feature models can have particular widths that are optimal
for generalization at a given data density, while making neural networks as
wide or as narrow as possible is always optimal. Moreover, we show that the
leading-order correction to the kernel-limit learning curve cannot distinguish
between random feature models and deep networks in which all layers are
trained. Taken together, our findings begin to elucidate how architectural
details affect generalization performance in this simple class of deep
regression models."
2642,"Given this observation, we will further study the properties
of training LNNs when σ = 0 in the next subsection.","This is diﬀerent from pure linear regression case where wy will keep
constant after initialization.",The basic setup is same to what we have done in Figure 2.5.,2022-03-01 16:55:51+00:00,Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space,cs.LG,"['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']","[arxiv.Result.Author('Juncai He'), arxiv.Result.Author('Richard Tsai'), arxiv.Result.Author('Rachel Ward')]","The low dimensional manifold hypothesis posits that the data found in many
applications, such as those involving natural images, lie (approximately) on
low dimensional manifolds embedded in a high dimensional Euclidean space. In
this setting, a typical neural network defines a function that takes a finite
number of vectors in the embedding space as input. However, one often needs to
consider evaluating the optimized network at points outside the training
distribution. This paper considers the case in which the training data is
distributed in a linear subspace of $\mathbb R^d$. We derive estimates on the
variation of the learning function, defined by a neural network, in the
direction transversal to the subspace. We study the potential regularization
effects associated with the network's depth and noise in the codimension of the
data manifold. We also present additional side effects in training due to the
presence of noise."
2643,"Given this observation, we will further study the properties
of training LNNs when σ = 0 in the next subsection.","This is diﬀerent from pure linear regression case where wy will keep
constant after initialization.",The basic setup is same to what we have done in Figure 2.5.,2022-03-01 16:55:51+00:00,Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space,cs.LG,"['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']","[arxiv.Result.Author('Juncai He'), arxiv.Result.Author('Richard Tsai'), arxiv.Result.Author('Rachel Ward')]","The low dimensional manifold hypothesis posits that the data found in many
applications, such as those involving natural images, lie (approximately) on
low dimensional manifolds embedded in a high dimensional Euclidean space. In
this setting, a typical neural network defines a function that takes a finite
number of vectors in the embedding space as input. However, one often needs to
consider evaluating the optimized network at points outside the training
distribution. This paper considers the case in which the training data is
distributed in a linear subspace of $\mathbb R^d$. We derive estimates on the
variation of the learning function, defined by a neural network, in the
direction transversal to the subspace. We study the potential regularization
effects associated with the network's depth and noise in the codimension of the
data manifold. We also present additional side effects in training due to the
presence of noise."
2644,"Given this observation, we will further study the properties
of training LNNs when σ = 0 in the next subsection.","This is diﬀerent from pure linear regression case where wy will keep
constant after initialization.",The basic setup is same to what we have done in Figure 2.5.,2022-03-01 16:55:51+00:00,Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space,cs.LG,"['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']","[arxiv.Result.Author('Juncai He'), arxiv.Result.Author('Richard Tsai'), arxiv.Result.Author('Rachel Ward')]","The low dimensional manifold hypothesis posits that the data found in many
applications, such as those involving natural images, lie (approximately) on
low dimensional manifolds embedded in a high dimensional Euclidean space. In
this setting, a typical neural network defines a function that takes a finite
number of vectors in the embedding space as input. However, one often needs to
consider evaluating the optimized network at points outside the training
distribution. This paper considers the case in which the training data is
distributed in a linear subspace of $\mathbb R^d$. We derive estimates on the
variation of the learning function, defined by a neural network, in the
direction transversal to the subspace. We study the potential regularization
effects associated with the network's depth and noise in the codimension of the
data manifold. We also present additional side effects in training due to the
presence of noise."
2647,"The fact that we know that the
approach scales is useful in motivating further research eﬀort, both small-compute and large-compute.","Indeed, one could implement the same algorithms on a smaller scale and still generate interesting
recall and generalisation capabilities across a targeted range of tasks.",8.3.,2022-03-01 19:32:27+00:00,Learning Robust Real-Time Cultural Transmission without Human Data,cs.LG,"['cs.LG', 'cs.AI', 'cs.MA']","[arxiv.Result.Author('Cultural General Intelligence Team'), arxiv.Result.Author('Avishkar Bhoopchand'), arxiv.Result.Author('Bethanie Brownfield'), arxiv.Result.Author('Adrian Collister'), arxiv.Result.Author('Agustin Dal Lago'), arxiv.Result.Author('Ashley Edwards'), arxiv.Result.Author('Richard Everett'), arxiv.Result.Author('Alexandre Frechette'), arxiv.Result.Author('Yanko Gitahy Oliveira'), arxiv.Result.Author('Edward Hughes'), arxiv.Result.Author('Kory W. Mathewson'), arxiv.Result.Author('Piermaria Mendolicchio'), arxiv.Result.Author('Julia Pawar'), arxiv.Result.Author('Miruna Pislar'), arxiv.Result.Author('Alex Platonov'), arxiv.Result.Author('Evan Senter'), arxiv.Result.Author('Sukhdeep Singh'), arxiv.Result.Author('Alexander Zacherl'), arxiv.Result.Author('Lei M. Zhang')]","Cultural transmission is the domain-general social skill that allows agents
to acquire and use information from each other in real-time with high fidelity
and recall. In humans, it is the inheritance process that powers cumulative
cultural evolution, expanding our skills, tools and knowledge across
generations. We provide a method for generating zero-shot, high recall cultural
transmission in artificially intelligent agents. Our agents succeed at
real-time cultural transmission from humans in novel contexts without using any
pre-collected human data. We identify a surprisingly simple set of ingredients
sufficient for generating cultural transmission and develop an evaluation
methodology for rigorously assessing it. This paves the way for cultural
evolution as an algorithm for developing artificial general intelligence."
2664,There are a number of avenues for further research.,"In doing so, we add further mathematical credence to the
great potential of deep neural networks and deep learning for solving inverse
problems.","First, on the theoretical
side, is extending these results to instances where the analysis operator does
not form a frame.",2022-03-02 00:44:25+00:00,"Stable, accurate and efficient deep neural networks for inverse problems with analysis-sparse models",cs.LG,"['cs.LG', 'cs.CV', 'cs.IT', 'cs.NA', 'eess.IV', 'math.IT', 'math.NA', 'E.4; H.1.1; G.1.6; I.2.6; I.4.5; I.5.1; J.2']","[arxiv.Result.Author('Maksym Neyra-Nesterenko'), arxiv.Result.Author('Ben Adcock')]","Solving inverse problems is a fundamental component of science, engineering
and mathematics. With the advent of deep learning, deep neural networks have
significant potential to outperform existing state-of-the-art, model-based
methods for solving inverse problems. However, it is known that current
data-driven approaches face several key issues, notably instabilities and
hallucinations, with potential impact in critical tasks such as medical
imaging. This raises the key question of whether or not one can construct
stable and accurate deep neural networks for inverse problems. In this work, we
present a novel construction of an accurate, stable and efficient neural
network for inverse problems with general analysis-sparse models. To construct
the network, we unroll NESTA, an accelerated first-order method for convex
optimization. Combined with a compressed sensing analysis, we prove accuracy
and stability. Finally, a restart scheme is employed to enable exponential
decay of the required network depth, yielding a shallower, and consequently
more efficient, network. We showcase this approach in the case of Fourier
imaging, and verify its stability and performance via a series of numerical
experiments. The key impact of this work is to provide theoretical guarantees
for computing and developing stable neural networks in practice."
2665,There are a number of avenues for further research.,"In doing so, we add further
mathematical credence to the great potential of deep neural networks and deep
learning for solving inverse problems.","First, on the theoretical
side, is extending these results to instances where the analysis operator does
not form a frame.",2022-03-02 00:44:25+00:00,"NESTANets: Stable, accurate and efficient neural networks for analysis-sparse inverse problems",cs.LG,"['cs.LG', 'cs.CV', 'cs.IT', 'cs.NA', 'eess.IV', 'math.IT', 'math.NA', 'E.4; H.1.1; G.1.6; I.2.6; I.4.5; I.5.1; J.2']","[arxiv.Result.Author('Maksym Neyra-Nesterenko'), arxiv.Result.Author('Ben Adcock')]","Solving inverse problems is a fundamental component of science, engineering
and mathematics. With the advent of deep learning, deep neural networks have
significant potential to outperform existing state-of-the-art, model-based
methods for solving inverse problems. However, it is known that current
data-driven approaches face several key issues, notably hallucinations,
instabilities and unpredictable generalization, with potential impact in
critical tasks such as medical imaging. This raises the key question of whether
or not one can construct deep neural networks for inverse problems with
explicit stability and accuracy guarantees. In this work, we present a novel
construction of accurate, stable and efficient neural networks for inverse
problems with general analysis-sparse models, termed NESTANets. To construct
the network, we first unroll NESTA, an accelerated first-order method for
convex optimization. The slow convergence of this method leads to deep networks
with low efficiency. Therefore, to obtain shallow, and consequently more
efficient, networks we combine NESTA with a novel restart scheme. We then use
compressed sensing techniques to demonstrate accuracy and stability. We
showcase this approach in the case of Fourier imaging, and verify its stability
and performance via a series of numerical experiments. The key impact of this
work is demonstrating the construction of efficient neural networks based on
unrolling with guaranteed stability and accuracy."
2677,"These
values correspond to the assumed PIs of 90% with lower and          In further research, we plan to enrich the input information
upper bounds q = 0.05 and q = 0.95, respectively.",1.47% are below PIs and 5.08%± 1.61% are above PIs.,with a learned context vector.,2022-03-02 08:39:33+00:00,ES-dRNN with Dynamic Attention for Short-Term Load Forecasting,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Author('Slawek Smyl'), arxiv.Result.Author('Grzegorz Dudek'), arxiv.Result.Author('Paweł Pełka')]","Short-term load forecasting (STLF) is a challenging problem due to the
complex nature of the time series expressing multiple seasonality and varying
variance. This paper proposes an extension of a hybrid forecasting model
combining exponential smoothing and dilated recurrent neural network (ES-dRNN)
with a mechanism for dynamic attention. We propose a new gated recurrent cell
-- attentive dilated recurrent cell, which implements an attention mechanism
for dynamic weighting of input vector components. The most relevant components
are assigned greater weights, which are subsequently dynamically fine-tuned.
This attention mechanism helps the model to select input information and, along
with other mechanisms implemented in ES-dRNN, such as adaptive time series
processing, cross-learning, and multiple dilation, leads to a significant
improvement in accuracy when compared to well-established statistical and
state-of-the-art machine learning forecasting models. This was confirmed in the
extensive experimental study concerning STLF for 35 European countries."
2680,"In further research, we plan to develop the opposed response-based approach
for other types of learners, e.g.","Further improve-
ment of the winning solution was achieved by weighting the training patterns
according to their similarity to the query pattern.","decision trees, and other types of problems (re-
gression, classiﬁcation).",2022-03-02 09:43:18+00:00,Boosted Ensemble Learning based on Randomized NNs for Time Series Forecasting,cs.LG,['cs.LG'],[arxiv.Result.Author('Grzegorz Dudek')],"Time series forecasting is a challenging problem particularly when a time
series expresses multiple seasonality, nonlinear trend and varying variance. In
this work, to forecast complex time series, we propose ensemble learning which
is based on randomized neural networks, and boosted in three ways. These
comprise ensemble learning based on residuals, corrected targets and opposed
response. The latter two methods are employed to ensure similar forecasting
tasks are solved by all ensemble members, which justifies the use of exactly
the same base models at all stages of ensembling. Unification of the tasks for
all members simplifies ensemble learning and leads to increased forecasting
accuracy. This was confirmed in an experimental study involving forecasting
time series with triple seasonality, in which we compare our three variants of
ensemble boosting. The strong points of the proposed ensembles based on RandNNs
are extremely rapid training and pattern-based time series representation,
which extracts relevant information from time series."
2688,"Whereas the overall               traces that integrate with the RL-Ecosystem (Gym [14])
                                       aims of algorithm development are often to improve generality
                                       and especially sample efﬁciency, the employed methods are
   • We provide an agenda for further research on how to           on reinforcement learning here.","In Game AI, competitions have been an important                   show that they enable a compression ratio of up to 104 : 1
                                       part of scientiﬁc conferences for a long time already, and                for ofﬂine RL datasets (Section IV-A)
                                       especially game problem benchmarks are currently more and
                                       more spreading out to core AI conferences, e.g., with the              • We provide Plug-n-Play code [13] to collect minimal
                                       MineRL competition at NeurIPS [2], [3].","In reinforcement learning,
      obtain veriﬁable RL experiment results using minimal         previous works suggest guidelines on how to design and report
      traces (Section V)                                           a well reproducible experiment [11].",2022-03-02 12:55:27+00:00,Reliable validation of Reinforcement Learning Benchmarks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Matthias Müller-Brockhausen'), arxiv.Result.Author('Aske Plaat'), arxiv.Result.Author('Mike Preuss')]","Reinforcement Learning (RL) is one of the most dynamic research areas in Game
AI and AI as a whole, and a wide variety of games are used as its prominent
test problems. However, it is subject to the replicability crisis that
currently affects most algorithmic AI research. Benchmarking in Reinforcement
Learning could be improved through verifiable results. There are numerous
benchmark environments whose scores are used to compare different algorithms,
such as Atari. Nevertheless, reviewers must trust that figures represent
truthful values, as it is difficult to reproduce an exact training curve. We
propose improving this situation by providing access to the original
experimental data to validate study results. To that end, we rely on the
concept of minimal traces. These allow re-simulation of action sequences in
deterministic RL environments and, in turn, enable reviewers to verify, re-use,
and manually inspect experimental results without needing large compute
clusters. It also permits validation of presented reward graphs, an inspection
of individual episodes, and re-use of result data (baselines) for proper
comparison in follow-up papers. We offer plug-and-play code that works with Gym
so that our measures fit well in the existing RL and reproducibility
eco-system. Our approach is freely available, easy to use, and adds minimal
overhead, as minimal traces allow a data compression ratio of up to $\approx
10^4:1$ (94GB to 8MB for Atari Pong) compared to a regular MDP trace used in
offline RL datasets. The paper presents proof-of-concept results for a variety
of games."
2696,"While these results are not state of the art, we believe this direc-
                                       tion is worthy of further study, and may be a good ﬁrst step towards the utlimate goal of achieving
                                       simple, equivariant networks with strong theoretical properties and empirical success.","We present preliminary
                                       experimental results for this model.","1.1 PRELIMINARIES

                                       Group actions and equivariance Given two (possibly different) vector spaces W1, W2, and a
                                       group G which acts on these vector spaces, we say that f : W1 → W2 is equivariant if

                                                                             f (gw) = gf (w), ∀w ∈ W1, g ∈ G.
                                       We say f is invariant in the special case where the action of G on W2 is trivial, that is gw2 = w2
                                       for all g ∈ G and w2 ∈ W2.",2022-03-02 16:19:01+00:00,A simple and universal rotation equivariant point-cloud network,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Finkelshtein'), arxiv.Result.Author('Chaim Baskin'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('Nadav Dym')]","Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}"
2697,"We ﬁnd that adding these mappings for k = 2 improves our results, and generalizing this to higher
dimensions is an interesting challenge for further research.","When
k = 2 these linear layers are spanned by the linear mappings

V → V, V → V T , V → trace(V )I3, V ∈ T2 = R3×3.","3 THEORETICAL PROPERTIES

We now discuss the expressive power of the architecture F(K, C) deﬁned above.",2022-03-02 16:19:01+00:00,A simple and universal rotation equivariant point-cloud network,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Finkelshtein'), arxiv.Result.Author('Chaim Baskin'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('Nadav Dym')]","Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}"
2698,"While these results are not state of the art, we believe this direc-
                                       tion is worthy of further study, and may be a good ﬁrst step towards the utlimate goal of achieving
                                       simple, equivariant networks with strong theoretical properties and empirical success.","We present preliminary
                                       experimental results for this model.","1.1 PRELIMINARIES

                                       Group actions and equivariance Given two (possibly different) vector spaces W1, W2, and a
                                       group G which acts on these vector spaces, we say that f : W1 → W2 is equivariant if

                                                                             f (gw) = gf (w), ∀w ∈ W1, g ∈ G.
                                       We say f is invariant in the special case where the action of G on W2 is trivial, that is gw2 = w2
                                       for all g ∈ G and w2 ∈ W2.",2022-03-02 16:19:01+00:00,A Simple and Universal Rotation Equivariant Point-cloud Network,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Finkelshtein'), arxiv.Result.Author('Chaim Baskin'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('Nadav Dym')]","Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}"
2699,"We ﬁnd that adding these mappings for k = 2 improves our results, and generalizing this to higher
dimensions is an interesting challenge for further research.","When
k = 2 these linear layers are spanned by the linear mappings

V → V, V → V T , V → trace(V )I3, V ∈ T2 = R3×3.","3 THEORETICAL PROPERTIES

We now discuss the expressive power of the architecture F(K, C) deﬁned above.",2022-03-02 16:19:01+00:00,A Simple and Universal Rotation Equivariant Point-cloud Network,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Finkelshtein'), arxiv.Result.Author('Chaim Baskin'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('Nadav Dym')]","Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}"
2700,"While our results are currently not state of the     Tensor product mappings are a standard method to equiv-
art, we believe this direction is worthy of further study, and
may be a good ﬁrst step towards the utlimate goal of achiev-     ariantly map lower order representations to higher order
ing simple, equivariant networks with strong theoretical
properties and empirical success.","These results are presented
in Table 1.",representations.,2022-03-02 16:19:01+00:00,A Simple and Universal Rotation Equivariant Point-cloud Network,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Finkelshtein'), arxiv.Result.Author('Chaim Baskin'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('Nadav Dym')]","Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}"
2701,"We ﬁnd that adding these mappings for k = 2 improves
our results, and generalizing this to higher dimensions is an
interesting challenge for further research.","When k = 2 these                   Implementation We trained and tested our model on
linear layers are spanned by the linear mappings                         NVidia GTX A6000 GPUs with python 3.9, Cuda 11.3,

V → V, V → V T , V → trace(V )I3, V ∈ T2 = R3×3.","A simple and universal rotation equivariant point-cloud network

Methods                                                           z/z z/SO(3) SO(3)/SO(3)

SFCNN Rao et al.",2022-03-02 16:19:01+00:00,A Simple and Universal Rotation Equivariant Point-cloud Network,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Finkelshtein'), arxiv.Result.Author('Chaim Baskin'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('Nadav Dym')]","Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}"
2702,"At the same time, we believe the inability of our method, as
                                                                  well as other semi-lifted approaches such as TFN, to outper-
All methods we compared to, as well as our own method,            form methods with low-dimensional representations is wor-
are invariant to rigid motions and permutations, and as a         thy of further study and raises many interesting questions
result are hardly effected when the test data is augmented by     for further reseach.","In general We ﬁnd that our model does not perform
as well as many of the models we compared too.","For example: Are the low dimensional
general 3D rotations (z/SO(3)) or when both test and train        models universal?",2022-03-02 16:19:01+00:00,A Simple and Universal Rotation Equivariant Point-cloud Network,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Finkelshtein'), arxiv.Result.Author('Chaim Baskin'), arxiv.Result.Author('Haggai Maron'), arxiv.Result.Author('Nadav Dym')]","Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}"
2703,"In
the current section, a further study of the correlation of the modal coordinates
is performed.","The adversarial loss, as discussed,
enforces the statistical independence of the modal coordinates and, if overlooked
in some degree, the result may be that the modal coordinates are correlated.","28
Figure 26: Superposition/inverse modal transformation for the experimental system(red) and
original displacements (blue), state 14.",2022-03-02 16:46:41+00:00,On the application of generative adversarial networks for nonlinear modal analysis,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('G. Tsialiamanis'), arxiv.Result.Author('M. D. Champneys'), arxiv.Result.Author('N. Dervilis'), arxiv.Result.Author('D. J. Wagg'), arxiv.Result.Author('K. Worden')]","Linear modal analysis is a useful and effective tool for the design and
analysis of structures. However, a comprehensive basis for nonlinear modal
analysis remains to be developed. In the current work, a machine learning
scheme is proposed with a view to performing nonlinear modal analysis. The
scheme is focussed on defining a one-to-one mapping from a latent `modal' space
to the natural coordinate space, whilst also imposing orthogonality of the mode
shapes. The mapping is achieved via the use of the recently-developed
cycle-consistent generative adversarial network (cycle-GAN) and an assembly of
neural networks targeted on maintaining the desired orthogonality. The method
is tested on simulated data from structures with cubic nonlinearities and
different numbers of degrees of freedom, and also on data from an experimental
three-degree-of-freedom set-up with a column-bumper nonlinearity. The results
reveal the method's efficiency in separating the `modes'. The method also
provides a nonlinear superposition function, which in most cases has very good
accuracy."
2704,We further study the importance of parameter sharing in HIGHMMT.,In which components of the HIGHMMT model is parameter sharing important?,"From the ablation studies in Table 13, using separate
parameters for either unimodal or multimodal layers results in worse performance.",2022-03-02 18:56:20+00:00,HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Paul Pu Liang'), arxiv.Result.Author('Yiwei Lyu'), arxiv.Result.Author('Xiang Fan'), arxiv.Result.Author('Shengtong Mo'), arxiv.Result.Author('Dani Yogatama'), arxiv.Result.Author('Louis-Philippe Morency'), arxiv.Result.Author('Ruslan Salakhutdinov')]","Learning multimodal representations involves discovering correspondences and
integrating information from multiple heterogeneous sources of data. While
recent research has begun to explore the design of more general-purpose
multimodal models (contrary to prior focus on domain and modality-specific
architectures), these methods are still largely focused on a small set of
modalities in the language, vision, and audio space. In order to accelerate
generalization towards diverse and understudied modalities, we investigate
methods for high-modality (a large set of diverse modalities) and
partially-observable (each task only defined on a small subset of modalities)
scenarios. To tackle these challenges, we design a general multimodal model
that enables multitask and transfer learning: multitask learning with shared
parameters enables stable parameter counts (addressing scalability), and
cross-modal transfer learning enables information sharing across modalities and
tasks (addressing partial observability). Our resulting model generalizes
across text, image, video, audio, time-series, sensors, tables, and set
modalities from different research areas, improves the tradeoff between
performance and efficiency, transfers to new modalities and tasks, and reveals
surprising insights on the nature of information sharing in multitask models.
We release our code and benchmarks which we hope will present a unified
platform for subsequent theoretical and empirical analysis:
https://github.com/pliang279/HighMMT."
2705,We further study the importance of parameter sharing in HIGHMMT.,In which components of the HIGHMMT model is parameter sharing important?,"From the ablation studies in Table 13, using separate
parameters for either unimodal or multimodal layers results in worse performance.",2022-03-02 18:56:20+00:00,HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Paul Pu Liang'), arxiv.Result.Author('Yiwei Lyu'), arxiv.Result.Author('Xiang Fan'), arxiv.Result.Author('Shentong Mo'), arxiv.Result.Author('Dani Yogatama'), arxiv.Result.Author('Louis-Philippe Morency'), arxiv.Result.Author('Ruslan Salakhutdinov')]","Learning multimodal representations involves discovering correspondences and
integrating information from multiple heterogeneous sources of data. While
recent research has begun to explore the design of more general-purpose
multimodal models (contrary to prior focus on domain and modality-specific
architectures), these methods are still largely focused on a small set of
modalities in the language, vision, and audio space. In order to accelerate
generalization towards diverse and understudied modalities, we investigate
methods for high-modality (a large set of diverse modalities) and
partially-observable (each task only defined on a small subset of modalities)
scenarios. To tackle these challenges, we design a general multimodal model
that enables multitask and transfer learning: multitask learning with shared
parameters enables stable parameter counts (addressing scalability), and
cross-modal transfer learning enables information sharing across modalities and
tasks (addressing partial observability). Our resulting model generalizes
across text, image, video, audio, time-series, sensors, tables, and set
modalities from different research areas, improves the tradeoff between
performance and efficiency, transfers to new modalities and tasks, and reveals
surprising insights on the nature of information sharing in multitask models.
We release our code and benchmarks which we hope will present a unified
platform for subsequent theoretical and empirical analysis:
https://github.com/pliang279/HighMMT."
2706,We further study the importance of parameter sharing in HighMMT.,"In which components of the HighMMT model is parameter sharing
important?","From the ablation studies in Table 12,
using separate parameters for either unimodal or multimodal layers results in worse performance.",2022-03-02 18:56:20+00:00,HighMMT: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Paul Pu Liang'), arxiv.Result.Author('Yiwei Lyu'), arxiv.Result.Author('Xiang Fan'), arxiv.Result.Author('Jeffrey Tsaw'), arxiv.Result.Author('Yudong Liu'), arxiv.Result.Author('Shentong Mo'), arxiv.Result.Author('Dani Yogatama'), arxiv.Result.Author('Louis-Philippe Morency'), arxiv.Result.Author('Ruslan Salakhutdinov')]","Many real-world problems are inherently multimodal, from the communicative
modalities humans use to express social and emotional states to the force,
proprioception, and visual sensors ubiquitous on robots. While there has been
an explosion of interest in multimodal representation learning, these methods
are still largely focused on a small set of modalities, primarily in the
language, vision, and audio space. In order to accelerate generalization
towards diverse and understudied modalities, this paper studies efficient
representation learning for high-modality scenarios. Since adding new models
for every new modality or task becomes prohibitively expensive, a critical
technical challenge is heterogeneity quantification: how can we measure which
modalities encode similar information and interactions in order to permit
parameter sharing with previous modalities? We propose two new
information-theoretic metrics for heterogeneity quantification: (1) modality
heterogeneity studies how similar 2 modalities $\{X_1,X_2\}$ are by measuring
how much information can be transferred from $X_1$ to $X_2$, while (2)
interaction heterogeneity studies how similarly pairs of modalities
$\{X_1,X_2\}, \{X_3,X_4\}$ interact by measuring how much interaction
information can be transferred from $\{X_1,X_2\}$ to $\{X_3,X_4\}$. We show the
importance of these proposed metrics in high-modality scenarios as a way to
automatically prioritize the fusion of modalities that contain unique
information or interactions. The result is a single model, HighMMT, that scales
up to $10$ modalities and $15$ tasks from $5$ different research areas. Not
only does HighMMT outperform prior methods on the tradeoff between performance
and efficiency, it also demonstrates a crucial scaling behavior: performance
continues to improve with each modality added, and transfers to entirely new
modalities and tasks during fine-tuning."
2737,"[2020]
further study what makes good views for contrastive learning.",Tian et al.,"They propose an “InfoMin principle”, where
anchors and positives should share the least information necessary for the contrastive model to do well on
the downstream task.",2022-03-03 05:03:28+00:00,Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations,cs.LG,['cs.LG'],"[arxiv.Result.Author('Michael Zhang'), arxiv.Result.Author('Nimit S. Sohoni'), arxiv.Result.Author('Hongyang R. Zhang'), arxiv.Result.Author('Chelsea Finn'), arxiv.Result.Author('Christopher Ré')]","Spurious correlations pose a major challenge for robust machine learning.
Models trained with empirical risk minimization (ERM) may learn to rely on
correlations between class labels and spurious attributes, leading to poor
performance on data groups without these correlations. This is particularly
challenging to address when spurious attribute labels are unavailable. To
improve worst-group performance on spuriously correlated data without training
attribute labels, we propose Correct-N-Contrast (CNC), a contrastive approach
to directly learn representations robust to spurious correlations. As ERM
models can be good spurious attribute predictors, CNC works by (1) using a
trained ERM model's outputs to identify samples with the same class but
dissimilar spurious features, and (2) training a robust model with contrastive
learning to learn similar representations for same-class samples. To support
CNC, we introduce new connections between worst-group error and a
representation alignment loss that CNC aims to minimize. We empirically observe
that worst-group error closely tracks with alignment loss, and prove that the
alignment loss over a class helps upper-bound the class's worst-group vs.
average error gap. On popular benchmarks, CNC reduces alignment loss
drastically, and achieves state-of-the-art worst-group accuracy by 3.6% average
absolute lift. CNC is also competitive with oracle methods that require group
labels."
2749,"Moreover, the
repository may be extended by new implementations and datasets, thus facilitating further research.","The repository that we developed for the presented empirical comparison includes implementations of all al-
gorithms, as well as all datasets, and is publicly available,3 allowing for experiment reproducibility.","In the future, new algorithms will be added to this framework, such as RelClass [28] and DTEC [42].",2022-03-03 10:43:56+00:00,Early Time-Series Classification Algorithms: An Empirical Comparison,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Charilaos Akasiadis'), arxiv.Result.Author('Evgenios Kladis'), arxiv.Result.Author('Evangelos Michelioudakis'), arxiv.Result.Author('Elias Alevizos'), arxiv.Result.Author('Alexander Artikis')]","Early Time-Series Classification (ETSC) is the task of predicting the class
of incoming time-series by observing as few measurements as possible. Such
methods can be employed to obtain classification forecasts in many
time-critical applications. However, available techniques are not equally
suitable for every problem, since differentiations in the data characteristics
can impact algorithm performance in terms of earliness, accuracy, F1-score, and
training time. We evaluate six existing ETSC algorithms on publicly available
data, as well as on two newly introduced datasets originating from the life
sciences and maritime domains. Our goal is to provide a framework for the
evaluation and comparison of ETSC algorithms and to obtain intuition on how
such approaches perform on real-life applications. The presented framework may
also serve as a benchmark for new related techniques."
2750,"On the other hand, grouping patients needs to be
handled carefully and further research is needed to ensure fairness and reliability with respect to
underlying and hidden attributes of different groups.","Being able to correctly model
the dependencies of groups of patients is important and offers the potential of correctly identifying
underlying causes of diseases on a group level.","8 REPRODUCIBILITY STATEMENT

For all theoretical statements, we provide detailed derivations and state the necessary assumptions.",2022-03-03 10:44:50+00:00,Learning Group Importance using the Differentiable Hypergeometric Distribution,cs.LG,"['cs.LG', 'stat.CO', 'stat.ML']","[arxiv.Result.Author('Thomas M. Sutter'), arxiv.Result.Author('Laura Manduchi'), arxiv.Result.Author('Alain Ryser'), arxiv.Result.Author('Julia E. Vogt')]","Partitioning a set of elements into subsets of a priori unknown sizes is
essential in many applications. These subset sizes are rarely explicitly
learned - be it the cluster sizes in clustering applications or the number of
shared versus independent generative latent factors in weakly-supervised
learning. Probability distributions over correct combinations of subset sizes
are non-differentiable due to hard constraints, which prohibit gradient-based
optimization. In this work, we propose the differentiable hypergeometric
distribution. The hypergeometric distribution models the probability of
different group sizes based on their relative importance. We introduce
reparameterizable gradients to learn the importance between groups and
highlight the advantage of explicitly learning the size of subsets in two
typical applications: weakly-supervised learning and clustering. In both
applications, we outperform previous approaches, which rely on suboptimal
heuristics to model the unknown size of groups."
2753,"Therefore, there is scope for further research in this direction.","But, they have
       been mostly applied to centralized data although many real world data is distributed in
       nature.","• Random projection based techniques are useful once the intrinsic structure of the data and
       number of dimensions to be used for projection is known.",2022-03-03 12:47:46+00:00,Anomaly Detection in Big Data,cs.LG,['cs.LG'],[arxiv.Result.Author('Chandresh Kumar Maurya')],"Anomaly is defined as a state of the system that do not conform to the normal
behavior. For example, the emission of neutrons in a nuclear reactor channel
above the specified threshold is an anomaly. Big data refers to the data set
that is \emph{high volume, streaming, heterogeneous, distributed} and often
\emph{sparse}. Big data is not uncommon these days. For example, as per
Internet live stats, the number of tweets posted per day has gone above 500
millions. Due to data explosion in data laden domains, traditional anomaly
detection techniques developed for small data sets scale poorly on large-scale
data sets. Therefore, we take an alternative approach to tackle anomaly
detection in big data. Essentially, there are two ways to scale anomaly
detection in big data. The first is based on the \emph{online} learning and the
second is based on the \emph{distributed} learning. Our aim in the thesis is to
tackle big data problems while detecting anomaly efficiently. To that end, we
first take \emph{streaming} issue of the big data and propose
Passive-Aggressive GMEAN (PAGMEAN) algorithms. Although, online learning
algorithm can scale well over large number of data points and dimensions, they
can not process data when it is distributed at multiple locations; which is
quite common these days. Therefore, we propose anomaly detection algorithm
which is inherently distributed using ADMM. Finally, we present a case study on
anomaly detection in nuclear power plant data."
2755,"Investigating such speed/memory tradeoﬀ is outside of the scope of this paper, but is

a promising direction for further research.","This empirical ﬁnding suggests that algorithms using a

subquadratic amount of memory may provably converge slower than algorithms without memory

constraints.","6 Proof technique

For the least squares problem, the analysis of stochastic gradient methods is well studied and
techniques have been thoroughly reﬁned.",2022-03-03 14:39:33+00:00,Accelerated SGD for Non-Strongly-Convex Least Squares,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Aditya Varre'), arxiv.Result.Author('Nicolas Flammarion')]","We consider stochastic approximation for the least squares regression problem
in the non-strongly convex setting. We present the first practical algorithm
that achieves the optimal prediction error rates in terms of dependence on the
noise of the problem, as $O(d/t)$ while accelerating the forgetting of the
initial conditions to $O(d/t^2)$. Our new algorithm is based on a simple
modification of the accelerated gradient descent. We provide convergence
results for both the averaged and the last iterate of the algorithm. In order
to describe the tightness of these new bounds, we present a matching lower
bound in the noiseless setting and thus show the optimality of our algorithm."
2779,"CRL-SEQ with

linear models or neural nets), running more experiments and further studying the role of ﬁnite sample effects &

representation divergences.","Finally, it

would be interesting to explore the tightness of these bounds for concrete analytical settings (e.g.","22
D.1 Proof of Theorem 4

Proof Applying Lemma 1 for each task 1 ≤ t ≤ T and union bounding, with probability 1 − 2T e−τ , for all T
applications of (CRL-SEQ), we have that

L(ht, φtnew + φtfrz) − L ,t ≤ MMse,qt + MMtemp +  O(C(H) + CnΦew + τ )                             (18)
                                                             N

                                                  excess empirical error

       t        O(C(H) +N CnΦew + τ ) + MMtemp.",2022-03-03 21:23:08+00:00,Provable and Efficient Continual Representation Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yingcong Li'), arxiv.Result.Author('Mingchen Li'), arxiv.Result.Author('M. Salman Asif'), arxiv.Result.Author('Samet Oymak')]","In continual learning (CL), the goal is to design models that can learn a
sequence of tasks without catastrophic forgetting. While there is a rich set of
techniques for CL, relatively little understanding exists on how
representations built by previous tasks benefit new tasks that are added to the
network. To address this, we study the problem of continual representation
learning (CRL) where we learn an evolving representation as new tasks arrive.
Focusing on zero-forgetting methods where tasks are embedded in subnetworks
(e.g., PackNet), we first provide experiments demonstrating CRL can
significantly boost sample efficiency when learning new tasks. To explain this,
we establish theoretical guarantees for CRL by providing sample complexity and
generalization error bounds for new tasks by formalizing the statistical
benefits of previously-learned representations. Our analysis and experiments
also highlight the importance of the order in which we learn the tasks.
Specifically, we show that CL benefits if the initial tasks have large sample
size and high ""representation diversity"". Diversity ensures that adding new
tasks incurs small representation mismatch and can be learned with few samples
while training only few additional nonzero weights. Finally, we ask whether one
can ensure each task subnetwork to be efficient during inference time while
retaining the benefits of representation learning. To this end, we propose an
inference-efficient variation of PackNet called Efficient Sparse PackNet (ESPN)
which employs joint channel & weight pruning. ESPN embeds tasks in
channel-sparse subnets requiring up to 80% less FLOPs to compute while
approximately retaining accuracy and is very competitive with a variety of
baselines. In summary, this work takes a step towards data and
compute-efficient CL with a representation learning perspective. GitHub page:
https://github.com/ucr-optml/CtRL"
2780,"CRL-SEQ with linear models or neural nets),
run more experiments and further study the role of ﬁnite sample eﬀects & representation
divergences.","Finally, it would be interesting to explore the tightness of these
bounds for concrete analytical settings (e.g.",7.,2022-03-03 21:23:08+00:00,Provable and Efficient Continual Representation Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yingcong Li'), arxiv.Result.Author('Mingchen Li'), arxiv.Result.Author('M. Salman Asif'), arxiv.Result.Author('Samet Oymak')]","In continual learning (CL), the goal is to design models that can learn a
sequence of tasks without catastrophic forgetting. While there is a rich set of
techniques for CL, relatively little understanding exists on how
representations built by previous tasks benefit new tasks that are added to the
network. To address this, we study the problem of continual representation
learning (CRL) where we learn an evolving representation as new tasks arrive.
Focusing on zero-forgetting methods where tasks are embedded in subnetworks
(e.g., PackNet), we first provide experiments demonstrating CRL can
significantly boost sample efficiency when learning new tasks. To explain this,
we establish theoretical guarantees for CRL by providing sample complexity and
generalization error bounds for new tasks by formalizing the statistical
benefits of previously-learned representations. Our analysis and experiments
also highlight the importance of the order in which we learn the tasks.
Specifically, we show that CL benefits if the initial tasks have large sample
size and high ""representation diversity"". Diversity ensures that adding new
tasks incurs small representation mismatch and can be learned with few samples
while training only few additional nonzero weights. Finally, we ask whether one
can ensure each task subnetwork to be efficient during inference time while
retaining the benefits of representation learning. To this end, we propose an
inference-efficient variation of PackNet called Efficient Sparse PackNet (ESPN)
which employs joint channel & weight pruning. ESPN embeds tasks in
channel-sparse subnets requiring up to 80% less FLOPs to compute while
approximately retaining accuracy and is very competitive with a variety of
baselines. In summary, this work takes a step towards data and
compute-efficient CL with a representation learning perspective. GitHub page:
https://github.com/ucr-optml/CtRL"
2786,"As we transition to larger
# Decoder Params 0       0     ∼0 0.98                  models and higher latency, our results show that
                                                        the Transformer-XL architecture is nearly optimal
   We further study the decoder parameter count         on WikiText-103 when performing inference on a
proxy in scenarios where the range of model sizes       CPU.","As seen, in the low-latency regime, LTS consis-
Partial Training   500   231   66 0.92                  tently ﬁnds models that have signiﬁcantly lower
                  5,000  2690  768 0.96                 perplexity compared to naive scaling of the base-
                                                        line Transformer-XL.","When targeting a GPU, the pareto-frontier
provided for search is limited.",2022-03-04 02:10:43+00:00,LiteTransformerSearch: Training-free On-device Search for Efficient Autoregressive Language Models,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Mojan Javaheripi'), arxiv.Result.Author('Shital Shah'), arxiv.Result.Author('Subhabrata Mukherjee'), arxiv.Result.Author('Tomasz L. Religa'), arxiv.Result.Author('Caio C. T. Mendes'), arxiv.Result.Author('Gustavo H. de Rosa'), arxiv.Result.Author('Sebastien Bubeck'), arxiv.Result.Author('Farinaz Koushanfar'), arxiv.Result.Author('Debadeepta Dey')]","The transformer architecture is ubiquitously used as the building block of
most large-scale language models. However, it remains a painstaking guessing
game of trial and error to set its myriad of architectural hyperparameters,
e.g., number of layers, number of attention heads, and inner size of the feed
forward network, and find architectures with the optimal trade-off between task
performance like perplexity and compute constraints like memory and latency.
This challenge is further exacerbated by the proliferation of various hardware.
In this work, we leverage the somewhat surprising empirical observation that
the number of non-embedding parameters in autoregressive transformers has a
high rank correlation with task performance, irrespective of the architectural
hyperparameters. Since architectural hyperparameters affect the latency and
memory footprint in a hardware-dependent manner, the above observation
organically induces a simple search algorithm that can be directly run on
target devices. We rigorously show that the latency and perplexity
pareto-frontier can be found without need for any model training, using
non-embedding parameters as a proxy for perplexity. We evaluate our method,
dubbed Lightweight Transformer Search (LTS), on diverse devices from ARM CPUs
to Nvidia GPUs and show that the perplexity of Transformer-XL can be achieved
with up to 2x lower latency. LTS extracts the pareto-frontier in less than 3
hours while running on a commodity laptop. We effectively remove the carbon
footprint of training for hundreds of GPU hours, offering a strong simple
baseline for future NAS methods in autoregressive language modeling."
2787,"We further study the decoder parameter proxy in scenarios where the range of model sizes provided
for search is limited.","In addition to the small
distance between the prxoy-estimated Pareto-frontier and the ground truth, our zero-cost proxy holds
a high SRC of 0.98 over the entire Pareto, i.e., all 1200 sampled architectures.","We categorize the total 1200 sampled architectures into different bins based
on the decoder parameters.",2022-03-04 02:10:43+00:00,LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Mojan Javaheripi'), arxiv.Result.Author('Gustavo H. de Rosa'), arxiv.Result.Author('Subhabrata Mukherjee'), arxiv.Result.Author('Shital Shah'), arxiv.Result.Author('Tomasz L. Religa'), arxiv.Result.Author('Caio C. T. Mendes'), arxiv.Result.Author('Sebastien Bubeck'), arxiv.Result.Author('Farinaz Koushanfar'), arxiv.Result.Author('Debadeepta Dey')]","The Transformer architecture is ubiquitously used as the building block of
large-scale autoregressive language models. However, finding architectures with
the optimal trade-off between task performance (perplexity) and hardware
constraints like peak memory utilization and latency is non-trivial. This is
exacerbated by the proliferation of various hardware. We leverage the somewhat
surprising empirical observation that the number of decoder parameters in
autoregressive Transformers has a high rank correlation with task performance,
irrespective of the architecture topology. This observation organically induces
a simple Neural Architecture Search (NAS) algorithm that uses decoder
parameters as a proxy for perplexity without need for any model training. The
search phase of our training-free algorithm, dubbed Lightweight Transformer
Search (LTS), can be run directly on target devices since it does not require
GPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of
perplexity versus any hardware performance cost. We evaluate LTS on diverse
devices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer
backbones: GPT-2 and Transformer-XL. Results show that the perplexity of
16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster
runtime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero
and one-shot settings, LTS Pareto-frontier models achieve higher average
accuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x
lower latency. LTS extracts the Pareto-frontier in under 3 hours while running
on a commodity laptop. We effectively remove the carbon footprint of hundreds
of GPU hours of training during search, offering a strong simple baseline for
future NAS methods in autoregressive language modeling."
2821,"This opens up avenues for further research on more efﬁcient, and more powerful, message passing
parameretisations for KGs.","On the other hand, the effectiveness of these random transformations illustrates that, for KGs,
the R-GCN’s message passing and aggregation paradigm is more signiﬁcant than the actual param-
eters of the transformations, which have to be obtained through an expensive training procedure.","3 Background

In this section, we touch upon the concepts that are fundamental to our RR-GCN approach: (i) we
brieﬂy outline prior research directions in Knowledge Graph Embedding (KGE); (ii) we provide the
necessary theory of R-GCNs, on which our algorithm is based; and (iii) we discuss the concept of
learning from random transformations.",2022-03-04 16:55:25+00:00,R-GCN: The R Could Stand for Random,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Vic Degraeve'), arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Femke Ongenae'), arxiv.Result.Author('Sofie Van Hoecke')]","The inception of Relational Graph Convolutional Networks (R-GCNs) marked a
milestone in the Semantic Web domain as it allows for end-to-end training of
machine learning models that operate on Knowledge Graphs (KGs). R-GCNs generate
a representation for a node of interest by repeatedly aggregating parametrised,
relation-specific transformations of its neighbours. However, in this paper, we
argue that the the R-GCN's main contribution lies in this ""message passing""
paradigm, rather than the learned parameters. To this end, we introduce the
""Random Relational Graph Convolutional Network"" (RR-GCN), which constructs
embeddings for nodes in the KG by aggregating randomly transformed random
information from neigbours, i.e., with no learned parameters. We empirically
show that RR-GCNs can compete with fully trained R-GCNs in both node
classification and link prediction settings. The implications of these results
are two-fold: on the one hand, our technique can be used as a quick baseline
that novel KG embedding methods should be able to beat. On the other hand, it
demonstrates that further research might reveal more parameter-efficient
inductive biases for KGs."
2822,"This opens up many new inter-
esting avenues for further research, both in improving the RR-GCNs themselves and KG embedding
in general, which we discuss subsequently.","aggregation
functions (such as PPV) before considering differentiable variants.","12
8 Future Work

Apart from being used as a baseline that any trained method should be able to beat, we envision that
RR-GCNs might be helpful as a tool for improving and/or developing other embedding techniques.",2022-03-04 16:55:25+00:00,R-GCN: The R Could Stand for Random,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Vic Degraeve'), arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Femke Ongenae'), arxiv.Result.Author('Sofie Van Hoecke')]","The inception of Relational Graph Convolutional Networks (R-GCNs) marked a
milestone in the Semantic Web domain as it allows for end-to-end training of
machine learning models that operate on Knowledge Graphs (KGs). R-GCNs generate
a representation for a node of interest by repeatedly aggregating parametrised,
relation-specific transformations of its neighbours. However, in this paper, we
argue that the the R-GCN's main contribution lies in this ""message passing""
paradigm, rather than the learned parameters. To this end, we introduce the
""Random Relational Graph Convolutional Network"" (RR-GCN), which constructs
embeddings for nodes in the KG by aggregating randomly transformed random
information from neigbours, i.e., with no learned parameters. We empirically
show that RR-GCNs can compete with fully trained R-GCNs in both node
classification and link prediction settings. The implications of these results
are two-fold: on the one hand, our technique can be used as a quick baseline
that novel KG embedding methods should be able to beat. On the other hand, it
demonstrates that further research might reveal more parameter-efficient
inductive biases for KGs."
2823,"We envision that further research could trade in some
of these memory requirements for compute by aggregating many small node embeddings.","Even though for some datasets (such as DBLP), RR-GCNs are more memory efﬁcient than their
trained counterparts, the memory requirements are still substantial for graphs with many nodes as
downstream tasks require large embeddings.","Acknowledgements

We thank the anonymous reviewers of ESWC 2022 (Extended Semantic Web Conference) for their
valuable suggestions and comments on our initial manuscript.",2022-03-04 16:55:25+00:00,R-GCN: The R Could Stand for Random,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Vic Degraeve'), arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Femke Ongenae'), arxiv.Result.Author('Sofie Van Hoecke')]","The inception of Relational Graph Convolutional Networks (R-GCNs) marked a
milestone in the Semantic Web domain as it allows for end-to-end training of
machine learning models that operate on Knowledge Graphs (KGs). R-GCNs generate
a representation for a node of interest by repeatedly aggregating parametrised,
relation-specific transformations of its neighbours. However, in this paper, we
argue that the the R-GCN's main contribution lies in this ""message passing""
paradigm, rather than the learned parameters. To this end, we introduce the
""Random Relational Graph Convolutional Network"" (RR-GCN), which constructs
embeddings for nodes in the KG by aggregating randomly transformed random
information from neigbours, i.e., with no learned parameters. We empirically
show that RR-GCNs can compete with fully trained R-GCNs in both node
classification and link prediction settings. The implications of these results
are two-fold: on the one hand, our technique can be used as a quick baseline
that novel KG embedding methods should be able to beat. On the other hand, it
demonstrates that further research might reveal more parameter-efficient
inductive biases for KGs."
2824,"This opens up avenues for further research on more eﬃcient, and more
powerful, message passing parameretisations for KGs.","On the other hand, the
eﬀectiveness of these random transformations illustrates that, for KGs, the r-
gcn’s message passing and aggregation paradigm is more signiﬁcant than the
actual parameters, which have to be obtained through an expensive training pro-
cedure.","3 Background

In this section, we touch upon the concepts that are fundamental to our rr-gcn
approach: (i) we brieﬂy outline prior research directions in Knowledge Graph
Embedding (KGE); (ii) we provide the necessary theory of r-gcns; and (iii) we
discuss the concept of learning from random transformations.",2022-03-04 16:55:25+00:00,R-GCN: The R Could Stand for Random,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Vic Degraeve'), arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Femke Ongenae'), arxiv.Result.Author('Sofie Van Hoecke')]","The inception of the Relational Graph Convolutional Network (R-GCN) marked a
milestone in the Semantic Web domain as a widely cited method that generalises
end-to-end hierarchical representation learning to Knowledge Graphs (KGs).
R-GCNs generate representations for nodes of interest by repeatedly aggregating
parameterised, relation-specific transformations of their neighbours. However,
in this paper, we argue that the the R-GCN's main contribution lies in this
""message passing"" paradigm, rather than the learned weights. To this end, we
introduce the ""Random Relational Graph Convolutional Network"" (RR-GCN), which
leaves all parameters untrained and thus constructs node embeddings by
aggregating randomly transformed random representations from neighbours, i.e.,
with no learned parameters. We empirically show that RR-GCNs can compete with
fully trained R-GCNs in both node classification and link prediction settings."
2825,"This opens up many
new interesting avenues for further research, both in improving the rr-gcns
themselves and KG embedding in general, which we discuss subsequently.","aggregation functions
(such as ppv) before considering diﬀerentiable variants.","8 Future Work

Apart from being used as a baseline that any trained method should be able to
beat, we envision that rr-gcns might be helpful as a tool for improving and/or
developing other embedding techniques.",2022-03-04 16:55:25+00:00,R-GCN: The R Could Stand for Random,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Vic Degraeve'), arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Femke Ongenae'), arxiv.Result.Author('Sofie Van Hoecke')]","The inception of the Relational Graph Convolutional Network (R-GCN) marked a
milestone in the Semantic Web domain as a widely cited method that generalises
end-to-end hierarchical representation learning to Knowledge Graphs (KGs).
R-GCNs generate representations for nodes of interest by repeatedly aggregating
parameterised, relation-specific transformations of their neighbours. However,
in this paper, we argue that the the R-GCN's main contribution lies in this
""message passing"" paradigm, rather than the learned weights. To this end, we
introduce the ""Random Relational Graph Convolutional Network"" (RR-GCN), which
leaves all parameters untrained and thus constructs node embeddings by
aggregating randomly transformed random representations from neighbours, i.e.,
with no learned parameters. We empirically show that RR-GCNs can compete with
fully trained R-GCNs in both node classification and link prediction settings."
2872,"Quite literally, as in the application we discuss in
                                       our paper (the 2020 U.S Presidential election), to test the many different theories and potential explanations for why
                                       voters decided to remove then President Trump from ofﬁce, researchers need to use methodologies that can quickly
                                       and efﬁciently reduce the feature space from hundreds of possible features to a smaller set that can then be the focus
                                       of further study.","1 The Problem

                                       Social science research today often encounters a difﬁcult methodological situation — larger and larger datasets, which
                                       contain high-dimensional features, which are highly correlated [7].","In our paper we present a variant of the popular Random Forest, Fuzzy Forests, which we argue is well suited for
                                       exactly this type of applied machine learning problem [6].",2022-03-05 21:25:18+00:00,Fuzzy Forests For Feature Selection in High-Dimensional Survey Data: An Application to the 2020 U.S. Presidential Election,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Sreemanti Dey'), arxiv.Result.Author('R. Michael Alvarez')]","An increasingly common methodological issue in the field of social science is
high-dimensional and highly correlated datasets that are unamenable to the
traditional deductive framework of study. Analysis of candidate choice in the
2020 Presidential Election is one area in which this issue presents itself: in
order to test the many theories explaining the outcome of the election, it is
necessary to use data such as the 2020 Cooperative Election Study Common
Content, with hundreds of highly correlated features. We present the Fuzzy
Forests algorithm, a variant of the popular Random Forests ensemble method, as
an efficient way to reduce the feature space in such cases with minimal bias,
while also maintaining predictive performance on par with common algorithms
like Random Forests and logit. Using Fuzzy Forests, we isolate the top
correlates of candidate choice and find that partisan polarization was the
strongest factor driving the 2020 presidential election."
2894,"Given recent advancement in the research of deep neural net-
works, a promising direction of further research would be to investigate how neural approximation and its
generalization properties (Chen et al., 2021a, 2020; Chen & Xu, 2021; Min et al., 2021a) would beneﬁt risk-
sensitive RL.","i=1

Future directions and broad impact.","Understanding and designing eﬃcient algorithms for risk-sensitive RL in other settings, such
as shortest path problems (Min et al., 2021b), oﬀ-policy evaluation (Min et al., 2021c) and oﬄine learning
(Chen et al., 2021b), may also be of great interest.",2022-03-07 03:07:09+00:00,Cascaded Gaps: Towards Gap-Dependent Regret for Risk-Sensitive Reinforcement Learning,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Yingjie Fei'), arxiv.Result.Author('Ruitu Xu')]","In this paper, we study gap-dependent regret guarantees for risk-sensitive
reinforcement learning based on the entropic risk measure. We propose a novel
definition of sub-optimality gaps, which we call cascaded gaps, and we discuss
their key components that adapt to the underlying structures of the problem.
Based on the cascaded gaps, we derive non-asymptotic and logarithmic regret
bounds for two model-free algorithms under episodic Markov decision processes.
We show that, in appropriate settings, these bounds feature exponential
improvement over existing ones that are independent of gaps. We also prove
gap-dependent lower bounds, which certify the near optimality of the upper
bounds."
2911,"Furthermore, considering the equal-                       [19] M. H. D. M. Ribeiro and L. dos Santos Coelho, “Ensemble approach
weighted averaging adopted by the top submissions of the M5                             based on bagging, boosting and stacking for short-term prediction in
competition, further research is needed to assess the perfor-                           agribusiness time series,” Applied Soft Computing, vol.","683–712, 2003.
the performance of a stacking ensemble that is conscious of
the data’s temporality.","86, p. 105837,
mance of feature weighted model averaging for hierarchical                              2020.
time-series data.",2022-03-07 10:51:40+00:00,"Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Pieter Cawood'), arxiv.Result.Author('Terence van Zyl')]","Techniques of hybridisation and ensemble learning are popular model fusion
techniques for improving the predictive power of forecasting methods. With
limited research that instigates combining these two promising approaches, this
paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural
Network (ES-RNN) in the pool of base models for different ensembles. We compare
against some state of the art ensembling techniques and arithmetic model
averaging as a benchmark. We experiment with the M4 forecasting data set of
100,000 time-series, and the results show that the Feature-based Forecast Model
Averaging (FFORMA), on average, is the best technique for late data fusion with
the ES-RNN. However, considering the M4's Daily subset of data, stacking was
the only successful ensemble at dealing with the case where all base model
performances are similar. Our experimental results indicate that we attain
state of the art forecasting results compared to N-BEATS as a benchmark. We
conclude that model averaging is a more robust ensemble than model selection
and stacking strategies. Further, the results show that gradient boosting is
superior for implementing ensemble learning strategies."
2912,"Furthermore, considering the equal-weighted averaging adopted by the top submissions of
the M5 competition, further research is needed to assess the performance of feature weighted
model averaging for multivariate time-series data.","Consequently, we recommend that future research consider
investigating the performance of a stacking ensemble that is conscious of the data’s temporality.","Author Contributions: Conceptualization, P.C.",2022-03-07 10:51:40+00:00,"Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Pieter Cawood'), arxiv.Result.Author('Terence van Zyl')]","Techniques of hybridisation and ensemble learning are popular model fusion
techniques for improving the predictive power of forecasting methods. With
limited research that instigates combining these two promising approaches, this
paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural
Network (ES-RNN) in the pool of base models for different ensembles. We compare
against some state of the art ensembling techniques and arithmetic model
averaging as a benchmark. We experiment with the M4 forecasting data set of
100,000 time-series, and the results show that the Feature-based Forecast Model
Averaging (FFORMA), on average, is the best technique for late data fusion with
the ES-RNN. However, considering the M4's Daily subset of data, stacking was
the only successful ensemble at dealing with the case where all base model
performances are similar. Our experimental results indicate that we attain
state of the art forecasting results compared to N-BEATS as a benchmark. We
conclude that model averaging is a more robust ensemble than model selection
and stacking strategies. Further, the results show that gradient boosting is
superior for implementing ensemble learning strategies."
2913,"Furthermore, considering the equal-weighted averaging adopted by the top submissions of
the M5 Competition, further research is needed to assess the performance of feature weighted
model averaging for multivariate time-series data.","Consequently, we recommend that future research consider
investigating the performance of a stacking ensemble that is conscious of the data’s temporality.","Author Contributions: Conceptualization, P.C.",2022-03-07 10:51:40+00:00,"Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Pieter Cawood'), arxiv.Result.Author('Terence van Zyl')]","Techniques of hybridisation and ensemble learning are popular model fusion
techniques for improving the predictive power of forecasting methods. With
limited research that instigates combining these two promising approaches, this
paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural
Network (ES-RNN) in the pool of base models for different ensembles. We compare
against some state of the art ensembling techniques and arithmetic model
averaging as a benchmark. We experiment with the M4 forecasting data set of
100,000 time-series, and the results show that the Feature-based Forecast Model
Averaging (FFORMA), on average, is the best technique for late data fusion with
the ES-RNN. However, considering the M4's Daily subset of data, stacking was
the only successful ensemble at dealing with the case where all base model
performances are similar. Our experimental results indicate that we attain
state of the art forecasting results compared to N-BEATS as a benchmark. We
conclude that model averaging is a more robust ensemble than model selection
and stacking strategies. Further, the results show that gradient boosting is
superior for implementing ensemble learning strategies."
2936,"This method may inspire
further research.","No matter what format the data is, vector or matrix, we both develop a method
to encode the data to a density matrix with the corresponding size.","Second, quantum progressive training is perfectly carried out in our model.",2022-03-04 08:33:28+00:00,Quantum Deep Learning for Mutant COVID-19 Strain Prediction,cs.LG,"['cs.LG', 'quant-ph']","[arxiv.Result.Author('Yu-Xin Jin'), arxiv.Result.Author('Jun-Jie Hu'), arxiv.Result.Author('Qi Li'), arxiv.Result.Author('Zhi-Cheng Luo'), arxiv.Result.Author('Fang-Yan Zhang'), arxiv.Result.Author('Hao Tang'), arxiv.Result.Author('Kun Qian'), arxiv.Result.Author('Xian-Min Jin')]","New COVID-19 epidemic strains like Delta and Omicron with increased
transmissibility and pathogenicity emerge and spread across the whole world
rapidly while causing high mortality during the pandemic period. Early
prediction of possible variants (especially spike protein) of COVID-19 epidemic
strains based on available mutated SARS-CoV-2 RNA sequences may lead to early
prevention and treatment. Here, combining the advantage of quantum and
quantum-inspired algorithms with the wide application of deep learning, we
propose a development tool named DeepQuantum, and use this software to realize
the goal of predicting spike protein variation structure of COVID-19 epidemic
strains. In addition, this hybrid quantum-classical model for the first time
achieves quantum-inspired blur convolution similar to classical depthwise
convolution and also successfully applies quantum progressive training with
quantum circuits, both of which guarantee that our model is the quantum
counterpart of the famous style-based GAN. The results state that the
fidelities of random generating spike protein variation structure are always
beyond 96% for Delta, 94% for Omicron. The training loss curve is more stable
and converges better with multiple loss functions compared with the
corresponding classical algorithm. At last, evidences that quantum-inspired
algorithms promote the classical deep learning and hybrid models effectively
predict the mutant strains are strong."
2949,"Additional or improved measures and
tasks should be investigated in further research, e.g.","This toolbox is by no means conclusive and
should act as a starting point for a more standardized means of evaluation.",the beneﬁts of mutual information.,2022-03-04 14:16:50+00:00,A Typology to Explore the Mitigation of Shortcut Behavior,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC']","[arxiv.Result.Author('Felix Friedrich'), arxiv.Result.Author('Wolfgang Stammer'), arxiv.Result.Author('Patrick Schramowski'), arxiv.Result.Author('Kristian Kersting')]","As machine learning models become increasingly larger, trained weakly
supervised on large, possibly uncurated data sets, it becomes increasingly
important to establish mechanisms for inspecting, interacting, and revising
models to mitigate learning shortcuts and guarantee their learned knowledge is
aligned with human knowledge. The recently proposed XIL framework was developed
for this purpose, and several such methods have been introduced, each with
individual motivations and methodological details. In this work, we provide a
unification of various XIL methods into a single typology by establishing a
common set of basic modules. In doing so, we pave the way for a principled
comparison of existing, but, importantly, also future XIL approaches. In
addition, we discuss existing and introduce novel measures and benchmarks for
evaluating the overall abilities of a XIL method. Given this extensive toolbox,
including our typology, measures, and benchmarks, we finally compare several
recent XIL methods methodologically and quantitatively. In our evaluations, all
methods prove to revise a model successfully. However, we found remarkable
differences in individual benchmark tasks, revealing valuable
application-relevant aspects for integrating these benchmarks in developing
future methods."
2953,"We would recommend
pursuing further study into crafting efficient heuristics to navigate the subspace:
based on the maximum accuracy obtained from a sampled point, we observe
that there do exist points within the subspace that offer at least 10% gains on
accuracy w.r.t.","This implies
various storage strategies, ranging from storing a single centre point-estimate, or
storing the 3 boundary point-estimates for each attacker.",clean labels.,2022-03-07 20:30:44+00:00,Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks,cs.LG,"['cs.LG', 'cs.CR', 'cs.MA']","[arxiv.Result.Author('Siddhartha Datta'), arxiv.Result.Author('Nigel Shadbolt')]","Recent exploration of the multi-agent backdoor attack demonstrated the
backfiring effect, a natural defense against backdoor attacks where backdoored
inputs are randomly classified. This yields a side-effect of low accuracy
w.r.t. clean labels, which motivates this paper's work on the construction of
multi-agent backdoor defenses that maximize accuracy w.r.t. clean labels and
minimize that of poison labels. Founded upon agent dynamics and low-loss
subspace construction, we contribute three defenses that yield improved
multi-agent backdoor robustness."
2954,"We would recommend                                                                                                                                                               distributions share one common source distribution, but a
pursuing further study into crafting efﬁcient heuristics to                                                                                                                                                             heuristic-sampling method could be investigated for search-
navigate the subspace: based on the maximum accuracy ob-                                                                                                                                                                ing for speciﬁc subnetworks for speciﬁc target distributions.","This implies various storage strategies, ranging from                                                                                                                                                           cluding inference with the centre or sampling across the
storing a single centre point-estimate, or storing the 3 bound-                                                                                                                                                         entire subspace, empirically performs well when the shifted
ary point-estimates for each attacker.","tained from a sampled point, we observe that there do exist
points within the subspace that offer at least 10% gains on                                                                                                                                                                Most work in mode connectivity and subspace construc-
accuracy w.r.t.",2022-03-07 20:30:44+00:00,Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks,cs.LG,"['cs.LG', 'cs.CR', 'cs.MA']","[arxiv.Result.Author('Siddhartha Datta'), arxiv.Result.Author('Nigel Shadbolt')]","Recent exploration of the multi-agent backdoor attack demonstrated the
backfiring effect, a natural defense against backdoor attacks where backdoored
inputs are randomly classified. This yields a side-effect of low accuracy
w.r.t. clean labels, which motivates this paper's work on the construction of
multi-agent backdoor defenses that maximize accuracy w.r.t. clean labels and
minimize that of poison labels. Founded upon agent dynamics and low-loss
subspace construction, we contribute three defenses that yield improved
multi-agent backdoor robustness."
2963,"We
make the task and prepared dataset, along with all models considered in this work, available for the
research community to facilitate reproduction of this work and further research1.","Each program relies on an external resource, the stdin input stream,
and we pair the programs with a natural language description of the behavior of the stream.","To make progress
on this challenging task, we identify a promising class of models from prior work, interpreter-
inspired models, and we demonstrate they perform well on the task.",2022-03-07 23:17:17+00:00,Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions,cs.LG,"['cs.LG', 'cs.PL']","[arxiv.Result.Author('David Bieber'), arxiv.Result.Author('Rishab Goel'), arxiv.Result.Author('Daniel Zheng'), arxiv.Result.Author('Hugo Larochelle'), arxiv.Result.Author('Daniel Tarlow')]","The execution behavior of a program often depends on external resources, such
as program inputs or file contents, and so cannot be run in isolation.
Nevertheless, software developers benefit from fast iteration loops where
automated tools identify errors as early as possible, even before programs can
be compiled and run. This presents an interesting machine learning challenge:
can we predict runtime errors in a ""static"" setting, where program execution is
not possible? Here, we introduce a real-world dataset and task for predicting
runtime errors, which we show is difficult for generic models like
Transformers. We approach this task by developing an interpreter-inspired
architecture with an inductive bias towards mimicking program executions, which
models exception handling and ""learns to execute"" descriptions of the contents
of external resources. Surprisingly, we show that the model can also predict
the location of the error, despite being trained only on labels indicating the
presence/absence and kind of error. In total, we present a practical and
difficult-yet-approachable challenge problem related to learning program
execution and we demonstrate promising new capabilities of interpreter-inspired
machine learning models for code."
3004,"In addition, we would like to further study the application of score matching causal discovery methods to generative
model other than additive (Gaussian) noise.",Amortization [44] is a promising direction to alleviate this issue.,"Due to the one-to-one correspondence between the score of a distribution
and its density function, it should be possible to recover the graph from the score for any identiﬁable model.",2022-03-08 21:34:46+00:00,Score matching enables causal discovery of nonlinear additive noise models,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Paul Rolland'), arxiv.Result.Author('Volkan Cevher'), arxiv.Result.Author('Matthäus Kleindessner'), arxiv.Result.Author('Chris Russel'), arxiv.Result.Author('Bernhard Schölkopf'), arxiv.Result.Author('Dominik Janzing'), arxiv.Result.Author('Francesco Locatello')]","This paper demonstrates how to recover causal graphs from the score of the
data distribution in non-linear additive (Gaussian) noise models. Using score
matching algorithms as a building block, we show how to design a new generation
of scalable causal discovery methods. To showcase our approach, we also propose
a new efficient method for approximating the score's Jacobian, enabling to
recover the causal graph. Empirically, we find that the new algorithm, called
SCORE, is competitive with state-of-the-art causal discovery methods while
being significantly faster."
3005,"We further study the running times of individual
components of CBS as shown in Figure 2.","For high
sparsity, in which weight update is less beneﬁcial, we observe that the running time often makes CBS-S more advantageous
because a simple approximation of the Hessian sufﬁces for weight selection.","Both RMP and LS use the sample loss on a subset of training data (5000 samples
      The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks

          0.90          100  200        300  400  500                          600       700                                                 800
          0.80
          0.70
          0.60
          0.50

               0

                             H −1         GradieTnitms e in SecRoMndPs-others  RMP-inference
                                        LS-inference
                             LS-others

                             Figure 2.",2022-03-09 00:58:04+00:00,The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Xin Yu'), arxiv.Result.Author('Thiago Serra'), arxiv.Result.Author('Srikumar Ramalingam'), arxiv.Result.Author('Shandian Zhe')]","Neural networks tend to achieve better accuracy with training if they are
larger -- even if the resulting models are overparameterized. Nevertheless,
carefully removing such excess parameters before, during, or after training may
also produce models with similar or even improved accuracy. In many cases, that
can be curiously achieved by heuristics as simple as removing a percentage of
the weights with the smallest absolute value -- even though magnitude is not a
perfect proxy for weight relevance. With the premise that obtaining
significantly better performance from pruning depends on accounting for the
combined effect of removing multiple weights, we revisit one of the classic
approaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose
a tractable heuristic for solving the combinatorial extension of OBS, in which
we select weights for simultaneous removal, as well as a systematic update of
the remaining weights. Our selection method outperforms other methods under
high sparsity, and the weight update is advantageous even when combined with
the other methods."
3037,"We believe
there are many directions to further study the properties of block-sparse classiﬁers, such as introducing
non-linear embedding dictionaries.","Finally, we experimentally veriﬁed the
validity of the structured block-sparse optimization approach on the YaleB and MNIST datasets.","References

 [1] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd.",2022-03-09 16:57:19+00:00,Reverse Engineering $\ell_p$ attacks: A block-sparse optimization approach with recovery guarantees,cs.LG,['cs.LG'],"[arxiv.Result.Author('Darshan Thaker'), arxiv.Result.Author('Paris Giampouras'), arxiv.Result.Author('René Vidal')]","Deep neural network-based classifiers have been shown to be vulnerable to
imperceptible perturbations to their input, such as $\ell_p$-bounded norm
adversarial attacks. This has motivated the development of many defense
methods, which are then broken by new attacks, and so on. This paper focuses on
a different but related problem of reverse engineering adversarial attacks.
Specifically, given an attacked signal, we study conditions under which one can
determine the type of attack ($\ell_1$, $\ell_2$ or $\ell_\infty$) and recover
the clean signal. We pose this problem as a block-sparse recovery problem,
where both the signal and the attack are assumed to lie in a union of subspaces
that includes one subspace per class and one subspace per attack type. We
derive geometric conditions on the subspaces under which any attacked signal
can be decomposed as the sum of a clean signal plus an attack. In addition, by
determining the subspaces that contain the signal and the attack, we can also
classify the signal and determine the attack type. Experiments on digit and
face classification demonstrate the effectiveness of the proposed approach."
3095,"And       It is particular challenging when there is imbalanced distribution
further study on their algorithm design choices can reveal the key       among different channels of a tensor.",algorithms helps us to select a proper quantization algorithm.,"To resolve that, cross layer
reason of an algorithm’s success and provide insights for further        equalization (CLE) [21] and outlier channel splitting (OCS) [27] are
improvement of the algorithm.",2022-03-10 17:22:08+00:00,An Empirical Study of Low Precision Quantization for TinyML,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shaojie Zhuo'), arxiv.Result.Author('Hongyu Chen'), arxiv.Result.Author('Ramchalam Kinattinkara Ramakrishnan'), arxiv.Result.Author('Tommy Chen'), arxiv.Result.Author('Chen Feng'), arxiv.Result.Author('Yicheng Lin'), arxiv.Result.Author('Parker Zhang'), arxiv.Result.Author('Liang Shen')]","Tiny machine learning (tinyML) has emerged during the past few years aiming
to deploy machine learning models to embedded AI processors with highly
constrained memory and computation capacity. Low precision quantization is an
important model compression technique that can greatly reduce both memory
consumption and computation cost of model inference. In this study, we focus on
post-training quantization (PTQ) algorithms that quantize a model to low-bit
(less than 8-bit) precision with only a small set of calibration data and
benchmark them on different tinyML use cases. To achieve a fair comparison, we
build a simulated quantization framework to investigate recent PTQ algorithms.
Furthermore, we break down those algorithms into essential components and
re-assembled a generic PTQ pipeline. With ablation study on different
alternatives of components in the pipeline, we reveal key design choices when
performing low precision quantization. We hope this work could provide useful
data points and shed lights on the future research of low precision
quantization."
3127,"Noted that this work is compatible with ours and we
leave this topic to further research.","share the exponent part of the binary
coding of the weights and thereby convert ﬂoating-point operations to ﬁxed-
point integer operations.",Sun et al [4].,2022-03-11 01:32:03+00:00,DNN Training Acceleration via Exploring GPGPU Friendly Sparsity,cs.LG,"['cs.LG', 'cs.AR']","[arxiv.Result.Author('Zhuoran Song'), arxiv.Result.Author('Yihong Xu'), arxiv.Result.Author('Han Li'), arxiv.Result.Author('Naifeng Jing'), arxiv.Result.Author('Xiaoyao Liang'), arxiv.Result.Author('Li Jiang')]","The training phases of Deep neural network~(DNN) consumes enormous processing
time and energy. Compression techniques utilizing the sparsity of DNNs can
effectively accelerate the inference phase of DNNs. However, it is hardly used
in the training phase because the training phase involves dense
matrix-multiplication using General-Purpose Computation on Graphics Processors
(GPGPU), which endorse the regular and structural data layout. In this paper,
we first propose the Approximate Random Dropout that replaces the conventional
random dropout of neurons and synapses with a regular and online generated
row-based or tile-based dropout patterns to eliminate the unnecessary
computation and data access for the multilayer perceptron~(MLP) and long
short-term memory~(LSTM). We then develop a SGD-based Search Algorithm that
produces the distribution of row-based or tile-based dropout patterns to
compensate for the potential accuracy loss. Moreover, aiming at the convolution
neural network~(CNN) training acceleration, we first explore the importance and
sensitivity of input feature maps; and then propose the sensitivity-aware
dropout method to dynamically drop the input feature maps based on their
sensitivity so as to achieve greater forward and backward training acceleration
while reserving better NN accuracy. To facilitate DNN programming, we build a
DNN training computation framework that unifies the proposed techniques in the
software stack. As a result, the GPGPU only needs to support the basic operator
-- matrix multiplication and can achieve significant performance improvement
regardless of DNN model."
3128,"A further study may focus on the application of DualVDT and combining more
informative differential equation model on speciﬁc ﬁelds.","The results shows the advance both in analytical and
experimental.","References

 [1] Nanxin Chen et al.",2022-03-11 06:02:16+00:00,Dual reparametrized Variational Generative Model for Time-Series Forecasting,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']",[arxiv.Result.Author('Ziang Chen')],"This paper propose DualVDT, a generative model for Time-series forecasting.
Introduced dual reparametrized variational mechanisms on variational
autoencoder (VAE) to tighter the evidence lower bound (ELBO) of the model,
prove the advance performance analytically. This mechanism leverage the latent
score based generative model (SGM), explicitly denoising the perturbation
accumulated on latent vector through reverse time stochastic differential
equation and variational ancestral sampling. The posterior of denoised latent
distribution fused with dual reparametrized variational density. The KL
divergence in ELBO will reduce to reach the better results of the model. This
paper also proposed a latent attention mechanisms to extract multivariate
dependency explicitly. Build the local temporal dependency simultaneously in
factor wised through constructed local topology and temporal wised. The proven
and experiment on multiple datasets illustrate, DualVDT, with a novel dual
reparametrized structure, which denoise the latent perturbation through the
reverse dynamics combining local-temporal inference, has the advanced
performance both analytically and experimentally."
3150,"Normalization and regularization could also be the              [2] Wouter Beek, Laurens Rietveld, Hamid R Bazoobandi, Jan Wielemaker, and Stefan
subject of further research.","We ran our experiments without normalization, because the                       Accessed: 2021-04-17.
class labels are dependent on the structural information of the
neighborhood.",This is motivated by the deviating results            Schlobach.,2022-03-11 13:45:34+00:00,Graph Summarization with Graph Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Maximilian Blasi'), arxiv.Result.Author('Manuel Freudenreich'), arxiv.Result.Author('Johannes Horvath'), arxiv.Result.Author('David Richerby'), arxiv.Result.Author('Ansgar Scherp')]","The goal of graph summarization is to represent large graphs in a structured
and compact way. A graph summary based on equivalence classes preserves
pre-defined features of a graph's vertex within a $k$-hop neighborhood such as
the vertex labels and edge labels. Based on these neighborhood characteristics,
the vertex is assigned to an equivalence class. The calculation of the assigned
equivalence class must be a permutation invariant operation on the pre-defined
features. This is achieved by sorting on the feature values, e. g., the edge
labels, which is computationally expensive, and subsequently hashing the
result. Graph Neural Networks (GNN) fulfill the permutation invariance
requirement. We formulate the problem of graph summarization as a subgraph
classification task on the root vertex of the $k$-hop neighborhood. We adapt
different GNN architectures, both based on the popular message-passing protocol
and alternative approaches, to perform the structural graph summarization task.
We compare different GNNs with a standard multi-layer perceptron (MLP) and
Bloom filter as non-neural method. For our experiments, we consider four
popular graph summary models on a large web graph. This resembles challenging
multi-class vertex classification tasks with the numbers of classes ranging
from $576$ to multiple hundreds of thousands. Our results show that the
performance of GNNs are close to each other. In three out of four experiments,
the non-message-passing GraphMLP model outperforms the other GNNs. The
performance of the standard MLP is extraordinary good, especially in the
presence of many classes. Finally, the Bloom filter outperforms all neural
architectures by a large margin, except for the dataset with the fewest number
of $576$ classes."
3151,"From the results it is noticeable that the models with
be the subject of further research.",Normalization and regularization could also     expected.,"This is motivated by the      the lowest hidden layer size also have the lowest accuracy: for
deviating results we received for the MSX-6 499.",2022-03-11 13:45:34+00:00,Graph Summarization with Graph Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Maximilian Blasi'), arxiv.Result.Author('Manuel Freudenreich'), arxiv.Result.Author('Johannes Horvath'), arxiv.Result.Author('David Richerby'), arxiv.Result.Author('Ansgar Scherp')]","The goal of graph summarization is to represent large graphs in a structured
and compact way. A graph summary based on equivalence classes preserves
pre-defined features of a graph's vertex within a $k$-hop neighborhood such as
the vertex labels and edge labels. Based on these neighborhood characteristics,
the vertex is assigned to an equivalence class. The calculation of the assigned
equivalence class must be a permutation invariant operation on the pre-defined
features. This is achieved by sorting on the feature values, e. g., the edge
labels, which is computationally expensive, and subsequently hashing the
result. Graph Neural Networks (GNN) fulfill the permutation invariance
requirement. We formulate the problem of graph summarization as a subgraph
classification task on the root vertex of the $k$-hop neighborhood. We adapt
different GNN architectures, both based on the popular message-passing protocol
and alternative approaches, to perform the structural graph summarization task.
We compare different GNNs with a standard multi-layer perceptron (MLP) and
Bloom filter as non-neural method. For our experiments, we consider four
popular graph summary models on a large web graph. This resembles challenging
multi-class vertex classification tasks with the numbers of classes ranging
from $576$ to multiple hundreds of thousands. Our results show that the
performance of GNNs are close to each other. In three out of four experiments,
the non-message-passing GraphMLP model outperforms the other GNNs. The
performance of the standard MLP is extraordinary good, especially in the
presence of many classes. Finally, the Bloom filter outperforms all neural
architectures by a large margin, except for the dataset with the fewest number
of $576$ classes."
3152,"To combat the skewed
be the subject of further research.",Normalization and regularization could also      result that contradicted our expectation.,"This is motivated by the       class distribution another less invasive measure could also be
deviating results we received for the MSX-6 499.",2022-03-11 13:45:34+00:00,Graph Summarization with Graph Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Maximilian Blasi'), arxiv.Result.Author('Manuel Freudenreich'), arxiv.Result.Author('Johannes Horvath'), arxiv.Result.Author('David Richerby'), arxiv.Result.Author('Ansgar Scherp')]","The goal of graph summarization is to represent large graphs in a structured
and compact way. A graph summary based on equivalence classes preserves
pre-defined features of a graph's vertex within a $k$-hop neighborhood such as
the vertex labels and edge labels. Based on these neighborhood characteristics,
the vertex is assigned to an equivalence class. The calculation of the assigned
equivalence class must be a permutation invariant operation on the pre-defined
features. This is achieved by sorting on the feature values, e. g., the edge
labels, which is computationally expensive, and subsequently hashing the
result. Graph Neural Networks (GNN) fulfill the permutation invariance
requirement. We formulate the problem of graph summarization as a subgraph
classification task on the root vertex of the $k$-hop neighborhood. We adapt
different GNN architectures, both based on the popular message-passing protocol
and alternative approaches, to perform the structural graph summarization task.
We compare different GNNs with a standard multi-layer perceptron (MLP) and
Bloom filter as non-neural method. For our experiments, we consider four
popular graph summary models on a large web graph. This resembles challenging
multi-class vertex classification tasks with the numbers of classes ranging
from $576$ to multiple hundreds of thousands. Our results show that the
performance of GNNs are close to each other. In three out of four experiments,
the non-message-passing GraphMLP model outperforms the other GNNs. The
performance of the standard MLP is extraordinary good, especially in the
presence of many classes. Finally, the Bloom filter outperforms all neural
architectures by a large margin, except for the dataset with the fewest number
of $576$ classes."
3164,"In fact, equivariant architectures already provide state-of-the-art predictions in
scientiﬁc contexts, as seen, for example, in chemistry and high energy physics [45, 32], and we believe
that further research of equivariant architectures will only increase the performance gap between them and
generic networks applied to physics problems.","All this is not to say that pure model performance is unimportant, or that it needs to be sacriﬁced in
favor of other properties.","Nevertheless, the value of these models is not only in their raw
performance, and we hope that this paper will help convince other scientists in the community that a broader
approach to network design that is less focused on short-term performance gains can facilitate bridge-building
between machine learning and physics.",2022-03-11 18:27:04+00:00,Symmetry Group Equivariant Architectures for Physics,cs.LG,"['cs.LG', 'astro-ph.IM', 'cs.AI', 'hep-ex', 'hep-ph']","[arxiv.Result.Author('Alexander Bogatskiy'), arxiv.Result.Author('Sanmay Ganguly'), arxiv.Result.Author('Thomas Kipf'), arxiv.Result.Author('Risi Kondor'), arxiv.Result.Author('David W. Miller'), arxiv.Result.Author('Daniel Murnane'), arxiv.Result.Author('Jan T. Offermann'), arxiv.Result.Author('Mariel Pettee'), arxiv.Result.Author('Phiala Shanahan'), arxiv.Result.Author('Chase Shimmin'), arxiv.Result.Author('Savannah Thais')]","Physical theories grounded in mathematical symmetries are an essential
component of our understanding of a wide range of properties of the universe.
Similarly, in the domain of machine learning, an awareness of symmetries such
as rotation or permutation invariance has driven impressive performance
breakthroughs in computer vision, natural language processing, and other
important applications. In this report, we argue that both the physics
community and the broader machine learning community have much to understand
and potentially to gain from a deeper investment in research concerning
symmetry group equivariant machine learning architectures. For some
applications, the introduction of symmetries into the fundamental structural
design can yield models that are more economical (i.e. contain fewer, but more
expressive, learned parameters), interpretable (i.e. more explainable or
directly mappable to physical quantities), and/or trainable (i.e. more
efficient in both data and computational requirements). We discuss various
figures of merit for evaluating these models as well as some potential benefits
and limitations of these methods for a variety of physics applications.
Research and investment into these approaches will lay the foundation for
future architectures that are potentially more robust under new computational
paradigms and will provide a richer description of the physical systems to
which they are applied."
3186,"We further study
its benefits to generalization capability by analysing the cross-                                                                                                              72                                                                             596                598
entropy losses on training set and validation set.","500                                                        655                        628
                                                                                                                               Accuracy (%)                                    74                                                                                                             653
   Figure 2 demonstrates the confidence-aware consistency loss                                                                                                                                                                                                                                      629
could significantly improve model’s performance.","Here we measure
model’s generalization capability with its generalization gap [16]—                                                                                                            70                                                                  561
the gap between training loss and validation loss.",2022-03-12 09:41:23+00:00,GRAND+: Scalable Graph Random Neural Networks,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Wenzheng Feng'), arxiv.Result.Author('Yuxiao Dong'), arxiv.Result.Author('Tinglin Huang'), arxiv.Result.Author('Ziqi Yin'), arxiv.Result.Author('Xu Cheng'), arxiv.Result.Author('Evgeny Kharlamov'), arxiv.Result.Author('Jie Tang')]","Graph neural networks (GNNs) have been widely adopted for semi-supervised
learning on graphs. A recent study shows that the graph random neural network
(GRAND) model can generate state-of-the-art performance for this problem.
However, it is difficult for GRAND to handle large-scale graphs since its
effectiveness relies on computationally expensive data augmentation procedures.
In this work, we present a scalable and high-performance GNN framework GRAND+
for semi-supervised graph learning. To address the above issue, we develop a
generalized forward push (GFPush) algorithm in GRAND+ to pre-compute a general
propagation matrix and employ it to perform graph data augmentation in a
mini-batch manner. We show that both the low time and space complexities of
GFPush enable GRAND+ to efficiently scale to large graphs. Furthermore, we
introduce a confidence-aware consistency loss into the model optimization of
GRAND+, facilitating GRAND+'s generalization superiority. We conduct extensive
experiments on seven public datasets of different sizes. The results
demonstrate that GRAND+ 1) is able to scale to large graphs and costs less
running time than existing scalable GNNs, and 2) can offer consistent accuracy
improvements over both full-batch and scalable GNNs across all datasets."
3201,"As a result, the authors in [35] recognize this and
   This paper builds upon an existing daily discord detection           encourage further research into ﬁnding a systematic way of de-
algorithm (ALDI) presented in [35].","ALDI++: Automating outlier and discord detection with-             the building energy data itself, such parameter selection is un-
      out parameter tuning                                              certain and requires numerous interactive processes to calculate
                                                                        the results.","The proposed frame-                 termining the parameter p−value or circumventing it during the
work, named ALDI++, bypasses the need for any user-deﬁned
parameter, and its performance is evaluated using the largest
publicly available building energy meter dataset, the Building

                                                                     2
discord and non-discord day calculations.",2022-03-13 10:39:11+00:00,ALDI++: Automatic and parameter-less discord and outlier detection for building energy load profiles,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Matias Quintana'), arxiv.Result.Author('Till Stoeckmann'), arxiv.Result.Author('June Young Park'), arxiv.Result.Author('Marian Turowski'), arxiv.Result.Author('Veit Hagenmeyer'), arxiv.Result.Author('Clayton Miller')]","Data-driven building energy prediction is an integral part of the process for
measurement and verification, building benchmarking, and building-to-grid
interaction. The ASHRAE Great Energy Predictor III (GEPIII) machine learning
competition used an extensive meter data set to crowdsource the most accurate
machine learning workflow for whole building energy prediction. A significant
component of the winning solutions was the pre-processing phase to remove
anomalous training data. Contemporary pre-processing methods focus on filtering
statistical threshold values or deep learning methods requiring training data
and multiple hyper-parameters. A recent method named ALDI (Automated Load
profile Discord Identification) managed to identify these discords using matrix
profile, but the technique still requires user-defined parameters. We develop
ALDI++, a method based on the previous work that bypasses user-defined
parameters and takes advantage of discord similarity. We evaluate ALDI++
against a statistical threshold, variational auto-encoder, and the original
ALDI as baselines in classifying discords and energy forecasting scenarios. Our
results demonstrate that while the classification performance improvement over
the original method is marginal, ALDI++ helps achieve the best forecasting
error improving 6% over the winning's team approach with six times less
computation time."
3222,"Based on Strategy C, we further study
prediction of each behavior like purchase as a binary classification
problem and negative samples are randomly selected.",We formulate the       Impact of Relax Factor.,the impact of the relax factor.,2022-03-14 01:08:31+00:00,MetaBalance: Improving Multi-Task Recommendations via Adapting Gradient Magnitudes of Auxiliary Tasks,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Yun He'), arxiv.Result.Author('Xue Feng'), arxiv.Result.Author('Cheng Cheng'), arxiv.Result.Author('Geng Ji'), arxiv.Result.Author('Yunsong Guo'), arxiv.Result.Author('James Caverlee')]","In many personalized recommendation scenarios, the generalization ability of
a target task can be improved via learning with additional auxiliary tasks
alongside this target task on a multi-task network. However, this method often
suffers from a serious optimization imbalance problem. On the one hand, one or
more auxiliary tasks might have a larger influence than the target task and
even dominate the network weights, resulting in worse recommendation accuracy
for the target task. On the other hand, the influence of one or more auxiliary
tasks might be too weak to assist the target task. More challenging is that
this imbalance dynamically changes throughout the training process and varies
across the parts of the same network. We propose a new method: MetaBalance to
balance auxiliary losses via directly manipulating their gradients w.r.t the
shared parameters in the multi-task network. Specifically, in each training
iteration and adaptively for each part of the network, the gradient of an
auxiliary loss is carefully reduced or enlarged to have a closer magnitude to
the gradient of the target loss, preventing auxiliary tasks from being so
strong that dominate the target task or too weak to help the target task.
Moreover, the proximity between the gradient magnitudes can be flexibly
adjusted to adapt MetaBalance to different scenarios. The experiments show that
our proposed method achieves a significant improvement of 8.34% in terms of
NDCG@10 upon the strongest baseline on two real-world datasets. The code of our
approach can be found at here: https://github.com/facebookresearch/MetaBalance"
3242,"Finally, we further study the influence of adversarial attacks for
VAE on robustness of classifier, and compare with that of adversarial attacks for classifier in
section IV-D.

A.","After that, ablation studies
about the architectures of classifier, attacker strength and the cost function of global training
are performed in section IV-C.","Implementation Details
   Datasets.",2022-03-11 08:48:26+00:00,Learning from Attacks: Attacking Variational Autoencoder for Improving Image Classification,cs.LG,"['cs.LG', 'cs.CV', 'eess.IV']","[arxiv.Result.Author('Jianzhang Zheng'), arxiv.Result.Author('Fan Yang'), arxiv.Result.Author('Hao Shen'), arxiv.Result.Author('Xuan Tang'), arxiv.Result.Author('Mingsong Chen'), arxiv.Result.Author('Liang Song'), arxiv.Result.Author('Xian Wei')]","Adversarial attacks are often considered as threats to the robustness of Deep
Neural Networks (DNNs). Various defending techniques have been developed to
mitigate the potential negative impact of adversarial attacks against task
predictions. This work analyzes adversarial attacks from a different
perspective. Namely, adversarial examples contain implicit information that is
useful to the predictions i.e., image classification, and treat the adversarial
attacks against DNNs for data self-expression as extracted abstract
representations that are capable of facilitating specific learning tasks. We
propose an algorithmic framework that leverages the advantages of the DNNs for
data self-expression and task-specific predictions, to improve image
classification. The framework jointly learns a DNN for attacking Variational
Autoencoder (VAE) networks and a DNN for classification, coined as Attacking
VAE for Improve Classification (AVIC). The experiment results show that AVIC
can achieve higher accuracy on standard datasets compared to the training with
clean examples and the traditional adversarial training."
3245,"He, A further study on the inequality constraints in stochastic configuration

       networks, Information Sciences.","Jia, R.Q.",487 (2019) 77-83.,2022-03-11 13:17:14+00:00,A New Learning Paradigm for Stochastic Configuration Network: SCN+,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yanshuang Ao'), arxiv.Result.Author('Xinyu Zhou'), arxiv.Result.Author('Wei Dai')]","Learning using privileged information (LUPI) paradigm, which pioneered
teacher-student interaction mechanism, makes the learning models use additional
information in training stage. This paper is the first to propose an
incremental learning algorithm with LUPI paradigm for stochastic configuration
network (SCN), named SCN+. This novel algorithm can leverage privileged
information into SCN in the training stage, which provides a new method to
train SCN. Moreover, the convergences have been studied in this paper. Finally,
experimental results indicate that SCN+ indeed performs favorably."
3304,"F MORE CASE STUDIES AND INTERPRETABILITY ANALYSES

In the following, we further study the interpretable effects of DEPTS with more cases.","Given limited data,
all our critical designs, such as properly initializing periodic coefﬁcients, ﬁne-tuning periodic co-
efﬁcients, and conducting expansion learning to decouple the dependencies of xt on zt, play much
crucial roles in producing accurate forecasts.","Fig-
ures 9, 10, 11 and 12 show the additional two cases on ELECTRICITY, TRAFFIC, CAISO, and NP,
respectively.",2022-03-15 06:51:58+00:00,DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Wei Fan'), arxiv.Result.Author('Shun Zheng'), arxiv.Result.Author('Xiaohan Yi'), arxiv.Result.Author('Wei Cao'), arxiv.Result.Author('Yanjie Fu'), arxiv.Result.Author('Jiang Bian'), arxiv.Result.Author('Tie-Yan Liu')]","Periodic time series (PTS) forecasting plays a crucial role in a variety of
industries to foster critical tasks, such as early warning, pre-planning,
resource scheduling, etc. However, the complicated dependencies of the PTS
signal on its inherent periodicity as well as the sophisticated composition of
various periods hinder the performance of PTS forecasting. In this paper, we
introduce a deep expansion learning framework, DEPTS, for PTS forecasting.
DEPTS starts with a decoupled formulation by introducing the periodic state as
a hidden variable, which stimulates us to make two dedicated modules to tackle
the aforementioned two challenges. First, we develop an expansion module on top
of residual learning to perform a layer-by-layer expansion of those complicated
dependencies. Second, we introduce a periodicity module with a parameterized
periodic function that holds sufficient capacity to capture diversified
periods. Moreover, our two customized modules also have certain interpretable
capabilities, such as attributing the forecasts to either local momenta or
global periodicity and characterizing certain core periodic properties, e.g.,
amplitudes and frequencies. Extensive experiments on both synthetic data and
real-world data demonstrate the effectiveness of DEPTS on handling PTS. In most
cases, DEPTS achieves significant improvements over the best baseline.
Specifically, the error reduction can even reach up to 20% for a few cases.
Finally, all codes are publicly available."
3337,"However, the datasets utilized and the experiments carried
                                        heterogeneous graph neural networks on PDNS-Net reveals that                                 out in such research works suffer from several limitations: (1) the
                                        further research is required to improve the performance of these                             collection and labeling methodology are not clear, (2) primarily
                                        models on large heterogeneous graphs.",Our preliminary evaluation of both popular homogeneous and                              20].,"homogeneous GNN models are utilized, (3) the datasets are small
                                                                                                                                     and (4) the datasets are not publicly available.",2022-03-15 14:57:20+00:00,PDNS-Net: A Large Heterogeneous Graph Benchmark Dataset of Network Resolutions for Graph Learning,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Udesh Kumarasinghe'), arxiv.Result.Author('Fatih Deniz'), arxiv.Result.Author('Mohamed Nabeel')]","In order to advance the state of the art in graph learning algorithms, it is
necessary to construct large real-world datasets. While there are many
benchmark datasets for homogeneous graphs, only a few of them are available for
heterogeneous graphs. Furthermore, the latter graphs are small in size
rendering them insufficient to understand how graph learning algorithms perform
in terms of classification metrics and computational resource utilization. We
introduce, PDNS-Net, the largest public heterogeneous graph dataset containing
447K nodes and 897K edges for the malicious domain classification task.
Compared to the popular heterogeneous datasets IMDB and DBLP, PDNS-Net is 38
and 17 times bigger respectively. We provide a detailed analysis of PDNS-Net
including the data collection methodology, heterogeneous graph construction,
descriptive statistics and preliminary graph classification performance. The
dataset is publicly available at https://github.com/qcri/PDNS-Net. Our
preliminary evaluation of both popular homogeneous and heterogeneous graph
neural networks on PDNS-Net reveals that further research is required to
improve the performance of these models on large heterogeneous graphs."
3338,"We assess that further study is required
                                                                       to diagnose the poor performance of homogeneous models on the
   In this section, we introduce experimental results on the two       large dataset PDNS-Net.",the homogeneous models.,"datasets described in this paper, namely mPDNS-Net and PDNS-
Net, on above-mentioned different variants of GNNs designed for        5 CONCLUSION
homogeneous and heterogeneous graphs.",2022-03-15 14:57:20+00:00,PDNS-Net: A Large Heterogeneous Graph Benchmark Dataset of Network Resolutions for Graph Learning,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Udesh Kumarasinghe'), arxiv.Result.Author('Fatih Deniz'), arxiv.Result.Author('Mohamed Nabeel')]","In order to advance the state of the art in graph learning algorithms, it is
necessary to construct large real-world datasets. While there are many
benchmark datasets for homogeneous graphs, only a few of them are available for
heterogeneous graphs. Furthermore, the latter graphs are small in size
rendering them insufficient to understand how graph learning algorithms perform
in terms of classification metrics and computational resource utilization. We
introduce, PDNS-Net, the largest public heterogeneous graph dataset containing
447K nodes and 897K edges for the malicious domain classification task.
Compared to the popular heterogeneous datasets IMDB and DBLP, PDNS-Net is 38
and 17 times bigger respectively. We provide a detailed analysis of PDNS-Net
including the data collection methodology, heterogeneous graph construction,
descriptive statistics and preliminary graph classification performance. The
dataset is publicly available at https://github.com/qcri/PDNS-Net. Our
preliminary evaluation of both popular homogeneous and heterogeneous graph
neural networks on PDNS-Net reveals that further research is required to
improve the performance of these models on large heterogeneous graphs."
3343,"In any case, the reasons behind the
observed diﬀerences in suitability of XAI methods for model improvement are subject to further research.","This may be due to the fact that
Guided Backpropagation is the only of the applied XAI-methods that does not take intermediate features
into account, and instead only expresses a modiﬁed sensitivity (which — except for the inﬂuence of ReLUs
being turned on or oﬀ — exclusively relies on the model parameters).","4.2 Example 2 (Model Equality)

Datasets that reﬂect realistic circumstances — such as the Adience [74] benchmark dataset of unﬁltered
faces or the Pascal VOC 2012 challenge dataset [118] — often do not contain the same amount of examples
for each class.",2022-03-15 15:44:28+00:00,Beyond Explaining: Opportunities and Challenges of XAI-Based Model Improvement,cs.LG,['cs.LG'],"[arxiv.Result.Author('Leander Weber'), arxiv.Result.Author('Sebastian Lapuschkin'), arxiv.Result.Author('Alexander Binder'), arxiv.Result.Author('Wojciech Samek')]","Explainable Artificial Intelligence (XAI) is an emerging research field
bringing transparency to highly complex and opaque machine learning (ML)
models. Despite the development of a multitude of methods to explain the
decisions of black-box classifiers in recent years, these tools are seldomly
used beyond visualization purposes. Only recently, researchers have started to
employ explanations in practice to actually improve models. This paper offers a
comprehensive overview over techniques that apply XAI practically for improving
various properties of ML models, and systematically categorizes these
approaches, comparing their respective strengths and weaknesses. We provide a
theoretical perspective on these methods, and show empirically through
experiments on toy and realistic settings how explanations can help improve
properties such as model generalization ability or reasoning, among others. We
further discuss potential caveats and drawbacks of these methods. We conclude
that while model improvement based on XAI can have significant beneficial
effects even on complex and not easily quantifyable model properties, these
methods need to be applied carefully, since their success can vary depending on
a multitude of factors, such as the model and dataset used, or the employed
explanation method."
3344,"Nevertheless, it requires careful consideration and further research.","As such, practically employing explanations to
improve ML-models is not only a promising concept, but extremely helpful in improving non-trivial model
properties.","While most approaches
focus on augmenting a single component of the training process, diﬀerent augmentations (e.g., data and
loss function) do not interfere with each other, and combining multiple ones could provide a signiﬁcant
improvement to a speciﬁc property, however, this is subject to future work.",2022-03-15 15:44:28+00:00,Beyond Explaining: Opportunities and Challenges of XAI-Based Model Improvement,cs.LG,['cs.LG'],"[arxiv.Result.Author('Leander Weber'), arxiv.Result.Author('Sebastian Lapuschkin'), arxiv.Result.Author('Alexander Binder'), arxiv.Result.Author('Wojciech Samek')]","Explainable Artificial Intelligence (XAI) is an emerging research field
bringing transparency to highly complex and opaque machine learning (ML)
models. Despite the development of a multitude of methods to explain the
decisions of black-box classifiers in recent years, these tools are seldomly
used beyond visualization purposes. Only recently, researchers have started to
employ explanations in practice to actually improve models. This paper offers a
comprehensive overview over techniques that apply XAI practically for improving
various properties of ML models, and systematically categorizes these
approaches, comparing their respective strengths and weaknesses. We provide a
theoretical perspective on these methods, and show empirically through
experiments on toy and realistic settings how explanations can help improve
properties such as model generalization ability or reasoning, among others. We
further discuss potential caveats and drawbacks of these methods. We conclude
that while model improvement based on XAI can have significant beneficial
effects even on complex and not easily quantifyable model properties, these
methods need to be applied carefully, since their success can vary depending on
a multitude of factors, such as the model and dataset used, or the employed
explanation method."
3351,"By obviating
the need for shortcut connections, we believe our method could enable further research into deep
models and their representations.","Unlike the most closely related approach (DKS), our method is fully compatible with
ReLU-family activation functions, and in fact achieves its best performance with them.","In addition, our method may enable new architectures to be trained
for which existing techniques, such as shortcuts and normalization layers, are insufﬁcient.",2022-03-15 17:49:08+00:00,Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Guodong Zhang'), arxiv.Result.Author('Aleksandar Botev'), arxiv.Result.Author('James Martens')]","Training very deep neural networks is still an extremely challenging task.
The common solution is to use shortcut connections and normalization layers,
which are both crucial ingredients in the popular ResNet architecture. However,
there is strong evidence to suggest that ResNets behave more like ensembles of
shallower networks than truly deep ones. Recently, it was shown that deep
vanilla networks (i.e. networks without normalization layers or shortcut
connections) can be trained as fast as ResNets by applying certain
transformations to their activation functions. However, this method (called
Deep Kernel Shaping) isn't fully compatible with ReLUs, and produces networks
that overfit significantly more than ResNets on ImageNet. In this work, we
rectify this situation by developing a new type of transformation that is fully
compatible with a variant of ReLUs -- Leaky ReLUs. We show in experiments that
our method, which introduces negligible extra computational cost, achieves
validation accuracies with deep vanilla networks that are competitive with
ResNets (of the same width/depth), and significantly higher than those obtained
with the Edge of Chaos (EOC) method. And unlike with EOC, the validation
accuracies we obtain do not get worse with depth."
3354,"Blockchain-based
avenues for further study of such variations.","However, this work                    [15] Weishan Zhang, Qinghua Lu, Qiuyu Yu, Zhaotong Li, Yue Liu, Sin Kit
is the ﬁrst attempt to quantify such effects and unlock new                           Lo, Shiping Chen, Xiwei Xu, and Liming Zhu.",While SemiPFL                           federated learning for device failure detection in industrial iot.,2022-03-15 18:09:15+00:00,SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.DC']","[arxiv.Result.Author('Arvin Tashakori'), arxiv.Result.Author('Wenwen Zhang'), arxiv.Result.Author('Z. Jane Wang'), arxiv.Result.Author('Peyman Servati')]","Recent advances in wearable devices and Internet-of-Things (IoT) have led to
massive growth in sensor data generated in edge devices. Labeling such massive
data for classification tasks has proven to be challenging. In addition, data
generated by different users bear various personal attributes and edge
heterogeneity, rendering it impractical to develop a global model that adapts
well to all users. Concerns over data privacy and communication costs also
prohibit centralized data accumulation and training. We propose SemiPFL that
supports edge users having no label or limited labeled datasets and a sizable
amount of unlabeled data that is insufficient to train a well-performing model.
In this work, edge users collaborate to train a Hyper-network in the server,
generating personalized autoencoders for each user. After receiving updates
from edge users, the server produces a set of base models for each user, which
the users locally aggregate them using their own labeled dataset. We
comprehensively evaluate our proposed framework on various public datasets from
a wide range of application scenarios, from wearable health to IoT, and
demonstrate that SemiPFL outperforms state-of-art federated learning frameworks
under the same assumptions regarding user performance, network footprint, and
computational consumption. We also show that the solution performs well for
users without label or having limited labeled datasets and increasing
performance for increased labeled data and number of users, signifying the
effectiveness of SemiPFL for handling data heterogeneity and limited
annotation. We also demonstrate the stability of SemiPFL for handling user
hardware resource heterogeneity in three real-time scenarios."
3357,"We hope that the AUTOMATA framework will popularize the
trend of using subset selection for hyper-parameter tuning and encourage further research on efﬁcient subset selection
approaches for faster hyper-parameter tuning, helping us move closer to the goal of Green AI [43].","AUTOMATA signiﬁcantly decreases
CO2 emissions by making hyper-parameter tuning fast and energy-efﬁcient, in turn reducing environmental impact
of such hyper-parameter tuning on society at large.","One of the limitations
of AUTOMATA is that in scenarios in which no performance loss is desired, we do not know the minimum subset size
to improve speed and, therefore, rely on larger subset sizes such as 10%, 30%.",2022-03-15 19:25:01+00:00,AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Krishnateja Killamsetty'), arxiv.Result.Author('Guttu Sai Abhishek'), arxiv.Result.Author('Aakriti'), arxiv.Result.Author('Alexandre V. Evfimievski'), arxiv.Result.Author('Lucian Popa'), arxiv.Result.Author('Ganesh Ramakrishnan'), arxiv.Result.Author('Rishabh Iyer')]","Deep neural networks have seen great success in recent years; however,
training a deep model is often challenging as its performance heavily depends
on the hyper-parameters used. In addition, finding the optimal hyper-parameter
configuration, even with state-of-the-art (SOTA) hyper-parameter optimization
(HPO) algorithms, can be time-consuming, requiring multiple training runs over
the entire dataset for different possible sets of hyper-parameters. Our central
insight is that using an informative subset of the dataset for model training
runs involved in hyper-parameter optimization, allows us to find the optimal
hyper-parameter configuration significantly faster. In this work, we propose
AUTOMATA, a gradient-based subset selection framework for hyper-parameter
tuning. We empirically evaluate the effectiveness of AUTOMATA in
hyper-parameter tuning through several experiments on real-world datasets in
the text, vision, and tabular domains. Our experiments show that using
gradient-based data subsets for hyper-parameter tuning achieves significantly
faster turnaround times and speedups of 3$\times$-30$\times$ while achieving
comparable performance to the hyper-parameters found using the entire dataset."
3415,"This seems to be an interesting direction for
further research.","It is also found that perplexity analysis can reveal imperfections of a dataset, which
can potentially help with data cleaning.","11
Acknowledgements

Research on this paper was supported by Huawei Technologies Co., Ltd and Hong Kong
Research Grants Council under grant 16204920.",2022-03-16 06:07:51+00:00,Example Perplexity,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Nevin L. Zhang'), arxiv.Result.Author('Weiyan Xie'), arxiv.Result.Author('Zhi Lin'), arxiv.Result.Author('Guanfang Dong'), arxiv.Result.Author('Xiao-Hui Li'), arxiv.Result.Author('Caleb Chen Cao'), arxiv.Result.Author('Yunpeng Wang')]","Some examples are easier for humans to classify than others. The same should
be true for deep neural networks (DNNs). We use the term example perplexity to
refer to the level of difficulty of classifying an example. In this paper, we
propose a method to measure the perplexity of an example and investigate what
factors contribute to high example perplexity. The related codes and resources
are available at https://github.com/vaynexie/Example-Perplexity."
3419,"We further study the rate-distortion performance on two down-
stream tasks: object detection and semantic segmentation, reusing         Figures 6 and 7 show the results for object detection and
the proposed model pretrained on the ImageNet dataset.","The training setup and hyperparameters used to
                                                                      ﬁne-tune the models are described in the supplementary material.","As             semantic segmentation, where the left ﬁgure shows the super-
suggested by He et al.",2022-03-16 18:43:18+00:00,SC2: Supervised Compression for Split Computing,cs.LG,"['cs.LG', 'cs.CV', 'eess.IV']","[arxiv.Result.Author('Yoshitomo Matsubara'), arxiv.Result.Author('Ruihan Yang'), arxiv.Result.Author('Marco Levorato'), arxiv.Result.Author('Stephan Mandt')]","Split computing distributes the execution of a neural network (e.g., for a
classification task) between a mobile device and a more powerful edge server. A
simple alternative to splitting the network is to carry out the supervised task
purely on the edge server while compressing and transmitting the full data, and
most approaches have barely outperformed this baseline. This paper proposes a
new approach for discretizing and entropy-coding intermediate feature
activations to efficiently transmit them from the mobile device to the edge
server. We show that a efficient splittable network architecture results from a
three-way tradeoff between (a) minimizing the computation on the mobile device,
(b) minimizing the size of the data to be transmitted, and (c) maximizing the
model's prediction performance. We propose an architecture based on this
tradeoff and train the splittable network and entropy model in a knowledge
distillation framework. In an extensive set of experiments involving three
vision tasks, three datasets, nine baselines, and more than 180 trained models,
we show that our approach improves supervised rate-distortion tradeoffs while
maintaining a considerably smaller encoder size. We also release sc2bench, an
installable Python package, to encourage and facilitate future studies on
supervised compression for split computing (SC2)."
3446,"In further research, we plan to enrich the input information with a learned
context vector.","Fourth is a composed asymmetrical loss
function based on quantiles, which enables RNN to produce both point forecasts
and PI and also to reduce the forecast bias.","This represents information extracted from other time series,
which can help predict a given time series.",2022-03-17 08:47:49+00:00,Recurrent Neural Networks for Forecasting Time Series with Multiple Seasonality: A Comparative Study,cs.LG,['cs.LG'],"[arxiv.Result.Author('Grzegorz Dudek'), arxiv.Result.Author('Slawek Smyl'), arxiv.Result.Author('Paweł Pełka')]","This paper compares recurrent neural networks (RNNs) with different types of
gated cells for forecasting time series with multiple seasonality. The cells we
compare include classical long short term memory (LSTM), gated recurrent unit
(GRU), modified LSTM with dilation, and two new cells we proposed recently,
which are equipped with dilation and attention mechanisms. To model the
temporal dependencies of different scales, our RNN architecture has multiple
dilated recurrent layers stacked with hierarchical dilations. The proposed RNN
produces both point forecasts and predictive intervals (PIs) for them. An
empirical study concerning short-term electrical load forecasting for 35
European countries confirmed that the new gated cells with dilation and
attention performed best."
3518,"Finally,
section 6 concludes the paper, summarizing the main results achieved, also pointing out
some inconclusive results regarding the potential of pen-up strokes that could lead to
further research efforts.","In section 5 the experimental
setup and the obtained results are presented and its significance is discussed.",2.,2022-03-18 10:37:19+00:00,Gender classification by means of online uppercase handwriting: A text-dependent allographic approach,cs.LG,['cs.LG'],"[arxiv.Result.Author('Enric Sesa-Nogueras'), arxiv.Result.Author('Marcos Faundez-Zanuy'), arxiv.Result.Author('Josep Roure-Alcobé')]","This paper presents a gender classification schema based on online
handwriting. Using samples acquired with a digital tablet that captures the
dynamics of the writing, it classifies the writer as a male or a female. The
method proposed is allographic, regarding strokes as the structural units of
handwriting. Strokes performed while the writing device is not exerting any
pressure on the writing surface, pen-up (in-air) strokes, are also taken into
account. The method is also text-dependent meaning that training and testing is
done with exactly the same text. Text-dependency allows classification be
performed with very small amounts of text. Experimentation, performed with
samples from the BiosecurID database, yields results that fall in the range of
the classification averages expected from human judges. With only four
repetitions of a single uppercase word, the average rate of well classified
writers is 68%; with sixteen words, the rate rises to an average 72.6%.
Statistical analysis reveals that the aforementioned rates are highly
significant. In order to explore the classification potential of the pen-up
strokes, these are also considered. Although in this case results are not
conclusive, an outstanding average of 74% of well classified writers is
obtained when information from pen-up strokes is combined with information from
pen-down ones."
3524,Generalization gets very hard and the           further research.,"This is a task for
represented.",model is overfitting.,2022-03-18 12:39:35+00:00,Why we need biased AI -- How including cognitive and ethical machine biases can enhance AI systems,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Sarah Fabi'), arxiv.Result.Author('Thilo Hagendorff')]","This paper stresses the importance of biases in the field of artificial
intelligence (AI) in two regards. First, in order to foster efficient
algorithmic decision-making in complex, unstable, and uncertain real-world
environments, we argue for the structurewise implementation of human cognitive
biases in learning algorithms. Secondly, we argue that in order to achieve
ethical machine behavior, filter mechanisms have to be applied for selecting
biased training stimuli that represent social or behavioral traits that are
ethically desirable. We use insights from cognitive science as well as ethics
and apply them to the AI field, combining theoretical considerations with seven
case studies depicting tangible bias implementation scenarios. Ultimately, this
paper is the first tentative step to explicitly pursue the idea of a
re-evaluation of the ethical significance of machine biases, as well as putting
the idea forth to implement cognitive biases into machines."
3540,"Although they support their
theoretical analysis with experiments on cifar-10 (where they replicate 1/r of the dataset r times
and show that greater the value of r less the eﬀectiveness of mini-batching to speed up) they never

                                                                                       46
actually measure the gradient diversity in their experiments, or further study its properties.","They show that the greater is the gradient diversity, the more eﬀective are large mini-batches in
speeding up SGD (see also Theorem 3 and the remark following it).","Note
that they compute gradient diversity only on the training set since they use it for reasoning about
optimization only.",2022-03-18 16:09:53+00:00,On the Generalization Mystery in Deep Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Satrajit Chatterjee'), arxiv.Result.Author('Piotr Zielinski')]","The generalization mystery in deep learning is the following: Why do
over-parameterized neural networks trained with gradient descent (GD)
generalize well on real datasets even though they are capable of fitting random
datasets of comparable size? Furthermore, from among all solutions that fit the
training data, how does GD find one that generalizes well (when such a
well-generalizing solution exists)?
  We argue that the answer to both questions lies in the interaction of the
gradients of different examples during training. Intuitively, if the
per-example gradients are well-aligned, that is, if they are coherent, then one
may expect GD to be (algorithmically) stable, and hence generalize well. We
formalize this argument with an easy to compute and interpretable metric for
coherence, and show that the metric takes on very different values on real and
random datasets for several common vision networks. The theory also explains a
number of other phenomena in deep learning, such as why some examples are
reliably learned earlier than others, why early stopping works, and why it is
possible to learn from noisy labels. Moreover, since the theory provides a
causal explanation of how GD finds a well-generalizing solution when one
exists, it motivates a class of simple modifications to GD that attenuate
memorization and improve generalization.
  Generalization in deep learning is an extremely broad phenomenon, and
therefore, it requires an equally general explanation. We conclude with a
survey of alternative lines of attack on this problem, and argue that the
proposed approach is the most viable one on this basis."
3541,"Although they support their
theoretical analysis with experiments on cifar-10 (where they replicate 1/r of the dataset r times
and show that greater the value of r less the eﬀectiveness of mini-batching to speed up) they never

                                                                                       46
actually measure the gradient diversity in their experiments, or further study its properties.","They show that the greater is the gradient diversity, the more eﬀective are large mini-batches in
speeding up SGD (see also Theorem 3 and the remark following it).","Note
that they compute gradient diversity only on the training set since they use it for reasoning about
optimization only.",2022-03-18 16:09:53+00:00,On the Generalization Mystery in Deep Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Satrajit Chatterjee'), arxiv.Result.Author('Piotr Zielinski')]","The generalization mystery in deep learning is the following: Why do
over-parameterized neural networks trained with gradient descent (GD)
generalize well on real datasets even though they are capable of fitting random
datasets of comparable size? Furthermore, from among all solutions that fit the
training data, how does GD find one that generalizes well (when such a
well-generalizing solution exists)? We argue that the answer to both questions
lies in the interaction of the gradients of different examples during training.
Intuitively, if the per-example gradients are well-aligned, that is, if they
are coherent, then one may expect GD to be (algorithmically) stable, and hence
generalize well. We formalize this argument with an easy to compute and
interpretable metric for coherence, and show that the metric takes on very
different values on real and random datasets for several common vision
networks. The theory also explains a number of other phenomena in deep
learning, such as why some examples are reliably learned earlier than others,
why early stopping works, and why it is possible to learn from noisy labels.
Moreover, since the theory provides a causal explanation of how GD finds a
well-generalizing solution when one exists, it motivates a class of simple
modifications to GD that attenuate memorization and improve generalization.
Generalization in deep learning is an extremely broad phenomenon, and
therefore, it requires an equally general explanation. We conclude with a
survey of alternative lines of attack on this problem, and argue that the
proposed approach is the most viable one on this basis."
3542,"Although they support their
theoretical analysis with experiments on cifar-10 (where they replicate 1/r of the dataset r times
and show that greater the value of r less the eﬀectiveness of mini-batching to speed up) they never

                                                                                       47
actually measure the gradient diversity in their experiments, or further study its properties.","They show that the greater is the gradient diversity, the more eﬀective are large mini-batches in
speeding up SGD (see also Theorem 3 and the remark following it).","Note
that they compute gradient diversity only on the training set since they use it for reasoning about
optimization only.",2022-03-18 16:09:53+00:00,On the Generalization Mystery in Deep Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Satrajit Chatterjee'), arxiv.Result.Author('Piotr Zielinski')]","The generalization mystery in deep learning is the following: Why do
over-parameterized neural networks trained with gradient descent (GD)
generalize well on real datasets even though they are capable of fitting random
datasets of comparable size? Furthermore, from among all solutions that fit the
training data, how does GD find one that generalizes well (when such a
well-generalizing solution exists)? We argue that the answer to both questions
lies in the interaction of the gradients of different examples during training.
Intuitively, if the per-example gradients are well-aligned, that is, if they
are coherent, then one may expect GD to be (algorithmically) stable, and hence
generalize well. We formalize this argument with an easy to compute and
interpretable metric for coherence, and show that the metric takes on very
different values on real and random datasets for several common vision
networks. The theory also explains a number of other phenomena in deep
learning, such as why some examples are reliably learned earlier than others,
why early stopping works, and why it is possible to learn from noisy labels.
Moreover, since the theory provides a causal explanation of how GD finds a
well-generalizing solution when one exists, it motivates a class of simple
modifications to GD that attenuate memorization and improve generalization.
Generalization in deep learning is an extremely broad phenomenon, and
therefore, it requires an equally general explanation. We conclude with a
survey of alternative lines of attack on this problem, and argue that the
proposed approach is the most viable one on this basis."
3552,"We believe that our work provides a starting point for further research into improved learning meth-
ods for physical problems.",This was demonstrated with a range of experiments.,"Highly interesting avenues for future work are efﬁcient methods for
the half-inversion of the Jacobian matrix, or applying HIGs to physical systems exhibiting chaotic
behavior or to more sophisticated training setups (Battaglia et al., 2013; Ummenhofer et al., 2020;
Pfaff et al., 2020).",2022-03-18 19:11:04+00:00,Half-Inverse Gradients for Physical Deep Learning,cs.LG,"['cs.LG', 'physics.comp-ph']","[arxiv.Result.Author('Patrick Schnell'), arxiv.Result.Author('Philipp Holl'), arxiv.Result.Author('Nils Thuerey')]","Recent works in deep learning have shown that integrating differentiable
physics simulators into the training process can greatly improve the quality of
results. Although this combination represents a more complex optimization task
than supervised neural network training, the same gradient-based optimizers are
typically employed to minimize the loss function. However, the integrated
physics solvers have a profound effect on the gradient flow as manipulating
scales in magnitude and direction is an inherent property of many physical
processes. Consequently, the gradient flow is often highly unbalanced and
creates an environment in which existing gradient-based optimizers perform
poorly. In this work, we analyze the characteristics of both physical and
neural network optimizations to derive a new method that does not suffer from
this phenomenon. Our method is based on a half-inversion of the Jacobian and
combines principles of both classical network and physics optimizers to solve
the combined optimization task. Compared to state-of-the-art neural network
optimizers, our method converges more quickly and yields better solutions,
which we demonstrate on three complex learning problems involving nonlinear
oscillators, the Schroedinger equation and the Poisson problem."
3577,"An interesting area of further research would be to study strategies to
optimise rule applications, even though in our experiments reduction time was
negligible (less than a second).","The plug-in
uses a brute-force approach on rules and nodes until no rule can be applied
anymore.","6 Evaluation

In this section, we illustrate the practical applicability of the reduction rules
using a real-life experiment, mimicking the setting of a user applying a pro-
cess discovery technique and aiming for a model that is as small as possible
(for human understanding and further machine processing).",2022-03-19 23:40:45+00:00,Language-Preserving Reduction Rules for Block-Structured Workflow Nets,cs.LG,['cs.LG'],[arxiv.Result.Author('Sander J. J. Leemans')],"Process models are used by human analysts to model and analyse behaviour, and
by machines to verify properties such as soundness, liveness or other
reachability properties, and to compare their expressed behaviour with recorded
behaviour within business processes of organisations. For both human and
machine use, small models are preferable over large and complex models: for
ease of human understanding and to reduce the time spent by machines in state
space explorations. Reduction rules that preserve the behaviour of models have
been defined for Petri nets, however in this paper we show that a subclass of
Petri nets returned by process discovery techniques, that is, block-structured
workflow nets, can be further reduced by considering their block structure in
process trees. We revisit an existing set of reduction rules for process trees
and show that the rules are correct, terminating, confluent and complete, and
for which classes of process trees they are and are not complete. In a
real-life experiment, we show that these rules can reduce process models
discovered from real-life event logs further compared with rules that consider
only Petri net structures."
3624,"We further study whether BNS-GCN
different number of partitions.","From Figure 7, we observe that full-graph training                           showing that the proposed BNS-GCN is orthogonal to the
without boundary node sampling (p = 1) or completely                                adopted graph partitioning technique and is not necessarily
isolated training (p = 0) can overﬁt rapidly, regardless of                         limited to METIS.","With boundary node sam-                             consistently improves training efﬁciency on top of different
pling (p = 0.1/0.01), this overﬁtting issue is mitigated,                           partition algorithms, and demonstrate the result in Table 8.",2022-03-21 13:44:37+00:00,BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Boundary Node Sampling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cheng Wan'), arxiv.Result.Author('Youjie Li'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Nam Sung Kim'), arxiv.Result.Author('Yingyan Lin')]","Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art
method for graph-based learning tasks. However, training GCNs at scale is still
challenging, hindering both the exploration of more sophisticated GCN
architectures and their applications to real-world large graphs. While it might
be natural to consider graph partition and distributed training for tackling
this challenge, this direction has only been slightly scratched the surface in
the previous works due to the limitations of existing designs. In this work, we
first analyze why distributed GCN training is ineffective and identify the
underlying cause to be the excessive number of boundary nodes of each
partitioned subgraph, which easily explodes the memory and communication costs
for GCN training. Furthermore, we propose a simple yet effective method dubbed
BNS-GCN that adopts random Boundary-Node-Sampling to enable efficient and
scalable distributed GCN training. Experiments and ablation studies
consistently validate the effectiveness of BNS-GCN, e.g., boosting the
throughput by up to 16.2x and reducing the memory usage by up to 58%, while
maintaining a full-graph accuracy. Furthermore, both theoretical and empirical
analysis show that BNS-GCN enjoys a better convergence than existing
sampling-based methods. We believe that our BNS-GCN has opened up a new
paradigm for enabling GCN training at scale. The code is available at
https://github.com/RICE-EIC/BNS-GCN."
3625,"We further study whether BNS-GCN
different number of partitions.","From Figure 7, we observe that full-graph training                           showing that the proposed BNS-GCN is orthogonal to the
without boundary node sampling (p = 1) or completely                                adopted graph partitioning technique and is not necessarily
isolated training (p = 0) can overﬁt rapidly, regardless of                         limited to METIS.","With boundary node sam-                             consistently improves training efﬁciency on top of different
pling (p = 0.1/0.01), this overﬁtting issue is mitigated,                           partition algorithms, and demonstrate the result in Table 8.",2022-03-21 13:44:37+00:00,BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node Sampling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cheng Wan'), arxiv.Result.Author('Youjie Li'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Nam Sung Kim'), arxiv.Result.Author('Yingyan Lin')]","Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art
method for graph-based learning tasks. However, training GCNs at scale is still
challenging, hindering both the exploration of more sophisticated GCN
architectures and their applications to real-world large graphs. While it might
be natural to consider graph partition and distributed training for tackling
this challenge, this direction has only been slightly scratched the surface in
the previous works due to the limitations of existing designs. In this work, we
first analyze why distributed GCN training is ineffective and identify the
underlying cause to be the excessive number of boundary nodes of each
partitioned subgraph, which easily explodes the memory and communication costs
for GCN training. Furthermore, we propose a simple yet effective method dubbed
BNS-GCN that adopts random Boundary-Node-Sampling to enable efficient and
scalable distributed GCN training. Experiments and ablation studies
consistently validate the effectiveness of BNS-GCN, e.g., boosting the
throughput by up to 16.2x and reducing the memory usage by up to 58%, while
maintaining a full-graph accuracy. Furthermore, both theoretical and empirical
analysis show that BNS-GCN enjoys a better convergence than existing
sampling-based methods. We believe that our BNS-GCN has opened up a new
paradigm for enabling GCN training at scale. The code is available at
https://github.com/RICE-EIC/BNS-GCN."
3632,"We hope our work will elicit further research into building robust multimodal
representations of the physical world.","We also performed an analysis of where and why current models
fail, highlighting the increased difficulty of reasoning about physical commonsense, the lack of fine-grained
temporal information due to limitations in current models’ video and audio processing, and the need for more
advanced audiovisual models.","3

2 Related Work

We cover related work in commonsense reasoning, particularly on physical understanding, which has been
studied in domains spanning psychology, language, vision, robotics, and multimodal machine learning.",2022-03-21 17:05:23+00:00,PACS: A Dataset for Physical Audiovisual CommonSense Reasoning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Samuel Yu'), arxiv.Result.Author('Peter Wu'), arxiv.Result.Author('Paul Pu Liang'), arxiv.Result.Author('Ruslan Salakhutdinov'), arxiv.Result.Author('Louis-Philippe Morency')]","In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, they should be able to reason about the
physical world by understanding the physical properties and affordances of
available objects, how they can be manipulated, and how they interact with
other physical objects. This research field of physical commonsense reasoning
is fundamentally a multi-sensory task since physical properties are manifested
through multiple modalities, two of them being vision and acoustics. Our paper
takes a step towards real-world physical commonsense reasoning by contributing
PACS: the first audiovisual benchmark annotated for physical commonsense
attributes. PACS contains a total of 13,400 question-answer pairs, involving
1,377 unique physical commonsense questions and 1,526 videos. Our dataset
provides new opportunities to advance the research field of physical reasoning
by bringing audio as a core component of this multimodal problem. Using PACS,
we evaluate multiple state-of-the-art models on this new challenging task.
While some models show promising results (70% accuracy), they all fall short of
human performance (95% accuracy). We conclude the paper by demonstrating the
importance of multimodal reasoning and providing possible avenues for future
research."
3633,"We hope our work will elicit further research
into building robust multimodal representations of the physical world.","We also performed an analysis of where
and why current models fail, highlighting the increased difficulty of reasoning
about physical commonsense, the lack of fine-grained temporal information due
to limitations in current models’ video and audio processing, and the need for
more advanced audiovisual models.","3

2 Related Work

We cover related work in commonsense reasoning, particularly on physical un-
derstanding, which has been studied in domains spanning psychology, language,
vision, robotics, and multimodal machine learning.",2022-03-21 17:05:23+00:00,PACS: A Dataset for Physical Audiovisual CommonSense Reasoning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Samuel Yu'), arxiv.Result.Author('Peter Wu'), arxiv.Result.Author('Paul Pu Liang'), arxiv.Result.Author('Ruslan Salakhutdinov'), arxiv.Result.Author('Louis-Philippe Morency')]","In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, it must be able to robustly reason about
the physical world. Fundamental to this reasoning is physical common sense:
understanding the physical properties and affordances of available objects, how
they can be manipulated, and how they interact with other objects. Physical
commonsense reasoning is fundamentally a multi-sensory task, since physical
properties are manifested through multiple modalities - two of them being
vision and acoustics. Our paper takes a step towards real-world physical
commonsense reasoning by contributing PACS: the first audiovisual benchmark
annotated for physical commonsense attributes. PACS contains 13,400
question-answer pairs, involving 1,377 unique physical commonsense questions
and 1,526 videos. Our dataset provides new opportunities to advance the
research field of physical reasoning by bringing audio as a core component of
this multimodal problem. Using PACS, we evaluate multiple state-of-the-art
models on our new challenging task. While some models show promising results
(70% accuracy), they all fall short of human performance (95% accuracy). We
conclude the paper by demonstrating the importance of multimodal reasoning and
providing possible avenues for future research."
3634,"We hope our work will elicit further research
into building robust multimodal representations of the physical world.","We also performed an analysis of where
and why current models fail, highlighting the increased difficulty of reasoning
about physical commonsense, the lack of fine-grained temporal information due
to limitations in current models’ video and audio processing, and the need for
more advanced audiovisual models.","3

2 Related Work

We cover related work in commonsense reasoning, particularly on physical un-
derstanding, which has been studied in domains spanning psychology, language,
vision, robotics, and multimodal machine learning.",2022-03-21 17:05:23+00:00,PACS: A Dataset for Physical Audiovisual CommonSense Reasoning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Samuel Yu'), arxiv.Result.Author('Peter Wu'), arxiv.Result.Author('Paul Pu Liang'), arxiv.Result.Author('Ruslan Salakhutdinov'), arxiv.Result.Author('Louis-Philippe Morency')]","In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, it must be able to robustly reason about
the physical world. Fundamental to this reasoning is physical common sense:
understanding the physical properties and affordances of available objects, how
they can be manipulated, and how they interact with other objects. Physical
commonsense reasoning is fundamentally a multi-sensory task, since physical
properties are manifested through multiple modalities - two of them being
vision and acoustics. Our paper takes a step towards real-world physical
commonsense reasoning by contributing PACS: the first audiovisual benchmark
annotated for physical commonsense attributes. PACS contains 13,400
question-answer pairs, involving 1,377 unique physical commonsense questions
and 1,526 videos. Our dataset provides new opportunities to advance the
research field of physical reasoning by bringing audio as a core component of
this multimodal problem. Using PACS, we evaluate multiple state-of-the-art
models on our new challenging task. While some models show promising results
(70% accuracy), they all fall short of human performance (95% accuracy). We
conclude the paper by demonstrating the importance of multimodal reasoning and
providing possible avenues for future research."
3635,"In contrast, here we want to explore any model that shows
unusual abilities in any way for further study.","In particular, if one knows for an
application which metric best represents the goals of the end user, then one may use only that
metric in model veriﬁcation/selection.","Procedure to generate summary score for each spatial ﬁlter F and model M:

  1.",2022-03-21 17:18:43+00:00,Can we integrate spatial verification methods into neural-network loss functions for atmospheric science?,cs.LG,"['cs.LG', 'cs.AI', 'physics.ao-ph', 'stat.AP', 'stat.ME']","[arxiv.Result.Author('Ryan Lagerquist'), arxiv.Result.Author('Imme Ebert-Uphoff')]","In the last decade, much work in atmospheric science has focused on spatial
verification (SV) methods for gridded prediction, which overcome serious
disadvantages of pixelwise verification. However, neural networks (NN) in
atmospheric science are almost always trained to optimize pixelwise loss
functions, even when ultimately assessed with SV methods. This establishes a
disconnect between model verification during vs. after training. To address
this issue, we develop spatially enhanced loss functions (SELF) and demonstrate
their use for a real-world problem: predicting the occurrence of thunderstorms
(henceforth, ""convection"") with NNs. In each SELF we use either a neighbourhood
filter, which highlights convection at scales larger than a threshold, or a
spectral filter (employing Fourier or wavelet decomposition), which is more
flexible and highlights convection at scales between two thresholds. We use
these filters to spatially enhance common verification scores, such as the
Brier score. We train each NN with a different SELF and compare their
performance at many scales of convection, from discrete storm cells to tropical
cyclones. Among our many findings are that (a) for a low (high) risk threshold,
the ideal SELF focuses on small (large) scales; (b) models trained with a
pixelwise loss function perform surprisingly well; (c) however, models trained
with a spectral filter produce much better-calibrated probabilities than a
pixelwise model. We provide a general guide to using SELFs, including technical
challenges and the final Python code, as well as demonstrating their use for
the convection problem. To our knowledge this is the most in-depth guide to
SELFs in the geosciences."
3685,"4.4 Effectiveness of Model Architecture Complexity

To further study the effectiveness of Federated Knowledge Alignment (FedKA) when clients apply a larger model,
we employed the Resnet18 [He et al., 2015] without pretraining to perform feature extraction.",The result shows that a larger model can contribute to better improvement in the global model performance.,"Based on the same
hyperparameter setting, we evaluate the model performance in Digit-Five (Table 4) and Ofﬁce-Caltech10 (Table 5),
respectively.",2022-03-22 11:42:25+00:00,Feature Distribution Matching for Federated Domain Generalization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yuwei Sun'), arxiv.Result.Author('Ng Chong'), arxiv.Result.Author('Hideya Ochiai')]","Multi-source domain adaptation has been intensively studied. The distribution
shift in features inherent to specific domains brings in negative transfer
degrading a model's generality to unseen tasks. In Federated Learning (FL), to
leverage knowledge from different domains, learned model parameters are shared
to train a global model. Nonetheless, the data confidentiality of FL hinders
the effectiveness of traditional domain adaptation methods that require prior
knowledge of different domain data. To this end, we propose a new federated
domain generation method called Federated Knowledge Alignment (FedKA). FedKA
leverages feature distribution matching in a global workspace such that the
global model can learn domain-invariant client features under the constraint of
unknown domain data. A federated voting mechanism is devised to generate target
domain pseudo-labels based on the consensus from clients facilitating global
model fine-tuning. We performed extensive experiments including an ablation
study to evaluate the effectiveness of the proposed method in both image
classification tasks and a text classification task based on model
architectures with different complexities. The empirical results show that
FedKA can achieve performance gains of 8.8% and 3.5% in Digit-Five and
Office-Caltech10, respectively, and a gain of 0.7% in Amazon Review with
extremely limited training data."
3686,"4.4 Effectiveness of Model Architecture Complexity

To further study the effectiveness of Federated Knowledge Alignment (FedKA) when applying different encoder
models, we employed Resnet18 (He et al., 2015) without pretraining to perform feature extraction.","The result shows that FedKA can better beneﬁt the improvement in global model performance
with a more complex encoder model.","Based on the same
hyperparameter setting, we evaluate the model performance in Digit-Five (Table 4) and Ofﬁce-Caltech10 (Table 5).",2022-03-22 11:42:25+00:00,Feature Distribution Matching for Federated Domain Generalization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yuwei Sun'), arxiv.Result.Author('Ng Chong'), arxiv.Result.Author('Hideya Ochiai')]","Multi-source domain adaptation has been intensively studied. The distribution
shift in features inherent to specific domains causes the negative transfer
problem, degrading a model's generality to unseen tasks. In Federated Learning
(FL), learned model parameters are shared to train a global model that
leverages the underlying knowledge across client models trained on separate
data domains. Nonetheless, the data confidentiality of FL hinders the
effectiveness of traditional domain adaptation methods that require prior
knowledge of different domain data. We propose a new federated domain
generalization method called Federated Knowledge Alignment (FedKA). FedKA
leverages feature distribution matching in a global workspace such that the
global model can learn domain-invariant client features under the constraint of
unknown client data. FedKA employs a federated voting mechanism that generates
target domain pseudo-labels based on the consensus from clients to facilitate
global model fine-tuning. We performed extensive experiments, including an
ablation study, to evaluate the effectiveness of the proposed method in both
image and text classification tasks using different model architectures. The
empirical results show that FedKA achieves performance gains of 8.8% and 3.5%
in Digit-Five and Office-Caltech10, respectively, and a gain of 0.7% in Amazon
Review with extremely limited training data. Moreover, we studied the
effectiveness of FedKA in alleviating the negative transfer of FL based on a
new criterion called Group Effect. The results show that FedKA can reduce
negative transfer, improving the performance gain via model aggregation by 4
times."
3716,"Given the larger
prevalence of face image datasets with gender annotations, it is encouraging that
debiasing on gender also reduces racial bias but further research is needed into
cross-attribute debiasing generalization.","Even though FairFace is well-balanced across
gender, race, and their intersection, racial bias in the pretrained baseline is more
than twice the gender bias (on both MaxSkew and NDKL).",Debiasing effects on harmful image classification.,2022-03-22 17:59:04+00:00,A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning,cs.LG,"['cs.LG', 'cs.CL', 'cs.CV', 'cs.CY']","[arxiv.Result.Author('Hugo Berg'), arxiv.Result.Author('Siobhan Mackenzie Hall'), arxiv.Result.Author('Yash Bhalgat'), arxiv.Result.Author('Wonsuk Yang'), arxiv.Result.Author('Hannah Rose Kirk'), arxiv.Result.Author('Aleksandar Shtedritski'), arxiv.Result.Author('Max Bain')]","Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations."
3717,"Given the larger
prevalence of face image datasets with gender annotations, it is encouraging that
debiasing on gender also reduces racial bias but further research is needed into
cross-attribute debiasing generalization.","Even though FairFace is well-balanced across
gender, race, and their intersection, racial bias in the pretrained baseline is more
than twice the gender bias (on both MaxSkew and NDKL).",Debiasing effects on harmful image classification.,2022-03-22 17:59:04+00:00,A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning,cs.LG,"['cs.LG', 'cs.CL', 'cs.CV', 'cs.CY']","[arxiv.Result.Author('Hugo Berg'), arxiv.Result.Author('Siobhan Mackenzie Hall'), arxiv.Result.Author('Yash Bhalgat'), arxiv.Result.Author('Wonsuk Yang'), arxiv.Result.Author('Hannah Rose Kirk'), arxiv.Result.Author('Aleksandar Shtedritski'), arxiv.Result.Author('Max Bain')]","Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations."
3718,"Given the greater prevalence of face                   NLP evidence, CV investigations have also shown
image datasets with gender annotations, it is en-                 evidence of gender bias (Zhao et al., 2017), racial
couraging that debiasing on gender also reduces                   bias (Wilson et al., 2019), and their intersection
racial bias but further research is needed into cross-            (Buolamwini and Gebru, 2018; Steed and Caliskan,
attribute debiasing generalization.","Similar to the body of
and NDKL).",2021).,2022-03-22 17:59:04+00:00,A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning,cs.LG,"['cs.LG', 'cs.CL', 'cs.CV', 'cs.CY']","[arxiv.Result.Author('Hugo Berg'), arxiv.Result.Author('Siobhan Mackenzie Hall'), arxiv.Result.Author('Yash Bhalgat'), arxiv.Result.Author('Wonsuk Yang'), arxiv.Result.Author('Hannah Rose Kirk'), arxiv.Result.Author('Aleksandar Shtedritski'), arxiv.Result.Author('Max Bain')]","Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these multimodal harms due to
lacking measurement robustness and feature degradation. To address these
challenges, we investigate bias measures and apply ranking metrics for
image-text representations. We then investigate debiasing methods and show that
prepending learned embeddings to text queries that are jointly trained with
adversarial debiasing and a contrastive loss reduces various bias measures with
minimal degradation to the image-text representation."
3719,"Given the greater prevalence of face                   NLP evidence, CV investigations have also shown
image datasets with gender annotations, it is en-                 evidence of gender bias (Zhao et al., 2017), racial
couraging that debiasing on gender also reduces                   bias (Wilson et al., 2019), and their intersection
racial bias but further research is needed into cross-            (Buolamwini and Gebru, 2018; Steed and Caliskan,
attribute debiasing generalization.","Similar to the body of
and NDKL).",2021).,2022-03-22 17:59:04+00:00,A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning,cs.LG,"['cs.LG', 'cs.CL', 'cs.CV', 'cs.CY']","[arxiv.Result.Author('Hugo Berg'), arxiv.Result.Author('Siobhan Mackenzie Hall'), arxiv.Result.Author('Yash Bhalgat'), arxiv.Result.Author('Wonsuk Yang'), arxiv.Result.Author('Hannah Rose Kirk'), arxiv.Result.Author('Aleksandar Shtedritski'), arxiv.Result.Author('Max Bain')]","Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these multimodal harms due to
lacking measurement robustness and feature degradation. To address these
challenges, we investigate bias measures and apply ranking metrics for
image-text representations. We then investigate debiasing methods and show that
prepending learned embeddings to text queries that are jointly trained with
adversarial debiasing and a contrastive loss reduces various bias measures with
minimal degradation to the image-text representation."
3746,"However,
understanding which functional phenomena are being captured into such communities of nodes
needs further study.","Persistence summaries suggest
that grouping neurons in terms of their activation structure is feasible for DNNs.","This could lead to the discovery of new architectural properties useful to
develop better networks.",2022-03-23 11:15:36+00:00,Towards explaining the generalization gap in neural networks using topological data analysis,cs.LG,"['cs.LG', 'math.AT', '55N31, 68T07', 'I.2.6']","[arxiv.Result.Author('Rubén Ballester'), arxiv.Result.Author('Xavier Arnal Clemente'), arxiv.Result.Author('Carles Casacuberta'), arxiv.Result.Author('Meysam Madadi'), arxiv.Result.Author('Ciprian A. Corneanu'), arxiv.Result.Author('Sergio Escalera')]","Understanding how neural networks generalize on unseen data is crucial for
designing more robust and reliable models. In this paper, we study the
generalization gap of neural networks using methods from topological data
analysis. For this purpose, we compute homological persistence diagrams of
weighted graphs constructed from neuron activation correlations after a
training phase, aiming to capture patterns that are linked to the
generalization capacity of the network. We compare the usefulness of different
numerical summaries from persistence diagrams and show that a combination of
some of them can accurately predict and partially explain the generalization
gap without the need of a test set. Evaluation on two computer vision
recognition tasks (CIFAR10 and SVHN) shows competitive generalization gap
prediction when compared against state-of-the-art methods."
3794,"Thus, we argue that using compound

                                                                                                    19
augmentation approach, e.g., discard a portion of nodes and mask a portion of nodes at the same time
for one view, could be a direction for further research on graph contrastive learning.","For
example, for COLLAB dataset, the combination of FD and RWS only achieves 68.27% accuracy
while the combination of NS and ER achieves 75.07% accuracy.","Self-supervised
classiﬁcation performance may be stabilized and improved by adaptively or learnablely compounding
the augmentation approahces.",2022-03-24 02:58:36+00:00,On Understanding and Mitigating the Dimensional Collapse of Graph Contrastive Learning: a Non-Maximum Removal Approach,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jiawei Sun'), arxiv.Result.Author('Ruoxin Chen'), arxiv.Result.Author('Jie Li'), arxiv.Result.Author('Chentao Wu'), arxiv.Result.Author('Yue Ding'), arxiv.Result.Author('Junchi Yan')]","Graph Contrastive Learning (GCL) has shown promising performance in graph
representation learning (GRL) without the supervision of manual annotations.
GCL can generate graph-level embeddings by maximizing the Mutual Information
(MI) between different augmented views of the same graph (positive pairs).
However, the GCL is limited by dimensional collapse, i.e., embedding vectors
only occupy a low-dimensional subspace. In this paper, we show that the
smoothing effect of the graph pooling and the implicit regularization of the
graph convolution are two causes of the dimensional collapse in GCL. To
mitigate the above issue, we propose a non-maximum removal graph contrastive
learning approach (nmrGCL), which removes ""prominent'' dimensions (i.e.,
contribute most in similarity measurement) for positive pair in the pre-text
task. Comprehensive experiments on various benchmark datasets are conducted to
demonstrate the effectiveness of nmrGCL, and the results show that our model
outperforms the state-of-the-art methods. Source code will be made publicly
available."
3805,"5.1 Further research

Our baseline is a stepping stone for further research, where multiple avenues should be explored.","Hence, using the Dutch Draw as a general, simple and informative baseline should be the new gold
standard in any model evaluation process.","We
discuss four possible research directions.",2022-03-24 14:20:27+00:00,The Dutch Draw: Constructing a Universal Baseline for Binary Prediction Models,cs.LG,"['cs.LG', 'cs.PF', 'math.ST', 'stat.ME', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Etienne van de Bijl'), arxiv.Result.Author('Jan Klein'), arxiv.Result.Author('Joris Pries'), arxiv.Result.Author('Sandjai Bhulai'), arxiv.Result.Author('Mark Hoogendoorn'), arxiv.Result.Author('Rob van der Mei')]","Novel prediction methods should always be compared to a baseline to know how
well they perform. Without this frame of reference, the performance score of a
model is basically meaningless. What does it mean when a model achieves an
$F_1$ of 0.8 on a test set? A proper baseline is needed to evaluate the
`goodness' of a performance score. Comparing with the latest state-of-the-art
model is usually insightful. However, being state-of-the-art can change rapidly
when newer models are developed. Contrary to an advanced model, a simple dummy
classifier could be used. However, the latter could be beaten too easily,
making the comparison less valuable. This paper presents a universal baseline
method for all binary classification models, named the Dutch Draw (DD). This
approach weighs simple classifiers and determines the best classifier to use as
a baseline. We theoretically derive the DD baseline for many commonly used
evaluation measures and show that in most situations it reduces to (almost)
always predicting either zero or one. Summarizing, the DD baseline is: (1)
general, as it is applicable to all binary classification problems; (2) simple,
as it is quickly determined without training or parameter-tuning; (3)
informative, as insightful conclusions can be drawn from the results. The DD
baseline serves two purposes. First, to enable comparisons across research
papers by this robust and universal baseline. Secondly, to provide a sanity
check during the development process of a prediction model. It is a major
warning sign when a model is outperformed by the DD baseline."
3828,"For two manipulation graphs, given by → and
tail which graph structure, combinations of graphs structures        , we let their H-PX -distance be deﬁned as
and hypothesis classes or classes of cost function lead to the
strategic component sets having ﬁnite VC-dimension is an          dH,PX (→, ) = sup Ex∼PX [|ℓ→,⊥(h, x) − ℓ ,⊥(h, x)|]
intriguing direction for further research.","We believe, in many natural situations the con-
ditions in that theorem will hold, and analyzing in more de-      Deﬁnition 4.","h∈H
   We close this section with two results, both stated in The-
orem 5, on the learnability under the strategic loss in the gen-     We will now bound the strategic manipulation loss L→ P (h)
eral case where the VC-dimension of the strategic compo-          with respect to the true graph → in terms of the strategic
nent sets may be inﬁnite.",2022-03-25 02:26:16+00:00,Learning Losses for Strategic Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tosca Lechner'), arxiv.Result.Author('Ruth Urner')]","Strategic classification, i.e. classification under possible strategic
manipulations of features, has received a lot of attention from both the
machine learning and the game theory community. Most works focus on analysing
properties of the optimal decision rule under such manipulations. In our work
we take a learning theoretic perspective, focusing on the sample complexity
needed to learn a good decision rule which is robust to strategic manipulation.
We perform this analysis by introducing a novel loss function, the
\emph{strategic manipulation loss}, which takes into account both the accuracy
of the final decision rule and its vulnerability to manipulation. We analyse
the sample complexity for a known graph of possible manipulations in terms of
the complexity of the function class and the manipulation graph. Additionally,
we initialize the study of learning under unknown manipulation capabilities of
the involved agents. Using techniques from transfer learning theory, we define
a similarity measure for manipulation graphs and show that learning outcomes
are robust with respect to small changes in the manipulation graph. Lastly, we
analyse the (sample complexity of) learning of the manipulation capability of
agents with respect to this similarity measure, providing novel guarantees for
strategic classification with respect to an unknown manipulation graph."
3830,"Therefore, in this part, we further study the effect of other types of data
augmentations, and we show that our ARC metric is also effective for evaluating not only other
kinds of data augmentations, but also their composed ones.","Nevertheless,
in practice, the augmentations adopted in contrastive learning is composed of a list of different
kinds of augmentations.",Comparing different kinds of augmentations.,2022-03-25 05:36:26+00:00,Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Yifei Wang'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Jiansheng Yang'), arxiv.Result.Author('Zhouchen Lin')]","Recently, contrastive learning has risen to be a promising approach for
large-scale self-supervised learning. However, theoretical understanding of how
it works is still unclear. In this paper, we propose a new guarantee on the
downstream performance without resorting to the conditional independence
assumption that is widely adopted in previous work but hardly holds in
practice. Our new theory hinges on the insight that the support of different
intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Based on this augmentation overlap perspective, theoretically, we
obtain asymptotically closed bounds for downstream performance under weaker
assumptions, and empirically, we propose an unsupervised model selection metric
ARC that aligns well with downstream accuracy. Our theory suggests an
alternative understanding of contrastive learning: the role of aligning
positive samples is more like a surrogate task than an ultimate goal, and the
overlapped augmented views (i.e., the chaos) create a ladder for contrastive
learning to gradually learn class-separated representations. The code for
computing ARC is available at https://github.com/zhangq327/ARC."
3831,"Therefore, in this part, we further study the effect of other types of data
augmentations, and we show that our ARC metric is also effective for evaluating not only other
kinds of data augmentations, but also their composed ones.","Nevertheless,
in practice, the augmentations adopted in contrastive learning is composed of a list of different
kinds of augmentations.",Figure 9: Downstream accuracy (ACC) v.s.,2022-03-25 05:36:26+00:00,Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Yifei Wang'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Jiansheng Yang'), arxiv.Result.Author('Zhouchen Lin')]","Recently, contrastive learning has risen to be a promising approach for
large-scale self-supervised learning. However, theoretical understanding of how
it works is still unclear. In this paper, we propose a new guarantee on the
downstream performance without resorting to the conditional independence
assumption that is widely adopted in previous work but hardly holds in
practice. Our new theory hinges on the insight that the support of different
intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Based on this augmentation overlap perspective, theoretically, we
obtain asymptotically closed bounds for downstream performance under weaker
assumptions, and empirically, we propose an unsupervised model selection metric
ARC that aligns well with downstream accuracy. Our theory suggests an
alternative understanding of contrastive learning: the role of aligning
positive samples is more like a surrogate task than an ultimate goal, and the
overlapped augmented views (i.e., the chaos) create a ladder for contrastive
learning to gradually learn class-separated representations. The code for
computing ARC is available at https://github.com/zhangq327/ARC."
3873,"• In order to help researchers to ﬁnd further research directions and po-
      sition their work, a general taxonomy for concept analysis approaches is
      developed and applied to state-of-the-art research methods (cf.",Section 3).,"Section 4
      and Figure 1).",2022-03-25 20:57:16+00:00,Concept Embedding Analysis: A Review,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML', 'I.2.4; I.2.6; I.2.10; I.2.m']",[arxiv.Result.Author('Gesina Schwalbe')],"Deep neural networks (DNNs) have found their way into many applications with
potential impact on the safety, security, and fairness of
human-machine-systems. Such require basic understanding and sufficient trust by
the users. This motivated the research field of explainable artificial
intelligence (XAI), i.e. finding methods for opening the ""black-boxes"" DNNs
represent. For the computer vision domain in specific, practical assessment of
DNNs requires a globally valid association of human interpretable concepts with
internals of the model. The research field of concept (embedding) analysis (CA)
tackles this problem: CA aims to find global, assessable associations of
humanly interpretable semantic concepts (e.g., eye, bearded) with internal
representations of a DNN. This work establishes a general definition of CA and
a taxonomy for CA methods, uniting several ideas from literature. That allows
to easily position and compare CA approaches. Guided by the defined notions,
the current state-of-the-art research regarding CA methods and interesting
applications are reviewed. More than thirty relevant methods are discussed,
compared, and categorized. Finally, for practitioners, a survey of fifteen
datasets is provided that have been used for supervised concept analysis. Open
challenges and research directions are pointed out at the end."
3882,"Besides, we further explain the multi-modal big models in multi-lingual form and propose some
    directions that worth further studying.","Except the text and image modalities, big models for other modalities, such as video and audio, are also introduced
    in this section.","Key Technologies

 – Theory and Interpretability for Big Model (Section 9)
    Big models have received great empirical successes in recent years.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3883,It is foreseeable that there will be more and more work to carry out further research on this in the future.,"At the
same time, it can also provide a new way of solving problems for downstream tasks, which has signiﬁcant research
value.",2.4.2 Why and How Unsupervised Training Helps?,2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3884,"Despite these solutions, further research is worth being conducted on this issue.","Some of them are open-source, making
it possible to adapt to speciﬁc applications.","What is more, for LSICS system-
level optimization, local optimizations do not imply global optimization.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3885,"73

8.5.2 Uniﬁed Model Architecture

Although VLP models have achieved great successes, there are still problems that could be further researched.","In addition, since videos have an extra time dimension than images with only spatial dimensions, it is also important
to design algorithms to eﬀectively model the time series information while learning cross-modal correlations at the
same time.","One
major issue is the conﬂict between single-stream-based and two-stream-based models.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3886,How to develop an eﬀective multilingual BM still needs further research.,"Taking mBART as an example, it has 12
transformer layers and 80,000 vocabulary size to achieve better performance, which is much larger than a typical NMT
model.","Second, most multilingual pre-training
methods achieve substantial improvements on low or medium resource translation tasks, but the improvements on rich
resource tasks are not signiﬁcant enough.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3887,"Therefore, scaling up the model is only a means to touch the upper
limit of machine intelligence, and further research and exploration on the nature and laws of the model need to be
conducted.","In order to achieve higher levels of machine intelligence, larger models perform better and better within a certain
range but with lower and lower marginal beneﬁts.","18.2.6 Interpretability

DNNs can be regarded as black-box models because people know little about how models output the ﬁnal result
according to the input information.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3888,"Besides, we further explain the multi-modal big models in multi-lingual form and propose some
    directions that worth further studying.","Except the text and image modalities, big models for other modalities, such as video and audio, are also introduced
    in this section.","Key Technologies

 – Theory and Interpretability for Big Model (Section 9)
    Big models have received great empirical successes in recent years.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3889,It is foreseeable that there will be more and more work to carry out further research on this in the future.,"At the
same time, it can also provide a new way of solving problems for downstream tasks, which has signiﬁcant research
value.",2.4.2 Why and How Unsupervised Training Helps?,2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3890,"Despite these solutions, further research is worth being conducted on this issue.","Some of them are open-source, making
it possible to adapt to speciﬁc applications.","What is more, for LSICS system-
level optimization, local optimizations do not imply global optimization.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3891,"73

8.5.2 Uniﬁed Model Architecture

Although VLP models have achieved great successes, there are still problems that could be further researched.","In addition, since videos have an extra time dimension than images with only spatial dimensions, it is also important
to design algorithms to eﬀectively model the time series information while learning cross-modal correlations at the
same time.","One
major issue is the conﬂict between single-stream-based and two-stream-based models.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3892,How to develop an eﬀective multilingual BM still needs further research.,"Taking mBART as an example, it has 12
transformer layers and 80,000 vocabulary size to achieve better performance, which is much larger than a typical NMT
model.","Second, most multilingual pre-training
methods achieve substantial improvements on low or medium resource translation tasks, but the improvements on rich
resource tasks are not signiﬁcant enough.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3893,"Therefore, scaling up the model is only a means to touch the upper
limit of machine intelligence, and further research and exploration on the nature and laws of the model need to be
conducted.","In order to achieve higher levels of machine intelligence, larger models perform better and better within a certain
range but with lower and lower marginal beneﬁts.","18.2.6 Interpretability

DNNs can be regarded as black-box models because people know little about how models output the ﬁnal result
according to the input information.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3894,"Besides, we further explain the multi-modal big models in multi-lingual form and propose some
    directions that worth further studying.","Except the text and image modalities, big models for other modalities, such as video and audio, are also introduced
    in this section.","Key Technologies

 – Theory and Interpretability for Big Model (Section 9)
    Big models have received great empirical successes in recent years.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3895,It is foreseeable that there will be more and more work to carry out further research on this in the future.,"At the
same time, it can also provide a new way of solving problems for downstream tasks, which has signiﬁcant research
value.",2.4.2 Why and How Unsupervised Training Helps?,2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3896,"Despite these solutions, further research is worth being conducted on this issue.","Some of them are open-source, making
it possible to adapt to speciﬁc applications.","What is more, for LSICS system-
level optimization, local optimizations do not imply global optimization.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3897,"73

8.5.2 Uniﬁed Model Architecture

Although VLP models have achieved great successes, there are still problems that could be further researched.","In addition, since videos have an extra time dimension than images with only spatial dimensions, it is also important
to design algorithms to eﬀectively model the time series information while learning cross-modal correlations at the
same time.","One
major issue is the conﬂict between single-stream-based and two-stream-based models.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3898,How to develop an eﬀective multilingual BM still needs further research.,"Taking mBART as an example, it has 12
transformer layers and 80,000 vocabulary size to achieve better performance, which is much larger than a typical NMT
model.","Second, most multilingual pre-training
methods achieve substantial improvements on low or medium resource translation tasks, but the improvements on rich
resource tasks are not signiﬁcant enough.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3899,"Therefore, scaling up the model is only a means to touch the upper
limit of machine intelligence, and further research and exploration on the nature and laws of the model need to be
conducted.","In order to achieve higher levels of machine intelligence, larger models perform better and better within a certain
range but with lower and lower marginal beneﬁts.","18.2.6 Interpretability

DNNs can be regarded as black-box models because people know little about how models output the ﬁnal result
according to the input information.",2022-03-26 15:38:00+00:00,A Roadmap for Big Model,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Sha Yuan'), arxiv.Result.Author('Hanyu Zhao'), arxiv.Result.Author('Shuai Zhao'), arxiv.Result.Author('Jiahong Leng'), arxiv.Result.Author('Yangxiao Liang'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Jifan Yu'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Zhou Shao'), arxiv.Result.Author('Jiaao He'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhenghao Liu'), arxiv.Result.Author('Ning Ding'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Yizhao Gao'), arxiv.Result.Author('Liang Zhang'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Cong Fang'), arxiv.Result.Author('Yisen Wang'), arxiv.Result.Author('Mingsheng Long'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Yinpeng Dong'), arxiv.Result.Author('Tianyu Pang'), arxiv.Result.Author('Peng Cui'), arxiv.Result.Author('Lingxiao Huang'), arxiv.Result.Author('Zheng Liang'), arxiv.Result.Author('Huawei Shen'), arxiv.Result.Author('Hui Zhang'), arxiv.Result.Author('Quanshi Zhang'), arxiv.Result.Author('Qingxiu Dong'), arxiv.Result.Author('Zhixing Tan'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Long Zhou'), arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Yingwei Pan'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Zhou Yu'), arxiv.Result.Author('Rui Yan'), arxiv.Result.Author('Chence Shi'), arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Guoqiang Wang'), arxiv.Result.Author('Xiang Pan'), arxiv.Result.Author('Mengjie Li'), arxiv.Result.Author('Xiaoyu Chu'), arxiv.Result.Author('Zijun Yao'), arxiv.Result.Author('Fangwei Zhu'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Weicheng Xue'), arxiv.Result.Author('Zixuan Ma'), arxiv.Result.Author('Zhengyan Zhang'), arxiv.Result.Author('Shengding Hu'), arxiv.Result.Author('Yujia Qin'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Zheni Zeng'), arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Weize Chen'), arxiv.Result.Author('Weilin Zhao'), arxiv.Result.Author('Yuan Yao'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Wenzhao Zheng'), arxiv.Result.Author('Wenliang Zhao'), arxiv.Result.Author('Ziyi Wang'), arxiv.Result.Author('Borui Zhang'), arxiv.Result.Author('Nanyi Fei'), arxiv.Result.Author('Anwen Hu'), arxiv.Result.Author('Zenan Ling'), arxiv.Result.Author('Haoyang Li'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Weidong Zhan'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Hao Sun'), arxiv.Result.Author('Jiawen Deng'), arxiv.Result.Author('Chujie Zheng'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Xigang Cao'), arxiv.Result.Author('Jidong Zhai'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun'), arxiv.Result.Author('Jiwen Lu'), arxiv.Result.Author('Zhiwu Lu'), arxiv.Result.Author('Qin Jin'), arxiv.Result.Author('Ruihua Song'), arxiv.Result.Author('Ji-Rong Wen'), arxiv.Result.Author('Zhouchen Lin'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Hang Su'), arxiv.Result.Author('Jun Zhu'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Jiajun Zhang'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiaodong He'), arxiv.Result.Author('Minlie Huang'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Jie Tang')]","With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&Interpretability, Commonsense Reasoning, Reliability&Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view."
3912,"However, this work                     constructs the network with four 3 × 3 convolutional lay-
may also suffer from some negative impacts, which is worthy                  ers (each followed by ReLU and 2 × 2 max-pooling) and
of further research and exploration.","Network Structure
for saving the cost and time of data annotation, boosting
the reusability of knowledge across domains, and greatly                        For Digits-DG, the network is the same as [75] which
improving the generalization ability.","Speciﬁcally, more jobs                  a softmax classiﬁcation layer, denoted as ConvNet.",2022-03-27 08:08:33+00:00,Causality Inspired Representation Learning for Domain Generalization,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Fangrui Lv'), arxiv.Result.Author('Jian Liang'), arxiv.Result.Author('Shuang Li'), arxiv.Result.Author('Bin Zang'), arxiv.Result.Author('Chi Harold Liu'), arxiv.Result.Author('Ziteng Wang'), arxiv.Result.Author('Di Liu')]","Domain generalization (DG) is essentially an out-of-distribution problem,
aiming to generalize the knowledge learned from multiple source domains to an
unseen target domain. The mainstream is to leverage statistical models to model
the dependence between data and labels, intending to learn representations
independent of domain. Nevertheless, the statistical models are superficial
descriptions of reality since they are only required to model dependence
instead of the intrinsic causal mechanism. When the dependence changes with the
target distribution, the statistic models may fail to generalize. In this
regard, we introduce a general structural causal model to formalize the DG
problem. Specifically, we assume that each input is constructed from a mix of
causal factors (whose relationship with the label is invariant across domains)
and non-causal factors (category-independent), and only the former cause the
classification judgments. Our goal is to extract the causal factors from inputs
and then reconstruct the invariant causal mechanisms. However, the theoretical
idea is far from practical of DG since the required causal/non-causal factors
are unobserved. We highlight that ideal causal factors should meet three basic
properties: separated from the non-causal ones, jointly independent, and
causally sufficient for the classification. Based on that, we propose a
Causality Inspired Representation Learning (CIRL) algorithm that enforces the
representations to satisfy the above properties and then uses them to simulate
the causal factors, which yields improved generalization ability. Extensive
experimental results on several widely used datasets verify the effectiveness
of our approach."
3931,"Lastly, in Section 5, we give our       query model, and is generally designed based on the type
conclusions and suggestions for further research.","This query is extracted by the
of various attention models.",of output that is desired of the model.,2022-03-27 10:06:23+00:00,A General Survey on Attention Mechanisms in Deep Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Gianni Brauwers'), arxiv.Result.Author('Flavius Frasincar')]","Attention is an important mechanism that can be employed for a variety of
deep learning models across many different domains and tasks. This survey
provides an overview of the most important attention mechanisms proposed in the
literature. The various attention mechanisms are explained by means of a
framework consisting of a general attention model, uniform notation, and a
comprehensive taxonomy of attention mechanisms. Furthermore, the various
measures for evaluating attention models are reviewed, and methods to
characterize the structure of attention models based on the proposed framework
are discussed. Last, future work in the field of attention models is
considered."
3944,"6.4 Ablation Analysis

To further study the inﬂuence of each individual component of our proposed
method, we conduct ablation experiments to investigate the inﬂuence of individual
model components with diﬀerent data inputs.","In
general, all models performed better when supplied with available MPTS data
and clinical notes covering more hours.","The results of ablation analysis on
both datasets are presented in Table 5a and 5b, respectively.",2022-03-28 03:19:03+00:00,Integrating Physiological Time Series and Clinical Notes with Transformer for Early Prediction of Sepsis,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yuqing Wang'), arxiv.Result.Author('Yun Zhao'), arxiv.Result.Author('Rachael Callcut'), arxiv.Result.Author('Linda Petzold')]","Sepsis is a leading cause of death in the Intensive Care Units (ICU). Early
detection of sepsis is critical for patient survival. In this paper, we propose
a multimodal Transformer model for early sepsis prediction, using the
physiological time series data and clinical notes for each patient within $36$
hours of ICU admission. Specifically, we aim to predict sepsis using only the
first 12, 18, 24, 30 and 36 hours of laboratory measurements, vital signs,
patient demographics, and clinical notes. We evaluate our model on two large
critical care datasets: MIMIC-III and eICU-CRD. The proposed method is compared
with six baselines. In addition, ablation analysis and case studies are
conducted to study the influence of each individual component of the model and
the contribution of each data modality for early sepsis prediction.
Experimental results demonstrate the effectiveness of our method, which
outperforms competitive baselines on all metrics."
3951,"-0.2                                  (b) NB-201    (c) NB-201   (d) NB-201
A further study into the relationship between Θθt and the                                                (CIFAR10)    (CIFAR100)   (ImageNet)
trainability of DNNs leads Xiao et al.","also hypothesize that the
maximum feasible learning rate is given by: η ∼ 2/λmax.","[57] to conclude that               (a) NB-101
Eq.",2022-03-28 08:43:04+00:00,Demystifying the Neural Tangent Kernel from a Practical Perspective: Can it be trusted for Neural Architecture Search without training?,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jisoo Mok'), arxiv.Result.Author('Byunggook Na'), arxiv.Result.Author('Ji-Hoon Kim'), arxiv.Result.Author('Dongyoon Han'), arxiv.Result.Author('Sungroh Yoon')]","In Neural Architecture Search (NAS), reducing the cost of architecture
evaluation remains one of the most crucial challenges. Among a plethora of
efforts to bypass training of each candidate architecture to convergence for
evaluation, the Neural Tangent Kernel (NTK) is emerging as a promising
theoretical framework that can be utilized to estimate the performance of a
neural architecture at initialization. In this work, we revisit several
at-initialization metrics that can be derived from the NTK and reveal their key
shortcomings. Then, through the empirical analysis of the time evolution of
NTK, we deduce that modern neural architectures exhibit highly non-linear
characteristics, making the NTK-based metrics incapable of reliably estimating
the performance of an architecture without some amount of training. To take
such non-linear characteristics into account, we introduce Label-Gradient
Alignment (LGA), a novel NTK-based metric whose inherent formulation allows it
to capture the large amount of non-linear advantage present in modern neural
architectures. With minimal amount of training, LGA obtains a meaningful level
of rank correlation with the post-training test accuracy of an architecture.
Lastly, we demonstrate that LGA, complemented with few epochs of training,
successfully guides existing search algorithms to achieve competitive search
performances with significantly less search cost. The code is available at:
https://github.com/nutellamok/DemystifyingNTK."
4032,"The limitations of the current work and the aspects
that need further research are: (1) Since the conventional thermodynamic equations do not respect
the Lorentz invariance, the physical constraint of the invariance for the modified thermodynamic
equations still require study.","Overall,
we offer a new way for scientific discovery (extraction of PDE) and the conception of Neural
Networks incorporating physical constraints.","(2) The main purpose of the methodology in this paper is for the partial

                                                                    14
differential equation.",2022-03-29 14:01:03+00:00,Discovering Governing Equations by Machine Learning implemented with Invariance,cs.LG,['cs.LG'],"[arxiv.Result.Author('Chao Chen'), arxiv.Result.Author('Xiaowei Jin'), arxiv.Result.Author('Hui Li')]","The partial differential equation (PDE) plays a significantly important role
in many fields of science and engineering. The conventional case of the
derivation of PDE mainly relies on first principles and empirical observation.
However, the development of machine learning technology allows us to mine
potential control equations from the massive amounts of stored data in a fresh
way. Although there has been considerable progress in the data-driven discovery
of PDE, the extant literature mostly focuses on the improvements of discovery
methods, without substantial breakthroughs in the discovery process itself,
including the principles for the construction of candidates and how to
incorporate physical priors. In this paper, through rigorous derivation of
formulas, novel physically enhanced machining learning discovery methods for
control equations: GSNN (Galileo Symbolic Neural Network) and LSNN (Lorentz
Symbolic Neural Network) are firstly proposed based on Galileo invariance and
Lorentz invariance respectively, setting forth guidelines for building the
candidates of discovering equations. The adoption of mandatory embedding of
physical constraints is fundamentally different from PINN in the form of the
loss function, thus ensuring that the designed Neural Network strictly obeys
the physical prior of invariance and enhancing the interpretability of the
network. By comparing the results with PDE-NET in numerical experiments of
Burgers equation and Sine-Gordon equation, it shows that the method presented
in this study has better accuracy, parsimony, and interpretability."
4062,"Therefore, further research on how to

                                                    Proc.","It may be because healthcare does not have natural graph
scenarios like other fields (e.g., transportation network, robotics swarms).",ACM Meas.,2022-03-29 22:27:59+00:00,Graph Neural Networks in IoT: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Guimin Dong'), arxiv.Result.Author('Mingyue Tang'), arxiv.Result.Author('Zhiyuan Wang'), arxiv.Result.Author('Jiechao Gao'), arxiv.Result.Author('Sikun Guo'), arxiv.Result.Author('Lihua Cai'), arxiv.Result.Author('Robert Gutierrez'), arxiv.Result.Author('Bradford Campbell'), arxiv.Result.Author('Laura E. Barnes'), arxiv.Result.Author('Mehdi Boukhechba')]","The Internet of Things (IoT) boom has revolutionized almost every corner of
people's daily lives: healthcare, home, transportation, manufacturing, supply
chain, and so on. With the recent development of sensor and communication
technologies, IoT devices including smart wearables, cameras, smartwatches, and
autonomous vehicles can accurately measure and perceive their surrounding
environment. Continuous sensing generates massive amounts of data and presents
challenges for machine learning. Deep learning models (e.g., convolution neural
networks and recurrent neural networks) have been extensively employed in
solving IoT tasks by learning patterns from multi-modal sensory data. Graph
Neural Networks (GNNs), an emerging and fast-growing family of neural network
models, can capture complex interactions within sensor topology and have been
demonstrated to achieve state-of-the-art results in numerous IoT learning
tasks. In this survey, we present a comprehensive review of recent advances in
the application of GNNs to the IoT field, including a deep dive analysis of GNN
design in various IoT sensing environments, an overarching list of public data
and source code from the collected publications, and future research
directions. To keep track of newly published works, we collect representative
papers and their open-source implementations and create a Github repository at
https://github.com/GuiminDong/GNN4IoT."
4063,"Therefore, further research on how to
construct better graph scenarios for healthcare related problems is in high demand to get more reasonable and
effective results.","It may be because healthcare does not have natural graph
scenarios like other fields (e.g., transportation network, robotics swarms).","5.2.3 Environment With the rapid development of environmental analytical tools and monitoring technologies,
more and more data are being generated daily in the field of environmental science and engineering (ESE).",2022-03-29 22:27:59+00:00,Graph Neural Networks in IoT: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Guimin Dong'), arxiv.Result.Author('Mingyue Tang'), arxiv.Result.Author('Zhiyuan Wang'), arxiv.Result.Author('Jiechao Gao'), arxiv.Result.Author('Sikun Guo'), arxiv.Result.Author('Lihua Cai'), arxiv.Result.Author('Robert Gutierrez'), arxiv.Result.Author('Bradford Campbell'), arxiv.Result.Author('Laura E. Barnes'), arxiv.Result.Author('Mehdi Boukhechba')]","The Internet of Things (IoT) boom has revolutionized almost every corner of
people's daily lives: healthcare, home, transportation, manufacturing, supply
chain, and so on. With the recent development of sensor and communication
technologies, IoT devices including smart wearables, cameras, smartwatches, and
autonomous vehicles can accurately measure and perceive their surrounding
environment. Continuous sensing generates massive amounts of data and presents
challenges for machine learning. Deep learning models (e.g., convolution neural
networks and recurrent neural networks) have been extensively employed in
solving IoT tasks by learning patterns from multi-modal sensory data. Graph
Neural Networks (GNNs), an emerging and fast-growing family of neural network
models, can capture complex interactions within sensor topology and have been
demonstrated to achieve state-of-the-art results in numerous IoT learning
tasks. In this survey, we present a comprehensive review of recent advances in
the application of GNNs to the IoT field, including a deep dive analysis of GNN
design in various IoT sensing environments, an overarching list of public data
and source code from the collected publications, and future research
directions. To keep track of newly published works, we collect representative
papers and their open-source implementations and create a Github repository at
https://github.com/GuiminDong/GNN4IoT."
4064,"To the best of our knowledge, we are                K) is also worth further research.","Learning to better measure the classiﬁer un-
work and the other four categories of baseline methods de-                  der scenarios with extremely scarce support nodes (very small
scribed in Section 5.1.","the ﬁrst to investigate the effectiveness of transductive ﬁne-
tuning on Graph Few-shot Learning (GFL) problems.",2022-03-29 22:30:00+00:00,A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Tan'), arxiv.Result.Author('Kaize Ding'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Huan Liu')]","Recently, increasing attention has been devoted to the graph few-shot
learning problem, where the target novel classes only contain a few labeled
nodes. Among many existing endeavors, episodic meta-learning has become the
most prevailing paradigm, and its episodic emulation of the test environment is
believed to equip the graph neural network models with adaptability to novel
node classes. However, in the image domain, recent results have shown that
feature reuse is more likely to be the key of meta-learning to few-shot
extrapolation. Based on such observation, in this work, we propose a simple
transductive fine-tuning based framework as a new paradigm for graph few-shot
learning. In the proposed paradigm, a graph encoder backbone is pretrained with
base classes, and a simple linear classifier is fine-tuned by the few labeled
samples and is tasked to classify the unlabeled ones. For pretraining, we
propose a supervised contrastive learning framework with data augmentation
strategies specific for few-shot node classification to improve the
extrapolation of a GNN encoder. Finally, extensive experiments conducted on
three benchmark datasets demonstrate the superior advantage of our framework
over the state-of-the-art methods."
4065,"To the best of our knowledge, we are                K) is also worth further research.","Learning to better measure the classiﬁer un-
work and the other four categories of baseline methods de-                  der scenarios with extremely scarce support nodes (very small
scribed in Section 5.1.","the ﬁrst to investigate the effectiveness of transductive ﬁne-
tuning on Graph Few-shot Learning (GFL) problems.",2022-03-29 22:30:00+00:00,A Simple Yet Effective Pretraining Strategy for Graph Few-shot Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Tan'), arxiv.Result.Author('Kaize Ding'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Huan Liu')]","Recently, increasing attention has been devoted to the graph few-shot
learning problem, where the target novel classes only contain a few labeled
nodes. Among many existing endeavors, episodic meta-learning has become the
most prevailing paradigm, and its episodic emulation of the test environment is
believed to equip the graph neural network models with adaptability to novel
node classes. However, in the image domain, recent results have shown that
feature reuse is more likely to be the key of meta-learning to few-shot
extrapolation. Based on such observation, in this work, we propose a simple
transductive fine-tuning based framework as a new paradigm for graph few-shot
learning. In the proposed paradigm, a graph encoder backbone is pretrained with
base classes, and a simple linear classifier is fine-tuned by the few labeled
samples and is tasked to classify the unlabeled ones. For pretraining, we
propose a supervised contrastive learning framework with data augmentation
strategies specific for few-shot node classification to improve the
extrapolation of a GNN encoder. Finally, extensive experiments conducted on
three benchmark datasets demonstrate the superior advantage of our framework
over the state-of-the-art methods."
4066,"Learning to better measure the classiﬁer under scenarios with
extremely scarce support nodes (very small K) is also worth further research.","To a large extent, the degradation lies in the less accurate classiﬁer due to fewer
training nodes.","4.3 Further Experiments

To further evaluate our framework, we conduct more experiments next.",2022-03-29 22:30:00+00:00,Supervised Graph Contrastive Learning for Few-shot Node Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Tan'), arxiv.Result.Author('Kaize Ding'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Huan Liu')]","Graphs are present in many real-world applications, such as financial fraud
detection, commercial recommendation, and social network analysis. But given
the high cost of graph annotation or labeling, we face a severe graph
label-scarcity problem, i.e., a graph might have a few labeled nodes. One
example of such a problem is the so-called \textit{few-shot node
classification}. A predominant approach to this problem resorts to
\textit{episodic meta-learning}. In this work, we challenge the status quo by
asking a fundamental question whether meta-learning is a must for few-shot node
classification tasks. We propose a new and simple framework under the standard
few-shot node classification setting as an alternative to meta-learning to
learn an effective graph encoder. The framework consists of supervised graph
contrastive learning with novel mechanisms for data augmentation, subgraph
encoding, and multi-scale contrast on graphs. Extensive experiments on three
benchmark datasets (CoraFull, Reddit, Ogbn) show that the new framework
significantly outperforms state-of-the-art meta-learning based methods."
4067,There are many promising directions worth further research.,We hope our work will shed new light on few-shot node classiﬁcation tasks.,"For example, when
base classes also have very limited labeled nodes or even no label at all, from
Table 4.2, we can see that self-supervised pretraining can also help improve the
performance.",2022-03-29 22:30:00+00:00,Supervised Graph Contrastive Learning for Few-shot Node Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Tan'), arxiv.Result.Author('Kaize Ding'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Huan Liu')]","Graphs are present in many real-world applications, such as financial fraud
detection, commercial recommendation, and social network analysis. But given
the high cost of graph annotation or labeling, we face a severe graph
label-scarcity problem, i.e., a graph might have a few labeled nodes. One
example of such a problem is the so-called \textit{few-shot node
classification}. A predominant approach to this problem resorts to
\textit{episodic meta-learning}. In this work, we challenge the status quo by
asking a fundamental question whether meta-learning is a must for few-shot node
classification tasks. We propose a new and simple framework under the standard
few-shot node classification setting as an alternative to meta-learning to
learn an effective graph encoder. The framework consists of supervised graph
contrastive learning with novel mechanisms for data augmentation, subgraph
encoding, and multi-scale contrast on graphs. Extensive experiments on three
benchmark datasets (CoraFull, Reddit, Ogbn) show that the new framework
significantly outperforms state-of-the-art meta-learning based methods."
4068,"Learning to better measure the classiﬁer under scenarios with
extremely scarce support nodes (very small K) is also worth further research.","To a large extent, the degradation lies in the less accurate classiﬁer due to fewer
training nodes.","4.3 Further Experiments

To further evaluate our framework, we conduct more experiments next.",2022-03-29 22:30:00+00:00,Supervised Graph Contrastive Learning for Few-shot Node Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Tan'), arxiv.Result.Author('Kaize Ding'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Huan Liu')]","Graphs are present in many real-world applications, such as financial fraud
detection, commercial recommendation, and social network analysis. But given
the high cost of graph annotation or labeling, we face a severe graph
label-scarcity problem, i.e., a graph might have a few labeled nodes. One
example of such a problem is the so-called \textit{few-shot node
classification}. A predominant approach to this problem resorts to
\textit{episodic meta-learning}. In this work, we challenge the status quo by
asking a fundamental question whether meta-learning is a must for few-shot node
classification tasks. We propose a new and simple framework under the standard
few-shot node classification setting as an alternative to meta-learning to
learn an effective graph encoder. The framework consists of supervised graph
contrastive learning with novel mechanisms for data augmentation, subgraph
encoding, and multi-scale contrast on graphs. Extensive experiments on three
benchmark datasets (CoraFull, Reddit, Ogbn) show that the new framework
significantly outperforms state-of-the-art meta-learning based methods."
4069,There are many promising directions worth further research.,We hope our work will shed new light on few-shot node classiﬁcation tasks.,"For example, when
Supervised Graph Contrastive Learning for Few-shot Node Classiﬁcation  15

base classes also have very limited labeled nodes or even no label at all, from
Table 4.2, we can see that self-supervised pretraining can also help improve the
performance.",2022-03-29 22:30:00+00:00,Supervised Graph Contrastive Learning for Few-shot Node Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Tan'), arxiv.Result.Author('Kaize Ding'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Huan Liu')]","Graphs are present in many real-world applications, such as financial fraud
detection, commercial recommendation, and social network analysis. But given
the high cost of graph annotation or labeling, we face a severe graph
label-scarcity problem, i.e., a graph might have a few labeled nodes. One
example of such a problem is the so-called \textit{few-shot node
classification}. A predominant approach to this problem resorts to
\textit{episodic meta-learning}. In this work, we challenge the status quo by
asking a fundamental question whether meta-learning is a must for few-shot node
classification tasks. We propose a new and simple framework under the standard
few-shot node classification setting as an alternative to meta-learning to
learn an effective graph encoder. The framework consists of supervised graph
contrastive learning with novel mechanisms for data augmentation, subgraph
encoding, and multi-scale contrast on graphs. Extensive experiments on three
benchmark datasets (CoraFull, Reddit, Ogbn) show that the new framework
significantly outperforms state-of-the-art meta-learning based methods."
4078,"control ﬂow encoding),
with further research in predictive monitoring [5, 47, 3] advising to expand the
attribute space with other event and case attributes (i.e.","In the work of van Dongen [46], the focus is on
the order and execution of the activity in the trace (i.e.","data payload encod-
ing).",2022-03-30 05:59:50+00:00,Explainable Artificial Intelligence in Process Mining: Assessing the Explainability-Performance Trade-Off in Outcome-Oriented Predictive Process Monitoring,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Alexander Stevens'), arxiv.Result.Author('Johannes De Smedt')]","Recently, a shift has been made in the field of Outcome-Oriented Predictive
Process Monitoring (OOPPM) to use models from the eXplainable Artificial
Intelligence paradigm, however the evaluation still occurs mainly through
performance-based metrics not accounting for the implications and lack of
actionability of the explanations. In this paper, we define explainability by
the interpretability of the explanations (through the widely-used XAI
properties parsimony and functional complexity) and the faithfulness of the
explainability model (through monotonicity and level of disagreement). The
introduced properties are analysed along the event, case, and control flow
perspective that are typical of a process-based analysis. This allows to
quantitatively compare, inter alia, inherently created explanations (e.g.,
logistic regression coefficients) with post-hoc explanations (e.g., Shapley
values). Moreover, this paper contributes a guideline named X-MOP to
practitioners to select the appropriate model based on the event log
specifications and the task at hand, by providing insight into how the varying
preprocessing, model complexity and post-hoc explainability techniques typical
in OOPPM influence the explainability of the model. To this end, we benchmark
seven classifiers on thirteen real-life events logs."
4079,"control ﬂow encoding), with further research in
complexity [10], [31].","Further-            focus is on the order and execution of the activity in the
more, it is often quantiﬁed by the related concept of model       trace (i.e.","predictive monitoring [2], [4], [39] advising to expand the
Explainability model.",2022-03-30 05:59:50+00:00,Explainable Predictive Process Monitoring: Evaluation Metrics and Guidelines for Process Outcome Prediction,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Alexander Stevens'), arxiv.Result.Author('Johannes De Smedt')]","Although a recent shift has been made in the field of predictive process
monitoring to use models from the explainable artificial intelligence field,
the evaluation still occurs mainly through performance-based metrics, thus not
accounting for the actionability and implications of the explanations. In this
paper, we define explainability through the interpretability of the
explanations and the faithfulness of the explainability model in the field of
process outcome prediction. The introduced properties are analysed along the
event, case, and control flow perspective which are typical for a process-based
analysis. This allows comparing inherently created explanations with post-hoc
explanations. We benchmark seven classifiers on thirteen real-life events logs,
and these cover a range of transparent and non-transparent machine learning and
deep learning models, further complemented with explainability techniques.
Next, this paper contributes a set of guidelines named X-MOP which allows
selecting the appropriate model based on the event log specifications, by
providing insight into how the varying preprocessing, model complexity and
explainability techniques typical in process outcome prediction influence the
explainability of the model."
4086,"Assumptions to
achieve this controllable bias have been relaxed by further research to dissipativity and smoothness
(Raginsky et al., 2017; Xu et al., 2018), and recently to Log-Sobolev inequality (LSI) and smooth-
ness (Vempala and Wibisono, 2019).","Dalalyan (2017a,b) provided one of the ﬁrst non-
asymptotic rates of convergence of LMC for smooth log-concave distributions.","This relaxation of conditions is especially meaningful as the
objective distribution nowadays tends to become more and more complicated beyond the classical
assumption of log-concavity.",2022-03-30 11:39:00+00:00,Improved Convergence Rate of Stochastic Gradient Langevin Dynamics with Variance Reduction and its Application to Optimization,cs.LG,"['cs.LG', 'math.PR', 'stat.ML']","[arxiv.Result.Author('Yuri Kinoshita'), arxiv.Result.Author('Taiji Suzuki')]","The stochastic gradient Langevin Dynamics is one of the most fundamental
algorithms to solve sampling problems and non-convex optimization appearing in
several machine learning applications. Especially, its variance reduced
versions have nowadays gained particular attention. In this paper, we study two
variants of this kind, namely, the Stochastic Variance Reduced Gradient
Langevin Dynamics and the Stochastic Recursive Gradient Langevin Dynamics. We
prove their convergence to the objective distribution in terms of KL-divergence
under the sole assumptions of smoothness and Log-Sobolev inequality which are
weaker conditions than those used in prior works for these algorithms. With the
batch size and the inner loop length set to $\sqrt{n}$, the gradient complexity
to achieve an $\epsilon$-precision is
$\tilde{O}((n+dn^{1/2}\epsilon^{-1})\gamma^2 L^2\alpha^{-2})$, which is an
improvement from any previous analyses. We also show some essential
applications of our result to non-convex optimization."
4087,"Assumptions to
achieve this controllable bias have been relaxed by further research to dissipativity and smoothness
(Raginsky et al., 2017; Xu et al., 2018), and recently to Log-Sobolev inequality (LSI) and smooth-
ness (Vempala and Wibisono, 2019).","Dalalyan (2017a,b) provided one of the ﬁrst non-
asymptotic rates of convergence of LMC for smooth log-concave distributions.","This relaxation of conditions is especially meaningful as the
objective distribution nowadays tends to become more and more complicated beyond the classical
assumption of log-concavity.",2022-03-30 11:39:00+00:00,Improved Convergence Rate of Stochastic Gradient Langevin Dynamics with Variance Reduction and its Application to Optimization,cs.LG,"['cs.LG', 'math.PR', 'stat.ML']","[arxiv.Result.Author('Yuri Kinoshita'), arxiv.Result.Author('Taiji Suzuki')]","The stochastic gradient Langevin Dynamics is one of the most fundamental
algorithms to solve sampling problems and non-convex optimization appearing in
several machine learning applications. Especially, its variance reduced
versions have nowadays gained particular attention. In this paper, we study two
variants of this kind, namely, the Stochastic Variance Reduced Gradient
Langevin Dynamics and the Stochastic Recursive Gradient Langevin Dynamics. We
prove their convergence to the objective distribution in terms of KL-divergence
under the sole assumptions of smoothness and Log-Sobolev inequality which are
weaker conditions than those used in prior works for these algorithms. With the
batch size and the inner loop length set to $\sqrt{n}$, the gradient complexity
to achieve an $\epsilon$-precision is
$\tilde{O}((n+dn^{1/2}\epsilon^{-1})\gamma^2 L^2\alpha^{-2})$, which is an
improvement from any previous analyses. We also show some essential
applications of our result to non-convex optimization."
4088,"Assumptions to
obtain a non-asymptotic analysis and this controllable bias have been relaxed by further research to
dissipativity and smoothness (Raginsky et al., 2017; Xu et al., 2018), and recently to Log-Sobolev
inequality (LSI) and smoothness (Vempala and Wibisono, 2019).","Dalalyan (2017a,b) provided one of the ﬁrst
non-asymptotic rates of convergence of LMC for smooth log-concave distributions.","This relaxation of conditions
is especially meaningful as the objective distribution nowadays tends to become more and more
complicated beyond the classical assumption of log-concavity.",2022-03-30 11:39:00+00:00,Improved Convergence Rate of Stochastic Gradient Langevin Dynamics with Variance Reduction and its Application to Optimization,cs.LG,"['cs.LG', 'math.PR', 'stat.ML']","[arxiv.Result.Author('Yuri Kinoshita'), arxiv.Result.Author('Taiji Suzuki')]","The stochastic gradient Langevin Dynamics is one of the most fundamental
algorithms to solve sampling problems and non-convex optimization appearing in
several machine learning applications. Especially, its variance reduced
versions have nowadays gained particular attention. In this paper, we study two
variants of this kind, namely, the Stochastic Variance Reduced Gradient
Langevin Dynamics and the Stochastic Recursive Gradient Langevin Dynamics. We
prove their convergence to the objective distribution in terms of KL-divergence
under the sole assumptions of smoothness and Log-Sobolev inequality which are
weaker conditions than those used in prior works for these algorithms. With the
batch size and the inner loop length set to $\sqrt{n}$, the gradient complexity
to achieve an $\epsilon$-precision is
$\tilde{O}((n+dn^{1/2}\epsilon^{-1})\gamma^2 L^2\alpha^{-2})$, which is an
improvement from any previous analyses. We also show some essential
applications of our result to non-convex optimization."
4098,This trade-oﬀ deserves further study.,"For making
predictions, however, such convoluted and likely incorrect models perform better due to
their increased generalization.","We expect there exist
better generalization methods for software systems that do lead to both improved insight
and improved performance.",2022-03-28 21:13:24+00:00,FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata,cs.LG,"['cs.LG', 'cs.LO', 'cs.SE']","[arxiv.Result.Author('Sicco Verwer'), arxiv.Result.Author('Christian Hammerschmidt')]","We present the efficient implementations of probabilistic deterministic
finite automaton learning methods available in FlexFringe. These implement
well-known strategies for state-merging including several modifications to
improve their performance in practice. We show experimentally that these
algorithms obtain competitive results and significant improvements over a
default implementation. We also demonstrate how to use FlexFringe to learn
interpretable models from software logs and use these for anomaly detection.
Although less interpretable, we show that learning smaller more convoluted
models improves the performance of FlexFringe on anomaly detection,
outperforming an existing solution based on neural nets."
4119,"We pro-
                                                                    vide further study of this parameter in Appendix-B.","An
The idea here is to select a learning rate η (t) such that          ablative study of this parameter is shown in Figure 3.","We dub
the stable rank is increased over each epoch i.e.",2022-03-31 00:16:44+00:00,Exploiting Explainable Metrics for Augmented SGD,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Mahdi S. Hosseini'), arxiv.Result.Author('Mathieu Tuli'), arxiv.Result.Author('Konstantinos N. Plataniotis')]","Explaining the generalization characteristics of deep learning is an emerging
topic in advanced machine learning. There are several unanswered questions
about how learning under stochastic optimization really works and why certain
strategies are better than others. In this paper, we address the following
question: \textit{can we probe intermediate layers of a deep neural network to
identify and quantify the learning quality of each layer?} With this question
in mind, we propose new explainability metrics that measure the redundant
information in a network's layers using a low-rank factorization framework and
quantify a complexity measure that is highly correlated with the generalization
performance of a given optimizer, network, and dataset. We subsequently exploit
these metrics to augment the Stochastic Gradient Descent (SGD) optimizer by
adaptively adjusting the learning rate in each layer to improve in
generalization performance. Our augmented SGD -- dubbed RMSGD -- introduces
minimal computational overhead compared to SOTA methods and outperforms them by
exhibiting strong generalization characteristics across application,
architecture, and dataset."
4145,"Theorem G.2 tells that the random walk on a hypergraph can not be always equivalent to a random walk on
an undigraph, and open up the possibility to construct a hypergraph is not equivalent to an undigraph, which may inspire
researcher to further study for higher-order interactions and other insurmountable bottlenecks on hypergraphs.",Remark.,Proof.,2022-03-31 10:46:47+00:00,Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jiying Zhang'), arxiv.Result.Author('Fuyang Li'), arxiv.Result.Author('Xi Xiao'), arxiv.Result.Author('Tingyang Xu'), arxiv.Result.Author('Yu Rong'), arxiv.Result.Author('Junzhou Huang'), arxiv.Result.Author('Yatao Bian')]","As a powerful tool for modeling complex relationships, hypergraphs are
gaining popularity from the graph learning community. However, commonly used
frameworks in deep hypergraph learning focus on hypergraphs with
\textit{edge-independent vertex weights}(EIVWs), without considering
hypergraphs with \textit{edge-dependent vertex weights} (EDVWs) that have more
modeling power. To compensate for this, in this paper, we present General
Hypergraph Spectral Convolution(GHSC), a general learning framework that not
only can handle EDVW and EIVW hypergraphs, but more importantly, enables
theoretically explicitly utilizing the existing powerful Graph Convolutional
Neural Networks (GCNNs) such that largely ease the design of Hypergraph Neural
Networks. In this framework, the graph Laplacian of the given undirected GCNNs
is replaced with a unified hypergraph Laplacian that incorporates vertex weight
information from a random walk perspective by equating our defined generalized
hypergraphs with simple undirected graphs. Extensive experiments from various
domains including social network analysis, visual objective classification,
protein learning demonstrate that the proposed framework can achieve
state-of-the-art performance."
4146,"Theorem G.1 tells that the random walk on a hypergraph can not be always
equivalent to a random walk on an undigraph, and open up the possibility to construct a
hypergraph is not equivalent to an undigraph, which may inspire researcher to further study
for higher-order interactions and other insurmountable bottlenecks on hypergraphs.",Remark.,"38
                   Hypergraph                                                Directed Clique Graph

    𝟏=                         1                                                                                  444
                                                                                                         = 4 10 7 3
                               20 1
                                                                                                            12 4 10 7 3
                               2                                                                                        6 33
                                 1

    𝟐=

Figure 6: An example of hypergraph and its equivalent weighted directed graph, where
            qi(·) = Qi(·, e), i ∈ {1, 2}.",2022-03-31 10:46:47+00:00,Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jiying Zhang'), arxiv.Result.Author('Fuyang Li'), arxiv.Result.Author('Xi Xiao'), arxiv.Result.Author('Tingyang Xu'), arxiv.Result.Author('Yu Rong'), arxiv.Result.Author('Junzhou Huang'), arxiv.Result.Author('Yatao Bian')]","As a powerful tool for modeling complex relationships, hypergraphs are
gaining popularity from the graph learning community. However, commonly used
frameworks in deep hypergraph learning focus on hypergraphs with
\textit{edge-independent vertex weights}(EIVWs), without considering
hypergraphs with \textit{edge-dependent vertex weights} (EDVWs) that have more
modeling power. To compensate for this, in this paper, we present General
Hypergraph Spectral Convolution(GHSC), a general learning framework that not
only can handle EDVW and EIVW hypergraphs, but more importantly, enables
theoretically explicitly utilizing the existing powerful Graph Convolutional
Neural Networks (GCNNs) such that largely ease the design of Hypergraph Neural
Networks. In this framework, the graph Laplacian of the given undirected GCNNs
is replaced with a unified hypergraph Laplacian that incorporates vertex weight
information from a random walk perspective by equating our defined generalized
hypergraphs with simple undirected graphs. Extensive experiments from various
domains including social network analysis, visual objective classification,
protein learning demonstrate that the proposed framework can achieve
state-of-the-art performance."
4147,"Theorem E.4 tells that the random walk on a hypergraph can not be always equivalent to a random walk on
an undigraph, and open up the possibility to construct a hypergraph is not equivalent to an undigraph, which may inspire
researcher to further study for higher-order interactions and other insurmountable bottlenecks on hypergraphs.",Remark.,Proof.,2022-03-31 10:46:47+00:00,Hypergraph Convolutional Networks via Equivalency between Hypergraphs and Undirected Graphs,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jiying Zhang'), arxiv.Result.Author('Fuyang Li'), arxiv.Result.Author('Xi Xiao'), arxiv.Result.Author('Tingyang Xu'), arxiv.Result.Author('Yu Rong'), arxiv.Result.Author('Junzhou Huang'), arxiv.Result.Author('Yatao Bian')]","As a powerful tool for modeling complex relationships, hypergraphs are
gaining popularity from the graph learning community. However, commonly used
frameworks in deep hypergraph learning focus on hypergraphs with
edge-independent vertex weights (EIVWs), without considering hypergraphs with
edge-dependent vertex weights (EDVWs) that have more modeling power. To
compensate for this, we present General Hypergraph Spectral Convolution (GHSC),
a general learning framework that not only handles EDVW and EIVW hypergraphs,
but more importantly, enables theoretically explicitly utilizing the existing
powerful Graph Convolutional Neural Networks (GCNNs) such that largely ease the
design of Hypergraph Neural Networks. In this framework, the graph Laplacian of
the given undirected GCNNs is replaced with a unified hypergraph Laplacian that
incorporates vertex weight information from a random walk perspective by
equating our defined generalized hypergraphs with simple undirected graphs.
Extensive experiments from various domains including social network analysis,
visual objective classification, and protein learning demonstrate the
state-of-the-art performance of the proposed framework."
4160,This is subject to further research.,"If the differential equation in (1) is linear and time-invariant,         to evaluate the norm on the underlying inﬁnite-dimensional
i.e., the right-hand side of (1) is given by                                 space.","f (t, x) = Ax                                (8)                  To obtain a computable error bound, we have to estimate

for some matrix A ∈ Rn×n, then we can further improve the                    the integral I(t, δ) in (6), which is done here exemplarily by
error estimation as follows.",2022-03-31 14:23:04+00:00,Certified machine learning: A posteriori error estimation for physics-informed neural networks,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Birgit Hillebrecht'), arxiv.Result.Author('Benjamin Unger')]","Physics-informed neural networks (PINNs) are one popular approach to
introduce a priori knowledge about physical systems into the learning
framework. PINNs are known to be robust for smaller training sets, derive
better generalization problems, and are faster to train. In this paper, we show
that using PINNs in comparison with purely data-driven neural networks is not
only favorable for training performance but allows us to extract significant
information on the quality of the approximated solution. Assuming that the
underlying differential equation for the PINN training is an ordinary
differential equation, we derive a rigorous upper limit on the PINN prediction
error. This bound is applicable even for input data not included in the
training phase and without any prior knowledge about the true solution.
Therefore, our a posteriori error estimation is an essential step to certify
the PINN. We apply our error estimator exemplarily to two academic toy
problems, whereof one falls in the category of model-predictive control and
thereby shows the practical use of the derived results."
4161,"Nevertheless,
                                                                   a thorough investigation is subject to further research.","The expected machine learning error at time       be closer to the actual prediction error since we do not
t can be estimated a priori as                                     introduce any additional approximation steps.","EMexLp(t, x0) = eLt x0 − xˆ(0) + (eLt − 1) RLϕˆ .",2022-03-31 14:23:04+00:00,Certified machine learning: A posteriori error estimation for physics-informed neural networks,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Birgit Hillebrecht'), arxiv.Result.Author('Benjamin Unger')]","Physics-informed neural networks (PINNs) are one popular approach to
introduce a priori knowledge about physical systems into the learning
framework. PINNs are known to be robust for smaller training sets, derive
better generalization problems, and are faster to train. In this paper, we show
that using PINNs in comparison with purely data-driven neural networks is not
only favorable for training performance but allows us to extract significant
information on the quality of the approximated solution. Assuming that the
underlying differential equation for the PINN training is an ordinary
differential equation, we derive a rigorous upper limit on the PINN prediction
error. This bound is applicable even for input data not included in the
training phase and without any prior knowledge about the true solution.
Therefore, our a posteriori error estimation is an essential step to certify
the PINN. We apply our error estimator exemplarily to two academic toy
problems, whereof one falls in the category of model-predictive control and
thereby shows the practical use of the derived results."
4162,This is subject to further research.,"to evaluate the norm on the underlying inﬁnite-dimensional
                                                                             space.","If the differential equation in (1) is linear and time-invariant,
i.e., the right-hand side of (1) is given by                                    To obtain a computable error bound, we have to estimate

              f (t, x) = Ax                                (8)               the integral I(t, δ) in (6), which is done here exemplarily
                                                                             by trapezoidal rule.",2022-03-31 14:23:04+00:00,Certified machine learning: A posteriori error estimation for physics-informed neural networks,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Birgit Hillebrecht'), arxiv.Result.Author('Benjamin Unger')]","Physics-informed neural networks (PINNs) are one popular approach to
incorporate a priori knowledge about physical systems into the learning
framework. PINNs are known to be robust for smaller training sets, derive
better generalization problems, and are faster to train. In this paper, we show
that using PINNs in comparison with purely data-driven neural networks is not
only favorable for training performance but allows us to extract significant
information on the quality of the approximated solution. Assuming that the
underlying differential equation for the PINN training is an ordinary
differential equation, we derive a rigorous upper limit on the PINN prediction
error. This bound is applicable even for input data not included in the
training phase and without any prior knowledge about the true solution.
Therefore, our a posteriori error estimation is an essential step to certify
the PINN. We apply our error estimator exemplarily to two academic toy
problems, whereof one falls in the category of model-predictive control and
thereby shows the practical use of the derived results."
4163,The Lipschitz constant L is then immediately estimated    a thorough investigation is subject to further research.,"Nevertheless,
points.",as the maximum of the determined largest singular values.,2022-03-31 14:23:04+00:00,Certified machine learning: A posteriori error estimation for physics-informed neural networks,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Birgit Hillebrecht'), arxiv.Result.Author('Benjamin Unger')]","Physics-informed neural networks (PINNs) are one popular approach to
incorporate a priori knowledge about physical systems into the learning
framework. PINNs are known to be robust for smaller training sets, derive
better generalization problems, and are faster to train. In this paper, we show
that using PINNs in comparison with purely data-driven neural networks is not
only favorable for training performance but allows us to extract significant
information on the quality of the approximated solution. Assuming that the
underlying differential equation for the PINN training is an ordinary
differential equation, we derive a rigorous upper limit on the PINN prediction
error. This bound is applicable even for input data not included in the
training phase and without any prior knowledge about the true solution.
Therefore, our a posteriori error estimation is an essential step to certify
the PINN. We apply our error estimator exemplarily to two academic toy
problems, whereof one falls in the category of model-predictive control and
thereby shows the practical use of the derived results."
4164,This is subject to further research.,"to evaluate the norm on the underlying inﬁnite-dimensional
                                                                             space.",Remark III.2.,2022-03-31 14:23:04+00:00,Certified machine learning: A posteriori error estimation for physics-informed neural networks,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Birgit Hillebrecht'), arxiv.Result.Author('Benjamin Unger')]","Physics-informed neural networks (PINNs) are one popular approach to
incorporate a priori knowledge about physical systems into the learning
framework. PINNs are known to be robust for smaller training sets, derive
better generalization problems, and are faster to train. In this paper, we show
that using PINNs in comparison with purely data-driven neural networks is not
only favorable for training performance but allows us to extract significant
information on the quality of the approximated solution. Assuming that the
underlying differential equation for the PINN training is an ordinary
differential equation, we derive a rigorous upper limit on the PINN prediction
error. This bound is applicable even for input data not included in the
training phase and without any prior knowledge about the true solution.
Therefore, our a posteriori error estimation is an essential step to certify
the PINN. We apply our error estimator exemplarily to two academic toy
problems, whereof one falls in the category of model-predictive control and
thereby shows the practical use of the derived results."
4165,"Nevertheless,
                                                                  a thorough investigation is subject to further research.","The expected machine learning error at time      be closer to the actual prediction error since we do not
t can be estimated a priori as                                    introduce any additional approximation steps.","EMexLp(t, x0) = eLt x0 − xˆ(0) + (eLt − 1) RLϕˆ .",2022-03-31 14:23:04+00:00,Certified machine learning: A posteriori error estimation for physics-informed neural networks,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Birgit Hillebrecht'), arxiv.Result.Author('Benjamin Unger')]","Physics-informed neural networks (PINNs) are one popular approach to
incorporate a priori knowledge about physical systems into the learning
framework. PINNs are known to be robust for smaller training sets, derive
better generalization problems, and are faster to train. In this paper, we show
that using PINNs in comparison with purely data-driven neural networks is not
only favorable for training performance but allows us to extract significant
information on the quality of the approximated solution. Assuming that the
underlying differential equation for the PINN training is an ordinary
differential equation, we derive a rigorous upper limit on the PINN prediction
error. This bound is applicable even for input data not included in the
training phase and without any prior knowledge about the true solution.
Therefore, our a posteriori error estimation is an essential step to certify
the PINN. We apply our error estimator exemplarily to two academic toy
problems, whereof one falls in the category of model-predictive control and
thereby shows the practical use of the derived results."
4179,"Signiﬁ-
cant further research is required to quantify whether the two-to-one mapping from years to standard
deviations will be problematic.","A more plausible direction is a new
time and date scheme based on standard deviations from the Gaussian technology curve.",Acknowledgements.,2022-03-31 17:58:10+00:00,A 23 MW data centre is all you need,cs.LG,['cs.LG'],"[arxiv.Result.Author('Samuel Albanie'), arxiv.Result.Author('Dylan Campbell'), arxiv.Result.Author('João F. Henriques')]","The field of machine learning has achieved striking progress in recent years,
witnessing breakthrough results on language modelling, protein folding and
nitpickingly fine-grained dog breed classification. Some even succeeded at
playing computer games and board games, a feat both of engineering and of
setting their employers' expectations. The central contribution of this work is
to carefully examine whether this progress, and technology more broadly, can be
expected to continue indefinitely. Through a rigorous application of
statistical theory and failure to extrapolate beyond the training data, we
answer firmly in the negative and provide details: technology will peak at 3:07
am (BST) on 20th July, 2032. We then explore the implications of this finding,
discovering that individuals awake at this ungodly hour with access to a
sufficiently powerful computer possess an opportunity for myriad forms of
long-term linguistic 'lock in'. All we need is a large (>> 1W) data centre to
seize this pivotal moment. By setting our analogue alarm clocks, we propose a
tractable algorithm to ensure that, for the future of humanity, the British
spelling of colour becomes the default spelling across more than 80% of the
global word processing software market."
4228,"21
       The further research, extrapolated from the above conclusions, is to deeply optimize the base

learners of stacking and their combination algorithms to deliver faster and more accurate modeling.","The second layer compiles and aggregates these features
through linear regression with a regular term and effectively outputs simulations.","The other direction is to incorporate this article's in-the-now power modeling approach with

meteorological data with historical wind power to achieve efficacious short-term power forecasting.",2022-04-01 18:20:04+00:00,Cluster-based ensemble learning for wind power modeling with meteorological wind data,cs.LG,['cs.LG'],[arxiv.Result.Author('Hao Chen')],"Optimal implementation and monitoring of wind energy generation hinge on
reliable power modeling that is vital for understanding turbine control, farm
operational optimization, and grid load balance. Based on the idea of similar
wind condition leads to similar wind power; this paper constructs a modeling
scheme that orderly integrates three types of ensemble learning algorithms,
bagging, boosting, and stacking, and clustering approaches to achieve optimal
power modeling. It also investigates applications of different clustering
algorithms and methodology for determining cluster numbers in wind power
modeling. The results reveal that all ensemble models with clustering exploit
the intrinsic information of wind data and thus outperform models without it by
approximately 15% on average. The model with the best farthest first clustering
is computationally rapid and performs exceptionally well with an improvement of
around 30%. The modeling is further boosted by about 5% by introducing stacking
that fuses ensembles with varying clusters. The proposed modeling framework
thus demonstrates promise by delivering efficient and robust modeling
performance."
4230,"At a bare minimum, any forecast strategy using
data assimilation techniques should be able to produce better results than this single forecast if it is to be
worthy of consideration and further study.","A single forecast, without using data
assimilation is included in this comparison as a control.",1.,2022-04-01 20:11:28+00:00,Assimilation of Satellite Active Fires Data,cs.LG,"['cs.LG', 'physics.ao-ph', 'stat.AP']",[arxiv.Result.Author('James D. Haley')],"Wildland fires pose an increasingly serious problem in our society. The
number and severity of these fires has been rising for many years. Wildfires
pose direct threats to life and property as well as threats through ancillary
effects like reduced air quality. The aim of this thesis is to develop
techniques to help combat the impacts of wildfires by improving wildfire
modeling capabilities by using satellite fire observations. Already much work
has been done in this direction by other researchers. Our work seeks to expand
the body of knowledge using mathematically sound methods to utilize information
about wildfires that considers the uncertainties inherent in the satellite
data.
  In this thesis we explore methods for using satellite data to help initialize
and steer wildfire simulations. In particular, we develop a method for
constructing the history of a fire, a new technique for assimilating wildfire
data, and a method for modifying the behavior of a modeled fire by inferring
information about the fuels in the fire domain. These goals rely on being able
to estimate the time a fire first arrived at every location in a geographic
region of interest. Because detailed knowledge of real wildfires is typically
unavailable, the basic procedure for developing and testing the methods in this
thesis will be to first work with simulated data so that the estimates produced
can be compared with known solutions. The methods thus developed are then
applied to real-world scenarios. Analysis of these scenarios shows that the
work with constructing the history of fires and data assimilation improves
improves fire modeling capabilities. The research is significant because it
gives us a better understanding of the capabilities and limitations of using
satellite data to inform wildfire models and it points the way towards new
avenues for modeling fire behavior."
4231,"We also further study a fully
Bayesian approach in Alg.","2, although this uses optimistic safety indices.",3.,2022-04-01 22:08:03+00:00,Strategies for Safe Multi-Armed Bandits with Logarithmic Regret and Risk,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Tianrui Chen'), arxiv.Result.Author('Aditya Gangrade'), arxiv.Result.Author('Venkatesh Saligrama')]","We investigate a natural but surprisingly unstudied approach to the
multi-armed bandit problem under safety risk constraints. Each arm is
associated with an unknown law on safety risks and rewards, and the learner's
goal is to maximise reward whilst not playing unsafe arms, as determined by a
given threshold on the mean risk.
  We formulate a pseudo-regret for this setting that enforces this safety
constraint in a per-round way by softly penalising any violation, regardless of
the gain in reward due to the same. This has practical relevance to scenarios
such as clinical trials, where one must maintain safety for each round rather
than in an aggregated sense.
  We describe doubly optimistic strategies for this scenario, which maintain
optimistic indices for both safety risk and reward. We show that schema based
on both frequentist and Bayesian indices satisfy tight gap-dependent
logarithmic regret bounds, and further that these play unsafe arms only
logarithmically many times in total. This theoretical analysis is complemented
by simulation studies demonstrating the effectiveness of the proposed schema,
and probing the domains in which their use is appropriate."
4250,[71] introduced benchmarking suites to propel further research in TinyML.,"[9] and Sudarshan
et al.",Banbury et al.,2022-04-02 09:53:36+00:00,Intelligence at the Extreme Edge: A Survey on Reformable TinyML,cs.LG,"['cs.LG', 'eess.SP']","[arxiv.Result.Author('Visal Rajapakse'), arxiv.Result.Author('Ishan Karunanayake'), arxiv.Result.Author('Nadeem Ahmed')]","The rapid miniaturization of Machine Learning (ML) for low powered processing
has opened gateways to provide cognition at the extreme edge (E.g., sensors and
actuators). Dubbed Tiny Machine Learning (TinyML), this upsurging research
field proposes to democratize the use of Machine Learning (ML) and Deep
Learning (DL) on frugal Microcontroller Units (MCUs). MCUs are highly
energy-efficient pervasive devices capable of operating with less than a few
Milliwatts of power. Nevertheless, many solutions assume that TinyML can only
run inference. Despite this, growing interest in TinyML has led to work that
makes them reformable, i.e., work that permits TinyML to improve once deployed.
In line with this, roadblocks in MCU based solutions in general, such as
reduced physical access and long deployment periods of MCUs, deem reformable
TinyML to play a significant part in more effective solutions. In this work, we
present a survey on reformable TinyML solutions with the proposal of a novel
taxonomy for ease of separation. Here, we also discuss the suitability of each
hierarchical layer in the taxonomy for allowing reformability. In addition to
these, we explore the workflow of TinyML and analyze the identified deployment
schemes and the scarcely available benchmarking tools. Furthermore, we discuss
how reformable TinyML can impact a few selected industrial areas and discuss
the challenges and future directions."
4265,"Conclusion and recommendation of direction for further research

   Mobile health information service is a new tread in self-health management, especially during the
COVID-19 pandemic.","Fig.10: Total rewards for different values of 𝑚 (𝑙 = 2)
                           Fig.11: Total rewards for different values of 𝑙 (𝑚 = 60)

7.","In the mHealth information service, the recommender system record and
visualize the individual health conditions through wearable trackers for the individual, then deliver a
set of activity plans for daily exercise with motivational messages.",2022-04-03 01:19:20+00:00,Dynamic physical activity recommendation on personalised mobile health information service: A deep reinforcement learning approach,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Ji Fang'), arxiv.Result.Author('Vincent CS Lee'), arxiv.Result.Author('Haiyan Wang')]","Mobile health (mHealth) information service makes healthcare management
easier for users, who want to increase physical activity and improve health.
However, the differences in activity preference among the individual, adherence
problems, and uncertainty of future health outcomes may reduce the effect of
the mHealth information service. The current health service system usually
provides recommendations based on fixed exercise plans that do not satisfy the
user specific needs. This paper seeks an efficient way to make physical
activity recommendation decisions on physical activity promotion in
personalised mHealth information service by establishing data-driven model. In
this study, we propose a real-time interaction model to select the optimal
exercise plan for the individual considering the time-varying characteristics
in maximising the long-term health utility of the user. We construct a
framework for mHealth information service system comprising a personalised AI
module, which is based on the scientific knowledge about physical activity to
evaluate the individual exercise performance, which may increase the awareness
of the mHealth artificial intelligence system. The proposed deep reinforcement
learning (DRL) methodology combining two classes of approaches to improve the
learning capability for the mHealth information service system. A deep learning
method is introduced to construct the hybrid neural network combing long-short
term memory (LSTM) network and deep neural network (DNN) techniques to infer
the individual exercise behavior from the time series data. A reinforcement
learning method is applied based on the asynchronous advantage actor-critic
algorithm to find the optimal policy through exploration and exploitation."
4310,"To further study the influence of common cause
failure factors on system reliability, the following analysis is made.","The reason for the above differences is that there are

                                                                       22
common cause failure factors between components.",Fig.,2022-04-04 08:57:55+00:00,Algorithms for Bayesian network modeling and reliability inference of complex multistate systems: Part II-Dependent systems,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Xiaohu Zheng'), arxiv.Result.Author('Wen Yao'), arxiv.Result.Author('Xiaoqian Chen')]","In using the Bayesian network (BN) to construct the complex multistate
system's reliability model as described in Part I, the memory storage
requirements of the node probability table (NPT) will exceed the random access
memory (RAM) of the computer. However, the proposed inference algorithm of Part
I is not suitable for the dependent system. This Part II proposes a novel
method for BN reliability modeling and analysis to apply the compression idea
to the complex multistate dependent system. In this Part II, the dependent
nodes and their parent nodes are equivalent to a block, based on which the
multistate joint probability inference algorithm is proposed to calculate the
joint probability distribution of a block's all nodes. Then, based on the
proposed multistate compression algorithm of Part I, the dependent multistate
inference algorithm is proposed for the complex multistate dependent system.
The use and accuracy of the proposed algorithms are demonstrated in case 1.
Finally, the proposed algorithms are applied to the reliability modeling and
analysis of the satellite attitude control system. The results show that both
Part I and Part II's proposed algorithms make the reliability modeling and
analysis of the complex multistate system feasible."
4358,"Creating
a tradeoff between model scalability and graph completeness has a potential for further studying.","However, some of the
important neighbors of a node may be lost by sampling and the completeness of the graph may get corrupted.",Depth of model.,2022-04-04 21:18:48+00:00,A Survey on Graph Representation Learning Methods,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Shima Khoshraftar'), arxiv.Result.Author('Aijun An')]","Graphs representation learning has been a very active research area in recent
years. The goal of graph representation learning is to generate graph
representation vectors that capture the structure and features of large graphs
accurately. This is especially important because the quality of the graph
representation vectors will affect the performance of these vectors in
downstream tasks such as node classification, link prediction and anomaly
detection. Many techniques are proposed for generating effective graph
representation vectors. Two of the most prevalent categories of graph
representation learning are graph embedding methods without using graph neural
nets (GNN), which we denote as non-GNN based graph embedding methods, and graph
neural nets (GNN) based methods. Non-GNN graph embedding methods are based on
techniques such as random walks, temporal point processes and neural network
learning methods. GNN-based methods, on the other hand, are the application of
deep learning on graph data. In this survey, we provide an overview of these
two categories and cover the current state-of-the-art methods for both static
and dynamic graphs. Finally, we explore some open and ongoing research
directions for future work."
4359,"However,
some of them, such as over-squashing, were newly discovered and solutions to them are still preliminary, which have
potential for further research.",Different solutions have been proposed for these issues.,"In addition, the proposed solutions for the limitations of GNNs were all for static GNNs
and not much work has been done on solving the problems of dynamic and spatial-temporal GNNs.",2022-04-04 21:18:48+00:00,A Survey on Graph Representation Learning Methods,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Shima Khoshraftar'), arxiv.Result.Author('Aijun An')]","Graphs representation learning has been a very active research area in recent
years. The goal of graph representation learning is to generate graph
representation vectors that capture the structure and features of large graphs
accurately. This is especially important because the quality of the graph
representation vectors will affect the performance of these vectors in
downstream tasks such as node classification, link prediction and anomaly
detection. Many techniques are proposed for generating effective graph
representation vectors. Two of the most prevalent categories of graph
representation learning are graph embedding methods without using graph neural
nets (GNN), which we denote as non-GNN based graph embedding methods, and graph
neural nets (GNN) based methods. Non-GNN graph embedding methods are based on
techniques such as random walks, temporal point processes and neural network
learning methods. GNN-based methods, on the other hand, are the application of
deep learning on graph data. In this survey, we provide an overview of these
two categories and cover the current state-of-the-art methods for both static
and dynamic graphs. Finally, we explore some open and ongoing research
directions for future work."
4360,"A few works incorporate domain
knowledge into GNNs, such as [50, 51], but this area is still new and has the potential for further research.","For instance, we can enrich the input data by considering additional
relationships or constraints on existing relationships among the entities in the data.",Limited training data labels.,2022-04-04 21:18:48+00:00,A Survey on Graph Representation Learning Methods,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Shima Khoshraftar'), arxiv.Result.Author('Aijun An')]","Graphs representation learning has been a very active research area in recent
years. The goal of graph representation learning is to generate graph
representation vectors that capture the structure and features of large graphs
accurately. This is especially important because the quality of the graph
representation vectors will affect the performance of these vectors in
downstream tasks such as node classification, link prediction and anomaly
detection. Many techniques are proposed for generating effective graph
representation vectors. Two of the most prevalent categories of graph
representation learning are graph embedding methods without using graph neural
nets (GNN), which we denote as non-GNN based graph embedding methods, and graph
neural nets (GNN) based methods. Non-GNN graph embedding methods are based on
techniques such as random walks, temporal point processes and neural network
learning methods. GNN-based methods, on the other hand, are the application of
deep learning on graph data. In this survey, we provide an overview of these
two categories and cover the current state-of-the-art methods for both static
and dynamic graphs. Finally, we explore some open and ongoing research
directions for future work."
4361,"However, there are still opportunities for further studying these types of learning in GNNs in both static and dynamic
settings.","Solutions such as self-supervised learning, data augmentation, and
contrastive learning, have been proposed to solve the label scarcity problem and are employed in graph neural nets.",Transferring advances in deep learning models to GNNs.,2022-04-04 21:18:48+00:00,A Survey on Graph Representation Learning Methods,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Shima Khoshraftar'), arxiv.Result.Author('Aijun An')]","Graphs representation learning has been a very active research area in recent
years. The goal of graph representation learning is to generate graph
representation vectors that capture the structure and features of large graphs
accurately. This is especially important because the quality of the graph
representation vectors will affect the performance of these vectors in
downstream tasks such as node classification, link prediction and anomaly
detection. Many techniques are proposed for generating effective graph
representation vectors. Two of the most prevalent categories of graph
representation learning are graph embedding methods without using graph neural
nets (GNN), which we denote as non-GNN based graph embedding methods, and graph
neural nets (GNN) based methods. Non-GNN graph embedding methods are based on
techniques such as random walks, temporal point processes and neural network
learning methods. GNN-based methods, on the other hand, are the application of
deep learning on graph data. In this survey, we provide an overview of these
two categories and cover the current state-of-the-art methods for both static
and dynamic graphs. Finally, we explore some open and ongoing research
directions for future work."
4362,"These future directions include proposing solutions for limitations of GNNs
in both static and dynamic setting, learning GNNs with limited training data labels in both static and dynamic setting,
transferring advances in deep learning to GNNs and further studying GNN’s theory side.","Finally, we discussed some of the open issues in the node representation learning field that
are interesting future research directions.","As this field is growing fast,
we hope that this up-to-date survey can provide valuable additional information for researchers working in this area.",2022-04-04 21:18:48+00:00,A Survey on Graph Representation Learning Methods,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Shima Khoshraftar'), arxiv.Result.Author('Aijun An')]","Graphs representation learning has been a very active research area in recent
years. The goal of graph representation learning is to generate graph
representation vectors that capture the structure and features of large graphs
accurately. This is especially important because the quality of the graph
representation vectors will affect the performance of these vectors in
downstream tasks such as node classification, link prediction and anomaly
detection. Many techniques are proposed for generating effective graph
representation vectors. Two of the most prevalent categories of graph
representation learning are graph embedding methods without using graph neural
nets (GNN), which we denote as non-GNN based graph embedding methods, and graph
neural nets (GNN) based methods. Non-GNN graph embedding methods are based on
techniques such as random walks, temporal point processes and neural network
learning methods. GNN-based methods, on the other hand, are the application of
deep learning on graph data. In this survey, we provide an overview of these
two categories and cover the current state-of-the-art methods for both static
and dynamic graphs. Finally, we explore some open and ongoing research
directions for future work."
4366,"While we scratch the surface of exploring the incorporation of metadata in the active learning
process, there are several possible areas of further study.","We recommend further explorations of the utility of automatically
generated metadata in machine learning processes.","Modern active learning approaches
have been proposed which work well for deep learning with image data.",2022-04-05 01:01:32+00:00,An Exploration of Active Learning for Affective Digital Phenotyping,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Peter Washington'), arxiv.Result.Author('Cezmi Mutlu'), arxiv.Result.Author('Aaron Kline'), arxiv.Result.Author('Cathy Hou'), arxiv.Result.Author('Kaitlyn Dunlap'), arxiv.Result.Author('Jack Kent'), arxiv.Result.Author('Arman Husic'), arxiv.Result.Author('Nate Stockham'), arxiv.Result.Author('Brianna Chrisman'), arxiv.Result.Author('Kelley Paskov'), arxiv.Result.Author('Jae-Yoon Jung'), arxiv.Result.Author('Dennis P. Wall')]","Some of the most severe bottlenecks preventing widespread development of
machine learning models for human behavior include a dearth of labeled training
data and difficulty of acquiring high quality labels. Active learning is a
paradigm for using algorithms to computationally select a useful subset of data
points to label using metrics for model uncertainty and data similarity. We
explore active learning for naturalistic computer vision emotion data, a
particularly heterogeneous and complex data space due to inherently subjective
labels. Using frames collected from gameplay acquired from a therapeutic
smartphone game for children with autism, we run a simulation of active
learning using gameplay prompts as metadata to aid in the active learning
process. We find that active learning using information generated during
gameplay slightly outperforms random selection of the same number of labeled
frames. We next investigate a method to conduct active learning with subjective
data, such as in affective computing, and where multiple crowdsourced labels
can be acquired for each image. Using the Child Affective Facial Expression
(CAFE) dataset, we simulate an active learning process for crowdsourcing many
labels and find that prioritizing frames using the entropy of the crowdsourced
label distribution results in lower categorical cross-entropy loss compared to
random frame selection. Collectively, these results demonstrate pilot
evaluations of two novel active learning approaches for subjective affective
data collected in noisy settings."
4367,"While we scratch the surface of exploring the incorporation of metadata in the active learning
process, there are several possible areas of further study.","We recommend further explorations of the utility of automatically
generated metadata in machine learning processes.","Modern active learning approaches
have been proposed which work well for deep learning with image data.",2022-04-05 01:01:32+00:00,An Exploration of Active Learning for Affective Digital Phenotyping,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Peter Washington'), arxiv.Result.Author('Cezmi Mutlu'), arxiv.Result.Author('Aaron Kline'), arxiv.Result.Author('Cathy Hou'), arxiv.Result.Author('Kaitlyn Dunlap'), arxiv.Result.Author('Jack Kent'), arxiv.Result.Author('Arman Husic'), arxiv.Result.Author('Nate Stockham'), arxiv.Result.Author('Brianna Chrisman'), arxiv.Result.Author('Kelley Paskov'), arxiv.Result.Author('Jae-Yoon Jung'), arxiv.Result.Author('Dennis P. Wall')]","Some of the most severe bottlenecks preventing widespread development of
machine learning models for human behavior include a dearth of labeled training
data and difficulty of acquiring high quality labels. Active learning is a
paradigm for using algorithms to computationally select a useful subset of data
points to label using metrics for model uncertainty and data similarity. We
explore active learning for naturalistic computer vision emotion data, a
particularly heterogeneous and complex data space due to inherently subjective
labels. Using frames collected from gameplay acquired from a therapeutic
smartphone game for children with autism, we run a simulation of active
learning using gameplay prompts as metadata to aid in the active learning
process. We find that active learning using information generated during
gameplay slightly outperforms random selection of the same number of labeled
frames. We next investigate a method to conduct active learning with subjective
data, such as in affective computing, and where multiple crowdsourced labels
can be acquired for each image. Using the Child Affective Facial Expression
(CAFE) dataset, we simulate an active learning process for crowdsourcing many
labels and find that prioritizing frames using the entropy of the crowdsourced
label distribution results in lower categorical cross-entropy loss compared to
random frame selection. Collectively, these results demonstrate pilot
evaluations of two novel active learning approaches for subjective affective
data collected in noisy settings."
4403,"This also suggests a further research direction to
           bridge such a convergence-diversity gap.","By contrast, the objective of
           RSPO aims to ﬁnd diverse local optima.",2.,2022-04-04 12:38:58+00:00,Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Zihan Zhou'), arxiv.Result.Author('Wei Fu'), arxiv.Result.Author('Bingliang Zhang'), arxiv.Result.Author('Yi Wu')]","We present Reward-Switching Policy Optimization (RSPO), a paradigm to
discover diverse strategies in complex RL environments by iteratively finding
novel policies that are both locally optimal and sufficiently different from
existing ones. To encourage the learning policy to consistently converge
towards a previously undiscovered local optimum, RSPO switches between
extrinsic and intrinsic rewards via a trajectory-based novelty measurement
during the optimization process. When a sampled trajectory is sufficiently
distinct, RSPO performs standard policy optimization with extrinsic rewards.
For trajectories with high likelihood under existing policies, RSPO utilizes an
intrinsic diversity reward to promote exploration. Experiments show that RSPO
is able to discover a wide spectrum of strategies in a variety of domains,
ranging from single-agent particle-world tasks and MuJoCo continuous control to
multi-agent stag-hunt games and StarCraftII challenges."
4404,"This also suggests a further research direction to
           bridge such a convergence-diversity gap.","By contrast, the objective of
           RSPO aims to ﬁnd diverse local optima.",2.,2022-04-04 12:38:58+00:00,Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Zihan Zhou'), arxiv.Result.Author('Wei Fu'), arxiv.Result.Author('Bingliang Zhang'), arxiv.Result.Author('Yi Wu')]","We present Reward-Switching Policy Optimization (RSPO), a paradigm to
discover diverse strategies in complex RL environments by iteratively finding
novel policies that are both locally optimal and sufficiently different from
existing ones. To encourage the learning policy to consistently converge
towards a previously undiscovered local optimum, RSPO switches between
extrinsic and intrinsic rewards via a trajectory-based novelty measurement
during the optimization process. When a sampled trajectory is sufficiently
distinct, RSPO performs standard policy optimization with extrinsic rewards.
For trajectories with high likelihood under existing policies, RSPO utilizes an
intrinsic diversity reward to promote exploration. Experiments show that RSPO
is able to discover a wide spectrum of strategies in a variety of domains,
ranging from single-agent particle-world tasks and MuJoCo continuous control to
multi-agent stag-hunt games and StarCraftII challenges."
4405,"This also suggests a further research direction to
           bridge such a convergence-diversity gap.","By contrast, the objective of
           RSPO aims to ﬁnd diverse local optima.",2.,2022-04-04 12:38:58+00:00,Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Zihan Zhou'), arxiv.Result.Author('Wei Fu'), arxiv.Result.Author('Bingliang Zhang'), arxiv.Result.Author('Yi Wu')]","We present Reward-Switching Policy Optimization (RSPO), a paradigm to
discover diverse strategies in complex RL environments by iteratively finding
novel policies that are both locally optimal and sufficiently different from
existing ones. To encourage the learning policy to consistently converge
towards a previously undiscovered local optimum, RSPO switches between
extrinsic and intrinsic rewards via a trajectory-based novelty measurement
during the optimization process. When a sampled trajectory is sufficiently
distinct, RSPO performs standard policy optimization with extrinsic rewards.
For trajectories with high likelihood under existing policies, RSPO utilizes an
intrinsic diversity reward to promote exploration. Experiments show that RSPO
is able to discover a wide spectrum of strategies in a variety of domains,
ranging from single-agent particle-world tasks and MuJoCo continuous control to
multi-agent stag-hunt games and StarCraftII challenges."
4406,"More-
                                                                 over, to further study whether the learned latent feature can
pθ(X) = pθ(X|z)p(z)  (1)

The loss function of VAE is minimized following the
(a) PCA result of expression data

      PCA_dim2

(b) PCA result of latent feature                                            OV

      PCA_dim2

                                   PCA_dim1  PCA_dim1             PCA_dim1      PCA_dim1

                                    BRCA      GBM                   LGG            OV

Fig.",latent representation from the original expression data.,3.,2022-04-02 11:44:58+00:00,Cancer Subtyping via Embedded Unsupervised Learning on Transcriptomics Data,cs.LG,"['cs.LG', 'q-bio.GN']","[arxiv.Result.Author('Ziwei Yang'), arxiv.Result.Author('Lingwei Zhu'), arxiv.Result.Author('Zheng Chen'), arxiv.Result.Author('Ming Huang'), arxiv.Result.Author('Naoaki Ono'), arxiv.Result.Author('MD Altaf-Ul-Amin'), arxiv.Result.Author('Shigehiko Kanaya')]","Cancer is one of the deadliest diseases worldwide. Accurate diagnosis and
classification of cancer subtypes are indispensable for effective clinical
treatment. Promising results on automatic cancer subtyping systems have been
published recently with the emergence of various deep learning methods.
However, such automatic systems often overfit the data due to the high
dimensionality and scarcity. In this paper, we propose to investigate automatic
subtyping from an unsupervised learning perspective by directly constructing
the underlying data distribution itself, hence sufficient data can be generated
to alleviate the issue of overfitting. Specifically, we bypass the strong
Gaussianity assumption that typically exists but fails in the unsupervised
learning subtyping literature due to small-sized samples by vector
quantization. Our proposed method better captures the latent space features and
models the cancer subtype manifestation on a molecular basis, as demonstrated
by the extensive experimental results."
4436,"Some further research in this topic involve testing this
                                                                     method on False Positive minimization and using deeper
neural networks.","staying near the same F1 score, ensembling of the best models
was performed without the emphasis on the minimization of               Some of these methods include class weight change
                                                                     from keras documentation, data augumentation, and threshold
                                                                     change.","While the method was tested on famous
networks like VGG16, it can be applied to ever deeper neural
networks on larger datasets.",2022-04-06 00:33:40+00:00,Emphasis on the Minimization of False Negatives or False Positives in Binary Classification,cs.LG,"['cs.LG', 'cs.CV']",[arxiv.Result.Author('Sanskriti Singh')],"The minimization of specific cases in binary classification, such as false
negatives or false positives, grows increasingly important as humans begin to
implement more machine learning into current products. While there are a few
methods to put a bias towards the reduction of specific cases, these methods
aren't very effective, hence their minimal use in models. To this end, a new
method is introduced to reduce the False Negatives or False positives without
drastically changing the overall performance or F1 score of the model. This
method involving the careful change to the real value of the input after
pre-training the model. Presenting the results of this method being applied on
various datasets, some being more complex than others. Through experimentation
on multiple model architectures on these datasets, the best model was found. In
all the models, an increase in the recall or precision, minimization of False
Negatives or False Positives respectively, was shown without a large drop in F1
score."
4452,"Given its effectiveness, a
further study can be conducted on more non-stationary time series datasets to verify its general SSL capability.","VNIbCReg which is a combination of VIbCReg
and TNC is shown to be effective in learning representations of non-stationary time series.","Acknowledgments

We acknowledge support from the Norwegian Research Council project ML4ITS grant 312062 and the SFI Centre for
Geophysical Forecasting grant 309960.",2022-04-06 09:48:59+00:00,VNIbCReg: VIbCReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series,cs.LG,"['cs.LG', 'physics.geo-ph', 'stat.ML']","[arxiv.Result.Author('Daesoo Lee'), arxiv.Result.Author('Erlend Aune'), arxiv.Result.Author('Nadège Langet'), arxiv.Result.Author('Jo Eidsvik')]","One of the latest self-supervised learning (SSL) methods, VICReg, showed a
great performance both in the linear evaluation and the fine-tuning evaluation.
However, VICReg is proposed in computer vision and it learns by pulling
representations of random crops of an image while maintaining the
representation space by the variance and covariance loss. However, VICReg would
be ineffective on non-stationary time series where different parts/crops of
input should be differently encoded to consider the non-stationarity. Another
recent SSL proposal, Temporal Neighborhood Coding (TNC) is effective for
encoding non-stationary time series. This study shows that a combination of a
VICReg-style method and TNC is very effective for SSL on non-stationary time
series, where a non-stationary seismic signal time series is used as an
evaluation dataset."
4453,"Given its effectiveness, a
further study can be conducted on more non-stationary time series datasets to verify its general SSL capability.","VNIbCReg which is a combination of VIbCReg
and TNC is shown to be effective in learning representations of non-stationary time series.","Acknowledgments

We acknowledge support from the Norwegian Research Council project ML4ITS grant 312062 and the SFI Centre for
Geophysical Forecasting grant 309960.",2022-04-06 09:48:59+00:00,VNIbCReg: VICReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series,cs.LG,"['cs.LG', 'physics.geo-ph', 'stat.ML']","[arxiv.Result.Author('Daesoo Lee'), arxiv.Result.Author('Erlend Aune'), arxiv.Result.Author('Nadège Langet'), arxiv.Result.Author('Jo Eidsvik')]","One of the latest self-supervised learning (SSL) methods, VICReg, showed a
great performance both in the linear evaluation and the fine-tuning evaluation.
However, VICReg is proposed in computer vision and it learns by pulling
representations of random crops of an image while maintaining the
representation space by the variance and covariance loss. However, VICReg would
be ineffective on non-stationary time series where different parts/crops of
input should be differently encoded to consider the non-stationarity. Another
recent SSL proposal, Temporal Neighborhood Coding (TNC) is effective for
encoding non-stationary time series. This study shows that a combination of a
VICReg-style method and TNC is very effective for SSL on non-stationary time
series, where a non-stationary seismic signal time series is used as an
evaluation dataset."
4454,"Given its effectiveness, a
further study can be conducted on more non-stationary time series datasets to verify its general SSL capability.","VNIbCReg which is a combination of VIbCReg
and TNC is shown to be effective in learning representations of non-stationary time series.","Acknowledgments

We acknowledge support from the Norwegian Research Council project ML4ITS grant 312062 and the SFI Centre for
Geophysical Forecasting grant 309960.",2022-04-06 09:48:59+00:00,VNIbCReg: VICReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series,cs.LG,"['cs.LG', 'physics.geo-ph', 'stat.ML']","[arxiv.Result.Author('Daesoo Lee'), arxiv.Result.Author('Erlend Aune'), arxiv.Result.Author('Nadège Langet'), arxiv.Result.Author('Jo Eidsvik')]","One of the latest self-supervised learning (SSL) methods, VICReg, showed a
great performance both in the linear evaluation and the fine-tuning evaluation.
However, VICReg is proposed in computer vision and it learns by pulling
representations of random crops of an image while maintaining the
representation space by the variance and covariance loss. However, VICReg would
be ineffective on non-stationary time series where different parts/crops of
input should be differently encoded to consider the non-stationarity. Another
recent SSL proposal, Temporal Neighborhood Coding (TNC) is effective for
encoding non-stationary time series. This study shows that a combination of a
VICReg-style method and TNC is very effective for SSL on non-stationary time
series, where a non-stationary seismic signal time series is used as an
evaluation dataset."
4455,"Given its effectiveness, a
further study can be conducted on more non-stationary time series datasets to verify its general SSL capability.","VNIbCReg which is a combination of VIbCReg
and TNC is shown to be effective in learning representations of non-stationary time series.","Acknowledgments

We acknowledge support from the Norwegian Research Council project ML4ITS grant 312062 and the SFI Centre for
Geophysical Forecasting grant 309960.",2022-04-06 09:48:59+00:00,VNIbCReg: VICReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series,cs.LG,"['cs.LG', 'physics.geo-ph', 'stat.ML']","[arxiv.Result.Author('Daesoo Lee'), arxiv.Result.Author('Erlend Aune'), arxiv.Result.Author('Nadège Langet'), arxiv.Result.Author('Jo Eidsvik')]","One of the latest self-supervised learning (SSL) methods, VICReg, showed a
great performance both in the linear evaluation and the fine-tuning evaluation.
However, VICReg is proposed in computer vision and it learns by pulling
representations of random crops of an image while maintaining the
representation space by the variance and covariance loss. However, VICReg would
be ineffective on non-stationary time series where different parts/crops of
input should be differently encoded to consider the non-stationarity. Another
recent SSL proposal, Temporal Neighborhood Coding (TNC) is effective for
encoding non-stationary time series. This study shows that a combination of a
VICReg-style method and TNC is very effective for SSL on non-stationary time
series, where a non-stationary seismic signal time series is used as an
evaluation dataset."
4456,"Given its effectiveness, a
further study can be conducted on more non-stationary time series datasets to verify its general SSL capability.","VNIbCReg which is a combination of VIbCReg
and TNC is shown to be effective in learning representations of non-stationary time series.","Acknowledgments

We acknowledge support from the Norwegian Research Council project ML4ITS grant 312062 and the SFI Centre for
Geophysical Forecasting grant 309960.",2022-04-06 09:48:59+00:00,VNIbCReg: VICReg with Neighboring-Invariance and better-Covariance Evaluated on Non-stationary Seismic Signal Time Series,cs.LG,"['cs.LG', 'physics.geo-ph', 'stat.ML']","[arxiv.Result.Author('Daesoo Lee'), arxiv.Result.Author('Erlend Aune'), arxiv.Result.Author('Nadège Langet'), arxiv.Result.Author('Jo Eidsvik')]","One of the latest self-supervised learning (SSL) methods, VICReg, showed a
great performance both in the linear evaluation and the fine-tuning evaluation.
However, VICReg is proposed in computer vision and it learns by pulling
representations of random crops of an image while maintaining the
representation space by the variance and covariance loss. However, VICReg would
be ineffective on non-stationary time series where different parts/crops of
input should be differently encoded to consider the non-stationarity. Another
recent SSL proposal, Temporal Neighborhood Coding (TNC) is effective for
encoding non-stationary time series. This study shows that a combination of a
VICReg-style method and TNC is very effective for SSL on non-stationary time
series, where a non-stationary seismic signal time series is used as an
evaluation dataset."
4481,"Therefore, this list is not necessarily complete but gives an insight
into further research in graph learning on combined graph types.","To conclude this work, we give a selected list of graph type combinations that are used in several
research ﬁelds where GNNs are already established.","Graph Type            Models                Problem                          Data Category
undirected node-attributed               node classiﬁcation & graph
                            GCN [40] &  classiﬁcation, node regression       citation networks, knowledge
          weighted          Hyperbolic                                       graphs & synthetic, molecular,
                             GCN [48]            link prediction
     knowledge graph        KGIN [85]            link prediction,                       blockchain
    content-associated      HetG [104]  recommendation, (inductive)              recommender systems
                                           node classiﬁcation, node
       heterogeneous                                                                 review networks
                                                    clustering

   13Since, to the best of our knowledge, most of the models do not specify the connectedness, it is assumed here that they can handle
both connected and unconnected graphs.",2022-04-06 20:37:42+00:00,Graph Neural Networks Designed for Different Graph Types: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Josephine M. Thomas'), arxiv.Result.Author('Alice Moallemy-Oureh'), arxiv.Result.Author('Silvia Beddar-Wiesing'), arxiv.Result.Author('Clara Holzhüter')]","Graphs are ubiquitous in nature and can therefore serve as models for many
practical but also theoretical problems. Based on this, the young research
field of Graph Neural Networks (GNNs) has emerged. Despite the youth of the
field and the speed in which new models are developed, many good surveys have
been published in the last years. Nevertheless, an overview on which graph
types can be modeled by GNNs is missing. In this survey, we give a detailed
overview of already existing GNNs and, unlike previous surveys, categorize them
according to their ability to handle different graph types and properties. We
consider GNNs operating on static as well as on dynamic graphs of different
structural constitutions, with or without node or edge attributes. Moreover in
the dynamic case, we separate the models in discrete-time and continuous-time
dynamic graphs based on their architecture. While ordering the existing GNN
models, we find, that there are still graph types, that are not or only rarely
covered by existing GNN models. We point out where models are missing and give
potential reasons for their absence."
4482,"Therefore, this list is not necessarily complete but gives an insight into
further research on GNNs considering combined graph types.","To conclude this work, we give a selected list of graph-type combinations used in several research
ﬁelds where GNNs are already established.",The architectures listed in Tab.,2022-04-06 20:37:42+00:00,Graph Neural Networks Designed for Different Graph Types: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Josephine M. Thomas'), arxiv.Result.Author('Alice Moallemy-Oureh'), arxiv.Result.Author('Silvia Beddar-Wiesing'), arxiv.Result.Author('Clara Holzhüter')]","Graphs are ubiquitous in nature and can therefore serve as models for many
practical but also theoretical problems. For this purpose, they can be defined
as many different types which suitably reflect the individual contexts of the
represented problem. To address cutting-edge problems based on graph data, the
research field of Graph Neural Networks (GNNs) has emerged. Despite the field's
youth and the speed at which new models are developed, many recent surveys have
been published to keep track of them. Nevertheless, it has not yet been
gathered which GNN can process what kind of graph types. In this survey, we
give a detailed overview of already existing GNNs and, unlike previous surveys,
categorize them according to their ability to handle different graph types and
properties. We consider GNNs operating on static and dynamic graphs of
different structural constitutions, with or without node or edge attributes.
Moreover, we distinguish between GNN models for discrete-time or
continuous-time dynamic graphs and group the models according to their
architecture. We find that there are still graph types that are not or only
rarely covered by existing GNN models. We point out where models are missing
and give potential reasons for their absence."
4483,"Therefore, this list is not necessarily complete but gives an insight into
further research on GNNs considering combined graph types.","To conclude this work, we give a selected list of graph-type combinations used in several research
ﬁelds where GNNs are already established.",The architectures listed in Tab.,2022-04-06 20:37:42+00:00,Graph Neural Networks Designed for Different Graph Types: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Josephine M. Thomas'), arxiv.Result.Author('Alice Moallemy-Oureh'), arxiv.Result.Author('Silvia Beddar-Wiesing'), arxiv.Result.Author('Clara Holzhüter')]","Graphs are ubiquitous in nature and can therefore serve as models for many
practical but also theoretical problems. For this purpose, they can be defined
as many different types which suitably reflect the individual contexts of the
represented problem. To address cutting-edge problems based on graph data, the
research field of Graph Neural Networks (GNNs) has emerged. Despite the field's
youth and the speed at which new models are developed, many recent surveys have
been published to keep track of them. Nevertheless, it has not yet been
gathered which GNN can process what kind of graph types. In this survey, we
give a detailed overview of already existing GNNs and, unlike previous surveys,
categorize them according to their ability to handle different graph types and
properties. We consider GNNs operating on static and dynamic graphs of
different structural constitutions, with or without node or edge attributes.
Moreover, we distinguish between GNN models for discrete-time or
continuous-time dynamic graphs and group the models according to their
architecture. We find that there are still graph types that are not or only
rarely covered by existing GNN models. We point out where models are missing
and give potential reasons for their absence."
4504,We further study the property of Qc in the following lemma.,"In addition, we note that if Mc < M , the last M − Mc entries of Qc
are padded with zero by construction.",Lemma 2.,2022-04-07 09:12:00+00:00,Federated Learning from Only Unlabeled Data with Class-Conditional-Sharing Clients,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nan Lu'), arxiv.Result.Author('Zhao Wang'), arxiv.Result.Author('Xiaoxiao Li'), arxiv.Result.Author('Gang Niu'), arxiv.Result.Author('Qi Dou'), arxiv.Result.Author('Masashi Sugiyama')]","Supervised federated learning (FL) enables multiple clients to share the
trained model without sharing their labeled data. However, potential clients
might even be reluctant to label their own data, which could limit the
applicability of FL in practice. In this paper, we show the possibility of
unsupervised FL whose model is still a classifier for predicting class labels,
if the class-prior probabilities are shifted while the class-conditional
distributions are shared among the unlabeled data owned by the clients. We
propose federation of unsupervised learning (FedUL), where the unlabeled data
are transformed into surrogate labeled data for each of the clients, a modified
model is trained by supervised FL, and the wanted model is recovered from the
modified model. FedUL is a very general solution to unsupervised FL: it is
compatible with many supervised FL methods, and the recovery of the wanted
model can be theoretically guaranteed as if the data have been labeled.
Experiments on benchmark and real-world datasets demonstrate the effectiveness
of FedUL. Code is available at https://github.com/lunanbit/FedUL."
4505,We further study the property of Qc in the following lemma.,"In addition, we note that if Mc < M , the last M − Mc entries of Qc
are padded with zero by construction.",Lemma 2.,2022-04-07 09:12:00+00:00,Federated Learning from Only Unlabeled Data with Class-Conditional-Sharing Clients,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nan Lu'), arxiv.Result.Author('Zhao Wang'), arxiv.Result.Author('Xiaoxiao Li'), arxiv.Result.Author('Gang Niu'), arxiv.Result.Author('Qi Dou'), arxiv.Result.Author('Masashi Sugiyama')]","Supervised federated learning (FL) enables multiple clients to share the
trained model without sharing their labeled data. However, potential clients
might even be reluctant to label their own data, which could limit the
applicability of FL in practice. In this paper, we show the possibility of
unsupervised FL whose model is still a classifier for predicting class labels,
if the class-prior probabilities are shifted while the class-conditional
distributions are shared among the unlabeled data owned by the clients. We
propose federation of unsupervised learning (FedUL), where the unlabeled data
are transformed into surrogate labeled data for each of the clients, a modified
model is trained by supervised FL, and the wanted model is recovered from the
modified model. FedUL is a very general solution to unsupervised FL: it is
compatible with many supervised FL methods, and the recovery of the wanted
model can be theoretically guaranteed as if the data have been labeled.
Experiments on benchmark and real-world datasets demonstrate the effectiveness
of FedUL. Code is available at https://github.com/lunanbit/FedUL."
4506,"For example,            This necessitates further research on providing more powerful
inception-v3 has a size of 91 MB [253], while vgg-19 has a size of              compression techniques for DL networks.",DL models can vary significantly in their overall size.,548 MB [254]).,2022-04-07 09:47:10+00:00,Enabling All In-Edge Deep Learning: A Literature Review,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Praveen Joshi'), arxiv.Result.Author('Mohammed Hasanuzzaman'), arxiv.Result.Author('Chandra Thapa'), arxiv.Result.Author('Haithem Afli'), arxiv.Result.Author('Ted Scully')]","In recent years, deep learning (DL) models have demonstrated remarkable
achievements on non-trivial tasks such as speech recognition and natural
language understanding. One of the significant contributors to its success is
the proliferation of end devices that acted as a catalyst to provide data for
data-hungry DL models. However, computing DL training and inference is the main
challenge. Usually, central cloud servers are used for the computation, but it
opens up other significant challenges, such as high latency, increased
communication costs, and privacy concerns. To mitigate these drawbacks,
considerable efforts have been made to push the processing of DL models to edge
servers. Moreover, the confluence point of DL and edge has given rise to edge
intelligence (EI). This survey paper focuses primarily on the fifth level of
EI, called all in-edge level, where DL training and inference (deployment) are
performed solely by edge servers. All in-edge is suitable when the end devices
have low computing resources, e.g., Internet-of-Things, and other requirements
such as latency and communication cost are important in mission-critical
applications, e.g., health care. Firstly, this paper presents all in-edge
computing architectures, including centralized, decentralized, and distributed.
Secondly, this paper presents enabling technologies, such as model parallelism
and split learning, which facilitate DL training and deployment at edge
servers. Thirdly, model adaptation techniques based on model compression and
conditional computation are described because the standard cloud-based DL
deployment cannot be directly applied to all in-edge due to its limited
computational resources. Fourthly, this paper discusses eleven key performance
metrics to evaluate the performance of DL at all in-edge efficiently. Finally,
several open research challenges in the area of all in-edge are presented."
4525,"We further study the performance of                  (0.01) for FedProx in the FMNIST dataset gives the worst
FedADMM by investigating the impact of different initialization            performance for MNIST in the 200-client setting.","tuning is dependent on system sizes and data distributions, as
                                                                           we also demonstrate in Table V. Note that the best ρ value
Local Initialization.","Moreover,
for the local training subproblems at selected clients.",2022-04-07 15:58:33+00:00,FedADMM: A Robust Federated Deep Learning Framework with Adaptivity to System Heterogeneity,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yonghai Gong'), arxiv.Result.Author('Yichuan Li'), arxiv.Result.Author('Nikolaos M. Freris')]","Federated Learning (FL) is an emerging framework for distributed processing
of large data volumes by edge devices subject to limited communication
bandwidths, heterogeneity in data distributions and computational resources, as
well as privacy considerations. In this paper, we introduce a new FL protocol
termed FedADMM based on primal-dual optimization. The proposed method leverages
dual variables to tackle statistical heterogeneity, and accommodates system
heterogeneity by tolerating variable amount of work performed by clients.
FedADMM maintains identical communication costs per round as FedAvg/Prox, and
generalizes them via the augmented Lagrangian. A convergence proof is
established for nonconvex objectives, under no restrictions in terms of data
dissimilarity or number of participants per round of the algorithm. We
demonstrate the merits through extensive experiments on real datasets, under
both IID and non-IID data distributions across clients. FedADMM consistently
outperforms all baseline methods in terms of communication efficiency, with the
number of rounds needed to reach a prescribed accuracy reduced by up to 87%.
The algorithm effectively adapts to heterogeneous data distributions through
the use of dual variables, without the need for hyperparameter tuning, and its
advantages are more pronounced in large-scale systems."
4526,"We further study the performance of                  (0.01) for FedProx in the FMNIST dataset gives the worst
FedADMM by investigating the impact of different initialization            performance for MNIST in the 200-client setting.","tuning is dependent on system sizes and data distributions, as
                                                                           we also demonstrate in Table V. Note that the best ρ value
Local Initialization.","Moreover,
for the local training subproblems at selected clients.",2022-04-07 15:58:33+00:00,FedADMM: A Robust Federated Deep Learning Framework with Adaptivity to System Heterogeneity,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yonghai Gong'), arxiv.Result.Author('Yichuan Li'), arxiv.Result.Author('Nikolaos M. Freris')]","Federated Learning (FL) is an emerging framework for distributed processing
of large data volumes by edge devices subject to limited communication
bandwidths, heterogeneity in data distributions and computational resources, as
well as privacy considerations. In this paper, we introduce a new FL protocol
termed FedADMM based on primal-dual optimization. The proposed method leverages
dual variables to tackle statistical heterogeneity, and accommodates system
heterogeneity by tolerating variable amount of work performed by clients.
FedADMM maintains identical communication costs per round as FedAvg/Prox, and
generalizes them via the augmented Lagrangian. A convergence proof is
established for nonconvex objectives, under no restrictions in terms of data
dissimilarity or number of participants per round of the algorithm. We
demonstrate the merits through extensive experiments on real datasets, under
both IID and non-IID data distributions across clients. FedADMM consistently
outperforms all baseline methods in terms of communication efficiency, with the
number of rounds needed to reach a prescribed accuracy reduced by up to 87%.
The algorithm effectively adapts to heterogeneous data distributions through
the use of dual variables, without the need for hyperparameter tuning, and its
advantages are more pronounced in large-scale systems."
4538,"As      fore we further study the ﬁnite sample behavior and
                                                             how to decide the sizes of training and validation sets.",", K}| ∀j Aval[i, j] = 1 ∧ Aval[k, j] = 1}.","the dimensions and samples are independent, the maxi-
                                                             Finite Sample Analysis.",2022-04-07 17:59:19+00:00,Equivariance Discovery by Learned Parameter-Sharing,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Raymond A. Yeh'), arxiv.Result.Author('Yuan-Ting Hu'), arxiv.Result.Author('Mark Hasegawa-Johnson'), arxiv.Result.Author('Alexander G. Schwing')]","Designing equivariance as an inductive bias into deep-nets has been a
prominent approach to build effective models, e.g., a convolutional neural
network incorporates translation equivariance. However, incorporating these
inductive biases requires knowledge about the equivariance properties of the
data, which may not be available, e.g., when encountering a new domain. To
address this, we study how to discover interpretable equivariances from data.
Specifically, we formulate this discovery process as an optimization problem
over a model's parameter-sharing schemes. We propose to use the partition
distance to empirically quantify the accuracy of the recovered equivariance.
Also, we theoretically analyze the method for Gaussian data and provide a bound
on the mean squared gap between the studied discovery scheme and the oracle
scheme. Empirically, we show that the approach recovers known equivariances,
such as permutations and shifts, on sum of numbers and spatially-invariant
data."
4584,"Correspondingly, the behavior, quality, and security of
                                       benchmark for further studying the quantized models.",We opensource our code and models as a new                                fication [47].,DNNs are also concerned by the software engineering community.,2022-04-08 11:19:16+00:00,Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Qiang Hu'), arxiv.Result.Author('Yuejun Guo'), arxiv.Result.Author('Maxime Cordy'), arxiv.Result.Author('Xiaofei Xie'), arxiv.Result.Author('Wei Ma'), arxiv.Result.Author('Mike Papadakis'), arxiv.Result.Author('Yves Le Traon')]","Deep Neural Networks (DNNs) have gained considerable attention in the past
decades due to their astounding performance in different applications, such as
natural language modeling, self-driving assistance, and source code
understanding. With rapid exploration, more and more complex DNN architectures
have been proposed along with huge pre-trained model parameters. The common way
to use such DNN models in user-friendly devices (e.g., mobile phones) is to
perform model compression before deployment. However, recent research has
demonstrated that model compression, e.g., model quantization, yields accuracy
degradation as well as outputs disagreements when tested on unseen data. Since
the unseen data always include distribution shifts and often appear in the
wild, the quality and reliability of quantized models are not ensured. In this
paper, we conduct a comprehensive study to characterize and help users
understand the behaviors of quantized models. Our study considers 4 datasets
spanning from image to text, 8 DNN architectures including feed-forward neural
networks and recurrent neural networks, and 42 shifted sets with both synthetic
and natural distribution shifts. The results reveal that 1) data with
distribution shifts happen more disagreements than without. 2)
Quantization-aware training can produce more stable models than standard,
adversarial, and Mixup training. 3) Disagreements often have closer top-1 and
top-2 output probabilities, and $Margin$ is a better indicator than the other
uncertainty metrics to distinguish disagreements. 4) Retraining with
disagreements has limited efficiency in removing disagreements. We opensource
our code and models as a new benchmark for further studying the quantized
models."
4585,"To support further research, we released our                          [18] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.",disagreement issue.,2016.,2022-04-08 11:19:16+00:00,Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Qiang Hu'), arxiv.Result.Author('Yuejun Guo'), arxiv.Result.Author('Maxime Cordy'), arxiv.Result.Author('Xiaofei Xie'), arxiv.Result.Author('Wei Ma'), arxiv.Result.Author('Mike Papadakis'), arxiv.Result.Author('Yves Le Traon')]","Deep Neural Networks (DNNs) have gained considerable attention in the past
decades due to their astounding performance in different applications, such as
natural language modeling, self-driving assistance, and source code
understanding. With rapid exploration, more and more complex DNN architectures
have been proposed along with huge pre-trained model parameters. The common way
to use such DNN models in user-friendly devices (e.g., mobile phones) is to
perform model compression before deployment. However, recent research has
demonstrated that model compression, e.g., model quantization, yields accuracy
degradation as well as outputs disagreements when tested on unseen data. Since
the unseen data always include distribution shifts and often appear in the
wild, the quality and reliability of quantized models are not ensured. In this
paper, we conduct a comprehensive study to characterize and help users
understand the behaviors of quantized models. Our study considers 4 datasets
spanning from image to text, 8 DNN architectures including feed-forward neural
networks and recurrent neural networks, and 42 shifted sets with both synthetic
and natural distribution shifts. The results reveal that 1) data with
distribution shifts happen more disagreements than without. 2)
Quantization-aware training can produce more stable models than standard,
adversarial, and Mixup training. 3) Disagreements often have closer top-1 and
top-2 output probabilities, and $Margin$ is a better indicator than the other
uncertainty metrics to distinguish disagreements. 4) Retraining with
disagreements has limited efficiency in removing disagreements. We opensource
our code and models as a new benchmark for further studying the quantized
models."
4607,"prototype that explores, based on small networks, whether the principle           The user evaluation indicates that further research is needed to improve
idea of axis-aligned slices is a feasible approach to understand the loss         the usability and interpretability, but that our approach could be useful
landscape.","Therefore, we decided to build a           and follows descent directions through valleys in late stages of training.",for analyzing autoencoders and ensemble networks.,2022-04-09 16:41:53+00:00,FuNNscope: Visual microscope for interactively exploring the loss landscape of fully connected neural networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Aleksandar Doknic'), arxiv.Result.Author('Torsten Möller')]","Despite their effective use in various fields, many aspects of neural
networks are poorly understood. One important way to investigate the
characteristics of neural networks is to explore the loss landscape. However,
most models produce a high-dimensional non-convex landscape which is difficult
to visualize. We discuss and extend existing visualization methods based on 1D-
and 2D slicing with a novel method that approximates the actual loss landscape
geometry by using charts with interpretable axes. Based on the assumption that
observations on small neural networks can generalize to more complex systems
and provide us with helpful insights, we focus on small models in the range of
a few dozen weights, which enables computationally cheap experiments and the
use of an interactive dashboard. We observe symmetries around the zero vector,
the influence of different layers on the global landscape, the different weight
sensitivities around a minimizer, and how gradient descent navigates high-loss
obstacles. The user study resulted in an average SUS (System Usability Scale)
score with suggestions for improvement and opened up a number of possible
application scenarios, such as autoencoders and ensemble networks."
4619,"695–703, 2019.
further study the multi-objective dispatching of hybrid power
grid under source-load side uncertainty.","4, pp.","[5] H. Huang, M. Zhou, S. Zhang, L. Zhang, G. Li, and
                                                                                      Y.",2022-04-10 06:18:23+00:00,Confidence Estimation Transformer for Long-term Renewable Energy Forecasting in Reinforcement Learning-based Power Grid Dispatching,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Xinhang Li'), arxiv.Result.Author('Zihao Li'), arxiv.Result.Author('Nan Yang'), arxiv.Result.Author('Zheng Yuan'), arxiv.Result.Author('Qinwen Wang'), arxiv.Result.Author('Yiying Yang'), arxiv.Result.Author('Yupeng Huang'), arxiv.Result.Author('Xuri Song'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Lin Zhang')]","The expansion of renewable energy could help realizing the goals of peaking
carbon dioxide emissions and carbon neutralization. Some existing grid
dispatching methods integrating short-term renewable energy prediction and
reinforcement learning (RL) have been proved to alleviate the adverse impact of
energy fluctuations risk. However, these methods omit the long-term output
prediction, which leads to stability and security problems on the optimal power
flow. This paper proposes a confidence estimation Transformer for long-term
renewable energy forecasting in reinforcement learning-based power grid
dispatching (Conformer-RLpatching). Conformer-RLpatching predicts long-term
active output of each renewable energy generator with an enhanced Transformer
to boost the performance of hybrid energy grid dispatching. Furthermore, a
confidence estimation method is proposed to reduce the prediction error of
renewable energy. Meanwhile, a dispatching necessity evaluation mechanism is
put forward to decide whether the active output of a generator needs to be
adjusted. Experiments carried out on the SG-126 power grid simulator show that
Conformer-RLpatching achieves great improvement over the second best algorithm
DDPG in security score by 25.8% and achieves a better total reward compared
with the golden medal team in the power grid dispatching competition sponsored
by State Grid Corporation of China under the same simulation environment. Codes
are outsourced in https://github.com/buptlxh/Conformer-RLpatching."
4632,"FDA approval and design of the tools for clinical centers also remains a large
and outstanding invitation for further research to continue.","The topic areas of
health that have saturation relative to others were highlighted, which serves to build a foundation
for future work.","Multi-site data integration is of the
utmost importance for creating models that are generalizable and representative of the populace
at large.",2022-04-10 21:56:07+00:00,Multimodal Machine Learning in Precision Health,cs.LG,['cs.LG'],"[arxiv.Result.Author('Adrienne Kline'), arxiv.Result.Author('Hanyin Wang'), arxiv.Result.Author('Yikuan Li'), arxiv.Result.Author('Saya Dennis'), arxiv.Result.Author('Meghan Hutch'), arxiv.Result.Author('Zhenxing Xu'), arxiv.Result.Author('Fei Wang'), arxiv.Result.Author('Feixiong Cheng'), arxiv.Result.Author('Yuan Luo')]","As machine learning and artificial intelligence are more frequently being
leveraged to tackle problems in the health sector, there has been increased
interest in utilizing them in clinical decision-support. This has historically
been the case in single modal data such as electronic health record data.
Attempts to improve prediction and resemble the multimodal nature of clinical
expert decision-making this has been met in the computational field of machine
learning by a fusion of disparate data. This review was conducted to summarize
this field and identify topics ripe for future research. We conducted this
review in accordance with the PRISMA (Preferred Reporting Items for Systematic
reviews and Meta-Analyses) extension for Scoping Reviews to characterize
multi-modal data fusion in health. We used a combination of content analysis
and literature searches to establish search strings and databases of PubMed,
Google Scholar, and IEEEXplore from 2011 to 2021. A final set of 125 articles
were included in the analysis. The most common health areas utilizing
multi-modal methods were neurology and oncology. However, there exist a wide
breadth of current applications. The most common form of information fusion was
early fusion. Notably, there was an improvement in predictive performance
performing heterogeneous data fusion. Lacking from the papers were clear
clinical deployment strategies and pursuit of FDA-approved tools. These
findings provide a map of the current literature on multimodal data fusion as
applied to health diagnosis/prognosis problems. Multi-modal machine learning,
while more robust in its estimations over unimodal methods, has drawbacks in
its scalability and the time-consuming nature of information concatenation."
4638,"We further study the role of prompting function on Split
CIFAR-100 and Split ImageNet-R.",Preﬁx.,"In prior prompt-based CL work, L2P, only Pro-T is applied
without further investigation.",2022-04-10 23:36:55+00:00,DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Zifeng Wang'), arxiv.Result.Author('Zizhao Zhang'), arxiv.Result.Author('Sayna Ebrahimi'), arxiv.Result.Author('Ruoxi Sun'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Chen-Yu Lee'), arxiv.Result.Author('Xiaoqi Ren'), arxiv.Result.Author('Guolong Su'), arxiv.Result.Author('Vincent Perot'), arxiv.Result.Author('Jennifer Dy'), arxiv.Result.Author('Tomas Pfister')]","Continual learning aims to enable a single model to learn a sequence of tasks
without catastrophic forgetting. Top-performing methods usually require a
rehearsal buffer to store past pristine examples for experience replay, which,
however, limits their practical value due to privacy and memory constraints. In
this work, we present a simple yet effective framework, DualPrompt, which
learns a tiny set of parameters, called prompts, to properly instruct a
pre-trained model to learn tasks arriving sequentially without buffering past
examples. DualPrompt presents a novel approach to attach complementary prompts
to the pre-trained backbone, and then formulates the objective as learning
task-invariant and task-specific ""instructions"". With extensive experimental
validation, DualPrompt consistently sets state-of-the-art performance under the
challenging class-incremental setting. In particular, DualPrompt outperforms
recent advanced continual learning methods with relatively large buffer sizes.
We also introduce a more challenging benchmark, Split ImageNet-R, to help
generalize rehearsal-free continual learning research. Source code is available
at https://github.com/google-research/l2p."
4639,"We further study the role of
prompting function on Split CIFAR-100 and Split ImageNet-R.",Prefix.,"In prior prompt-
based CL work, L2P, only Pro-T is applied without further investigation.",2022-04-10 23:36:55+00:00,DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Zifeng Wang'), arxiv.Result.Author('Zizhao Zhang'), arxiv.Result.Author('Sayna Ebrahimi'), arxiv.Result.Author('Ruoxi Sun'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Chen-Yu Lee'), arxiv.Result.Author('Xiaoqi Ren'), arxiv.Result.Author('Guolong Su'), arxiv.Result.Author('Vincent Perot'), arxiv.Result.Author('Jennifer Dy'), arxiv.Result.Author('Tomas Pfister')]","Continual learning aims to enable a single model to learn a sequence of tasks
without catastrophic forgetting. Top-performing methods usually require a
rehearsal buffer to store past pristine examples for experience replay, which,
however, limits their practical value due to privacy and memory constraints. In
this work, we present a simple yet effective framework, DualPrompt, which
learns a tiny set of parameters, called prompts, to properly instruct a
pre-trained model to learn tasks arriving sequentially without buffering past
examples. DualPrompt presents a novel approach to attach complementary prompts
to the pre-trained backbone, and then formulates the objective as learning
task-invariant and task-specific ""instructions"". With extensive experimental
validation, DualPrompt consistently sets state-of-the-art performance under the
challenging class-incremental setting. In particular, DualPrompt outperforms
recent advanced continual learning methods with relatively large buffer sizes.
We also introduce a more challenging benchmark, Split ImageNet-R, to help
generalize rehearsal-free continual learning research. Source code is available
at https://github.com/google-research/l2p."
4641,"4.3 Effect of Augmentations on Features

To further study the effect of the commonly used graph    Table 2: Distance between original node
augmentation method, attribute masking, on node at-       features and augmented node features with
tribute from spectral view.","Our conclusion is aligned with the previous works [38, 42] in the graph adversarial attack domain,

in which they ﬁnd that, for the graph structure modiﬁcation, perturbations on the low-frequency

components are smaller than that in the middle or high-frequency ones.",We denote the Fourier trans-  30% attribute dropping.,2022-04-11 05:37:03+00:00,Augmentation-Free Graph Contrastive Learning,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Haonan Wang'), arxiv.Result.Author('Jieyu Zhang'), arxiv.Result.Author('Qi Zhu'), arxiv.Result.Author('Wei Huang')]","Graph contrastive learning (GCL) is the most representative and prevalent
self-supervised learning approach for graph-structured data. Despite its
remarkable success, existing GCL methods highly rely on an augmentation scheme
to learn the representations invariant across different augmentation views. In
this work, we revisit such a convention in GCL through examining the effect of
augmentation techniques on graph data via the lens of spectral theory. We found
that graph augmentations preserve the low-frequency components and perturb the
middle- and high-frequency components of the graph, which contributes to the
success of GCL algorithms on homophilic graphs but hinders its application on
heterophilic graphs, due to the high-frequency preference of heterophilic data.
Motivated by this, we propose a novel, theoretically-principled, and
augmentation-free GCL method, named AF-GCL, that (1) leverages the features
aggregated by Graph Neural Network to construct the self-supervision signal
instead of augmentations and therefore (2) is less sensitive to the graph
homophily degree. Theoretically, We present the performance guarantee for
AF-GCL as well as an analysis for understanding the efficacy of AF-GCL.
Extensive experiments on 14 benchmark datasets with varying degrees of
heterophily show that AF-GCL presents competitive or better performance on
homophilic graphs and outperforms all existing state-of-the-art GCL methods on
heterophilic graphs with significantly less computational overhead."
4642,"4.3 Effect of Augmentations on Features

To further study the effect of the commonly used graph aug-              Table 2: Distance between original
mentation method, attribute masking, on node attribute from              node features and augmented node
spectral view.","Our conclusion is aligned

with the previous works [6, 4] in the graph adversarial attack domain, in which they ﬁnd that, for the

graph structure modiﬁcation, perturbations on the low-frequency components are smaller than that

in the middle or high-frequency ones.","We denote the Fourier transform and inverse               features with 30% attribute drop-
Fourier transform as F (·) and F −1(·).",2022-04-11 05:37:03+00:00,Augmentation-Free Graph Contrastive Learning with Performance Guarantee,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Haonan Wang'), arxiv.Result.Author('Jieyu Zhang'), arxiv.Result.Author('Qi Zhu'), arxiv.Result.Author('Wei Huang')]","Graph contrastive learning (GCL) is the most representative and prevalent
self-supervised learning approach for graph-structured data. Despite its
remarkable success, existing GCL methods highly rely on an augmentation scheme
to learn the representations invariant across different augmentation views. In
this work, we revisit such a convention in GCL through examining the effect of
augmentation techniques on graph data via the lens of spectral theory. We found
that graph augmentations preserve the low-frequency components and perturb the
middle-and high-frequency components of the graph, which contributes to the
success of GCL algorithms on homophilic graphs but hinder its application on
heterophilic graphs, due to the high-frequency preference of heterophilic data.
Motivated by this, we propose a novel, theoretically-principled, and
augmentation-free GCL method, named AF-GCL, that (1) leverages the features
aggregated by Graph Neural Network to construct the self-supervision signal
instead of augmentations and therefore (2) is less sensitive to the graph
homophily degree. Theoretically, We present the performance guarantee for
AF-GCL as well as an analysis for understanding the efficacy of AF-GCL.
Extensive experiments on 14 benchmark datasets with varying degrees of
heterophily show that AF-GCL presents competitive or better performance on
homophilic graphs and outperforms all existing state-of-the-art GCL methods on
heterophilic graphs with significantly less computational overhead."
4656,"On the other hand, to inspire the community
                                                                          for further research on federated HPO, we provide functionalities
   Firstly, from the perspective of infrastructure, FederatedScope        for users to develop new FL algorithms, and an easy-to-use monitor
adopts an event-driven architecture for flexibly and conveniently         to show the FL training process through several local and global
describing asynchronous federated learning with heterogeneous             metrics.",plications in practice.,information exchanging.,2022-04-11 11:24:21+00:00,FederatedScope: A Flexible Federated Learning Platform for Heterogeneity,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yuexiang Xie'), arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Daoyuan Chen'), arxiv.Result.Author('Dawei Gao'), arxiv.Result.Author('Liuyi Yao'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding'), arxiv.Result.Author('Jingren Zhou')]","Although remarkable progress has been made by the existing federated learning
(FL) platforms to provide fundamental functionalities for development, these
platforms cannot well tackle the challenges brought by the heterogeneity of FL
scenarios from both academia and industry. To fill this gap, in this paper, we
propose a flexible federated learning platform, named FederatedScope, for
handling various types of heterogeneity in FL. Considering both flexibility and
extendability, FederatedScope adopts an event-driven architecture to
conveniently support asynchronous training protocol in practical FL
applications, and abstracts the exchanged information as messages and the
behaviors of participants as handling functions. For a new FL application,
developers only need to specify the adopted FL algorithm by defining new types
of exchanged messages and the corresponding handling functions based on
participants' behaviors, which would be automatically executed in an
asynchronous way for balancing effectiveness and efficiency in FederatedScope.
Meanwhile, towards an easy-to-use platform, FederatedScope provides rich
built-in algorithms, including personalization, federated aggregation, privacy
protection, and privacy attack, for users to conveniently customize
participant-specific training, fusing, aggregating, and protecting. Besides, a
federated hyperparameter optimization module is integrated into FederatedScope
for users to automatically tune their FL systems for resolving the unstable
issues brought by heterogeneity. We conduct a series of experiments on the
provided easy-to-use and comprehensive FL benchmarks to validate the
correctness and efficiency of FederatedScope. We have released FederatedScope
for users on https://github.com/alibaba/FederatedScope to promote research and
industrial deployment of federated learning in a variety of real-world
applications."
4657,"On the other hand, to inspire the community
to handle the heterogeneity of FL considering both flexibility and        for further research on federated HPO, we provide functionalities
convenience; specifically, we study the design and implementation         for users to develop new FL algorithms, and an easy-to-use monitor
of FederatedScope from the following perspectives.","Motivated by these, in this paper,       PBT [37]) under low-fidelity setting [42] to enable HPO for FL ap-
we propose FederatedScope, a novel federated learning platform            plications in practice.","to show the FL training process through several local and global
                                                                          metrics.",2022-04-11 11:24:21+00:00,FederatedScope: A Flexible Federated Learning Platform for Heterogeneity,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yuexiang Xie'), arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Daoyuan Chen'), arxiv.Result.Author('Dawei Gao'), arxiv.Result.Author('Liuyi Yao'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding'), arxiv.Result.Author('Jingren Zhou')]","Although remarkable progress has been made by the existing federated learning
(FL) platforms to provide fundamental functionalities for development, these
platforms cannot well tackle the challenges brought by the heterogeneity of FL
scenarios from both academia and industry. To fill this gap, in this paper, we
propose a flexible federated learning platform, named FederatedScope, for
handling various types of heterogeneity in FL. Considering both flexibility and
extensibility, FederatedScope adopts an event-driven architecture to frame an
FL course into event-handler pairs: the behaviors of participants are described
in handlers, and triggered by events of message passing or meeting certain
conditions in training. For a new FL application, developers only need to
specify the adopted FL algorithm by defining new types of events and the
corresponding handling functions based on participants' behaviors, which would
be automatically executed in an asynchronous way for balancing effectiveness
and efficiency in FederatedScope. Meanwhile, towards an easy-to-use platform,
FederatedScope provides rich built-in algorithms, including personalization,
federated aggregation, privacy protection, and privacy attack, for users to
conveniently customize participant-specific training, fusing, aggregating, and
protecting. Besides, a federated hyperparameter optimization module is
integrated into FederatedScope for users to automatically tune their FL systems
for resolving the unstable issues brought by heterogeneity. We conduct a series
of experiments on the provided easy-to-use and comprehensive FL benchmarks to
validate the correctness and efficiency of FederatedScope. We have released
FederatedScope for users on https://github.com/alibaba/FederatedScope to
promote research and industrial deployment of federated learning in a variety
of real-world applications."
4715,"data that comes from the graph structures, and we design a federal            Users often have to implement their FL algorithms with declarative
random graph model for the corresponding further study.","Then
worth mentioning that we identify a unique covariate shift of graph           each specific part is executed by the corresponding participant.","programming (i.e., describing the computational graph), which
                                                                              raises the bar for developers.",2022-04-12 06:48:06+00:00,"FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yuexiang Xie'), arxiv.Result.Author('Liuyi Yao'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding'), arxiv.Result.Author('Jingren Zhou')]","The incredible development of federated learning (FL) has benefited various
tasks in the domains of computer vision and natural language processing, and
the existing frameworks such as TFF and FATE has made the deployment easy in
real-world applications. However, federated graph learning (FGL), even though
graph data are prevalent, has not been well supported due to its unique
characteristics and requirements. The lack of FGL-related framework increases
the efforts for accomplishing reproducible research and deploying in real-world
applications. Motivated by such strong demand, in this paper, we first discuss
the challenges in creating an easy-to-use FGL package and accordingly present
our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified
view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo
and ModelZoo for out-of-the-box FGL capability; (3) an efficient model
auto-tuning component; and (4) off-the-shelf privacy attack and defense
abilities. We validate the effectiveness of FS-G by conducting extensive
experiments, which simultaneously gains many valuable insights about FGL for
the community. Moreover, we employ FS-G to serve the FGL application in
real-world E-commerce scenarios, where the attained improvements indicate great
potential business benefits. We publicly release FS-G at
https://github.com/alibaba/FederatedScope to promote FGL's research and enable
broad applications that would otherwise be infeasible due to the lack of a
dedicated package."
4716,"graph data that comes from the graph structures, and we design a              Users often have to implement their FL algorithms with declara-
federal random graph model for the corresponding further study.","Then
It is worth mentioning that we identify a unique covariate shift of           each specific part is executed by the corresponding participant.","tive programming (i.e., describing the computational graph), which
                                                                              raises the bar for developers.",2022-04-12 06:48:06+00:00,"FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yuexiang Xie'), arxiv.Result.Author('Liuyi Yao'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding'), arxiv.Result.Author('Jingren Zhou')]","The incredible development of federated learning (FL) has benefited various
tasks in the domains of computer vision and natural language processing, and
the existing frameworks such as TFF and FATE has made the deployment easy in
real-world applications. However, federated graph learning (FGL), even though
graph data are prevalent, has not been well supported due to its unique
characteristics and requirements. The lack of FGL-related framework increases
the efforts for accomplishing reproducible research and deploying in real-world
applications. Motivated by such strong demand, in this paper, we first discuss
the challenges in creating an easy-to-use FGL package and accordingly present
our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified
view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo
and ModelZoo for out-of-the-box FGL capability; (3) an efficient model
auto-tuning component; and (4) off-the-shelf privacy attack and defense
abilities. We validate the effectiveness of FS-G by conducting extensive
experiments, which simultaneously gains many valuable insights about FGL for
the community. Moreover, we employ FS-G to serve the FGL application in
real-world E-commerce scenarios, where the attained improvements indicate great
potential business benefits. We publicly release FS-G, as submodules of
FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's
research and enable broad applications that would otherwise be infeasible due
to the lack of a dedicated package."
4717,"graph data that comes from the graph structures, and we design a              Users often have to implement their FL algorithms with declara-
federal random graph model for the corresponding further study.","Then
It is worth mentioning that we identify a unique covariate shift of           each specific part is executed by the corresponding participant.","tive programming (i.e., describing the computational graph), which
                                                                              raises the bar for developers.",2022-04-12 06:48:06+00:00,"FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yuexiang Xie'), arxiv.Result.Author('Liuyi Yao'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding'), arxiv.Result.Author('Jingren Zhou')]","The incredible development of federated learning (FL) has benefited various
tasks in the domains of computer vision and natural language processing, and
the existing frameworks such as TFF and FATE has made the deployment easy in
real-world applications. However, federated graph learning (FGL), even though
graph data are prevalent, has not been well supported due to its unique
characteristics and requirements. The lack of FGL-related framework increases
the efforts for accomplishing reproducible research and deploying in real-world
applications. Motivated by such strong demand, in this paper, we first discuss
the challenges in creating an easy-to-use FGL package and accordingly present
our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified
view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo
and ModelZoo for out-of-the-box FGL capability; (3) an efficient model
auto-tuning component; and (4) off-the-shelf privacy attack and defense
abilities. We validate the effectiveness of FS-G by conducting extensive
experiments, which simultaneously gains many valuable insights about FGL for
the community. Moreover, we employ FS-G to serve the FGL application in
real-world E-commerce scenarios, where the attained improvements indicate great
potential business benefits. We publicly release FS-G, as submodules of
FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's
research and enable broad applications that would otherwise be infeasible due
to the lack of a dedicated package."
4718,"In contrast, FGL algorithms [36, 37, 42] often require several    graph data that comes from the graph structures, and we design a
heterogeneous data (e.g., gradients, node embeddings, encrypted         federal random graph model for the corresponding further study.","exchanges homogeneous data (here model parameters) for one              It is worth mentioning that we identify a unique covariate shift of
pass.","adjacency lists, etc.)",2022-04-12 06:48:06+00:00,"FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yuexiang Xie'), arxiv.Result.Author('Liuyi Yao'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding'), arxiv.Result.Author('Jingren Zhou')]","The incredible development of federated learning (FL) has benefited various
tasks in the domains of computer vision and natural language processing, and
the existing frameworks such as TFF and FATE has made the deployment easy in
real-world applications. However, federated graph learning (FGL), even though
graph data are prevalent, has not been well supported due to its unique
characteristics and requirements. The lack of FGL-related framework increases
the efforts for accomplishing reproducible research and deploying in real-world
applications. Motivated by such strong demand, in this paper, we first discuss
the challenges in creating an easy-to-use FGL package and accordingly present
our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified
view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo
and ModelZoo for out-of-the-box FGL capability; (3) an efficient model
auto-tuning component; and (4) off-the-shelf privacy attack and defense
abilities. We validate the effectiveness of FS-G by conducting extensive
experiments, which simultaneously gains many valuable insights about FGL for
the community. Moreover, we employ FS-G to serve the FGL application in
real-world E-commerce scenarios, where the attained improvements indicate great
potential business benefits. We publicly release FS-G, as submodules of
FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's
research and enable broad applications that would otherwise be infeasible due
to the lack of a dedicated package."
4719,"In contrast, FGL algorithms [36, 37, 42] often require several    graph data that comes from the graph structures, and we design a
heterogeneous data (e.g., gradients, node embeddings, encrypted         federal random graph model for the corresponding further study.","exchanges homogeneous data (here model parameters) for one              It is worth mentioning that we identify a unique covariate shift of
pass.","adjacency lists, etc.)",2022-04-12 06:48:06+00:00,"FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhen Wang'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yuexiang Xie'), arxiv.Result.Author('Liuyi Yao'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding'), arxiv.Result.Author('Jingren Zhou')]","The incredible development of federated learning (FL) has benefited various
tasks in the domains of computer vision and natural language processing, and
the existing frameworks such as TFF and FATE has made the deployment easy in
real-world applications. However, federated graph learning (FGL), even though
graph data are prevalent, has not been well supported due to its unique
characteristics and requirements. The lack of FGL-related framework increases
the efforts for accomplishing reproducible research and deploying in real-world
applications. Motivated by such strong demand, in this paper, we first discuss
the challenges in creating an easy-to-use FGL package and accordingly present
our implemented package FederatedScope-GNN (FS-G), which provides (1) a unified
view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo
and ModelZoo for out-of-the-box FGL capability; (3) an efficient model
auto-tuning component; and (4) off-the-shelf privacy attack and defense
abilities. We validate the effectiveness of FS-G by conducting extensive
experiments, which simultaneously gains many valuable insights about FGL for
the community. Moreover, we employ FS-G to serve the FGL application in
real-world E-commerce scenarios, where the attained improvements indicate great
potential business benefits. We publicly release FS-G, as submodules of
FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's
research and enable broad applications that would otherwise be infeasible due
to the lack of a dedicated package."
4751,"This suggests that continual
                                                                        learning, especially in complex scenarios with a large num-
22:  γ∗, ξ∗    =  Optimize(gΩ, Xk  ∪  M  x  ,  Yk  ∪         using      ber of classes and high dimensional data, is far to be solved,
                                         k                              and further research should be devoted to this ﬁeld.","However, considering that top-1 ImageNet accuracy for a
20: Block classiﬁer parameters (φ, ψ)                                   ResNet-18, when trained on the entire dataset, is 69.76%1,
                                                                        even for the best methods the accuracy gap in the continual
21: Unlock generator parameters (γ, ξ)                Myk )             learning setup is very large.","Equation A.5

23: end for

    With this experiment we want to assess the performance              Appendix C. Classiﬁer hyper-parameters
of AR1 in a complex continual learning scenario, validating             Appendix C.1.",2022-04-12 14:38:00+00:00,Generative Negative Replay for Continual Learning,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Gabriele Graffieti'), arxiv.Result.Author('Davide Maltoni'), arxiv.Result.Author('Lorenzo Pellegrini'), arxiv.Result.Author('Vincenzo Lomonaco')]","Learning continually is a key aspect of intelligence and a necessary ability
to solve many real-life problems. One of the most effective strategies to
control catastrophic forgetting, the Achilles' heel of continual learning, is
storing part of the old data and replaying them interleaved with new
experiences (also known as the replay approach). Generative replay, which is
using generative models to provide replay patterns on demand, is particularly
intriguing, however, it was shown to be effective mainly under simplified
assumptions, such as simple scenarios and low-dimensional data. In this paper,
we show that, while the generated data are usually not able to improve the
classification accuracy for the old classes, they can be effective as negative
examples (or antagonists) to better learn the new classes, especially when the
learning experiences are small and contain examples of just one or few classes.
The proposed approach is validated on complex class-incremental and
data-incremental continual learning scenarios (CORe50 and ImageNet-1000)
composed of high-dimensional data and a large number of training experiences: a
setup where existing generative replay approaches usually fail."
4781,"We believe that the network dynamics model based on GRL deserves
further research and investigation.","To date, there has been little research on the use of
GRL methods for network dynamics, and the existing methods have focused only on problems such
as epidemic disease control.","33
.",2022-04-13 01:25:58+00:00,Reinforcement Learning on Graph: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nie Mingshuo'), arxiv.Result.Author('Chen Dongming'), arxiv.Result.Author('Wang Dongqi')]","Graph mining tasks arise from many different application domains, ranging
from social networks, transportation, E-commerce, etc., which have been
receiving great attention from the theoretical and algorithm design communities
in recent years, and there has been some pioneering work using the hotly
researched reinforcement learning (RL) techniques to address graph data mining
tasks. However, these graph mining algorithms and RL models are dispersed in
different research areas, which makes it hard to compare different algorithms
with each other. In this survey, we provide a comprehensive overview of RL
models and graph mining and generalize these algorithms to Graph Reinforcement
Learning (GRL) as a unified formulation. We further discuss the applications of
GRL methods across various domains and summarize the method description,
open-source codes, and benchmark datasets of GRL methods. Finally, we propose
possible important directions and challenges to be solved in the future. This
is the latest work on a comprehensive survey of GRL literature, and this work
provides a global view for researchers as well as a learning resource for
researchers outside the domain. In addition, we create an online open-source
for both interested researchers who want to enter this rapidly developing
domain and experts who would like to compare GRL methods."
4828,This experiment incidates that further study of the case p = 1 is warranted.,"Indeed, all Wp distances are based on smoothly distorting one image to the
other.","4.000 Dilation grid            0.4 Wassmap (p = 2)     Wassmap (p = 1)

    3.125                          0.2                  0.2
    2.250                          0.0                  0.0
    1.375                          0.2                  0.2
    0.5000.50 0.88 1.25 1.62 2.00  0.4 0.5 0.0 0.5      0.4 0.5 0.0 0.5

      Fig.",2022-04-13 21:43:28+00:00,Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML', '68T10, 49Q22']","[arxiv.Result.Author('Keaton Hamm'), arxiv.Result.Author('Nick Henscheid'), arxiv.Result.Author('Shujie Kang')]","In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a
nonlinear dimensionality reduction technique that provides solutions to some
drawbacks in existing global nonlinear dimensionality reduction algorithms in
imaging applications. Wassmap represents images via probability measures in
Wasserstein space, then uses pairwise Wasserstein distances between the
associated measures to produce a low-dimensional, approximately isometric
embedding. We show that the algorithm is able to exactly recover parameters of
some image manifolds including those generated by translations or dilations of
a fixed generating measure. Additionally, we show that a discrete version of
the algorithm retrieves parameters from manifolds generated from discrete
measures by providing a theoretical bridge to transfer recovery results from
functional data to discrete data. Testing of the proposed algorithms on various
image data manifolds show that Wassmap yields good embeddings compared with
other global and local techniques."
4849,"Our hope is that this will inspire further research into the particularly under-studied area of
GCEs, and prove useful as the development of explainability tools grows in the coming years.","We propose improvements to the AReS framework that speed up the generation
of GCEs by orders of magnitude, also witnessing signiﬁcant accuracy improvements on continuous
data.","5
Published as a workshop paper at ICLR 2022

Acknowledgments We thank the original authors Kaivalya Rawal and Himabindu Lakkaraju for
their helpful discussion of the proposed AReS framework in Rawal & Lakkaraju (2020).",2022-04-14 12:21:23+00:00,"Global Counterfactual Explanations: Investigations, Implementations and Improvements",cs.LG,"['cs.LG', 'cs.AI', 'cs.CY', 'stat.ML']","[arxiv.Result.Author('Dan Ley'), arxiv.Result.Author('Saumitra Mishra'), arxiv.Result.Author('Daniele Magazzeni')]","Counterfactual explanations have been widely studied in explainability, with
a range of application dependent methods emerging in fairness, recourse and
model understanding. However, the major shortcoming associated with these
methods is their inability to provide explanations beyond the local or
instance-level. While some works touch upon the notion of a global explanation,
typically suggesting to aggregate masses of local explanations in the hope of
ascertaining global properties, few provide frameworks that are either reliable
or computationally tractable. Meanwhile, practitioners are requesting more
efficient and interactive explainability tools. We take this opportunity to
investigate existing global methods, with a focus on implementing and improving
Actionable Recourse Summaries (AReS), the only known global counterfactual
explanation framework for recourse."
4951,"There are some new problems left from the work that de-          Diao, E., Ding, J., and Tarokh, V. Gradient assisted learn-
serve further research.","ing Representations (ICLR), 2020.","First, in many practical applica-           ing.",2022-04-17 19:02:25+00:00,Self-Aware Personalized Federated Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Huili Chen'), arxiv.Result.Author('Jie Ding'), arxiv.Result.Author('Eric Tramel'), arxiv.Result.Author('Shuang Wu'), arxiv.Result.Author('Anit Kumar Sahu'), arxiv.Result.Author('Salman Avestimehr'), arxiv.Result.Author('Tao Zhang')]","In the context of personalized federated learning (FL), the critical
challenge is to balance local model improvement and global model tuning when
the personal and global objectives may not be exactly aligned. Inspired by
Bayesian hierarchical models, we develop a self-aware personalized FL method
where each client can automatically balance the training of its local personal
model and the global model that implicitly contributes to other clients'
training. Such a balance is derived from the inter-client and intra-client
uncertainty quantification. A larger inter-client variation implies more
personalization is needed. Correspondingly, our method uses uncertainty-driven
local training steps and aggregation rule instead of conventional local
fine-tuning and sample size-based aggregation. With experimental studies on
synthetic data, Amazon Alexa audio data, and public datasets such as MNIST,
FEMNIST, CIFAR10, and Sent140, we show that our proposed method can achieve
significantly improved personalization performance compared with the existing
counterparts."
4987,"Hence,
further research in this area can improve the overall performance of the resulting
AutoML pipelines.","On the other hand, skip-
ping this phase or using the wrong feature engineering preprocessors makes it harder
to achieve relatively high accuracy, even for the most efﬁcient classiﬁers.","7 Conclusion

In this paper, we present a comprehensive evaluation and comparison of the perfor-
mance characteristics of six AutoML frameworks on 100 datasets from OpenML.",2022-04-18 15:03:53+00:00,AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Hassan Eldeeb'), arxiv.Result.Author('Mohamed Maher'), arxiv.Result.Author('Oleh Matsuk'), arxiv.Result.Author('Abdelrahman Aldallal'), arxiv.Result.Author('Radwa Elshawi'), arxiv.Result.Author('Sherif Sakr')]","Nowadays, machine learning is playing a crucial role in harnessing the power
of the massive amounts of data that we are currently producing every day in our
digital world. With the booming demand for machine learning applications, it
has been recognized that the number of knowledgeable data scientists can not
scale with the growing data volumes and application needs in our digital world.
In response to this demand, several automated machine learning (AutoML)
techniques and frameworks have been developed to fill the gap of human
expertise by automating the process of building machine learning pipelines. In
this study, we present a comprehensive evaluation and comparison of the
performance characteristics of six popular AutoML frameworks, namely,
Auto-Weka, AutoSKlearn, TPOT, Recipe, ATM, and SmartML across 100 data sets
from established AutoML benchmark suites. Our experimental evaluation considers
different aspects for its comparison including the performance impact of
several design decisions including time budget, size of search space,
meta-learning, and ensemble construction. The results of our study reveal
various interesting insights that can significantly guide and impact the design
of AutoML frameworks."
5064,"Furthermore, as there is a growing
trend in modelling towards the combination of deep learning algorithms with existing choice
models, further researchers can make a comparison between the performance of the Ordinal-
ResLogit model and other learning-based ordinal models.","For instance, in our study, the impact of night
conditions was not logical due to the design of the VR data.","30
References

Ben-Akiva, M.E., Lerman, S. R. (1985).",2022-04-20 02:14:28+00:00,Ordinal-ResLogit: Interpretable Deep Residual Neural Networks for Ordered Choices,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kimia Kamal'), arxiv.Result.Author('Bilal Farooq')]","This study presents an Ordinal version of Residual Logit (Ordinal-ResLogit)
model to investigate the ordinal responses. We integrate the standard ResLogit
model into COnsistent RAnk Logits (CORAL) framework, classified as a binary
classification algorithm, to develop a fully interpretable deep learning-based
ordinal regression model. As the formulation of the Ordinal-ResLogit model
enjoys the Residual Neural Networks concept, our proposed model addresses the
main constraint of machine learning algorithms, known as black-box. Moreover,
the Ordinal-ResLogit model, as a binary classification framework for ordinal
data, guarantees consistency among binary classifiers. We showed that the
resulting formulation is able to capture underlying unobserved heterogeneity
from the data as well as being an interpretable deep learning-based model.
Formulations for market share, substitution patterns, and elasticities are
derived. We compare the performance of the Ordinal-ResLogit model with an
Ordered Logit Model using a stated preference (SP) dataset on pedestrian wait
time and a revealed preference (RP) dataset on travel distance. Our results
show that Ordinal-ResLogit outperforms the traditional ordinal regression model
for both datasets. Furthermore, the results obtained from the Ordinal-ResLogit
RP model show that travel attributes such as driving and transit cost have
significant effects on choosing the location of non-mandatory trips. In terms
of the Ordinal-ResLogit SP model, our results highlight that the road-related
variables and traffic condition are contributing factors in the prediction of
pedestrian waiting time such that the mixed traffic condition significantly
increases the probability of choosing longer waiting times."
5065,"Furthermore, as there is a growing
trend in modelling towards the combination of deep learning algorithms with existing choice
models, further researchers can make a comparison between the performance of the Ordinal-
ResLogit model and other learning-based ordinal models.","For instance, in our study, the impact of night
conditions was not logical due to the design of the VR data.","Acknowledgments
The financial support of the Natural Sciences and Engineering Research Council of Canada
(NSERC grant #RGPIN-2020-04492) and Canada Research Program (grant # CRC-2017-00038)
is gratefully acknowledged.",2022-04-20 02:14:28+00:00,Ordinal-ResLogit: Interpretable Deep Residual Neural Networks for Ordered Choices,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kimia Kamal'), arxiv.Result.Author('Bilal Farooq')]","This study presents an Ordinal version of Residual Logit (Ordinal-ResLogit)
model to investigate the ordinal responses. We integrate the standard ResLogit
model into COnsistent RAnk Logits (CORAL) framework, classified as a binary
classification algorithm, to develop a fully interpretable deep learning-based
ordinal regression model. As the formulation of the Ordinal-ResLogit model
enjoys the Residual Neural Networks concept, our proposed model addresses the
main constraint of machine learning algorithms, known as black-box. Moreover,
the Ordinal-ResLogit model, as a binary classification framework for ordinal
data, guarantees consistency among binary classifiers. We showed that the
resulting formulation is able to capture underlying unobserved heterogeneity
from the data as well as being an interpretable deep learning-based model.
Formulations for market share, substitution patterns, and elasticities are
derived. We compare the performance of the Ordinal-ResLogit model with an
Ordered Logit Model using a stated preference (SP) dataset on pedestrian wait
time and a revealed preference (RP) dataset on travel distance. Our results
show that Ordinal-ResLogit outperforms the traditional ordinal regression model
for both datasets. Furthermore, the results obtained from the Ordinal-ResLogit
RP model show that travel attributes such as driving and transit cost have
significant effects on choosing the location of non-mandatory trips. In terms
of the Ordinal-ResLogit SP model, our results highlight that the road-related
variables and traffic condition are contributing factors in the prediction of
pedestrian waiting time such that the mixed traffic condition significantly
increases the probability of choosing longer waiting times."
5067,"In this work we look to further study this        pling bias can be explained from the data imbalance per-
interference as it is currently under explored.","As mentioned, we propose that the subnet sam-
tween subnets.",spective.,2022-04-20 03:33:10+00:00,Does Interference Exist When Training a Once-For-All Network?,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Jordan Shipard'), arxiv.Result.Author('Arnold Wiliem'), arxiv.Result.Author('Clinton Fookes')]","The Once-For-All (OFA) method offers an excellent pathway to deploy a trained
neural network model into multiple target platforms by utilising the
supernet-subnet architecture. Once trained, a subnet can be derived from the
supernet (both architecture and trained weights) and deployed directly to the
target platform with little to no retraining or fine-tuning. To train the
subnet population, OFA uses a novel training method called Progressive
Shrinking (PS) which is designed to limit the negative impact of interference
during training. It is believed that higher interference during training
results in lower subnet population accuracies. In this work we take a second
look at this interference effect. Surprisingly, we find that interference
mitigation strategies do not have a large impact on the overall subnet
population performance. Instead, we find the subnet architecture selection bias
during training to be a more important aspect. To show this, we propose a
simple-yet-effective method called Random Subnet Sampling (RSS), which does not
have mitigation on the interference effect. Despite no mitigation, RSS is able
to produce a better performing subnet population than PS in four
small-to-medium-sized datasets; suggesting that the interference effect does
not play a pivotal role in these datasets. Due to its simplicity, RSS provides
a $1.9\times$ reduction in training times compared to PS. A $6.1\times$
reduction can also be achieved with a reasonable drop in performance when the
number of RSS training epochs are reduced. Code available at
https://github.com/Jordan-HS/RSS-Interference-CVPRW2022."
5068,"[11] states that the weights in the        We follow the problem deﬁnition presented in the orig-
supernet are deeply coupled, although no further study to        inal OFA work [4].",Guo et al.,"Let archi ∈ A be the i-th subnet ar-
explain the underlying mechanism of coupling is provided.",2022-04-20 03:33:10+00:00,Does Interference Exist When Training a Once-For-All Network?,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Jordan Shipard'), arxiv.Result.Author('Arnold Wiliem'), arxiv.Result.Author('Clinton Fookes')]","The Once-For-All (OFA) method offers an excellent pathway to deploy a trained
neural network model into multiple target platforms by utilising the
supernet-subnet architecture. Once trained, a subnet can be derived from the
supernet (both architecture and trained weights) and deployed directly to the
target platform with little to no retraining or fine-tuning. To train the
subnet population, OFA uses a novel training method called Progressive
Shrinking (PS) which is designed to limit the negative impact of interference
during training. It is believed that higher interference during training
results in lower subnet population accuracies. In this work we take a second
look at this interference effect. Surprisingly, we find that interference
mitigation strategies do not have a large impact on the overall subnet
population performance. Instead, we find the subnet architecture selection bias
during training to be a more important aspect. To show this, we propose a
simple-yet-effective method called Random Subnet Sampling (RSS), which does not
have mitigation on the interference effect. Despite no mitigation, RSS is able
to produce a better performing subnet population than PS in four
small-to-medium-sized datasets; suggesting that the interference effect does
not play a pivotal role in these datasets. Due to its simplicity, RSS provides
a $1.9\times$ reduction in training times compared to PS. A $6.1\times$
reduction can also be achieved with a reasonable drop in performance when the
number of RSS training epochs are reduced. Code available at
https://github.com/Jordan-HS/RSS-Interference-CVPRW2022."
5087,"In future work, further study on leveraging the ﬂexibility of SAAC to
incorporate more safety constraints is anticipated.","Our algorithm leads to better
risk-sensitive performance than SAC and the risk-sensitive distributional RL baselines in all
these environments.","References

Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans.",2022-04-20 12:32:33+00:00,SAAC: Safe Reinforcement Learning as an Adversarial Game of Actor-Critics,cs.LG,"['cs.LG', 'cs.AI', 'cs.GT']","[arxiv.Result.Author('Yannis Flet-Berliac'), arxiv.Result.Author('Debabrota Basu')]","Although Reinforcement Learning (RL) is effective for sequential
decision-making problems under uncertainty, it still fails to thrive in
real-world systems where risk or safety is a binding constraint. In this paper,
we formulate the RL problem with safety constraints as a non-zero-sum game.
While deployed with maximum entropy RL, this formulation leads to a safe
adversarially guided soft actor-critic framework, called SAAC. In SAAC, the
adversary aims to break the safety constraint while the RL agent aims to
maximize the constrained value function given the adversary's policy. The
safety constraint on the agent's value function manifests only as a repulsion
term between the agent's and the adversary's policies. Unlike previous
approaches, SAAC can address different safety criteria such as safe
exploration, mean-variance risk sensitivity, and CVaR-like coherent risk
sensitivity. We illustrate the design of the adversary for these constraints.
Then, in each of these variations, we show the agent differentiates itself from
the adversary's unsafe actions in addition to learning to solve the task.
Finally, for challenging continuous control tasks, we demonstrate that SAAC
achieves faster convergence, better efficiency, and fewer failures to satisfy
the safety constraints than risk-averse distributional RL and risk-neutral soft
actor-critic algorithms."
5088,"In future work, further study on leveraging the ﬂexibility of SAAC to
incorporate more safety constraints is anticipated.","Our algorithm leads to better
risk-sensitive performance than SAC and the risk-sensitive distributional RL baselines in all
these environments.","References

Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans.",2022-04-20 12:32:33+00:00,SAAC: Safe Reinforcement Learning as an Adversarial Game of Actor-Critics,cs.LG,"['cs.LG', 'cs.AI', 'cs.GT']","[arxiv.Result.Author('Yannis Flet-Berliac'), arxiv.Result.Author('Debabrota Basu')]","Although Reinforcement Learning (RL) is effective for sequential
decision-making problems under uncertainty, it still fails to thrive in
real-world systems where risk or safety is a binding constraint. In this paper,
we formulate the RL problem with safety constraints as a non-zero-sum game.
While deployed with maximum entropy RL, this formulation leads to a safe
adversarially guided soft actor-critic framework, called SAAC. In SAAC, the
adversary aims to break the safety constraint while the RL agent aims to
maximize the constrained value function given the adversary's policy. The
safety constraint on the agent's value function manifests only as a repulsion
term between the agent's and the adversary's policies. Unlike previous
approaches, SAAC can address different safety criteria such as safe
exploration, mean-variance risk sensitivity, and CVaR-like coherent risk
sensitivity. We illustrate the design of the adversary for these constraints.
Then, in each of these variations, we show the agent differentiates itself from
the adversary's unsafe actions in addition to learning to solve the task.
Finally, for challenging continuous control tasks, we demonstrate that SAAC
achieves faster convergence, better efficiency, and fewer failures to satisfy
the safety constraints than risk-averse distributional RL and risk-neutral soft
actor-critic algorithms."
5090,"Finally, conclusions and directions for further research are given
in Section 6.","Section 4 describes the main methodological contributions of
this paper, while the results of the rich numerical experiments plan are reported
in Section 5.",2.,2022-04-20 16:33:01+00:00,Deep Reinforcement Learning for a Two-Echelon Supply Chain with Seasonal Demand,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', '68T07 (Primary), 90B06, 90B05 (Secondary)']","[arxiv.Result.Author('Francesco Stranieri'), arxiv.Result.Author('Fabio Stella')]","This paper leverages recent developments in reinforcement learning and deep
learning to solve the supply chain inventory management problem, a complex
sequential decision-making problem consisting of determining the optimal
quantity of products to produce and ship to different warehouses over a given
time horizon. A mathematical formulation of the stochastic two-echelon supply
chain environment is given, which allows an arbitrary number of warehouses and
product types to be managed. Additionally, an open-source library that
interfaces with deep reinforcement learning algorithms is developed and made
publicly available for solving the inventory management problem. Performances
achieved by state-of-the-art deep reinforcement learning algorithms are
compared through a rich set of numerical experiments on synthetically generated
data. The experimental plan is designed and performed, including different
structures, topologies, demands, capacities, and costs of the supply chain.
Results show that the PPO algorithm adapts very well to different
characteristics of the environment. The VPG algorithm almost always converges
to a local maximum, even if it typically achieves an acceptable performance
level. Finally, A3C is the fastest algorithm, but just like the VPG, it never
achieves the best performance when compared to PPO. In conclusion, numerical
experiments show that deep reinforcement learning performs consistently better
than standard inventory management strategies, such as the static (s,
Q)-policy. Thus, it can be considered a practical and effective option for
solving real-world instances of the stochastic two-echelon supply chain
problem."
5109,"PtychoNN [18] is proposed for phase
We also plan to evaluate performance in a cloud environment and to              retrieval in ptychography, using data generated in the early stages of
further study the scalability of fairDMS.",use in the current experiment.,experiments.,2022-04-20 23:04:05+00:00,fairDMS: Rapid Model Training by Data and Model Reuse,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ahsan Ali'), arxiv.Result.Author('Hemant Sharma'), arxiv.Result.Author('Rajkumar Kettimuthu'), arxiv.Result.Author('Peter Kenesei'), arxiv.Result.Author('Dennis Trujillo'), arxiv.Result.Author('Antonino Miceli'), arxiv.Result.Author('Ian Foster'), arxiv.Result.Author('Ryan Coffee'), arxiv.Result.Author('Jana Thayer'), arxiv.Result.Author('Zhengchun Liu')]","Extracting actionable information rapidly from data produced by instruments
such as the Linac Coherent Light Source (LCLS-II) and Advanced Photon Source
Upgrade (APS-U) is becoming ever more challenging due to high (up to TB/s) data
rates. Conventional physics-based information retrieval methods are
hard-pressed to detect interesting events fast enough to enable timely focusing
on a rare event or correction of an error. Machine learning~(ML) methods that
learn cheap surrogate classifiers present a promising alternative, but can fail
catastrophically when changes in instrument or sample result in degradation in
ML performance. To overcome such difficulties, we present a new data storage
and ML model training architecture designed to organize large volumes of data
and models so that when model degradation is detected, prior models and/or data
can be queried rapidly and a more suitable model retrieved and fine-tuned for
new conditions. We show that our approach can achieve up to 100x data labelling
speedup compared to the current state-of-the-art, 200x improvement in training
speed, and 92x speedup in-terms of end-to-end model updating time."
5114,"We further study the inﬂuence of clustering on the model per-                                     4.7 Experiments in Appendix
formance.","In our ap-
                                                                                                  proach, we choose = 4 that can yield a good performance
4.4 Inﬂuence of Clustering                                                                        and satisfactory user privacy protection ability.","The results without clustering, with Kmeans clus-
tering, or with Ward clustering are compared in Fig.",2022-04-21 02:37:10+00:00,FedCL: Federated Contrastive Learning for Privacy-Preserving Recommendation,cs.LG,['cs.LG'],"[arxiv.Result.Author('Chuhan Wu'), arxiv.Result.Author('Fangzhao Wu'), arxiv.Result.Author('Tao Qi'), arxiv.Result.Author('Yongfeng Huang'), arxiv.Result.Author('Xing Xie')]","Contrastive learning is widely used for recommendation model learning, where
selecting representative and informative negative samples is critical. Existing
methods usually focus on centralized data, where abundant and high-quality
negative samples are easy to obtain. However, centralized user data storage and
exploitation may lead to privacy risks and concerns, while decentralized user
data on a single client can be too sparse and biased for accurate contrastive
learning. In this paper, we propose a federated contrastive learning method
named FedCL for privacy-preserving recommendation, which can exploit
high-quality negative samples for effective model training with privacy well
protected. We first infer user embeddings from local user data through the
local model on each client, and then perturb them with local differential
privacy (LDP) before sending them to a central server for hard negative
sampling. Since individual user embedding contains heavy noise due to LDP, we
propose to cluster user embeddings on the server to mitigate the influence of
noise, and the cluster centroids are used to retrieve hard negative samples
from the item pool. These hard negative samples are delivered to user clients
and mixed with the observed negative samples from local data as well as
in-batch negatives constructed from positive samples for federated model
training. Extensive experiments on four benchmark datasets show FedCL can
empower various recommendation methods in a privacy-preserving way."
5119,"We believe that our method provides a basis
Here we study the impact of the number of inducing points                for further research in calibration of uncertainty estimates in
for the regression and classiﬁcation tasks deﬁned in the previ-          deep neural network models.","The ﬂexibility of our IGN framework enables the combina-
5 Ablation on number of inducing points                                  tion of Gaussian process training with complex deep learning
                                                                         architectures.","Furthermore, as part of future
ous sections.",2022-04-21 05:27:09+00:00,Inducing Gaussian Process Networks,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Alessandro Tibo'), arxiv.Result.Author('Thomas Dyhre Nielsen')]","Gaussian processes (GPs) are powerful but computationally expensive machine
learning models, requiring an estimate of the kernel covariance matrix for
every prediction. In large and complex domains, such as graphs, sets, or
images, the choice of suitable kernel can also be non-trivial to determine,
providing an additional obstacle to the learning task. Over the last decade,
these challenges have resulted in significant advances being made in terms of
scalability and expressivity, exemplified by, e.g., the use of inducing points
and neural network kernel approximations. In this paper, we propose inducing
Gaussian process networks (IGN), a simple framework for simultaneously learning
the feature space as well as the inducing points. The inducing points, in
particular, are learned directly in the feature space, enabling a seamless
representation of complex structured domains while also facilitating scalable
gradient-based learning methods. We consider both regression and (binary)
classification tasks and report on experimental results for real-world data
sets showing that IGNs provide significant advances over state-of-the-art
methods. We also demonstrate how IGNs can be used to effectively model complex
domains using neural network architectures."
5177,"Extensions and further research possibilities

In the preceding sections we demonstrated how the geometry of an implied volatility cube con-
taining missing values can be inferred by learning stochastic latent representations via variational
autoencoders in an approximate Gibbs sampling environment.","12
Interpolation of Missing Swaption Volatility Data using Variational Autoencoders

5.","The imputed estimates of missing
quotes were afterwards used to ﬁt the SABR stochastic model on volatilty smiles.",2022-04-21 20:37:44+00:00,Interpolation of Missing Swaption Volatility Data using Gibbs Sampling on Variational Autoencoders,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Ivo Richert'), arxiv.Result.Author('Robert Buch')]","Albeit of crucial interest for both financial practitioners and researchers,
market-implied volatility data of European swaptions often exhibit large
portions of missing quotes due to illiquidity of the various underlying
swaption instruments. In this case, standard stochastic interpolation tools
like the common SABR model often cannot be calibrated to observed implied
volatility smiles, due to data being only available for the at-the-money quote
of the respective underlying swaption. Here, we propose to infer the geometry
of the full unknown implied volatility cube by learning stochastic latent
representations of implied volatility cubes via variational autoencoders,
enabling inference about the missing volatility data conditional on the
observed data by an approximate Gibbs sampling approach. Imputed estimates of
missing quotes can afterwards be used to fit a standard stochastic volatility
model. Since training data for the employed variational autoencoder model is
usually sparsely available, we test the robustness of the approach for a model
trained on synthetic data on real market quotes and we show that SABR
interpolated volatilites calibrated to reconstructed volatility cubes with
artificially imputed missing values differ by not much more than two basis
points compared to SABR fits calibrated to the complete cube. Moreover, we show
how the imputation can be used to successfully set up delta-neutral portfolios
for hedging purposes."
5204,"Exploration of the utility for the ICD coding task of Transformer variants, which have been studied
for extreme multi-label classification in particular X-Transformer [49] and XR-Transformer [50],
could be a fruitful avenue for further research.","They found that the Transformer models
could not outperform traditional neural networks when the label size was greater than 300.","Although HiLAT provides label-wise explainability visualizations using the hierarchical attention
weights, there is a need to evaluate the quality of explanation objectively.",2022-04-22 14:12:22+00:00,Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Leibo Liu'), arxiv.Result.Author('Oscar Perez-Concha'), arxiv.Result.Author('Anthony Nguyen'), arxiv.Result.Author('Vicki Bennett'), arxiv.Result.Author('Louisa Jorm')]","International Classification of Diseases (ICD) coding plays an important role
in systematically classifying morbidity and mortality data. In this study, we
propose a hierarchical label-wise attention Transformer model (HiLAT) for the
explainable prediction of ICD codes from clinical documents. HiLAT firstly
fine-tunes a pretrained Transformer model to represent the tokens of clinical
documents. We subsequently employ a two-level hierarchical label-wise attention
mechanism that creates label-specific document representations. These
representations are in turn used by a feed-forward neural network to predict
whether a specific ICD code is assigned to the input clinical document of
interest. We evaluate HiLAT using hospital discharge summaries and their
corresponding ICD-9 codes from the MIMIC-III database. To investigate the
performance of different types of Transformer models, we develop
ClinicalplusXLNet, which conducts continual pretraining from XLNet-Base using
all the MIMIC-III clinical notes. The experiment results show that the F1
scores of the HiLAT+ClinicalplusXLNet outperform the previous state-of-the-art
models for the top-50 most frequent ICD-9 codes from MIMIC-III. Visualisations
of attention weights present a potential explainability tool for checking the
face validity of ICD code predictions."
5205,"Exploration of the utility for the ICD coding task of Transformer variants, which have been studied
for extreme multi-label classification in particular X-Transformer [50] and XR-Transformer [51],
could be a fruitful avenue for further research.","They found that the Transformer models
could not outperform traditional neural networks when the label size was greater than 300.","Although HiLAT provides label-wise explainability visualizations using the hierarchical attention
weights, there is a need to evaluate the quality of explanations objectively.",2022-04-22 14:12:22+00:00,Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Leibo Liu'), arxiv.Result.Author('Oscar Perez-Concha'), arxiv.Result.Author('Anthony Nguyen'), arxiv.Result.Author('Vicki Bennett'), arxiv.Result.Author('Louisa Jorm')]","International Classification of Diseases (ICD) coding plays an important role
in systematically classifying morbidity and mortality data. In this study, we
propose a hierarchical label-wise attention Transformer model (HiLAT) for the
explainable prediction of ICD codes from clinical documents. HiLAT firstly
fine-tunes a pretrained Transformer model to represent the tokens of clinical
documents. We subsequently employ a two-level hierarchical label-wise attention
mechanism that creates label-specific document representations. These
representations are in turn used by a feed-forward neural network to predict
whether a specific ICD code is assigned to the input clinical document of
interest. We evaluate HiLAT using hospital discharge summaries and their
corresponding ICD-9 codes from the MIMIC-III database. To investigate the
performance of different types of Transformer models, we develop
ClinicalplusXLNet, which conducts continual pretraining from XLNet-Base using
all the MIMIC-III clinical notes. The experiment results show that the F1
scores of the HiLAT+ClinicalplusXLNet outperform the previous state-of-the-art
models for the top-50 most frequent ICD-9 codes from MIMIC-III. Visualisations
of attention weights present a potential explainability tool for checking the
face validity of ICD code predictions."
5217,"The FL backend developed for this project has been open-
sourced as a separate software library, to encourage further research on FL98 and is avail-

able at https://github.com/intel/openfl.","The data augmentation was done via
GaNDLF by leveraging TorchIO97.","The optimization of the consensus model in-
ference workload was performed via OpenVINO99 (https://github.com/openvinotoolkit/

openvino/tree/2021.4.1), which is an open-source toolkit enabling acceleration of neural

network models through various optimization techniques.",2022-04-22 17:27:00+00:00,Federated Learning Enables Big Data for Rare Cancer Boundary Detection,cs.LG,"['cs.LG', 'eess.IV']","[arxiv.Result.Author('Sarthak Pati'), arxiv.Result.Author('Ujjwal Baid'), arxiv.Result.Author('Brandon Edwards'), arxiv.Result.Author('Micah Sheller'), arxiv.Result.Author('Shih-Han Wang'), arxiv.Result.Author('G Anthony Reina'), arxiv.Result.Author('Patrick Foley'), arxiv.Result.Author('Alexey Gruzdev'), arxiv.Result.Author('Deepthi Karkada'), arxiv.Result.Author('Christos Davatzikos'), arxiv.Result.Author('Chiharu Sako'), arxiv.Result.Author('Satyam Ghodasara'), arxiv.Result.Author('Michel Bilello'), arxiv.Result.Author('Suyash Mohan'), arxiv.Result.Author('Philipp Vollmuth'), arxiv.Result.Author('Gianluca Brugnara'), arxiv.Result.Author('Chandrakanth J Preetha'), arxiv.Result.Author('Felix Sahm'), arxiv.Result.Author('Klaus Maier-Hein'), arxiv.Result.Author('Maximilian Zenk'), arxiv.Result.Author('Martin Bendszus'), arxiv.Result.Author('Wolfgang Wick'), arxiv.Result.Author('Evan Calabrese'), arxiv.Result.Author('Jeffrey Rudie'), arxiv.Result.Author('Javier Villanueva-Meyer'), arxiv.Result.Author('Soonmee Cha'), arxiv.Result.Author('Madhura Ingalhalikar'), arxiv.Result.Author('Manali Jadhav'), arxiv.Result.Author('Umang Pandey'), arxiv.Result.Author('Jitender Saini'), arxiv.Result.Author('John Garrett'), arxiv.Result.Author('Matthew Larson'), arxiv.Result.Author('Robert Jeraj'), arxiv.Result.Author('Stuart Currie'), arxiv.Result.Author('Russell Frood'), arxiv.Result.Author('Kavi Fatania'), arxiv.Result.Author('Raymond Y Huang'), arxiv.Result.Author('Ken Chang'), arxiv.Result.Author('Carmen Balaña Quintero'), arxiv.Result.Author('Jaume Capellades'), arxiv.Result.Author('Josep Puig'), arxiv.Result.Author('Johannes Trenkler'), arxiv.Result.Author('Josef Pichler'), arxiv.Result.Author('Georg Necker'), arxiv.Result.Author('Andreas Haunschmidt'), arxiv.Result.Author('Stephan Meckel'), arxiv.Result.Author('Gaurav Shukla'), arxiv.Result.Author('Spencer Liem'), arxiv.Result.Author('Gregory S Alexander'), arxiv.Result.Author('Joseph Lombardo'), arxiv.Result.Author('Joshua D Palmer'), arxiv.Result.Author('Adam E Flanders'), arxiv.Result.Author('Adam P Dicker'), arxiv.Result.Author('Haris I Sair'), arxiv.Result.Author('Craig K Jones'), arxiv.Result.Author('Archana Venkataraman'), arxiv.Result.Author('Meirui Jiang'), arxiv.Result.Author('Tiffany Y So'), arxiv.Result.Author('Cheng Chen'), arxiv.Result.Author('Pheng Ann Heng'), arxiv.Result.Author('Qi Dou'), arxiv.Result.Author('Michal Kozubek'), arxiv.Result.Author('Filip Lux'), arxiv.Result.Author('Jan Michálek'), arxiv.Result.Author('Petr Matula'), arxiv.Result.Author('Miloš Keřkovský'), arxiv.Result.Author('Tereza Kopřivová'), arxiv.Result.Author('Marek Dostál'), arxiv.Result.Author('Václav Vybíhal'), arxiv.Result.Author('Michael A Vogelbaum'), arxiv.Result.Author('J Ross Mitchell'), arxiv.Result.Author('Joaquim Farinhas'), arxiv.Result.Author('Joseph A Maldjian'), arxiv.Result.Author('Chandan Ganesh Bangalore Yogananda'), arxiv.Result.Author('Marco C Pinho'), arxiv.Result.Author('Divya Reddy'), arxiv.Result.Author('James Holcomb'), arxiv.Result.Author('Benjamin C Wagner'), arxiv.Result.Author('Benjamin M Ellingson'), arxiv.Result.Author('Timothy F Cloughesy'), arxiv.Result.Author('Catalina Raymond'), arxiv.Result.Author('Talia Oughourlian'), arxiv.Result.Author('Akifumi Hagiwara'), arxiv.Result.Author('Chencai Wang'), arxiv.Result.Author('Minh-Son To'), arxiv.Result.Author('Sargam Bhardwaj'), arxiv.Result.Author('Chee Chong'), arxiv.Result.Author('Marc Agzarian'), arxiv.Result.Author('Alexandre Xavier Falcão'), arxiv.Result.Author('Samuel B Martins'), arxiv.Result.Author('Bernardo C A Teixeira'), arxiv.Result.Author('Flávia Sprenger'), arxiv.Result.Author('David Menotti'), arxiv.Result.Author('Diego R Lucio'), arxiv.Result.Author('Pamela LaMontagne'), arxiv.Result.Author('Daniel Marcus'), arxiv.Result.Author('Benedikt Wiestler'), arxiv.Result.Author('Florian Kofler'), arxiv.Result.Author('Ivan Ezhov'), arxiv.Result.Author('Marie Metz'), arxiv.Result.Author('Rajan Jain'), arxiv.Result.Author('Matthew Lee'), arxiv.Result.Author('Yvonne W Lui'), arxiv.Result.Author('Richard McKinley'), arxiv.Result.Author('Johannes Slotboom'), arxiv.Result.Author('Piotr Radojewski'), arxiv.Result.Author('Raphael Meier'), arxiv.Result.Author('Roland Wiest'), arxiv.Result.Author('Derrick Murcia'), arxiv.Result.Author('Eric Fu'), arxiv.Result.Author('Rourke Haas'), arxiv.Result.Author('John Thompson'), arxiv.Result.Author('David Ryan Ormond'), arxiv.Result.Author('Chaitra Badve'), arxiv.Result.Author('Andrew E Sloan'), arxiv.Result.Author('Vachan Vadmal'), arxiv.Result.Author('Kristin Waite'), arxiv.Result.Author('Rivka R Colen'), arxiv.Result.Author('Linmin Pei'), arxiv.Result.Author('Murat Ak'), arxiv.Result.Author('Ashok Srinivasan'), arxiv.Result.Author('J Rajiv Bapuraj'), arxiv.Result.Author('Arvind Rao'), arxiv.Result.Author('Nicholas Wang'), arxiv.Result.Author('Ota Yoshiaki'), arxiv.Result.Author('Toshio Moritani'), arxiv.Result.Author('Sevcan Turk'), arxiv.Result.Author('Joonsang Lee'), arxiv.Result.Author('Snehal Prabhudesai'), arxiv.Result.Author('Fanny Morón'), arxiv.Result.Author('Jacob Mandel'), arxiv.Result.Author('Konstantinos Kamnitsas'), arxiv.Result.Author('Ben Glocker'), arxiv.Result.Author('Luke V M Dixon'), arxiv.Result.Author('Matthew Williams'), arxiv.Result.Author('Peter Zampakis'), arxiv.Result.Author('Vasileios Panagiotopoulos'), arxiv.Result.Author('Panagiotis Tsiganos'), arxiv.Result.Author('Sotiris Alexiou'), arxiv.Result.Author('Ilias Haliassos'), arxiv.Result.Author('Evangelia I Zacharaki'), arxiv.Result.Author('Konstantinos Moustakas'), arxiv.Result.Author('Christina Kalogeropoulou'), arxiv.Result.Author('Dimitrios M Kardamakis'), arxiv.Result.Author('Yoon Seong Choi'), arxiv.Result.Author('Seung-Koo Lee'), arxiv.Result.Author('Jong Hee Chang'), arxiv.Result.Author('Sung Soo Ahn'), arxiv.Result.Author('Bing Luo'), arxiv.Result.Author('Laila Poisson'), arxiv.Result.Author('Ning Wen'), arxiv.Result.Author('Pallavi Tiwari'), arxiv.Result.Author('Ruchika Verma'), arxiv.Result.Author('Rohan Bareja'), arxiv.Result.Author('Ipsa Yadav'), arxiv.Result.Author('Jonathan Chen'), arxiv.Result.Author('Neeraj Kumar'), arxiv.Result.Author('Marion Smits'), arxiv.Result.Author('Sebastian R van der Voort'), arxiv.Result.Author('Ahmed Alafandi'), arxiv.Result.Author('Fatih Incekara'), arxiv.Result.Author('Maarten MJ Wijnenga'), arxiv.Result.Author('Georgios Kapsas'), arxiv.Result.Author('Renske Gahrmann'), arxiv.Result.Author('Joost W Schouten'), arxiv.Result.Author('Hendrikus J Dubbink'), arxiv.Result.Author('Arnaud JPE Vincent'), arxiv.Result.Author('Martin J van den Bent'), arxiv.Result.Author('Pim J French'), arxiv.Result.Author('Stefan Klein'), arxiv.Result.Author('Yading Yuan'), arxiv.Result.Author('Sonam Sharma'), arxiv.Result.Author('Tzu-Chi Tseng'), arxiv.Result.Author('Saba Adabi'), arxiv.Result.Author('Simone P Niclou'), arxiv.Result.Author('Olivier Keunen'), arxiv.Result.Author('Ann-Christin Hau'), arxiv.Result.Author('Martin Vallières'), arxiv.Result.Author('David Fortin'), arxiv.Result.Author('Martin Lepage'), arxiv.Result.Author('Bennett Landman'), arxiv.Result.Author('Karthik Ramadass'), arxiv.Result.Author('Kaiwen Xu'), arxiv.Result.Author('Silky Chotai'), arxiv.Result.Author('Lola B Chambless'), arxiv.Result.Author('Akshitkumar Mistry'), arxiv.Result.Author('Reid C Thompson'), arxiv.Result.Author('Yuriy Gusev'), arxiv.Result.Author('Krithika Bhuvaneshwar'), arxiv.Result.Author('Anousheh Sayah'), arxiv.Result.Author('Camelia Bencheqroun'), arxiv.Result.Author('Anas Belouali'), arxiv.Result.Author('Subha Madhavan'), arxiv.Result.Author('Thomas C Booth'), arxiv.Result.Author('Alysha Chelliah'), arxiv.Result.Author('Marc Modat'), arxiv.Result.Author('Haris Shuaib'), arxiv.Result.Author('Carmen Dragos'), arxiv.Result.Author('Aly Abayazeed'), arxiv.Result.Author('Kenneth Kolodziej'), arxiv.Result.Author('Michael Hill'), arxiv.Result.Author('Ahmed Abbassy'), arxiv.Result.Author('Shady Gamal'), arxiv.Result.Author('Mahmoud Mekhaimar'), arxiv.Result.Author('Mohamed Qayati'), arxiv.Result.Author('Mauricio Reyes'), arxiv.Result.Author('Ji Eun Park'), arxiv.Result.Author('Jihye Yun'), arxiv.Result.Author('Ho Sung Kim'), arxiv.Result.Author('Abhishek Mahajan'), arxiv.Result.Author('Mark Muzi'), arxiv.Result.Author('Sean Benson'), arxiv.Result.Author('Regina G H Beets-Tan'), arxiv.Result.Author('Jonas Teuwen'), arxiv.Result.Author('Alejandro Herrera-Trujillo'), arxiv.Result.Author('Maria Trujillo'), arxiv.Result.Author('William Escobar'), arxiv.Result.Author('Ana Abello'), arxiv.Result.Author('Jose Bernal'), arxiv.Result.Author('Jhon Gómez'), arxiv.Result.Author('Joseph Choi'), arxiv.Result.Author('Stephen Baek'), arxiv.Result.Author('Yusung Kim'), arxiv.Result.Author('Heba Ismael'), arxiv.Result.Author('Bryan Allen'), arxiv.Result.Author('John M Buatti'), arxiv.Result.Author('Aikaterini Kotrotsou'), arxiv.Result.Author('Hongwei Li'), arxiv.Result.Author('Tobias Weiss'), arxiv.Result.Author('Michael Weller'), arxiv.Result.Author('Andrea Bink'), arxiv.Result.Author('Bertrand Pouymayou'), arxiv.Result.Author('Hassan F Shaykh'), arxiv.Result.Author('Joel Saltz'), arxiv.Result.Author('Prateek Prasanna'), arxiv.Result.Author('Sampurna Shrestha'), arxiv.Result.Author('Kartik M Mani'), arxiv.Result.Author('David Payne'), arxiv.Result.Author('Tahsin Kurc'), arxiv.Result.Author('Enrique Pelaez'), arxiv.Result.Author('Heydy Franco-Maldonado'), arxiv.Result.Author('Francis Loayza'), arxiv.Result.Author('Sebastian Quevedo'), arxiv.Result.Author('Pamela Guevara'), arxiv.Result.Author('Esteban Torche'), arxiv.Result.Author('Cristobal Mendoza'), arxiv.Result.Author('Franco Vera'), arxiv.Result.Author('Elvis Ríos'), arxiv.Result.Author('Eduardo López'), arxiv.Result.Author('Sergio A Velastin'), arxiv.Result.Author('Godwin Ogbole'), arxiv.Result.Author('Dotun Oyekunle'), arxiv.Result.Author('Olubunmi Odafe-Oyibotha'), arxiv.Result.Author('Babatunde Osobu'), arxiv.Result.Author(""Mustapha Shu'aibu""), arxiv.Result.Author('Adeleye Dorcas'), arxiv.Result.Author('Mayowa Soneye'), arxiv.Result.Author('Farouk Dako'), arxiv.Result.Author('Amber L Simpson'), arxiv.Result.Author('Mohammad Hamghalam'), arxiv.Result.Author('Jacob J Peoples'), arxiv.Result.Author('Ricky Hu'), arxiv.Result.Author('Anh Tran'), arxiv.Result.Author('Danielle Cutler'), arxiv.Result.Author('Fabio Y Moraes'), arxiv.Result.Author('Michael A Boss'), arxiv.Result.Author('James Gimpel'), arxiv.Result.Author('Deepak Kattil Veettil'), arxiv.Result.Author('Kendall Schmidt'), arxiv.Result.Author('Brian Bialecki'), arxiv.Result.Author('Sailaja Marella'), arxiv.Result.Author('Cynthia Price'), arxiv.Result.Author('Lisa Cimino'), arxiv.Result.Author('Charles Apgar'), arxiv.Result.Author('Prashant Shah'), arxiv.Result.Author('Bjoern Menze'), arxiv.Result.Author('Jill S Barnholtz-Sloan'), arxiv.Result.Author('Jason Martin'), arxiv.Result.Author('Spyridon Bakas')]","Although machine learning (ML) has shown promise in numerous domains, there
are concerns about generalizability to out-of-sample data. This is currently
addressed by centrally sharing ample, and importantly diverse, data from
multiple sites. However, such centralization is challenging to scale (or even
not feasible) due to various limitations. Federated ML (FL) provides an
alternative to train accurate and generalizable ML models, by only sharing
numerical model updates. Here we present findings from the largest FL study
to-date, involving data from 71 healthcare institutions across 6 continents, to
generate an automatic tumor boundary detector for the rare disease of
glioblastoma, utilizing the largest dataset of such patients ever used in the
literature (25,256 MRI scans from 6,314 patients). We demonstrate a 33%
improvement over a publicly trained model to delineate the surgically
targetable tumor, and 23% improvement over the tumor's entire extent. We
anticipate our study to: 1) enable more studies in healthcare informed by large
and diverse data, ensuring meaningful results for rare diseases and
underrepresented populations, 2) facilitate further quantitative analyses for
glioblastoma via performance optimization of our consensus model for eventual
public release, and 3) demonstrate the effectiveness of FL at such scale and
task complexity as a paradigm shift for multi-site collaborations, alleviating
the need for data sharing."
5218,"The FL backend developed for this project has been open-
sourced as a separate software library, to encourage further research on FL98 and is avail-

able at https://github.com/intel/openfl.","The data augmentation was done via
GaNDLF by leveraging TorchIO97.","The optimization of the consensus model in-
ference workload was performed via OpenVINO99 (https://github.com/openvinotoolkit/

openvino/tree/2021.4.1), which is an open-source toolkit enabling acceleration of neural

network models through various optimization techniques.",2022-04-22 17:27:00+00:00,Federated Learning Enables Big Data for Rare Cancer Boundary Detection,cs.LG,"['cs.LG', 'eess.IV']","[arxiv.Result.Author('Sarthak Pati'), arxiv.Result.Author('Ujjwal Baid'), arxiv.Result.Author('Brandon Edwards'), arxiv.Result.Author('Micah Sheller'), arxiv.Result.Author('Shih-Han Wang'), arxiv.Result.Author('G Anthony Reina'), arxiv.Result.Author('Patrick Foley'), arxiv.Result.Author('Alexey Gruzdev'), arxiv.Result.Author('Deepthi Karkada'), arxiv.Result.Author('Christos Davatzikos'), arxiv.Result.Author('Chiharu Sako'), arxiv.Result.Author('Satyam Ghodasara'), arxiv.Result.Author('Michel Bilello'), arxiv.Result.Author('Suyash Mohan'), arxiv.Result.Author('Philipp Vollmuth'), arxiv.Result.Author('Gianluca Brugnara'), arxiv.Result.Author('Chandrakanth J Preetha'), arxiv.Result.Author('Felix Sahm'), arxiv.Result.Author('Klaus Maier-Hein'), arxiv.Result.Author('Maximilian Zenk'), arxiv.Result.Author('Martin Bendszus'), arxiv.Result.Author('Wolfgang Wick'), arxiv.Result.Author('Evan Calabrese'), arxiv.Result.Author('Jeffrey Rudie'), arxiv.Result.Author('Javier Villanueva-Meyer'), arxiv.Result.Author('Soonmee Cha'), arxiv.Result.Author('Madhura Ingalhalikar'), arxiv.Result.Author('Manali Jadhav'), arxiv.Result.Author('Umang Pandey'), arxiv.Result.Author('Jitender Saini'), arxiv.Result.Author('John Garrett'), arxiv.Result.Author('Matthew Larson'), arxiv.Result.Author('Robert Jeraj'), arxiv.Result.Author('Stuart Currie'), arxiv.Result.Author('Russell Frood'), arxiv.Result.Author('Kavi Fatania'), arxiv.Result.Author('Raymond Y Huang'), arxiv.Result.Author('Ken Chang'), arxiv.Result.Author('Carmen Balana'), arxiv.Result.Author('Jaume Capellades'), arxiv.Result.Author('Josep Puig'), arxiv.Result.Author('Johannes Trenkler'), arxiv.Result.Author('Josef Pichler'), arxiv.Result.Author('Georg Necker'), arxiv.Result.Author('Andreas Haunschmidt'), arxiv.Result.Author('Stephan Meckel'), arxiv.Result.Author('Gaurav Shukla'), arxiv.Result.Author('Spencer Liem'), arxiv.Result.Author('Gregory S Alexander'), arxiv.Result.Author('Joseph Lombardo'), arxiv.Result.Author('Joshua D Palmer'), arxiv.Result.Author('Adam E Flanders'), arxiv.Result.Author('Adam P Dicker'), arxiv.Result.Author('Haris I Sair'), arxiv.Result.Author('Craig K Jones'), arxiv.Result.Author('Archana Venkataraman'), arxiv.Result.Author('Meirui Jiang'), arxiv.Result.Author('Tiffany Y So'), arxiv.Result.Author('Cheng Chen'), arxiv.Result.Author('Pheng Ann Heng'), arxiv.Result.Author('Qi Dou'), arxiv.Result.Author('Michal Kozubek'), arxiv.Result.Author('Filip Lux'), arxiv.Result.Author('Jan Michálek'), arxiv.Result.Author('Petr Matula'), arxiv.Result.Author('Miloš Keřkovský'), arxiv.Result.Author('Tereza Kopřivová'), arxiv.Result.Author('Marek Dostál'), arxiv.Result.Author('Václav Vybíhal'), arxiv.Result.Author('Michael A Vogelbaum'), arxiv.Result.Author('J Ross Mitchell'), arxiv.Result.Author('Joaquim Farinhas'), arxiv.Result.Author('Joseph A Maldjian'), arxiv.Result.Author('Chandan Ganesh Bangalore Yogananda'), arxiv.Result.Author('Marco C Pinho'), arxiv.Result.Author('Divya Reddy'), arxiv.Result.Author('James Holcomb'), arxiv.Result.Author('Benjamin C Wagner'), arxiv.Result.Author('Benjamin M Ellingson'), arxiv.Result.Author('Timothy F Cloughesy'), arxiv.Result.Author('Catalina Raymond'), arxiv.Result.Author('Talia Oughourlian'), arxiv.Result.Author('Akifumi Hagiwara'), arxiv.Result.Author('Chencai Wang'), arxiv.Result.Author('Minh-Son To'), arxiv.Result.Author('Sargam Bhardwaj'), arxiv.Result.Author('Chee Chong'), arxiv.Result.Author('Marc Agzarian'), arxiv.Result.Author('Alexandre Xavier Falcão'), arxiv.Result.Author('Samuel B Martins'), arxiv.Result.Author('Bernardo C A Teixeira'), arxiv.Result.Author('Flávia Sprenger'), arxiv.Result.Author('David Menotti'), arxiv.Result.Author('Diego R Lucio'), arxiv.Result.Author('Pamela LaMontagne'), arxiv.Result.Author('Daniel Marcus'), arxiv.Result.Author('Benedikt Wiestler'), arxiv.Result.Author('Florian Kofler'), arxiv.Result.Author('Ivan Ezhov'), arxiv.Result.Author('Marie Metz'), arxiv.Result.Author('Rajan Jain'), arxiv.Result.Author('Matthew Lee'), arxiv.Result.Author('Yvonne W Lui'), arxiv.Result.Author('Richard McKinley'), arxiv.Result.Author('Johannes Slotboom'), arxiv.Result.Author('Piotr Radojewski'), arxiv.Result.Author('Raphael Meier'), arxiv.Result.Author('Roland Wiest'), arxiv.Result.Author('Derrick Murcia'), arxiv.Result.Author('Eric Fu'), arxiv.Result.Author('Rourke Haas'), arxiv.Result.Author('John Thompson'), arxiv.Result.Author('David Ryan Ormond'), arxiv.Result.Author('Chaitra Badve'), arxiv.Result.Author('Andrew E Sloan'), arxiv.Result.Author('Vachan Vadmal'), arxiv.Result.Author('Kristin Waite'), arxiv.Result.Author('Rivka R Colen'), arxiv.Result.Author('Linmin Pei'), arxiv.Result.Author('Murat Ak'), arxiv.Result.Author('Ashok Srinivasan'), arxiv.Result.Author('J Rajiv Bapuraj'), arxiv.Result.Author('Arvind Rao'), arxiv.Result.Author('Nicholas Wang'), arxiv.Result.Author('Ota Yoshiaki'), arxiv.Result.Author('Toshio Moritani'), arxiv.Result.Author('Sevcan Turk'), arxiv.Result.Author('Joonsang Lee'), arxiv.Result.Author('Snehal Prabhudesai'), arxiv.Result.Author('Fanny Morón'), arxiv.Result.Author('Jacob Mandel'), arxiv.Result.Author('Konstantinos Kamnitsas'), arxiv.Result.Author('Ben Glocker'), arxiv.Result.Author('Luke V M Dixon'), arxiv.Result.Author('Matthew Williams'), arxiv.Result.Author('Peter Zampakis'), arxiv.Result.Author('Vasileios Panagiotopoulos'), arxiv.Result.Author('Panagiotis Tsiganos'), arxiv.Result.Author('Sotiris Alexiou'), arxiv.Result.Author('Ilias Haliassos'), arxiv.Result.Author('Evangelia I Zacharaki'), arxiv.Result.Author('Konstantinos Moustakas'), arxiv.Result.Author('Christina Kalogeropoulou'), arxiv.Result.Author('Dimitrios M Kardamakis'), arxiv.Result.Author('Yoon Seong Choi'), arxiv.Result.Author('Seung-Koo Lee'), arxiv.Result.Author('Jong Hee Chang'), arxiv.Result.Author('Sung Soo Ahn'), arxiv.Result.Author('Bing Luo'), arxiv.Result.Author('Laila Poisson'), arxiv.Result.Author('Ning Wen'), arxiv.Result.Author('Pallavi Tiwari'), arxiv.Result.Author('Ruchika Verma'), arxiv.Result.Author('Rohan Bareja'), arxiv.Result.Author('Ipsa Yadav'), arxiv.Result.Author('Jonathan Chen'), arxiv.Result.Author('Neeraj Kumar'), arxiv.Result.Author('Marion Smits'), arxiv.Result.Author('Sebastian R van der Voort'), arxiv.Result.Author('Ahmed Alafandi'), arxiv.Result.Author('Fatih Incekara'), arxiv.Result.Author('Maarten MJ Wijnenga'), arxiv.Result.Author('Georgios Kapsas'), arxiv.Result.Author('Renske Gahrmann'), arxiv.Result.Author('Joost W Schouten'), arxiv.Result.Author('Hendrikus J Dubbink'), arxiv.Result.Author('Arnaud JPE Vincent'), arxiv.Result.Author('Martin J van den Bent'), arxiv.Result.Author('Pim J French'), arxiv.Result.Author('Stefan Klein'), arxiv.Result.Author('Yading Yuan'), arxiv.Result.Author('Sonam Sharma'), arxiv.Result.Author('Tzu-Chi Tseng'), arxiv.Result.Author('Saba Adabi'), arxiv.Result.Author('Simone P Niclou'), arxiv.Result.Author('Olivier Keunen'), arxiv.Result.Author('Ann-Christin Hau'), arxiv.Result.Author('Martin Vallières'), arxiv.Result.Author('David Fortin'), arxiv.Result.Author('Martin Lepage'), arxiv.Result.Author('Bennett Landman'), arxiv.Result.Author('Karthik Ramadass'), arxiv.Result.Author('Kaiwen Xu'), arxiv.Result.Author('Silky Chotai'), arxiv.Result.Author('Lola B Chambless'), arxiv.Result.Author('Akshitkumar Mistry'), arxiv.Result.Author('Reid C Thompson'), arxiv.Result.Author('Yuriy Gusev'), arxiv.Result.Author('Krithika Bhuvaneshwar'), arxiv.Result.Author('Anousheh Sayah'), arxiv.Result.Author('Camelia Bencheqroun'), arxiv.Result.Author('Anas Belouali'), arxiv.Result.Author('Subha Madhavan'), arxiv.Result.Author('Thomas C Booth'), arxiv.Result.Author('Alysha Chelliah'), arxiv.Result.Author('Marc Modat'), arxiv.Result.Author('Haris Shuaib'), arxiv.Result.Author('Carmen Dragos'), arxiv.Result.Author('Aly Abayazeed'), arxiv.Result.Author('Kenneth Kolodziej'), arxiv.Result.Author('Michael Hill'), arxiv.Result.Author('Ahmed Abbassy'), arxiv.Result.Author('Shady Gamal'), arxiv.Result.Author('Mahmoud Mekhaimar'), arxiv.Result.Author('Mohamed Qayati'), arxiv.Result.Author('Mauricio Reyes'), arxiv.Result.Author('Ji Eun Park'), arxiv.Result.Author('Jihye Yun'), arxiv.Result.Author('Ho Sung Kim'), arxiv.Result.Author('Abhishek Mahajan'), arxiv.Result.Author('Mark Muzi'), arxiv.Result.Author('Sean Benson'), arxiv.Result.Author('Regina G H Beets-Tan'), arxiv.Result.Author('Jonas Teuwen'), arxiv.Result.Author('Alejandro Herrera-Trujillo'), arxiv.Result.Author('Maria Trujillo'), arxiv.Result.Author('William Escobar'), arxiv.Result.Author('Ana Abello'), arxiv.Result.Author('Jose Bernal'), arxiv.Result.Author('Jhon Gómez'), arxiv.Result.Author('Joseph Choi'), arxiv.Result.Author('Stephen Baek'), arxiv.Result.Author('Yusung Kim'), arxiv.Result.Author('Heba Ismael'), arxiv.Result.Author('Bryan Allen'), arxiv.Result.Author('John M Buatti'), arxiv.Result.Author('Aikaterini Kotrotsou'), arxiv.Result.Author('Hongwei Li'), arxiv.Result.Author('Tobias Weiss'), arxiv.Result.Author('Michael Weller'), arxiv.Result.Author('Andrea Bink'), arxiv.Result.Author('Bertrand Pouymayou'), arxiv.Result.Author('Hassan F Shaykh'), arxiv.Result.Author('Joel Saltz'), arxiv.Result.Author('Prateek Prasanna'), arxiv.Result.Author('Sampurna Shrestha'), arxiv.Result.Author('Kartik M Mani'), arxiv.Result.Author('David Payne'), arxiv.Result.Author('Tahsin Kurc'), arxiv.Result.Author('Enrique Pelaez'), arxiv.Result.Author('Heydy Franco-Maldonado'), arxiv.Result.Author('Francis Loayza'), arxiv.Result.Author('Sebastian Quevedo'), arxiv.Result.Author('Pamela Guevara'), arxiv.Result.Author('Esteban Torche'), arxiv.Result.Author('Cristobal Mendoza'), arxiv.Result.Author('Franco Vera'), arxiv.Result.Author('Elvis Ríos'), arxiv.Result.Author('Eduardo López'), arxiv.Result.Author('Sergio A Velastin'), arxiv.Result.Author('Godwin Ogbole'), arxiv.Result.Author('Dotun Oyekunle'), arxiv.Result.Author('Olubunmi Odafe-Oyibotha'), arxiv.Result.Author('Babatunde Osobu'), arxiv.Result.Author(""Mustapha Shu'aibu""), arxiv.Result.Author('Adeleye Dorcas'), arxiv.Result.Author('Mayowa Soneye'), arxiv.Result.Author('Farouk Dako'), arxiv.Result.Author('Amber L Simpson'), arxiv.Result.Author('Mohammad Hamghalam'), arxiv.Result.Author('Jacob J Peoples'), arxiv.Result.Author('Ricky Hu'), arxiv.Result.Author('Anh Tran'), arxiv.Result.Author('Danielle Cutler'), arxiv.Result.Author('Fabio Y Moraes'), arxiv.Result.Author('Michael A Boss'), arxiv.Result.Author('James Gimpel'), arxiv.Result.Author('Deepak Kattil Veettil'), arxiv.Result.Author('Kendall Schmidt'), arxiv.Result.Author('Brian Bialecki'), arxiv.Result.Author('Sailaja Marella'), arxiv.Result.Author('Cynthia Price'), arxiv.Result.Author('Lisa Cimino'), arxiv.Result.Author('Charles Apgar'), arxiv.Result.Author('Prashant Shah'), arxiv.Result.Author('Bjoern Menze'), arxiv.Result.Author('Jill S Barnholtz-Sloan'), arxiv.Result.Author('Jason Martin'), arxiv.Result.Author('Spyridon Bakas')]","Although machine learning (ML) has shown promise in numerous domains, there
are concerns about generalizability to out-of-sample data. This is currently
addressed by centrally sharing ample, and importantly diverse, data from
multiple sites. However, such centralization is challenging to scale (or even
not feasible) due to various limitations. Federated ML (FL) provides an
alternative to train accurate and generalizable ML models, by only sharing
numerical model updates. Here we present findings from the largest FL study
to-date, involving data from 71 healthcare institutions across 6 continents, to
generate an automatic tumor boundary detector for the rare disease of
glioblastoma, utilizing the largest dataset of such patients ever used in the
literature (25,256 MRI scans from 6,314 patients). We demonstrate a 33%
improvement over a publicly trained model to delineate the surgically
targetable tumor, and 23% improvement over the tumor's entire extent. We
anticipate our study to: 1) enable more studies in healthcare informed by large
and diverse data, ensuring meaningful results for rare diseases and
underrepresented populations, 2) facilitate further quantitative analyses for
glioblastoma via performance optimization of our consensus model for eventual
public release, and 3) demonstrate the effectiveness of FL at such scale and
task complexity as a paradigm shift for multi-site collaborations, alleviating
the need for data sharing."
5231,"[7] Y. H. Cho, J. K. Kim, and S. H. Kim, “A personalized recommender
Besides, further research should be carried out to evaluate the                                                                                                                  system based on web usage mining and decision tree induction,” Expert
discriminative weights of categorical features.","I
nodes based on some discriminative feature selection methods
rather than exhaustive searches is a direction worth exploring.","systems with Applications, vol.",2022-04-23 07:33:57+00:00,A Novel Splitting Criterion Inspired by Geometric Mean Metric Learning for Decision Tree,cs.LG,['cs.LG'],"[arxiv.Result.Author('Dan Li'), arxiv.Result.Author('Songcan Chen')]","Decision tree (DT) attracts persistent research attention due to its
impressive empirical performance and interpretability in numerous applications.
However, the growth of traditional yet widely-used univariate decision trees
(UDTs) is quite time-consuming as they need to traverse all the features to
find the splitting value with the maximal reduction of the impurity at each
internal node. In this paper, we newly design a splitting criterion to speed up
the growth. The criterion is induced from Geometric Mean Metric Learning (GMML)
and then optimized under its diagonalized metric matrix constraint,
consequently, a closed-form rank of feature discriminant abilities can at once
be obtained and the top 1 feature at each node used to grow an intent DT
(called as dGMML-DT, where d is an abbreviation for diagonalization). We
evaluated the performance of the proposed methods and their corresponding
ensembles on benchmark datasets. The experiment shows that dGMML-DT achieves
comparable or better classification results more efficiently than the UDTs with
10x average speedup. Furthermore, dGMML-DT can straightforwardly be extended to
its multivariable counterpart (dGMML-MDT) without needing laborious operations."
5241,"To further study the sensitivity to the choices of the mentioned scaling factors and further analyze
memory usage of U-NO style architectures, we propose U-NO+, which has the same architecture

as the U-NO except that each Fourier operator layer contracts (or expands) the domain with a
more aggressive factor of 12 , as shown in Fig.","There are skip connections from the

ﬁrst three Fourier layers (G0, G1, G2) respectively to the last three (G6, G5, G4).",2.,2022-04-23 19:18:44+00:00,U-NO: U-shaped Neural Operators,cs.LG,['cs.LG'],"[arxiv.Result.Author('Md Ashiqur Rahman'), arxiv.Result.Author('Zachary E. Ross'), arxiv.Result.Author('Kamyar Azizzadenesheli')]","Neural operators generalize classical neural networks to maps between
infinite-dimensional spaces, e.g. function spaces. Prior works on neural
operators proposed a series of novel architectures to learn such maps and
demonstrated unprecedented success in solving partial differential equations
(PDEs). In this paper, we propose U-shaped Neural Operators U-NO, an
architecture that allows for deeper neural operators compared to prior works.
U-NOs exploit the problems structures in function predictions, demonstrate fast
training, data efficiency, and robustness w.r.t hyperparameters choices. We
study the performance of U-NO on PDE benchmarks, namely, Darcy's flow law and
the Navier-Stokes equations. We show that U-NO results in average of 14% and
38% prediction improvement on the Darcy's flow and Navier-Stokes equations,
respectively, over the state of art."
5242,"To further study the sensitivity to the choices of the mentioned scaling factors and further analyze
memory usage of U-NO style architectures, we propose U-NO+ that each Fourier operator layer

contracts (or expands) the domain with a more aggressive factor of 12 (shown in Fig.","There are skip connections from

the ﬁrst three Fourier layers (G0, G1, G2) respectively to the last three (G6, G5, G4).",2A).,2022-04-23 19:18:44+00:00,U-NO: U-shaped Neural Operators,cs.LG,['cs.LG'],"[arxiv.Result.Author('Md Ashiqur Rahman'), arxiv.Result.Author('Zachary E. Ross'), arxiv.Result.Author('Kamyar Azizzadenesheli')]","Neural operators generalize classical neural networks to maps between
infinite-dimensional spaces, e.g. function spaces. Prior works on neural
operators proposed a series of novel architectures to learn such maps and
demonstrated unprecedented success in learning solution operators of partial
differential equations. Due to their close proximity to fully connected
architectures, these models mainly suffer from high memory usage and are
generally limited to shallow deep learning models. In this paper, we propose
U-shaped Neural Operator (U-NO), a U-shaped memory enhanced architecture that
allows for deeper neural operators. U-NOs exploit the problem structures in
function predictions and demonstrate fast training, data efficiency, and
robustness with respect to hyperparameters choices. We study the performance of
U-NO on PDE benchmarks, namely, Darcy's flow law and the Navier-Stokes
equations. We show that U-NO results in an average of 14% and 34% prediction
improvement on Darcy's flow and turbulent Navier-Stokes equations,
respectively, over the state of art. On Navier-Stokes 3D spatio-temporal
operator learning task, we show U-NO provides 40% improvement over the state of
art methods."
5299,"For
results and facilitate further research.","The proposed models are implemented in python and are                   Based on three types of testing problems and ﬁve problem
publicly available on GitHub1 to reproduce the experimental          scales, ﬁfteen levels of testing problems are generated.","The DRL-MOA is also         example, T1O2S100 represents problem T1O2 with 100-city.",2022-04-25 14:02:34+00:00,Multi-objective Pointer Network for Combinatorial Optimization,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Le-yang Gao'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Chuang Liu'), arxiv.Result.Author('Zhao-hong Jia')]","Multi-objective combinatorial optimization problems (MOCOPs), one type of
complex optimization problems, widely exist in various real applications.
Although meta-heuristics have been successfully applied to address MOCOPs, the
calculation time is often much longer. Recently, a number of deep reinforcement
learning (DRL) methods have been proposed to generate approximate optimal
solutions to the combinatorial optimization problems. However, the existing
studies on DRL have seldom focused on MOCOPs. This study proposes a
single-model deep reinforcement learning framework, called multi-objective
Pointer Network (MOPN), where the input structure of PN is effectively improved
so that the single PN is capable of solving MOCOPs. In addition, two training
strategies, based on representative model and transfer learning, respectively,
are proposed to further enhance the performance of MOPN in different
application scenarios. Moreover, compared to classical meta-heuristics, MOPN
only consumes much less time on forward propagation to obtain the Pareto front.
Meanwhile, MOPN is insensitive to problem scale, meaning that a trained MOPN is
able to address MOCOPs with different scales. To verify the performance of
MOPN, extensive experiments are conducted on three multi-objective traveling
salesman problems, in comparison with one state-of-the-art model DRL-MOA and
three classical multi-objective meta-heuristics. Experimental results
demonstrate that the proposed model outperforms all the comparative methods
with only 20\% to 40\% training time of DRL-MOA."
5300,"[25] C. P. Tomazella and M. S. Nagano, “A comprehensive review of branch-
      and-bound algorithms: Guidelines and directions for further research on
      the ﬂowshop scheduling problem,” Expert Systems with Applications,
      vol.","788–803, 2022.","158, p. 113556, 2020.",2022-04-25 14:02:34+00:00,Multi-objective Pointer Network for Combinatorial Optimization,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Le-yang Gao'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Chuang Liu'), arxiv.Result.Author('Zhao-hong Jia')]","Multi-objective combinatorial optimization problems (MOCOPs), one type of
complex optimization problems, widely exist in various real applications.
Although meta-heuristics have been successfully applied to address MOCOPs, the
calculation time is often much longer. Recently, a number of deep reinforcement
learning (DRL) methods have been proposed to generate approximate optimal
solutions to the combinatorial optimization problems. However, the existing
studies on DRL have seldom focused on MOCOPs. This study proposes a
single-model deep reinforcement learning framework, called multi-objective
Pointer Network (MOPN), where the input structure of PN is effectively improved
so that the single PN is capable of solving MOCOPs. In addition, two training
strategies, based on representative model and transfer learning, respectively,
are proposed to further enhance the performance of MOPN in different
application scenarios. Moreover, compared to classical meta-heuristics, MOPN
only consumes much less time on forward propagation to obtain the Pareto front.
Meanwhile, MOPN is insensitive to problem scale, meaning that a trained MOPN is
able to address MOCOPs with different scales. To verify the performance of
MOPN, extensive experiments are conducted on three multi-objective traveling
salesman problems, in comparison with one state-of-the-art model DRL-MOA and
three classical multi-objective meta-heuristics. Experimental results
demonstrate that the proposed model outperforms all the comparative methods
with only 20\% to 40\% training time of DRL-MOA."
5301,"We suggest that   full micro data access to masked audit data to research how
                                          this problem setting is fertile ground for further research and   machine learning could improve audit selection.","Based on a unique
                                          important social consequences given that the current tax gap      multiyear collaboration with the IRS, we were provided with
                                          is estimated at nearly half a trillion dollars.","We investi-
                                          we highlight its interesting challenges.",2022-04-25 18:28:55+00:00,Integrating Reward Maximization and Population Estimation: Sequential Decision-Making for Internal Revenue Service Audit Selection,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Peter Henderson'), arxiv.Result.Author('Ben Chugg'), arxiv.Result.Author('Brandon Anderson'), arxiv.Result.Author('Kristen Altenburger'), arxiv.Result.Author('Alex Turk'), arxiv.Result.Author('John Guyton'), arxiv.Result.Author('Jacob Goldin'), arxiv.Result.Author('Daniel E. Ho')]","We introduce a new setting, optimize-and-estimate structured bandits. Here, a
policy must select a batch of arms, each characterized by its own context, that
would allow it to both maximize reward and maintain an accurate (ideally
unbiased) population estimate of the reward. This setting is inherent to many
public and private sector applications and often requires handling delayed
feedback, small data, and distribution shifts. We demonstrate its importance on
real data from the United States Internal Revenue Service (IRS). The IRS
performs yearly audits of the tax base. Two of its most important objectives
are to identify suspected misreporting and to estimate the ""tax gap"" -- the
global difference between the amount paid and true amount owed. Based on a
unique collaboration with the IRS, we cast these two processes as a unified
optimize-and-estimate structured bandit. We analyze optimize-and-estimate
approaches to the IRS problem and propose a novel mechanism for unbiased
population estimation that achieves rewards comparable to baseline approaches.
This approach has the potential to improve audit efficacy, while maintaining
policy-relevant estimates of the tax gap. This has important social
consequences given that the current tax gap is estimated at nearly half a
trillion dollars. We suggest that this problem setting is fertile ground for
further research and we highlight its interesting challenges. The results of
this and related research are currently being incorporated into the continual
improvement of the IRS audit selection methods."
5313,"[2020]
                                                                                                       further study a reﬁnement of the problem and give the ﬁrst
                                           ∗Corresponding author                                       theoretical analysis.","However, most of them assume the full preferences       ing problem in two-sided matching markets Liu et al.","They propose both ETC and UCB-type
                                                                                                       algorithms with guarantees on the stable regret, which is de-
                                                                                                       ﬁned as the difference between the cumulative reward of a
stable matching1 and the cumulative reward collected.",2022-04-26 03:01:17+00:00,Thompson Sampling for Bandit Learning in Matching Markets,cs.LG,"['cs.LG', 'cs.GT']","[arxiv.Result.Author('Fang Kong'), arxiv.Result.Author('Junming Yin'), arxiv.Result.Author('Shuai Li')]","The problem of two-sided matching markets has a wide range of real-world
applications and has been extensively studied in the literature. A line of
recent works have focused on the problem setting where the preferences of
one-side market participants are unknown \emph{a priori} and are learned by
iteratively interacting with the other side of participants. All these works
are based on explore-then-commit (ETC) and upper confidence bound (UCB)
algorithms, two common strategies in multi-armed bandits (MAB). Thompson
sampling (TS) is another popular approach, which attracts lots of attention due
to its easier implementation and better empirical performances. In many
problems, even when UCB and ETC-type algorithms have already been analyzed,
researchers are still trying to study TS for its benefits. However, the
convergence analysis of TS is much more challenging and remains open in many
problem settings. In this paper, we provide the first regret analysis for TS in
the new setting of iterative matching markets. Extensive experiments
demonstrate the practical advantages of the TS-type algorithm over the ETC and
UCB-type baselines."
5347,"When executing the convolution with
rations for a given DNN model and how these results can be       128 multipliers, the required clock cycles for the optimal and
incorporated into further research.","the suboptimal mappings perform increasingly worse with
                                                                 the amount of multipliers, which demonstrates the impact
   The example above demonstrates how Bifrost can be used        and importance of dataﬂow orchestration for larger and more
to evaluate the performance of different architecture conﬁgu-    complex architectures.","Using Bifrost, researchers   suboptimal mappings differ by a factor of 76.",2022-04-26 16:22:24+00:00,Bifrost: End-to-End Evaluation and Optimization of Reconfigurable DNN Accelerators,cs.LG,"['cs.LG', 'cs.AR', 'cs.DC', 'cs.PF']","[arxiv.Result.Author('Axel Stjerngren'), arxiv.Result.Author('Perry Gibson'), arxiv.Result.Author('José Cano')]","Reconfigurable accelerators for deep neural networks (DNNs) promise to
improve performance such as inference latency. STONNE is the first
cycle-accurate simulator for reconfigurable DNN inference accelerators which
allows for the exploration of accelerator designs and configuration space.
However, preparing models for evaluation and exploring configuration space in
STONNE is a manual developer-timeconsuming process, which is a barrier for
research. This paper introduces Bifrost, an end-to-end framework for the
evaluation and optimization of reconfigurable DNN inference accelerators.
Bifrost operates as a frontend for STONNE and leverages the TVM deep learning
compiler stack to parse models and automate offloading of accelerated
computations. We discuss Bifrost's advantages over STONNE and other tools, and
evaluate the MAERI and SIGMA architectures using Bifrost. Additionally, Bifrost
introduces a module leveraging AutoTVM to efficiently explore accelerator
designs and dataflow mapping space to optimize performance. This is
demonstrated by tuning the MAERI architecture and generating efficient dataflow
mappings for AlexNet, obtaining an average speedup of $50\times$ for the
convolutional layers and $11\times$ for the fully connected layers. Our code is
available at www.github.com/gicLAB/bifrost."
5348,"Although our
results empirically demonstrate that SWA results in low-rank representation, further research about
their connection is needed.","The effectiveness
of SWA in few-shot learning must be related to the property of the representation.","14
      Explicit regularizers can also be used to obtain simple input-output functions in deep neural
networks and low-rank representation, including L1 regularization, nuclear norm, spectral norm,
and Frobenius norm (Bartlett et al., 2017, Neyshabur et al., 2018, Sanyal et al., 2020).",2022-04-26 17:36:34+00:00,Meta-free representation learning for few-shot learning via stochastic weight averaging,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Kuilin Chen'), arxiv.Result.Author('Chi-Guhn Lee')]","Recent studies on few-shot classification using transfer learning pose
challenges to the effectiveness and efficiency of episodic meta-learning
algorithms. Transfer learning approaches are a natural alternative, but they
are restricted to few-shot classification. Moreover, little attention has been
on the development of probabilistic models with well-calibrated uncertainty
from few-shot samples, except for some Bayesian episodic learning algorithms.
To tackle the aforementioned issues, we propose a new transfer learning method
to obtain accurate and reliable models for few-shot regression and
classification. The resulting method does not require episodic meta-learning
and is called meta-free representation learning (MFRL). MFRL first finds
low-rank representation generalizing well on meta-test tasks. Given the learned
representation, probabilistic linear models are fine-tuned with few-shot
samples to obtain models with well-calibrated uncertainty. The proposed method
not only achieves the highest accuracy on a wide range of few-shot learning
benchmark datasets but also correctly quantifies the prediction uncertainty. In
addition, weight averaging and temperature scaling are effective in improving
the accuracy and reliability of few-shot learning in existing meta-learning
algorithms with a wide range of learning paradigms and model architectures."
5349,"Although our
results empirically demonstrate that SWA results in low-rank representation, further research about
their connection is needed.","The effectiveness
of SWA in few-shot learning must be related to the property of the representation.","14
      Explicit regularizers can also be used to obtain simple input-output functions in deep neural
networks and low-rank representation, including L1 regularization, nuclear norm, spectral norm,
and Frobenius norm (Bartlett et al., 2017, Neyshabur et al., 2018, Sanyal et al., 2020).",2022-04-26 17:36:34+00:00,Meta-free few-shot learning via representation learning with weight averaging,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Kuilin Chen'), arxiv.Result.Author('Chi-Guhn Lee')]","Recent studies on few-shot classification using transfer learning pose
challenges to the effectiveness and efficiency of episodic meta-learning
algorithms. Transfer learning approaches are a natural alternative, but they
are restricted to few-shot classification. Moreover, little attention has been
on the development of probabilistic models with well-calibrated uncertainty
from few-shot samples, except for some Bayesian episodic learning algorithms.
To tackle the aforementioned issues, we propose a new transfer learning method
to obtain accurate and reliable models for few-shot regression and
classification. The resulting method does not require episodic meta-learning
and is called meta-free representation learning (MFRL). MFRL first finds
low-rank representation generalizing well on meta-test tasks. Given the learned
representation, probabilistic linear models are fine-tuned with few-shot
samples to obtain models with well-calibrated uncertainty. The proposed method
not only achieves the highest accuracy on a wide range of few-shot learning
benchmark datasets but also correctly quantifies the prediction uncertainty. In
addition, weight averaging and temperature scaling are effective in improving
the accuracy and reliability of few-shot learning in existing meta-learning
algorithms with a wide range of learning paradigms and model architectures."
5359,"The initialization of these parameter
needs further research.","Second, βki ’s are the trainable parameters in Stan.","Third, there are many other issues that come with the optimization
process, and thus designing a better optimizer for PINN can be an important contribution.",2022-04-26 20:56:49+00:00,Self-scalable Tanh (Stan): Faster Convergence and Better Generalization in Physics-informed Neural Networks,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Author('Raghav Gnanasambandam'), arxiv.Result.Author('Bo Shen'), arxiv.Result.Author('Jihoon Chung'), arxiv.Result.Author('Xubo Yue'), arxiv.Result.Author('Zhenyu'), arxiv.Result.Author('Kong')]","Physics-informed Neural Networks (PINNs) are gaining attention in the
engineering and scientific literature for solving a range of differential
equations with applications in weather modeling, healthcare, manufacturing, and
so on. Poor scalability is one of the barriers to utilizing PINNs for many
real-world problems. To address this, a Self-scalable tanh (Stan) activation
function is proposed for the PINNs. The proposed Stan function is smooth,
non-saturating, and has a trainable parameter. During training, it can allow
easy flow of gradients to compute the required derivatives and also enable
systematic scaling of the input-output mapping. It is also shown theoretically
that the PINN with the proposed Stan function has no spurious stationary points
when using gradient descent algorithms. The proposed Stan is tested on a couple
of numerical studies involving general regression problems. It is subsequently
used for solving multiple forward problems, which involve second-order
derivatives and multiple dimensions, and an inverse problem where the thermal
diffusivity is predicted through heat conduction in a rod. Our results of these
case studies establish empirically that the Stan activation function can
achieve better training and more accurate predictions than the state-of-the-art
activation functions."
5373,"However, further study
influence is useful, which is the premise of SCGC*.","However, higher 𝑅 values are slightly       For comparison with prior work [2, 22], we chose to use the same
better, which suggests that incorporating more neighbourhood            AE with 500 − 500 − 2000 − 10 dimensions.","(2) As 𝜏 decreases  is needed to determine an optimal AE to better handle the novel
accuracy is consistently improved on all data sets, particularly on     IAC loss, including diffident architectures (ex.",2022-04-27 01:38:46+00:00,SCGC : Self-Supervised Contrastive Graph Clustering,cs.LG,"['cs.LG', 'cs.CV', 'cs.MM']","[arxiv.Result.Author('Gayan K. Kulatilleke'), arxiv.Result.Author('Marius Portmann'), arxiv.Result.Author('Shekhar S. Chandra')]","Graph clustering discovers groups or communities within networks. Deep
learning methods such as autoencoders (AE) extract effective clustering and
downstream representations but cannot incorporate rich structural information.
While Graph Neural Networks (GNN) have shown great success in encoding graph
structure, typical GNNs based on convolution or attention variants suffer from
over-smoothing, noise, heterophily, are computationally expensive and typically
require the complete graph being present. Instead, we propose Self-Supervised
Contrastive Graph Clustering (SCGC), which imposes graph-structure via
contrastive loss signals to learn discriminative node representations and
iteratively refined soft cluster labels. We also propose SCGC*, with a more
effective, novel, Influence Augmented Contrastive (IAC) loss to fuse richer
structural information, and half the original model parameters. SCGC(*) is
faster with simple linear units, completely eliminate convolutions and
attention of traditional GNNs, yet efficiently incorporates structure. It is
impervious to layer depth and robust to over-smoothing, incorrect edges and
heterophily. It is scalable by batching, a limitation in many prior GNN models,
and trivially parallelizable. We obtain significant improvements over
state-of-the-art on a wide range of benchmark graph datasets, including images,
sensor data, text, and citation networks efficiently. Specifically, 20% on ARI
and 18% on NMI for DBLP; overall 55% reduction in training time and overall,
81% reduction on inference time. Our code is available at :
https://github.com/gayanku/SCGC"
5398,of feature selection merit further study.,"Other methods        5: Fit step: Train Ci = C(target = f, Ci , D, HC ).","6: end for
3.3 Privacy and Budget
                                                                   7: for Range of |n|, desired samples do
It is quite difﬁcult to determine a “reasonable” budget for
a DP mechanism, and in practice what constitutes a reason-         8: Sample step: Generate sample s ∼ S
able level of privacy varies by context [Dwork et al., 2019].",2022-04-27 13:13:56+00:00,Spending Privacy Budget Fairly and Wisely,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Lucas Rosenblatt'), arxiv.Result.Author('Joshua Allen'), arxiv.Result.Author('Julia Stoyanovich')]","Differentially private (DP) synthetic data generation is a practical method
for improving access to data as a means to encourage productive partnerships.
One issue inherent to DP is that the ""privacy budget"" is generally ""spent""
evenly across features in the data set. This leads to good statistical parity
with the real data, but can undervalue the conditional probabilities and
marginals that are critical for predictive quality of synthetic data. Further,
loss of predictive quality may be non-uniform across the data set, with subsets
that correspond to minority groups potentially suffering a higher loss.
  In this paper, we develop ensemble methods that distribute the privacy budget
""wisely"" to maximize predictive accuracy of models trained on DP data, and
""fairly"" to bound potential disparities in accuracy across groups and reduce
inequality. Our methods are based on the insights that feature importance can
inform how privacy budget is allocated, and, further, that per-group feature
importance and fairness-related performance objectives can be incorporated in
the allocation. These insights make our methods tunable to social contexts,
allowing data owners to produce balanced synthetic data for predictive
analysis."
5405,"Conclusions
the formulation of scientific hypotheses for further study.","insights on an ex-ante impact assessment for the agricultural
practices studied in the context of climate change, enabling        5.","We presented an approach for assessing agricultural land
   Future work includes incorporating climatic projections          suitability using causal machine learning.",2022-04-27 14:13:47+00:00,Towards assessing agricultural land suitability with causal machine learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Georgios Giannarakis'), arxiv.Result.Author('Vasileios Sitokonstantinou'), arxiv.Result.Author('Roxanne Suzette Lorilla'), arxiv.Result.Author('Charalampos Kontoes')]","Understanding the suitability of agricultural land for applying specific
management practices is of great importance for sustainable and resilient
agriculture against climate change. Recent developments in the field of causal
machine learning enable the estimation of intervention impacts on an outcome of
interest, for samples described by a set of observed characteristics. We
introduce an extensible data-driven framework that leverages earth observations
and frames agricultural land suitability as a geospatial impact assessment
problem, where the estimated effects of agricultural practices on
agroecosystems serve as a land suitability score and guide decision making. We
formulate this as a causal machine learning task and discuss how this approach
can be used for agricultural planning in a changing climate. Specifically, we
extract the agricultural management practices of ""crop rotation"" and ""landscape
crop diversity"" from crop type maps, account for climate and land use data, and
use double machine learning to estimate their heterogeneous effect on Net
Primary Productivity (NPP), within the Flanders region of Belgium from 2010 to
2020. We find that the effect of crop rotation was insignificant, while
landscape crop diversity had a small negative effect on NPP. Finally, we
observe considerable effect heterogeneity in space for both practices and
analyze it."
5419,"After deﬁning
                                        the ML-EDM problem, ten challenges are identiﬁed and proposed to the
                                        scientiﬁc community to further research in this area.","This paper introduces a more
                                        general problem, called Machine Learning based Early Decision Making
                                        (ML-EDM), which consists in optimizing the decision times of models in
                                        a wide range of settings where data is collected over time.","These challenges
                                        open important application perspectives, discussed in this paper.",2022-04-27 14:46:10+00:00,Open challenges for Machine Learning based Early Decision-Making research,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexis Bondu'), arxiv.Result.Author('Youssef Achenchabe'), arxiv.Result.Author('Albert Bifet'), arxiv.Result.Author('Fabrice Clérot'), arxiv.Result.Author('Antoine Cornuéjols'), arxiv.Result.Author('Joao Gama'), arxiv.Result.Author('Georges Hébrail'), arxiv.Result.Author('Vincent Lemaire'), arxiv.Result.Author('Pierre-François Marteau')]","More and more applications require early decisions, i.e. taken as soon as
possible from partially observed data. However, the later a decision is made,
the more its accuracy tends to improve, since the description of the problem to
hand is enriched over time. Such a compromise between the earliness and the
accuracy of decisions has been particularly studied in the field of Early Time
Series Classification. This paper introduces a more general problem, called
Machine Learning based Early Decision Making (ML-EDM), which consists in
optimizing the decision times of models in a wide range of settings where data
is collected over time. After defining the ML-EDM problem, ten challenges are
identified and proposed to the scientific community to further research in this
area. These challenges open important application perspectives, discussed in
this paper."
5420,"This position paper aims to deﬁne the ﬁeld of ML-EDM, and proposes ten
challenges to the scientiﬁc community to further research in this area.","In this paper a more general problem is introduced, called
Machine Learning based Early Decision Making (ML-EDM), which consists in
optimizing the decision times of models in a wide range of settings where data
is collected over time.","In partic-
ular, ML-EDM has been deﬁned and positioned with respect to related ﬁelds,
such as machine learning and reinforcement learning.",2022-04-27 14:46:10+00:00,Open challenges for Machine Learning based Early Decision-Making research,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexis Bondu'), arxiv.Result.Author('Youssef Achenchabe'), arxiv.Result.Author('Albert Bifet'), arxiv.Result.Author('Fabrice Clérot'), arxiv.Result.Author('Antoine Cornuéjols'), arxiv.Result.Author('Joao Gama'), arxiv.Result.Author('Georges Hébrail'), arxiv.Result.Author('Vincent Lemaire'), arxiv.Result.Author('Pierre-François Marteau')]","More and more applications require early decisions, i.e. taken as soon as
possible from partially observed data. However, the later a decision is made,
the more its accuracy tends to improve, since the description of the problem to
hand is enriched over time. Such a compromise between the earliness and the
accuracy of decisions has been particularly studied in the field of Early Time
Series Classification. This paper introduces a more general problem, called
Machine Learning based Early Decision Making (ML-EDM), which consists in
optimizing the decision times of models in a wide range of settings where data
is collected over time. After defining the ML-EDM problem, ten challenges are
identified and proposed to the scientific community to further research in this
area. These challenges open important application perspectives, discussed in
this paper."
5421,"After deﬁning
                                                the ML-EDM problem, ten challenges are identiﬁed and proposed to the
                                                scientiﬁc community to further research in this area.","This paper introduces a more
                                                general problem, called Machine Learning based Early Decision Making
                                                (ML-EDM), which consists in optimizing the decision times of models in
                                                a wide range of settings where data is collected over time.","These challenges
                                                open important application perspectives, discussed in this paper.",2022-04-27 14:46:10+00:00,Open challenges for Machine Learning based Early Decision-Making research,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexis Bondu'), arxiv.Result.Author('Youssef Achenchabe'), arxiv.Result.Author('Albert Bifet'), arxiv.Result.Author('Fabrice Clérot'), arxiv.Result.Author('Antoine Cornuéjols'), arxiv.Result.Author('Joao Gama'), arxiv.Result.Author('Georges Hébrail'), arxiv.Result.Author('Vincent Lemaire'), arxiv.Result.Author('Pierre-François Marteau')]","More and more applications require early decisions, i.e. taken as soon as
possible from partially observed data. However, the later a decision is made,
the more its accuracy tends to improve, since the description of the problem to
hand is enriched over time. Such a compromise between the earliness and the
accuracy of decisions has been particularly studied in the field of Early Time
Series Classification. This paper introduces a more general problem, called
Machine Learning based Early Decision Making (ML-EDM), which consists in
optimizing the decision times of models in a wide range of settings where data
is collected over time. After defining the ML-EDM problem, ten challenges are
identified and proposed to the scientific community to further research in this
area. These challenges open important application perspectives, discussed in
this paper."
5422,"This position paper aims to deﬁne the ﬁeld of ML-EDM, and proposes ten
challenges to the scientiﬁc community to further research in this area.","In this paper a more general problem is introduced, called
Machine Learning based Early Decision Making (ML-EDM), which consists in
optimizing the decision times of models in a wide range of settings where data
is collected over time.","In partic-
ular, ML-EDM has been deﬁned and positioned with respect to related ﬁelds,
such as machine learning and reinforcement learning.",2022-04-27 14:46:10+00:00,Open challenges for Machine Learning based Early Decision-Making research,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexis Bondu'), arxiv.Result.Author('Youssef Achenchabe'), arxiv.Result.Author('Albert Bifet'), arxiv.Result.Author('Fabrice Clérot'), arxiv.Result.Author('Antoine Cornuéjols'), arxiv.Result.Author('Joao Gama'), arxiv.Result.Author('Georges Hébrail'), arxiv.Result.Author('Vincent Lemaire'), arxiv.Result.Author('Pierre-François Marteau')]","More and more applications require early decisions, i.e. taken as soon as
possible from partially observed data. However, the later a decision is made,
the more its accuracy tends to improve, since the description of the problem to
hand is enriched over time. Such a compromise between the earliness and the
accuracy of decisions has been particularly studied in the field of Early Time
Series Classification. This paper introduces a more general problem, called
Machine Learning based Early Decision Making (ML-EDM), which consists in
optimizing the decision times of models in a wide range of settings where data
is collected over time. After defining the ML-EDM problem, ten challenges are
identified and proposed to the scientific community to further research in this
area. These challenges open important application perspectives, discussed in
this paper."
5477,"Journal
compositional generalization that requires further study.","Never-          Guestrin, C., Koller, D., Parr, R., and Venkataraman, S.
theless, there remains a gap between our models and true           Efﬁcient solution algorithms for factored MDPs.","Ad-      of Artiﬁcial Intelligence Research, 19:399–468, 2003.
ditionally, our analysis has been based on a ﬁxed K, and
assumes object persistence, which clearly should be relaxed.",2022-04-28 17:22:45+00:00,Toward Compositional Generalization in Object-Oriented World Modeling,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Linfeng Zhao'), arxiv.Result.Author('Lingzhi Kong'), arxiv.Result.Author('Robin Walters'), arxiv.Result.Author('Lawson L. S. Wong')]","Compositional generalization is a critical ability in learning and
decision-making. We focus on the setting of reinforcement learning in
object-oriented environments to study compositional generalization in world
modeling. We (1) formalize the compositional generalization problem with an
algebraic approach and (2) study how a world model can achieve that. We
introduce a conceptual environment, Object Library, and two instances, and
deploy a principled pipeline to measure the generalization ability. Motivated
by the formulation, we analyze several methods with exact} or no compositional
generalization ability using our framework, and design a differentiable
approach, Homomorphic Object-oriented World Model (HOWM), that achieves
approximate but more efficient compositional generalization."
5478,"To further study the capability of C-SWMs, we increase the number of object slots to N , i.e., we use N
   slots to learn the full transition model of the object library.","Toward Compositional Generalization in Object-Oriented World Modeling

• ΣN -CSWM.","In the original C-SWM model, the number of object slot is
   equal to the number of objects in the scene, which is 5 in 2D shapes environment.",2022-04-28 17:22:45+00:00,Toward Compositional Generalization in Object-Oriented World Modeling,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Linfeng Zhao'), arxiv.Result.Author('Lingzhi Kong'), arxiv.Result.Author('Robin Walters'), arxiv.Result.Author('Lawson L. S. Wong')]","Compositional generalization is a critical ability in learning and
decision-making. We focus on the setting of reinforcement learning in
object-oriented environments to study compositional generalization in world
modeling. We (1) formalize the compositional generalization problem with an
algebraic approach and (2) study how a world model can achieve that. We
introduce a conceptual environment, Object Library, and two instances, and
deploy a principled pipeline to measure the generalization ability. Motivated
by the formulation, we analyze several methods with exact} or no compositional
generalization ability using our framework, and design a differentiable
approach, Homomorphic Object-oriented World Model (HOWM), that achieves
approximate but more efficient compositional generalization."
5479,"To further study the capability of C-SWMs, we increase the number of object slots to N , i.e., we use N
   slots to learn the full transition model of the object library.","Toward Compositional Generalization in Object-Oriented World Modeling

• ΣN -CSWM.","In the original C-SWM model, the number of object slot is
   equal to the number of objects in the scene, which is 5 in 2D shapes environment.",2022-04-28 17:22:45+00:00,Toward Compositional Generalization in Object-Oriented World Modeling,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Linfeng Zhao'), arxiv.Result.Author('Lingzhi Kong'), arxiv.Result.Author('Robin Walters'), arxiv.Result.Author('Lawson L. S. Wong')]","Compositional generalization is a critical ability in learning and
decision-making. We focus on the setting of reinforcement learning in
object-oriented environments to study compositional generalization in world
modeling. We (1) formalize the compositional generalization problem with an
algebraic approach and (2) study how a world model can achieve that. We
introduce a conceptual environment, Object Library, and two instances, and
deploy a principled pipeline to measure the generalization ability. Motivated
by the formulation, we analyze several methods with exact or no compositional
generalization ability using our framework, and design a differentiable
approach, Homomorphic Object-oriented World Model (HOWM), that achieves soft
but more efficient compositional generalization."
5481,"However, further study is needed to
report detailed results with ResNet18 with BFloat16 throughout         determine whether this is possible and effective.","For clarity, we              boosting overall beneﬁts.","the paper, concluding with overall performance and energy
efﬁciency measurements for all models.",2022-04-28 17:30:08+00:00,Schrödinger's FP: Dynamic Adaptation of Floating-Point Containers for Deep Learning Training,cs.LG,"['cs.LG', 'cs.AR']","[arxiv.Result.Author('Miloš Nikolić'), arxiv.Result.Author('Enrique Torres Sanchez'), arxiv.Result.Author('Jiahui Wang'), arxiv.Result.Author('Ali Hadi Zadeh'), arxiv.Result.Author('Mostafa Mahmoud'), arxiv.Result.Author('Ameer Abdelhadi'), arxiv.Result.Author('Andreas Moshovos')]","We introduce a software-hardware co-design approach to reduce memory traffic
and footprint during training with BFloat16 or FP32 boosting energy efficiency
and execution time performance. We introduce methods to dynamically adjust the
size and format of the floating-point containers used to store activations and
weights during training. The different value distributions lead us to different
approaches for exponents and mantissas. Gecko exploits the favourable exponent
distribution with a loss-less delta encoding approach to reduce the total
exponent footprint by up to $58\%$ in comparison to a 32 bit floating point
baseline. To content with the noisy mantissa distributions, we present two
lossy methods to eliminate as many as possible least significant bits while not
affecting accuracy. Quantum Mantissa, is a machine learning-first mantissa
compression method that taps on training's gradient descent algorithm to also
learn minimal mantissa bitlengths on a per-layer granularity, and obtain up to
$92\%$ reduction in total mantissa footprint. Alternatively, BitChop observes
changes in the loss function during training to adjust mantissa bit-length
network-wide yielding a reduction of $81\%$ in footprint. Schr\""{o}dinger's FP
implements hardware encoders/decoders that guided by Gecko/Quantum Mantissa or
Gecko/BitChop transparently encode/decode values when transferring to/from
off-chip memory boosting energy efficiency and reducing execution time."
5487,"We set d=32, m=5, and a=5
as default values and we further study the effect of different                 720 (6, 6, 4)
values in the Appendix.","We train the models for a max-
imum of 10 epoch, while utilizing early stopping with a pa-                    672 (7, 6, 4, 4)
tience of 3 and a batch size of 32.","Since the patch sizes are highly de-
pendent on the input size H, we vary the patch sizes among 2,                  Table 3: Patch Sizes and Number of Layers
3, 4, 6, 7, 12, and 24, and select the best patch sizes based on
the validation set.",2022-04-28 20:41:49+00:00,"Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting--Full Version",cs.LG,['cs.LG'],"[arxiv.Result.Author('Razvan-Gabriel Cirstea'), arxiv.Result.Author('Chenjuan Guo'), arxiv.Result.Author('Bin Yang'), arxiv.Result.Author('Tung Kieu'), arxiv.Result.Author('Xuanyi Dong'), arxiv.Result.Author('Shirui Pan')]","A variety of real-world applications rely on far future information to make
decisions, thus calling for efficient and accurate long sequence multivariate
time series forecasting. While recent attention-based forecasting models show
strong abilities in capturing long-term dependencies, they still suffer from
two key limitations. First, canonical self attention has a quadratic complexity
w.r.t. the input time series length, thus falling short in efficiency. Second,
different variables' time series often have distinct temporal dynamics, which
existing studies fail to capture, as they use the same model parameter space,
e.g., projection matrices, for all variables' time series, thus falling short
in accuracy. To ensure high efficiency and accuracy, we propose Triformer, a
triangular, variable-specific attention. (i) Linear complexity: we introduce a
novel patch attention with linear complexity. When stacking multiple layers of
the patch attentions, a triangular structure is proposed such that the layer
sizes shrink exponentially, thus maintaining linear complexity. (ii)
Variable-specific parameters: we propose a light-weight method to enable
distinct sets of model parameters for different variables' time series to
enhance accuracy without compromising efficiency and memory usage. Strong
empirical evidence on four datasets from multiple domains justifies our design
choices, and it demonstrates that Triformer outperforms state-of-the-art
methods w.r.t. both accuracy and efficiency. This is an extended version of
""Triformer: Triangular, Variable-Specific Attentions for Long Sequence
Multivariate Time Series Forecasting"", to appear in IJCAI 2022 [Cirstea et al.,
2022a], including additional experimental results."
5493,"Because of this design, our pipeline
not only addresses the issue of reproducibility, but it also promotes further research using
diﬀerent cohorts for downstream prediction tasks.","By
incorporating our standardized pipeline proposed in this paper, all steps taken to derive
a unique cohort can be recorded for reproducibility.","12
            An Extensive Data Processing Pipeline for MIMIC-IV

    The optional standalone evaluation and fairness module provided in our pipeline extends
the usability of our work beyond the MIMIC-IV dataset.",2022-04-29 01:09:38+00:00,An Extensive Data Processing Pipeline for MIMIC-IV,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mehak Gupta'), arxiv.Result.Author('Brennan Gallamoza'), arxiv.Result.Author('Nicolas Cutrona'), arxiv.Result.Author('Pranjal Dhakal'), arxiv.Result.Author('Raphael Poulain'), arxiv.Result.Author('Rahmatollah Beheshti')]","An increasing amount of research is being devoted to applying machine
learning methods to electronic health record (EHR) data for various clinical
tasks. This growing area of research has exposed the limitation of
accessibility of EHR datasets for all, as well as the reproducibility of
different modeling frameworks. One reason for these limitations is the lack of
standardized pre-processing pipelines. MIMIC is a freely available EHR dataset
in a raw format that has been used in numerous studies. The absence of
standardized pre-processing steps serves as a major barrier to the wider
adoption of the dataset. It also leads to different cohorts being used in
downstream tasks, limiting the ability to compare the results among similar
studies. Contrasting studies also use various distinct performance metrics,
which can greatly reduce the ability to compare model results. In this work, we
provide an end-to-end fully customizable pipeline to extract, clean, and
pre-process data; and to predict and evaluate the fourth version of the MIMIC
dataset (MIMIC-IV) for ICU and non-ICU-related clinical time-series prediction
tasks."
5494,"Because of this design, our pipeline
not only addresses the issue of reproducibility, but it also promotes further research using
diﬀerent cohorts for downstream prediction tasks.","By
incorporating our standardized pipeline proposed in this paper, all steps taken to derive
a unique cohort can be recorded for reproducibility.","12
                      An Extensive Data Processing Pipeline for MIMIC-IV

    The optional standalone evaluation and fairness module provided in our pipeline extends
the usability of our work beyond the MIMIC-IV dataset.",2022-04-29 01:09:38+00:00,An Extensive Data Processing Pipeline for MIMIC-IV,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mehak Gupta'), arxiv.Result.Author('Brennan Gallamoza'), arxiv.Result.Author('Nicolas Cutrona'), arxiv.Result.Author('Pranjal Dhakal'), arxiv.Result.Author('Raphael Poulain'), arxiv.Result.Author('Rahmatollah Beheshti')]","An increasing amount of research is being devoted to applying machine
learning methods to electronic health record (EHR) data for various clinical
tasks. This growing area of research has exposed the limitation of
accessibility of EHR datasets for all, as well as the reproducibility of
different modeling frameworks. One reason for these limitations is the lack of
standardized pre-processing pipelines. MIMIC is a freely available EHR dataset
in a raw format used in numerous studies. The absence of standardized
pre-processing steps serves as a significant barrier to the wider adoption of
the dataset. It also leads to different cohorts being used in downstream tasks,
limiting the ability to compare the results among similar studies. Contrasting
studies also use various distinct performance metrics, which can greatly reduce
the ability to compare model results. In this work, we provide an end-to-end
fully customizable pipeline to extract, clean, and pre-process data; and to
predict and evaluate the fourth version of the MIMIC dataset (MIMIC-IV) for ICU
and non-ICU-related clinical time-series prediction tasks. The tool is publicly
available at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline."
5495,"Our pipeline not only
addresses the issue of reproducibility (by recording all design choices) but also promotes
further research using diﬀerent cohorts for prediction tasks.","Our proposed pipeline is highly conﬁg-
urable and provides users with many options to deﬁne customized cohorts by allowing for
feature selection options and other user-deﬁned pre-processing steps.","Limitations: Our study trades oﬀ a more generalizable pipeline applicable to diﬀerent
versions of MIMIC or diﬀerent EHR datasets for a more specialized version focusing on
MIMIC-IV, with the goal of lowering the barriers to entering the ﬁeld.",2022-04-29 01:09:38+00:00,An Extensive Data Processing Pipeline for MIMIC-IV,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mehak Gupta'), arxiv.Result.Author('Brennan Gallamoza'), arxiv.Result.Author('Nicolas Cutrona'), arxiv.Result.Author('Pranjal Dhakal'), arxiv.Result.Author('Raphael Poulain'), arxiv.Result.Author('Rahmatollah Beheshti')]","An increasing amount of research is being devoted to applying machine
learning methods to electronic health record (EHR) data for various clinical
purposes. This growing area of research has exposed the challenges of the
accessibility of EHRs. MIMIC is a popular, public, and free EHR dataset in a
raw format that has been used in numerous studies. The absence of standardized
pre-processing steps can be, however, a significant barrier to the wider
adoption of this rare resource. Additionally, this absence can reduce the
reproducibility of the developed tools and limit the ability to compare the
results among similar studies. In this work, we provide a greatly customizable
pipeline to extract, clean, and pre-process the data available in the fourth
version of the MIMIC dataset (MIMIC-IV). The pipeline also presents an
end-to-end wizard-like package supporting predictive model creations and
evaluations. The pipeline covers a range of clinical prediction tasks which can
be broadly classified into four categories - readmission, length of stay,
mortality, and phenotype prediction. The tool is publicly available at
https://github.com/anonymousblind/MIMIC-IV-Data-Pipeline."
5496,"Our pipeline not only
addresses the issue of reproducibility (by recording all design choices) but also promotes
further research using diﬀerent cohorts for prediction tasks.","Our proposed pipeline is highly conﬁg-
urable and provides users with many options to deﬁne customized cohorts by allowing for
feature selection options and other user-deﬁned pre-processing steps.","Limitations: Our study trades oﬀ a more generalizable pipeline applicable to diﬀerent
versions of MIMIC or diﬀerent EHR datasets for a more specialized version focusing on
MIMIC-IV, with the goal of lowering the barriers to entering the ﬁeld.",2022-04-29 01:09:38+00:00,An Extensive Data Processing Pipeline for MIMIC-IV,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mehak Gupta'), arxiv.Result.Author('Brennan Gallamoza'), arxiv.Result.Author('Nicolas Cutrona'), arxiv.Result.Author('Pranjal Dhakal'), arxiv.Result.Author('Raphael Poulain'), arxiv.Result.Author('Rahmatollah Beheshti')]","An increasing amount of research is being devoted to applying machine
learning methods to electronic health record (EHR) data for various clinical
purposes. This growing area of research has exposed the challenges of the
accessibility of EHRs. MIMIC is a popular, public, and free EHR dataset in a
raw format that has been used in numerous studies. The absence of standardized
pre-processing steps can be, however, a significant barrier to the wider
adoption of this rare resource. Additionally, this absence can reduce the
reproducibility of the developed tools and limit the ability to compare the
results among similar studies. In this work, we provide a greatly customizable
pipeline to extract, clean, and pre-process the data available in the fourth
version of the MIMIC dataset (MIMIC-IV). The pipeline also presents an
end-to-end wizard-like package supporting predictive model creations and
evaluations. The pipeline covers a range of clinical prediction tasks which can
be broadly classified into four categories - readmission, length of stay,
mortality, and phenotype prediction. The tool is publicly available at
https://github.com/healthylaife/MIMIC-IV-Data-Pipeline."
5497,"Our pipeline not only
addresses the issue of reproducibility (by recording all design choices) but also promotes
further research using diﬀerent cohorts for prediction tasks.","Our proposed pipeline is highly conﬁg-
urable and provides users with many options to deﬁne customized cohorts by allowing for
feature selection options and other user-deﬁned pre-processing steps.","Limitations: Our study trades oﬀ a more generalizable pipeline applicable to diﬀerent
versions of MIMIC or diﬀerent EHR datasets for a more specialized version focusing on
MIMIC-IV, with the goal of lowering the barriers to entering the ﬁeld.",2022-04-29 01:09:38+00:00,An Extensive Data Processing Pipeline for MIMIC-IV,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mehak Gupta'), arxiv.Result.Author('Brennan Gallamoza'), arxiv.Result.Author('Nicolas Cutrona'), arxiv.Result.Author('Pranjal Dhakal'), arxiv.Result.Author('Raphael Poulain'), arxiv.Result.Author('Rahmatollah Beheshti')]","An increasing amount of research is being devoted to applying machine
learning methods to electronic health record (EHR) data for various clinical
purposes. This growing area of research has exposed the challenges of the
accessibility of EHRs. MIMIC is a popular, public, and free EHR dataset in a
raw format that has been used in numerous studies. The absence of standardized
pre-processing steps can be, however, a significant barrier to the wider
adoption of this rare resource. Additionally, this absence can reduce the
reproducibility of the developed tools and limit the ability to compare the
results among similar studies. In this work, we provide a greatly customizable
pipeline to extract, clean, and pre-process the data available in the fourth
version of the MIMIC dataset (MIMIC-IV). The pipeline also presents an
end-to-end wizard-like package supporting predictive model creations and
evaluations. The pipeline covers a range of clinical prediction tasks which can
be broadly classified into four categories - readmission, length of stay,
mortality, and phenotype prediction. The tool is publicly available at
https://github.com/healthylaife/MIMIC-IV-Data-Pipeline."
5508,"While not a focus of this work,
further study using downstream machine learning tasks can potentially pinpoint the beneﬁts of these temporal persistent
features.","From the persistent diagrams, the persistent features detected by the
persistent homology are not noticeably different from the temporal persistent homology.","For the simplicial analysis, we ﬁrst group the data by their time steps.",2022-04-29 12:46:14+00:00,Topological Data Analysis in Time Series: Temporal Filtration and Application to Single-Cell Genomics,cs.LG,"['cs.LG', 'math.AT', 'q-bio.GN', 'q-bio.QM']",[arxiv.Result.Author('Baihan Lin')],"The absence of a conventional association between the cell-cell cohabitation
and its emergent dynamics into cliques during development has hindered our
understanding of how cell populations proliferate, differentiate, and compete,
i.e. the cell ecology. With the recent advancement of the single-cell
RNA-sequencing (RNA-seq), we can potentially describe such a link by
constructing network graphs that characterize the similarity of the gene
expression profiles of the cell-specific transcriptional programs, and
analyzing these graphs systematically using the summary statistics informed by
the algebraic topology. We propose the single-cell topological simplicial
analysis (scTSA). Applying this approach to the single-cell gene expression
profiles from local networks of cells in different developmental stages with
different outcomes reveals a previously unseen topology of cellular ecology.
These networks contain an abundance of cliques of single-cell profiles bound
into cavities that guide the emergence of more complicated habitation forms. We
visualize these ecological patterns with topological simplicial architectures
of these networks, compared with the null models. Benchmarked on the
single-cell RNA-seq data of zebrafish embryogenesis spanning 38,731 cells, 25
cell types and 12 time steps, our approach highlights the gastrulation as the
most critical stage, consistent with consensus in developmental biology. As a
nonlinear, model-independent, and unsupervised framework, our approach can also
be applied to tracing multi-scale cell lineage, identifying critical stages, or
creating pseudo-time series."
5516,"the predictive performance is attributable to these fea-

                                                            9
   Identifying new features may reveal previously un-               The examination of feature importance highlighted
known connections between components of the system               an obscure biophysical link in the case of carbon diox-
for further study with the potential to improve under-           ide concentration and methane ﬂux which improve our
standing of the underlying biophysical processes.",Eﬀectively this means that the majority of         low the same physical processes as evaporation.,"This           understanding of the physical and biological processes
process is signiﬁcantly enabled by our objective and             involved.",2022-04-29 15:00:21+00:00,A Framework for Constructing Machine Learning Models with Feature Set Optimisation for Evapotranspiration Partitioning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Adam Stapleton'), arxiv.Result.Author('Elke Eichelmann'), arxiv.Result.Author('Mark Roantree')]","A deeper understanding of the drivers of evapotranspiration and the modelling
of its constituent parts (evaporation and transpiration) could be of
significant importance to the monitoring and management of water resources
globally over the coming decades. In this work, we developed a framework to
identify the best performing machine learning algorithm from a candidate set,
select optimal predictive features as well as ranking features in terms of
their importance to predictive accuracy. Our experiments used 3 separate
feature sets across 4 wetland sites as input into 8 candidate machine learning
algorithms, providing 96 sets of experimental configurations. Given this high
number of parameters, our results show strong evidence that there is no
singularly optimal machine learning algorithm or feature set across all of the
wetland sites studied despite their similarities. A key finding discovered when
examining feature importance is that methane flux, a feature whose relationship
with evapotranspiration is not generally examined, may contribute to further
biophysical process understanding."
5574,"We believe that this problem is pretty
                u¯k = √ (uˇk(x¯1), ..., uˇk(x¯m))                  important and worthy of further study.","One limitation of this paper is that we don’t pro-
                                                                   vide a true convergence rate for the excess risk of the empir-
                          1                                        ical discrete solution.","λk

to obtain the eigenvectors of X¯ for relaxed RatioCut and use

                    u¯k = (uˇk(x¯1), ..., uˇk(x¯m)
Acknowledgments

We appreciate all the anonymous reviewers for their invaluable comments.",2022-04-30 14:21:56+00:00,Understanding the Generalization Performance of Spectral Clustering Algorithms,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Shaojie Li'), arxiv.Result.Author('Sheng Ouyang'), arxiv.Result.Author('Yong Liu')]","The theoretical analysis of spectral clustering mainly focuses on
consistency, while there is relatively little research on its generalization
performance. In this paper, we study the excess risk bounds of the popular
spectral clustering algorithms: \emph{relaxed} RatioCut and \emph{relaxed}
NCut. Firstly, we show that their excess risk bounds between the empirical
continuous optimal solution and the population-level continuous optimal
solution have a $\mathcal{O}(1/\sqrt{n})$ convergence rate, where $n$ is the
sample size. Secondly, we show the fundamental quantity in influencing the
excess risk between the empirical discrete optimal solution and the
population-level discrete optimal solution. At the empirical level, algorithms
can be designed to reduce this quantity. Based on our theoretical analysis, we
propose two novel algorithms that can not only penalize this quantity, but also
cluster the out-of-sample data without re-eigendecomposition on the overall
sample. Experiments verify the effectiveness of the proposed algorithms."
5575,"We believe that this problem is pretty  [Liu et al., 2018] Fuchen Liu, David Choi, Lu Xie, and
important and worthy of further study.","One limitation of this paper is that we don’t pro-     ence on Machine Learning, pages 6392–6402, 2021.
vide a true convergence rate for the excess risk of the empir-
ical discrete solution.",Kathryn Roeder.,2022-04-30 14:21:56+00:00,Understanding the Generalization Performance of Spectral Clustering Algorithms,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Shaojie Li'), arxiv.Result.Author('Sheng Ouyang'), arxiv.Result.Author('Yong Liu')]","The theoretical analysis of spectral clustering mainly focuses on
consistency, while there is relatively little research on its generalization
performance. In this paper, we study the excess risk bounds of the popular
spectral clustering algorithms: \emph{relaxed} RatioCut and \emph{relaxed}
NCut. Firstly, we show that their excess risk bounds between the empirical
continuous optimal solution and the population-level continuous optimal
solution have a $\mathcal{O}(1/\sqrt{n})$ convergence rate, where $n$ is the
sample size. Secondly, we show the fundamental quantity in influencing the
excess risk between the empirical discrete optimal solution and the
population-level discrete optimal solution. At the empirical level, algorithms
can be designed to reduce this quantity. Based on our theoretical analysis, we
propose two novel algorithms that can not only penalize this quantity, but also
cluster the out-of-sample data without re-eigendecomposition on the overall
sample. Experiments verify the effectiveness of the proposed algorithms."
5581,"This diverse set of datasets can be used for further research in CL at a signiﬁcantly lower cost than
end-to-end training on raw data.","The
variability of the CL results across encoders and classiﬁers (MLP, NMC, SLDA) is also a good indicator of encoded
dataset diversity.",It does not mean that experimentation should not be done in an end-to-end setting.,2022-04-30 19:11:37+00:00,Foundational Models for Continual Learning: An Empirical Study of Latent Replay,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Oleksiy Ostapenko'), arxiv.Result.Author('Timothee Lesort'), arxiv.Result.Author('Pau Rodríguez'), arxiv.Result.Author('Md Rifat Arefin'), arxiv.Result.Author('Arthur Douillard'), arxiv.Result.Author('Irina Rish'), arxiv.Result.Author('Laurent Charlin')]","Rapid development of large-scale pre-training has resulted in foundation
models that can act as effective feature extractors on a variety of downstream
tasks and domains. Motivated by this, we study the efficacy of pre-trained
vision models as a foundation for downstream continual learning (CL) scenarios.
Our goal is twofold. First, we want to understand the compute-accuracy
trade-off between CL in the raw-data space and in the latent space of
pre-trained encoders. Second, we investigate how the characteristics of the
encoder, the pre-training algorithm and data, as well as of the resulting
latent space affect CL performance. For this, we compare the efficacy of
various pre-trained models in large-scale benchmarking scenarios with a vanilla
replay setting applied in the latent and in the raw-data space. Notably, this
study shows how transfer, forgetting, task similarity and learning are
dependent on the input data characteristics and not necessarily on the CL
algorithms. First, we show that under some circumstances reasonable CL
performance can readily be achieved with a non-parametric classifier at
negligible compute. We then show how models pre-trained on broader data result
in better performance for various replay sizes. We explain this with
representational similarity and transfer properties of these representations.
Finally, we show the effectiveness of self-supervised pre-training for
downstream domains that are out-of-distribution as compared to the pre-training
domain. We point out and validate several research directions that can further
increase the efficacy of latent CL including representation ensembling. The
diverse set of datasets used in this study can serve as a compute-efficient
playground for further CL research. The codebase is available under
https://github.com/oleksost/latent_CL."
5582,"This diverse set of datasets can be used for further research in CL at a signiﬁcantly lower cost than
end-to-end training on raw data.","The
variability of the CL results across encoders and classiﬁers (MLP, NMC, SLDA) is also a good indicator of encoded
dataset diversity.",It does not mean that experimentation should not be done in an end-to-end setting.,2022-04-30 19:11:37+00:00,Continual Learning with Foundation Models: An Empirical Study of Latent Replay,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Oleksiy Ostapenko'), arxiv.Result.Author('Timothee Lesort'), arxiv.Result.Author('Pau Rodríguez'), arxiv.Result.Author('Md Rifat Arefin'), arxiv.Result.Author('Arthur Douillard'), arxiv.Result.Author('Irina Rish'), arxiv.Result.Author('Laurent Charlin')]","Rapid development of large-scale pre-training has resulted in foundation
models that can act as effective feature extractors on a variety of downstream
tasks and domains. Motivated by this, we study the efficacy of pre-trained
vision models as a foundation for downstream continual learning (CL) scenarios.
Our goal is twofold. First, we want to understand the compute-accuracy
trade-off between CL in the raw-data space and in the latent space of
pre-trained encoders. Second, we investigate how the characteristics of the
encoder, the pre-training algorithm and data, as well as of the resulting
latent space affect CL performance. For this, we compare the efficacy of
various pre-trained models in large-scale benchmarking scenarios with a vanilla
replay setting applied in the latent and in the raw-data space. Notably, this
study shows how transfer, forgetting, task similarity and learning are
dependent on the input data characteristics and not necessarily on the CL
algorithms. First, we show that under some circumstances reasonable CL
performance can readily be achieved with a non-parametric classifier at
negligible compute. We then show how models pre-trained on broader data result
in better performance for various replay sizes. We explain this with
representational similarity and transfer properties of these representations.
Finally, we show the effectiveness of self-supervised pre-training for
downstream domains that are out-of-distribution as compared to the pre-training
domain. We point out and validate several research directions that can further
increase the efficacy of latent CL including representation ensembling. The
diverse set of datasets used in this study can serve as a compute-efficient
playground for further CL research. The codebase is available under
https://github.com/oleksost/latent_CL."
5599,"xi = xj , ∀i, j ∈ [N ], (2)

omnifaceted overview of DOL, which hopefully can motivate                           i=1

and facilitate further study in this ﬁeld.","To our best knowledge, this paper is the ﬁrst to report an      ft(x) = fi,t(xi), s.t.","where x := col(x1, .",2022-05-01 14:10:26+00:00,A Survey of Decentralized Online Learning,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Xiuxian Li'), arxiv.Result.Author('Lihua Xie'), arxiv.Result.Author('Na Li')]","Decentralized online learning (DOL) has been increasingly researched in the
last decade, mostly motivated by its wide applications in sensor networks,
commercial buildings, robotics (e.g., decentralized target tracking and
formation control), smart grids, deep learning, and so forth. In this problem,
there are a network of agents who may be cooperative (i.e., decentralized
online optimization) or noncooperative (i.e., online game) through local
information exchanges, and the local cost function of each agent is often
time-varying in dynamic and even adversarial environments. At each time, a
decision must be made by each agent based on historical information at hand
without knowing future information on cost functions. Although this problem has
been extensively studied in the last decade, a comprehensive survey is lacking.
Therefore, this paper provides a thorough overview of DOL from the perspective
of problem settings, communication, computation, and performances. In addition,
some potential future directions are also discussed in details."
5628,"Therefore, we envision further research in those areas, which may additionally
provide some indication of how widely representations can vary.","The effects of expertise or developmental stage in mental representations are of obvious interest to
cognitive scientists.","9 Conclusion

One of the central goals in the cognitive sciences is the development of computational models
of mental representations of object concepts.",2022-05-02 09:03:55+00:00,VICE: Variational Interpretable Concept Embeddings,cs.LG,"['cs.LG', 'stat.AP', 'stat.ML']","[arxiv.Result.Author('Lukas Muttenthaler'), arxiv.Result.Author('Charles Y. Zheng'), arxiv.Result.Author('Patrick McClure'), arxiv.Result.Author('Robert A. Vandermeulen'), arxiv.Result.Author('Martin N. Hebart'), arxiv.Result.Author('Francisco Pereira')]","A central goal in the cognitive sciences is the development of numerical
models for mental representations of object concepts. This paper introduces
Variational Interpretable Concept Embeddings (VICE), an approximate Bayesian
method for embedding object concepts in a vector space using data collected
from humans in a triplet odd-one-out task. VICE uses variational inference to
obtain sparse, non-negative representations of object concepts with uncertainty
estimates for the embedding values. These estimates are used to automatically
select the dimensions that best explain the data. We derive a PAC learning
bound for VICE that can be used to estimate generalization performance or
determine a sufficient sample size for experimental design. VICE rivals or
outperforms its predecessor, SPoSE, at predicting human behavior in the triplet
odd-one-out task. Furthermore, VICE's object representations are more
reproducible and consistent across random initializations, highlighting the
unique advantage of using VICE for deriving interpretable embeddings from human
behavior."
5646,"requires
    Lastly, further research is needed on novel approaches         smart data capture strategies to reduce capture overhead) in-
proposing rigorous methodologies and systems for repro-            terconnected by diﬀerent network capabilities (e.g.","heterogeneous hardware resources ranging from HPC/Cloud
                                                                   servers to resource constrained Edge devices (e.g.","requires
ducible experimental evaluations to enable the performance         provenance data transmission balancing to mitigate the net-
comparison of AI models and learning paradigms deployed            work overhead).",2022-04-29 08:06:05+00:00,Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Daniel Rosendo'), arxiv.Result.Author('Alexandru Costan'), arxiv.Result.Author('Patrick Valduriez'), arxiv.Result.Author('Gabriel Antoniu')]","The explosion of data volumes generated by an increasing number of
applications is strongly impacting the evolution of distributed digital
infrastructures for data analytics and machine learning (ML). While data
analytics used to be mainly performed on cloud infrastructures, the rapid
development of IoT infrastructures and the requirements for low-latency, secure
processing has motivated the development of edge analytics. Today, to balance
various trade-offs, ML-based analytics tends to increasingly leverage an
interconnected ecosystem that allows complex applications to be executed on
hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC
systems in what is called the Computing Continuum, the Digital Continuum, or
the Transcontinuum.Enabling learning-based analytics on such complex
infrastructures is challenging. The large scale and optimized deployment of
learning-based workflows across the Edge-to-Cloud Continuum requires extensive
and reproducible experimental analysis of the application execution on
representative testbeds. This is necessary to help understand the performance
trade-offs that result from combining a variety of learning paradigms and
supportive frameworks. A thorough experimental analysis requires the assessment
of the impact of multiple factors, such as: model accuracy, training time,
network overhead, energy consumption, processing latency, among others.This
review aims at providing a comprehensive vision of the main state-of-the-art
libraries and frameworks for machine learning and data analytics available
today. It describes the main learning paradigms enabling learning-based
analytics on the Edge-to-Cloud Continuum. The main simulation, emulation,
deployment systems, and testbeds for experimental research on the Edge-to-Cloud
Continuum available today are also surveyed. Furthermore, we analyze how the
selected systems provide support for experiment reproducibility. We conclude
our review with a detailed discussion of relevant open research challenges and
of future directions in this domain such as: holistic understanding of
performance; performance optimization of applications;efficient deployment of
Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures;
and reproducible analysis of experiments on the Computing Continuum."
5676,"Ablation Study on Model Architecture
    To further study the inﬂuence of MSTFGRN’s various modules, we cre-

ated four variants of the MSTFGRN-based model and compared MSTFGRN
to these four variants on the PeMS04 and PeMS08 datasets.",4.5.,"Below are the
distinctions between these four model kinds.",2022-05-03 13:23:38+00:00,Multi-Spatio-temporal Fusion Graph Recurrent Network for Traffic forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Wei Zhao'), arxiv.Result.Author('Shiqi Zhang'), arxiv.Result.Author('Bing Zhou'), arxiv.Result.Author('Bei Wang')]","Traffic forecasting is essential for the traffic construction of smart cities
in the new era. However, traffic data's complex spatial and temporal
dependencies make traffic forecasting extremely challenging. Most existing
traffic forecasting methods rely on the predefined adjacency matrix to model
the Spatio-temporal dependencies. Nevertheless, the road traffic state is
highly real-time, so the adjacency matrix should change dynamically with time.
This article presents a new Multi-Spatio-temporal Fusion Graph Recurrent
Network (MSTFGRN) to address the issues above. The network proposes a
data-driven weighted adjacency matrix generation method to compensate for
real-time spatial dependencies not reflected by the predefined adjacency
matrix. It also efficiently learns hidden Spatio-temporal dependencies by
performing a new two-way Spatio-temporal fusion operation on parallel
Spatio-temporal relations at different moments. Finally, global Spatio-temporal
dependencies are captured simultaneously by integrating a global attention
mechanism into the Spatio-temporal fusion module. Extensive trials on four
large-scale, real-world traffic datasets demonstrate that our method achieves
state-of-the-art performance compared to alternative baselines."
5677,"More generally, we hope that our work opens up further research on dimensionality re-
duction in diffusion processes, particularly to nonlinear manifolds and/or learned substructures, as a
means of both simplifying and improving score-based generative models.","Potential avenues of future work include applying sub-
space diffusion to other data domains and combining it with step-size based methods for accelerating
inference.","ACKNOWLEDGMENTS

We thank Yilun Du, Xiang Fu, Jason Yim, Shangyuan Tong, Yilun Xu, Felix Faltings, and Saro Pas-
saro for helpful feedback and discussions.",2022-05-03 13:43:47+00:00,Subspace Diffusion Generative Models,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Bowen Jing'), arxiv.Result.Author('Gabriele Corso'), arxiv.Result.Author('Renato Berlinghieri'), arxiv.Result.Author('Tommi Jaakkola')]","Score-based models generate samples by mapping noise to data (and vice versa)
via a high-dimensional diffusion process. We question whether it is necessary
to run this entire process at high dimensionality and incur all the
inconveniences thereof. Instead, we restrict the diffusion via projections onto
subspaces as the data distribution evolves toward noise. When applied to
state-of-the-art models, our framework simultaneously improves sample quality
-- reaching an FID of 2.17 on unconditional CIFAR-10 -- and reduces the
computational cost of inference for the same number of denoising steps. Our
framework is fully compatible with continuous-time diffusion and retains its
flexible capabilities, including exact log-likelihoods and controllable
generation. Code is available at
https://github.com/bjing2016/subspace-diffusion."
5678,"of deep learning, we need to further study the trade-off in                                                                                                                  2
depth.","Therefore, knowing the underlying reasons for this trade-off

                                       • J. Zhang, D. Zhang and J. Chen are with the Division of Computational
                                            physics and Intelligent modeling, Northwest Institute of Nuclear
                                            Technology, Xi’an 710024, China
                                            E-mail: zjacob@mail.ustc.edu.cn

                                       • L. Pang is with the Key Laboratory of Quark & Lepton Physics of Ministry
                                            of Education, Central China Normal University, Wuhan 430079, China.","direction of the FGSM attack increases both the test and
    To understand the phenomenon, various empirical               robust accuracies (Fig.",2022-05-03 13:48:12+00:00,On the uncertainty principle of neural networks,cs.LG,"['cs.LG', 'physics.comp-ph', 'quant-ph']","[arxiv.Result.Author('Jun-Jie Zhang'), arxiv.Result.Author('Dong-Xiao Zhang'), arxiv.Result.Author('Jian-Nan Chen'), arxiv.Result.Author('Long-Gang Pang')]","Despite the successes in many fields, it is found that neural networks are
vulnerability and difficult to be both accurate and robust (robust means that
the prediction of the trained network stays unchanged for inputs with
non-random perturbations introduced by adversarial attacks). Various empirical
and analytic studies have suggested that there is more or less a trade-off
between the accuracy and robustness of neural networks. If the trade-off is
inherent, applications based on the neural networks are vulnerable with
untrustworthy predictions. It is then essential to ask whether the trade-off is
an inherent property or not. Here, we show that the accuracy-robustness
trade-off is an intrinsic property whose underlying mechanism is deeply related
to the uncertainty principle in quantum mechanics. We find that for a neural
network to be both accurate and robust, it needs to resolve the features of the
two conjugated parts $x$ (the inputs) and $\Delta$ (the derivatives of the
normalized loss function $J$ with respect to $x$), respectively. Analogous to
the position-momentum conjugation in quantum mechanics, we show that the inputs
and their conjugates cannot be resolved by a neural network simultaneously."
5679,"1.2 Motivation
                                            Seeing the fact that more and more researchers are
                                        seeking to understand the neural networks, it is crucial                     To the best of our knowledge, the underlying theoretical
                                        for us to further study the accuracy-robustness trade-off                    reason for this accuracy-robustness trade-off is still
                                        of these networks as aforementioned.","[15], [16], [17].","Meanwhile, since                        unknown so far, and it is still not sure whether we can
                                        many researchers are using neural networks in their                          ultimately invent a neural network with both sufﬁcient
                                        investigations, it is also important for us to explore that if               accuracy and robustness.",2022-05-03 13:48:12+00:00,On the uncertainty principle of neural networks,cs.LG,"['cs.LG', 'physics.comp-ph', 'quant-ph']","[arxiv.Result.Author('Jun-Jie Zhang'), arxiv.Result.Author('Dong-Xiao Zhang'), arxiv.Result.Author('Jian-Nan Chen'), arxiv.Result.Author('Long-Gang Pang'), arxiv.Result.Author('Deyu Meng')]","Despite the successes in many fields, it is found that neural networks are
difficult to be both accurate and robust, i.e., high accuracy networks are
often vulnerable. Various empirical and analytic studies have substantiated
that there is more or less a trade-off between the accuracy and robustness of
neural networks. If the property is inherent, applications based on the neural
networks are vulnerable with untrustworthy predictions. To more deeply explore
and understand this issue, in this study we show that the accuracy-robustness
trade-off is an intrinsic property whose underlying mechanism is closely
related to the uncertainty principle in quantum mechanics. By relating the loss
function in neural networks to the wave function in quantum mechanics, we show
that the inputs and their conjugates cannot be resolved by a neural network
simultaneously. This work thus provides an insightful explanation for the
inevitability of the accuracy-robustness dilemma for general deep networks from
an entirely new perspective, and furthermore, reveals a potential possibility
to study various properties of neural networks with the mature mathematical
tools in quantum physics."
5680,"at the foundations of deep learning, it is then crucial to
                                                                                                                     further study this trade-off issue in depth.","If,
                                             of Ministry of Education, Central China Normal University, Wuhan        otherwise, it involves some intrinsic properties which stand
                                             430079, China.","• D. Meng is with School of Mathematics and Statistics and Ministry of
                                             Education Key Lab of Intelligent Networks and Network Security, Xi’an
                                             Jiaotong University, Shaanxi, P. R. China.",2022-05-03 13:48:12+00:00,On the uncertainty principle of neural networks,cs.LG,"['cs.LG', 'physics.comp-ph', 'quant-ph']","[arxiv.Result.Author('Jun-Jie Zhang'), arxiv.Result.Author('Dong-Xiao Zhang'), arxiv.Result.Author('Jian-Nan Chen'), arxiv.Result.Author('Long-Gang Pang'), arxiv.Result.Author('Deyu Meng')]","Despite the successes in many fields, it is found that neural networks are
difficult to be both accurate and robust, i.e., high accuracy networks are
often vulnerable. Various empirical and analytic studies have substantiated
that there is more or less a trade-off between the accuracy and robustness of
neural networks. If the property is inherent, applications based on the neural
networks are vulnerable with untrustworthy predictions. To more deeply explore
and understand this issue, in this study we show that the accuracy-robustness
trade-off is an intrinsic property whose underlying mechanism is closely
related to the uncertainty principle in quantum mechanics. By relating the loss
function in neural networks to the wave function in quantum mechanics, we show
that the inputs and their conjugates cannot be resolved by a neural network
simultaneously. This work thus provides an insightful explanation for the
inevitability of the accuracy-robustness dilemma for general deep networks from
an entirely new perspective, and furthermore, reveals a potential possibility
to study various properties of neural networks with the mature mathematical
tools in quantum physics."
5681,"1.2 Motivation
                                            Seeing the fact that more and more researchers are                       To the best of our knowledge, the underlying theoretical
                                        seeking to understand the neural networks, it is crucial                     reason for this accuracy-robustness trade-off is still
                                        for us to further study the accuracy-robustness trade-off                    unknown so far, and it is still not sure whether we can
                                        of these networks as aforementioned.","[15], [16], [17].","Meanwhile, since                        ultimately invent a neural network with both sufﬁcient
                                        many researchers are using neural networks in their                          accuracy and robustness.",2022-05-03 13:48:12+00:00,On the uncertainty principle of neural networks,cs.LG,"['cs.LG', 'physics.comp-ph', 'quant-ph']","[arxiv.Result.Author('Jun-Jie Zhang'), arxiv.Result.Author('Dong-Xiao Zhang'), arxiv.Result.Author('Jian-Nan Chen'), arxiv.Result.Author('Long-Gang Pang'), arxiv.Result.Author('Deyu Meng')]","Despite the successes in many fields, it is found that neural networks are
difficult to be both accurate and robust, i.e., high accuracy networks are
often vulnerable. Various empirical and analytic studies have substantiated
that there is more or less a trade-off between the accuracy and robustness of
neural networks. If the property is inherent, applications based on the neural
networks are vulnerable with untrustworthy predictions. To more deeply explore
and understand this issue, in this study we show that the accuracy-robustness
trade-off is an intrinsic property whose underlying mechanism is closely
related to the uncertainty principle in quantum mechanics. By relating the loss
function in neural networks to the wave function in quantum mechanics, we show
that the inputs and their conjugates cannot be resolved by a neural network
simultaneously. This work thus provides an insightful explanation for the
inevitability of the accuracy-robustness dilemma for general deep networks from
an entirely new perspective, and furthermore, reveals a potential possibility
to study various properties of neural networks with the mature mathematical
tools in quantum physics."
5682,further study this trade-off issue in depth.,"If,
                                        • L.-G. Pang is with the Key Laboratory of Quark & Lepton Physics            otherwise, it involves some intrinsic properties which stand
                                             of Ministry of Education, Central China Normal University, Wuhan        at the foundations of deep learning, it is then crucial to
                                             430079, China.","• D. Meng is with School of Mathematics and Statistics and Ministry of
                                             Education Key Lab of Intelligent Networks and Network Security, Xi’an
                                             Jiaotong University, Shaanxi, P. R. China.",2022-05-03 13:48:12+00:00,On the uncertainty principle of neural networks,cs.LG,"['cs.LG', 'physics.comp-ph', 'quant-ph']","[arxiv.Result.Author('Jun-Jie Zhang'), arxiv.Result.Author('Dong-Xiao Zhang'), arxiv.Result.Author('Jian-Nan Chen'), arxiv.Result.Author('Long-Gang Pang'), arxiv.Result.Author('Deyu Meng')]","Despite the successes in many fields, it is found that neural networks are
difficult to be both accurate and robust, i.e., high accuracy networks are
often vulnerable. Various empirical and analytic studies have substantiated
that there is more or less a trade-off between the accuracy and robustness of
neural networks. If the property is inherent, applications based on the neural
networks are vulnerable with untrustworthy predictions. To more deeply explore
and understand this issue, in this study we show that the accuracy-robustness
trade-off is an intrinsic property whose underlying mechanism is closely
related to the uncertainty principle in quantum mechanics. By relating the loss
function in neural networks to the wave function in quantum mechanics, we show
that the inputs and their conjugates cannot be resolved by a neural network
simultaneously. This work thus provides an insightful explanation for the
inevitability of the accuracy-robustness dilemma for general deep networks from
an entirely new perspective, and furthermore, reveals a potential possibility
to study various properties of neural networks with the mature mathematical
tools in quantum physics."
5685,"To improve its performance in this context,
                 50 2 3 161, 300 97.91                            the development and application of task-dependent in-
                 50 2 4 164, 800 98.26                            ner functions for speciﬁc feature extraction might be
                 50 4 2 315, 600 96.59                            an interesting direction of further research, for exam-
                 50 4 3 318, 100 97.87                            ple, the investigation of convolutional spline operators
                 50 4 4 321, 600 97.94                            such as in [21].","15
                  T N M Params % Acc                              purpose model, not designed nor tailored for this spe-
                 50 2 2 158, 800 96.25                            ciﬁc task.","100 2 2 317, 600 97.28
               100 2 3 322, 600 98.34                             Regression.",2022-05-03 14:06:36+00:00,ExSpliNet: An interpretable and expressive spline-based neural network,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Daniele Fakhoury'), arxiv.Result.Author('Emanuele Fakhoury'), arxiv.Result.Author('Hendrik Speleers')]","In this paper we present ExSpliNet, an interpretable and expressive neural
network model. The model combines ideas of Kolmogorov neural networks,
ensembles of probabilistic trees, and multivariate B-spline representations. We
give a probabilistic interpretation of the model and show its universal
approximation properties. We also discuss how it can be efficiently encoded by
exploiting B-spline properties. Finally, we test the effectiveness of the
proposed model on synthetic approximation problems and classical machine
learning benchmark datasets."
5716,"We publicly share six new TripAdvisor datasets [9] corresponding to the
       cities of Barcelona, Madrid, New York, New Delhi, London and Paris to
       support further research regarding dyadic data explanation tasks.",4.,"The remainder of the article is organised as follows: Section 2 presents the
state-of-the-art works in relation to the explanation of pairs (user, item) in
recommender systems and particularly in the hospitality context.",2022-05-03 20:04:32+00:00,Explain and Conquer: Personalised Text-based Reviews to Achieve Transparency,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.IR', 'cs.SI', 'I.2.7; I.5.1; I.5.2; I.5.3; I.5.4']","[arxiv.Result.Author('Iñigo López-Riobóo Botana'), arxiv.Result.Author('Verónica Bolón-Canedo'), arxiv.Result.Author('Bertha Guijarro-Berdiñas'), arxiv.Result.Author('Amparo Alonso-Betanzos')]","There are many contexts in which dyadic data are present. Social networks are
a well-known example. In these contexts, pairs of elements are linked building
a network that reflects interactions. Explaining why these relationships are
established is essential to obtain transparency, an increasingly important
notion. These explanations are often presented using text, thanks to the spread
of the natural language understanding tasks. Our aim is to represent and
explain pairs established by any agent (e.g., a recommender system or a paid
promotion mechanism), so that text-based personalisation is taken into account.
We have focused on the TripAdvisor platform, considering the applicability to
other dyadic data contexts. The items are a subset of users and restaurants and
the interactions the reviews posted by these users. We propose the PTER
(Personalised TExt-based Reviews) model. We predict, from the available reviews
for a given restaurant, those that fit to the specific user interactions. PTER
leverages the BERT (Bidirectional Encoders Representations from Transformers)
transformer-encoder model. We customised a deep neural network following the
feature-based approach, presenting a LTR (Learning To Rank) downstream task. We
carried out several comparisons of our proposal with a random baseline and
other models of the state of the art, following the EXTRA (EXplanaTion RAnking)
benchmark. Our method outperforms other collaborative filtering proposals."
5753,"We start
by discussing the intertwined historical development of attacks and defenses, and then highlight the corresponding
challenges, open questions, and promising avenues for further research.","7 DEVELOPMENT, CHALLENGES, AND FUTURE RESEARCH DIRECTIONS

In this section, we outline challenges and future research directions for poisoning attacks and defenses.","7.1 Development Timelines for Poisoning Attacks and Defenses

We start by discussing the historical development of poisoning attacks (represented in Fig.",2022-05-04 11:00:26+00:00,Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Antonio Emanuele Cinà'), arxiv.Result.Author('Kathrin Grosse'), arxiv.Result.Author('Ambra Demontis'), arxiv.Result.Author('Sebastiano Vascon'), arxiv.Result.Author('Werner Zellinger'), arxiv.Result.Author('Bernhard A. Moser'), arxiv.Result.Author('Alina Oprea'), arxiv.Result.Author('Battista Biggio'), arxiv.Result.Author('Marcello Pelillo'), arxiv.Result.Author('Fabio Roli')]","The success of machine learning is fueled by the increasing availability of
computing power and large training datasets. The training data is used to learn
new models or update existing ones, assuming that it is sufficiently
representative of the data that will be encountered at test time. This
assumption is challenged by the threat of poisoning, an attack that manipulates
the training data to compromise the model's performance at test time. Although
poisoning has been acknowledged as a relevant threat in industry applications,
and a variety of different attacks and defenses have been proposed so far, a
complete systematization and critical review of the field is still missing. In
this survey, we provide a comprehensive systematization of poisoning attacks
and defenses in machine learning, reviewing more than 200 papers published in
the field in the last 15 years. We start by categorizing the current threat
models and attacks, and then organize existing defenses accordingly. While we
focus mostly on computer-vision applications, we argue that our systematization
also encompasses state-of-the-art attacks and defenses for other data
modalities. Finally, we discuss existing resources for research in poisoning,
and shed light on the current limitations and open research questions in this
research field."
5754,"In conclusion, we believe our contribution can help clarify
what threats an ML system may encounter in adversarial settings and encourage further research developments in
deploying trustworthy systems even in the presence of data poisoning threats.","Finally, we trace the historical development of data literature since
the early developments dating back to more than 20 years ago and find the open challenges and possible research
directions that can pave the way for future development.","REFERENCES

  [1] Hojjat Aghakhani, Thorsten Eisenhofer, Lea Schönherr, Dorothea Kolossa, Thorsten Holz, Christopher Kruegel, and Giovanni Vigna.",2022-05-04 11:00:26+00:00,Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Antonio Emanuele Cinà'), arxiv.Result.Author('Kathrin Grosse'), arxiv.Result.Author('Ambra Demontis'), arxiv.Result.Author('Sebastiano Vascon'), arxiv.Result.Author('Werner Zellinger'), arxiv.Result.Author('Bernhard A. Moser'), arxiv.Result.Author('Alina Oprea'), arxiv.Result.Author('Battista Biggio'), arxiv.Result.Author('Marcello Pelillo'), arxiv.Result.Author('Fabio Roli')]","The success of machine learning is fueled by the increasing availability of
computing power and large training datasets. The training data is used to learn
new models or update existing ones, assuming that it is sufficiently
representative of the data that will be encountered at test time. This
assumption is challenged by the threat of poisoning, an attack that manipulates
the training data to compromise the model's performance at test time. Although
poisoning has been acknowledged as a relevant threat in industry applications,
and a variety of different attacks and defenses have been proposed so far, a
complete systematization and critical review of the field is still missing. In
this survey, we provide a comprehensive systematization of poisoning attacks
and defenses in machine learning, reviewing more than 200 papers published in
the field in the last 15 years. We start by categorizing the current threat
models and attacks, and then organize existing defenses accordingly. While we
focus mostly on computer-vision applications, we argue that our systematization
also encompasses state-of-the-art attacks and defenses for other data
modalities. Finally, we discuss existing resources for research in poisoning,
and shed light on the current limitations and open research questions in this
research field."
5755,"We start
by discussing the intertwined historical development of attacks and defenses, and then highlight the corresponding
challenges, open questions, and promising avenues for further research.","7 DEVELOPMENT, CHALLENGES, AND FUTURE RESEARCH DIRECTIONS
In this section, we outline challenges and future research directions for poisoning attacks and defenses.","7.1 Development Timelines for Poisoning Attacks and Defenses
We start by discussing the historical development of poisoning attacks (represented in Fig.",2022-05-04 11:00:26+00:00,Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Antonio Emanuele Cinà'), arxiv.Result.Author('Kathrin Grosse'), arxiv.Result.Author('Ambra Demontis'), arxiv.Result.Author('Sebastiano Vascon'), arxiv.Result.Author('Werner Zellinger'), arxiv.Result.Author('Bernhard A. Moser'), arxiv.Result.Author('Alina Oprea'), arxiv.Result.Author('Battista Biggio'), arxiv.Result.Author('Marcello Pelillo'), arxiv.Result.Author('Fabio Roli')]","The success of machine learning is fueled by the increasing availability of
computing power and large training datasets. The training data is used to learn
new models or update existing ones, assuming that it is sufficiently
representative of the data that will be encountered at test time. This
assumption is challenged by the threat of poisoning, an attack that manipulates
the training data to compromise the model's performance at test time. Although
poisoning has been acknowledged as a relevant threat in industry applications,
and a variety of different attacks and defenses have been proposed so far, a
complete systematization and critical review of the field is still missing. In
this survey, we provide a comprehensive systematization of poisoning attacks
and defenses in machine learning, reviewing more than 100 papers published in
the field in the last 15 years. We start by categorizing the current threat
models and attacks, and then organize existing defenses accordingly. While we
focus mostly on computer-vision applications, we argue that our systematization
also encompasses state-of-the-art attacks and defenses for other data
modalities. Finally, we discuss existing resources for research in poisoning,
and shed light on the current limitations and open research questions in this
research field."
5756,"In conclusion, we believe our contribution can help clarify
what threats an ML system may encounter in adversarial settings and encourage further research developments in
deploying trustworthy systems even in the presence of data poisoning threats.","Finally, we trace the historical development of data literature since
the early developments dating back to more than 20 years ago and find the open challenges and possible research
directions that can pave the way for future development.","ACKNOWLEDGMENTS

This work has been partially supported by the PRIN 2017 project RexLearn (grant no.",2022-05-04 11:00:26+00:00,Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Antonio Emanuele Cinà'), arxiv.Result.Author('Kathrin Grosse'), arxiv.Result.Author('Ambra Demontis'), arxiv.Result.Author('Sebastiano Vascon'), arxiv.Result.Author('Werner Zellinger'), arxiv.Result.Author('Bernhard A. Moser'), arxiv.Result.Author('Alina Oprea'), arxiv.Result.Author('Battista Biggio'), arxiv.Result.Author('Marcello Pelillo'), arxiv.Result.Author('Fabio Roli')]","The success of machine learning is fueled by the increasing availability of
computing power and large training datasets. The training data is used to learn
new models or update existing ones, assuming that it is sufficiently
representative of the data that will be encountered at test time. This
assumption is challenged by the threat of poisoning, an attack that manipulates
the training data to compromise the model's performance at test time. Although
poisoning has been acknowledged as a relevant threat in industry applications,
and a variety of different attacks and defenses have been proposed so far, a
complete systematization and critical review of the field is still missing. In
this survey, we provide a comprehensive systematization of poisoning attacks
and defenses in machine learning, reviewing more than 100 papers published in
the field in the last 15 years. We start by categorizing the current threat
models and attacks, and then organize existing defenses accordingly. While we
focus mostly on computer-vision applications, we argue that our systematization
also encompasses state-of-the-art attacks and defenses for other data
modalities. Finally, we discuss existing resources for research in poisoning,
and shed light on the current limitations and open research questions in this
research field."
5785,"The
                                                package is released under a MIT license and structured in such a way to
                                                foster further research into sampling, walking, and embedding strategies,
                                                which are vital components of the RDF2Vec algorithm.","By making the algorithm available in the most
                                                popular data science language, and by bundling all extensions into a
                                                single place, the use of RDF2Vec is simpliﬁed for data scientists.","Several optimi-
                                                sations have been implemented in pyRDF2Vec that allow for more eﬃcient
                                                walk extraction than the original algorithm.",2022-05-04 18:39:07+00:00,pyRDF2Vec: A Python Implementation and Extension of RDF2Vec,cs.LG,['cs.LG'],"[arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Bram Steenwinckel'), arxiv.Result.Author('Terencio Agozzino'), arxiv.Result.Author('Femke Ongenae')]","This paper introduces pyRDF2Vec, a Python software package that reimplements
the well-known RDF2Vec algorithm along with several of its extensions. By
making the algorithm available in the most popular data science language, and
by bundling all extensions into a single place, the use of RDF2Vec is
simplified for data scientists. The package is released under a MIT license and
structured in such a way to foster further research into sampling, walking, and
embedding strategies, which are vital components of the RDF2Vec algorithm.
Several optimisations have been implemented in \texttt{pyRDF2Vec} that allow
for more efficient walk extraction than the original algorithm. Furthermore,
best practices in terms of code styling, testing, and documentation were
applied such that the package is future-proof as well as to facilitate external
contributions."
5786,"The code is released under an
open-source license and is written in a way to facilitate further research into the
diﬀerent components of the RDF2Vec algorithm.","Moreover, various mechanisms are
built in which allows to better handle large KGs.","The remainder of this paper
is structured as follows.",2022-05-04 18:39:07+00:00,pyRDF2Vec: A Python Implementation and Extension of RDF2Vec,cs.LG,['cs.LG'],"[arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Bram Steenwinckel'), arxiv.Result.Author('Terencio Agozzino'), arxiv.Result.Author('Femke Ongenae')]","This paper introduces pyRDF2Vec, a Python software package that reimplements
the well-known RDF2Vec algorithm along with several of its extensions. By
making the algorithm available in the most popular data science language, and
by bundling all extensions into a single place, the use of RDF2Vec is
simplified for data scientists. The package is released under a MIT license and
structured in such a way to foster further research into sampling, walking, and
embedding strategies, which are vital components of the RDF2Vec algorithm.
Several optimisations have been implemented in \texttt{pyRDF2Vec} that allow
for more efficient walk extraction than the original algorithm. Furthermore,
best practices in terms of code styling, testing, and documentation were
applied such that the package is future-proof as well as to facilitate external
contributions."
5787,"That way, we hope to both facilitate and
stimulate further research into these components of the RDF2Vec algorithm.","It is important to Connector, Sampler, Walker, and Embedder expose interfaces
that can be implemented by users.","3.2 Optimizations and extensions

The pyRDF2Vec implementation has several extensions, that speed up walk ex-
traction and which provide information in addition to the embeddings based on
walks.",2022-05-04 18:39:07+00:00,pyRDF2Vec: A Python Implementation and Extension of RDF2Vec,cs.LG,['cs.LG'],"[arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Bram Steenwinckel'), arxiv.Result.Author('Terencio Agozzino'), arxiv.Result.Author('Femke Ongenae')]","This paper introduces pyRDF2Vec, a Python software package that reimplements
the well-known RDF2Vec algorithm along with several of its extensions. By
making the algorithm available in the most popular data science language, and
by bundling all extensions into a single place, the use of RDF2Vec is
simplified for data scientists. The package is released under a MIT license and
structured in such a way to foster further research into sampling, walking, and
embedding strategies, which are vital components of the RDF2Vec algorithm.
Several optimisations have been implemented in \texttt{pyRDF2Vec} that allow
for more efficient walk extraction than the original algorithm. Furthermore,
best practices in terms of code styling, testing, and documentation were
applied such that the package is future-proof as well as to facilitate external
contributions."
5788,"This allows
for further research into techniques similar to ensembling, where the informa-
tion obtained from several strategies is combined.","First, the Transformer takes a list of Walker strategies, with optionally associ-
ated Sampler strategies, which enables to combine several strategies.","This combination can be done
either (i) on corpus-level, by concatenating the walks extracted by the diﬀerent
strategies together before feeding them to the Embedder, (ii) on embedding level,
where embeddings are learned on the corpora of each strategy individually and
then aggregated, or (iii) on prediction level, where the embeddings learned on
each corpora are fed to a classiﬁer to make predictions for the downstream task
and then aggregated.",2022-05-04 18:39:07+00:00,pyRDF2Vec: A Python Implementation and Extension of RDF2Vec,cs.LG,['cs.LG'],"[arxiv.Result.Author('Gilles Vandewiele'), arxiv.Result.Author('Bram Steenwinckel'), arxiv.Result.Author('Terencio Agozzino'), arxiv.Result.Author('Femke Ongenae')]","This paper introduces pyRDF2Vec, a Python software package that reimplements
the well-known RDF2Vec algorithm along with several of its extensions. By
making the algorithm available in the most popular data science language, and
by bundling all extensions into a single place, the use of RDF2Vec is
simplified for data scientists. The package is released under a MIT license and
structured in such a way to foster further research into sampling, walking, and
embedding strategies, which are vital components of the RDF2Vec algorithm.
Several optimisations have been implemented in \texttt{pyRDF2Vec} that allow
for more efficient walk extraction than the original algorithm. Furthermore,
best practices in terms of code styling, testing, and documentation were
applied such that the package is future-proof as well as to facilitate external
contributions."
5825,They all warrant further study and       superior to the design of the robot itself.,"Rather than ﬁnding clever
neural networks.","Morpho-
expanding the work to ﬁgure out how much, if at           logical wobbling removes a lot of those difﬁculties.",2022-05-05 17:41:58+00:00,Morphological Wobbling Can Help Robots Learn,cs.LG,"['cs.LG', 'cs.RO', 'I.2.6; I.2.9']","[arxiv.Result.Author('Fabien C. Y. Benureau'), arxiv.Result.Author('Jun Tani')]","We propose to make the physical characteristics of a robot oscillate while it
learns to improve its behavioral performance. We consider quantities such as
mass, actuator strength, and size that are usually fixed in a robot, and show
that when those quantities oscillate at the beginning of the learning process
on a simulated 2D soft robot, the performance on a locomotion task can be
significantly improved. We investigate the dynamics of the phenomenon and
conclude that in our case, surprisingly, a high-frequency oscillation with a
large amplitude for a large portion of the learning duration leads to the
highest performance benefits. Furthermore, we show that morphological wobbling
significantly increases exploration of the search space."
5859,"This
        adaptive procedure (also called Bayesian experimental design or experimental design with Bayesian
        optimization [12]) or further related optimization methods might be promising tools to support experi-
        mental planning, process characterization, process transfer or optimization of cell culture processes but
        they still require further research and being embedded in software solutions easy to use for operators.","Instead of performing model-based in-silico experiments (process simulations), real lab experiments
        would be performed and fed back to update the black box model (here the Gaussian process).","17
Table 5: Pareto optimal solutions concerning the choice of ﬁlling volumes in shake ﬂask scales, for 3, 4 and
5 shake ﬂask scales, for two diﬀerent scenarios.",2022-05-06 14:33:02+00:00,Designing Robust Biotechnological Processes Regarding Variabilities using Multi-Objective Optimization Applied to a Biopharmaceutical Seed Train Design,cs.LG,"['cs.LG', 'q-bio.QM', 'stat.ML', '60G15, 62G05, 68T01, 92-04, 92-08, 92C37', 'I.2.6; I.5.1; J.3']","[arxiv.Result.Author('Tanja Hernández Rodríguez'), arxiv.Result.Author('Anton Sekulic'), arxiv.Result.Author('Markus Lange-Hegermann'), arxiv.Result.Author('Björn Frahm')]","Development and optimization of biopharmaceutical production processes with
cell cultures is cost- and time-consuming and often performed rather
empirically. Efficient optimization of multiple-objectives like process time,
viable cell density, number of operating steps & cultivation scales, required
medium, amount of product as well as product quality depicts a promising
approach. This contribution presents a workflow which couples uncertainty-based
upstream simulation and Bayes optimization using Gaussian processes. Its
application is demonstrated in a simulation case study for a relevant
industrial task in process development, the design of a robust cell culture
expansion process (seed train), meaning that despite uncertainties and
variabilities concerning cell growth, low variations of viable cell density
during the seed train are obtained. Compared to a non-optimized reference seed
train, the optimized process showed much lower deviation rates regarding viable
cell densities (<~10% instead of 41.7%) using 5 or 4 shake flask scales and
seed train duration could be reduced by 56 h from 576 h to 520 h. Overall, it
is shown that applying Bayes optimization allows for optimization of a
multi-objective optimization function with several optimizable input variables
and under a considerable amount of constraints with a low computational effort.
This approach provides the potential to be used in form of a decision tool,
e.g. for the choice of an optimal and robust seed train design or for further
optimization tasks within process development."
5861,"The rest of the paper is organized as follows: Section 2 presents the method-
ological framework adopted in this paper; Section 3 demonstrates the applica-
tion of the methodology on a synthetic interdependent power-water-transport
network; and Section 4 summarizes the ﬁndings and discusses the scope for
further research.","Propose unsupervised and supervised methods to ﬁnd the optimal num-
       ber of clusters in each infrastructure system in an interdependent net-
       work.",2.,2022-05-06 15:51:05+00:00,Application of Clustering Algorithms for Dimensionality Reduction in Infrastructure Resilience Prediction Models,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Srijith Balakrishnan'), arxiv.Result.Author('Beatrice Cassottana'), arxiv.Result.Author('Arun Verma')]","Recent studies increasingly adopt simulation-based machine learning (ML)
models to analyze critical infrastructure system resilience. For realistic
applications, these ML models consider the component-level characteristics that
influence the network response during emergencies. However, such an approach
could result in a large number of features and cause ML models to suffer from
the `curse of dimensionality'. We present a clustering-based method that
simultaneously minimizes the problem of high-dimensionality and improves the
prediction accuracy of ML models developed for resilience analysis in
large-scale interdependent infrastructure networks. The methodology has three
parts: (a) generation of simulation dataset, (b) network component clustering,
and (c) dimensionality reduction and development of prediction models. First,
an interdependent infrastructure simulation model simulates the network-wide
consequences of various disruptive events. The component-level features are
extracted from the simulated data. Next, clustering algorithms are used to
derive the cluster-level features by grouping component-level features based on
their topological and functional characteristics. Finally, ML algorithms are
used to develop models that predict the network-wide impacts of disruptive
events using the cluster-level features. The applicability of the method is
demonstrated using an interdependent power-water-transport testbed. The
proposed method can be used to develop decision-support tools for post-disaster
recovery of infrastructure networks."
5942,"Ultimately, the most significant motivating factor for further research into HCI may simply be
what is the greatest weakness of modern AutoML offerings.","Kedziora, B. Gabrys

for open-world AutonoML, careful design of advanced interfaces will be required to imbue a team
of stakeholders and computers with a shared amount of situational awareness [81, 157].","Simply put, in pursuing automation,
many current AutoML tools have obscured their internals, acting as black-box systems [319].",2022-05-09 09:28:43+00:00,The Roles and Modes of Human Interactions with Automated Machine Learning Systems,cs.LG,"['cs.LG', 'cs.AI', 'A.1; I.2.0; I.2.1; I.2.6; I.2.m; I.5.0; I.5.1; I.5.2; I.5.4']","[arxiv.Result.Author('Thanh Tung Khuat'), arxiv.Result.Author('David Jacob Kedziora'), arxiv.Result.Author('Bogdan Gabrys')]","As automated machine learning (AutoML) systems continue to progress in both
sophistication and performance, it becomes important to understand the `how'
and `why' of human-computer interaction (HCI) within these frameworks, both
current and expected. Such a discussion is necessary for optimal system design,
leveraging advanced data-processing capabilities to support decision-making
involving humans, but it is also key to identifying the opportunities and risks
presented by ever-increasing levels of machine autonomy. Within this context,
we focus on the following questions: (i) How does HCI currently look like for
state-of-the-art AutoML algorithms, especially during the stages of
development, deployment, and maintenance? (ii) Do the expectations of HCI
within AutoML frameworks vary for different types of users and stakeholders?
(iii) How can HCI be managed so that AutoML solutions acquire human trust and
broad acceptance? (iv) As AutoML systems become more autonomous and capable of
learning from complex open-ended environments, will the fundamental nature of
HCI evolve? To consider these questions, we project existing literature in HCI
into the space of AutoML; this connection has, to date, largely been
unexplored. In so doing, we review topics including user-interface design,
human-bias mitigation, and trust in artificial intelligence (AI). Additionally,
to rigorously gauge the future of HCI, we contemplate how AutoML may manifest
in effectively open-ended environments. This discussion necessarily reviews
projected developmental pathways for AutoML, such as the incorporation of
reasoning, although the focus remains on how and why HCI may occur in such a
framework rather than on any implementational details. Ultimately, this review
serves to identify key research directions aimed at better facilitating the
roles and modes of human interactions with both current and future AutoML
systems."
5985,"We further study the robustness and flexibility of JellyBean (G4)
                                                                                  with the following sensitivity analysis experiments.","6.4 Sensitivity Analysis
This is significantly smaller that that of Spark which may take up
to 300% (as shown in Figure 5).",Remark.,2022-05-10 07:32:32+00:00,Serving and Optimizing Machine Learning Workflows on Heterogeneous Infrastructures,cs.LG,"['cs.LG', 'cs.DB', 'cs.DC']","[arxiv.Result.Author('Yongji Wu'), arxiv.Result.Author('Matthew Lentz'), arxiv.Result.Author('Danyang Zhuo'), arxiv.Result.Author('Yao Lu')]","With the advent of ubiquitous deployment of smart devices and the Internet of
Things, data sources for machine learning inference have increasingly moved to
the edge of the network. Existing machine learning inference platforms
typically assume a homogeneous infrastructure and do not take into account the
more complex and tiered computing infrastructure that includes edge devices,
local hubs, edge datacenters, and cloud datacenters. On the other hand, recent
machine learning efforts have provided viable solutions for model compression,
pruning and quantization for heterogeneous environments; for a machine learning
model, now we may easily find or even generate a series of models with
different tradeoffs between accuracy and efficiency.
  We design and implement JellyBean, a framework for serving and optimizing
machine learning inference workflows on heterogeneous infrastructures. Given
service-level objectives (e.g., throughput, accuracy), JellyBean automatically
selects the most cost-efficient models that met the accuracy target and decides
how to deploy them across different tiers of infrastructures. Evaluations show
that JellyBean reduces the total serving cost of visual question answering by
up to 58%, and vehicle tracking from the NVIDIA AI City Challenge by up to 36%
compared with state-of-the-art model selection and worker assignment solutions.
JellyBean also outperforms prior ML serving systems (e.g., Spark on the cloud)
up to 5x in serving costs."
5986,"For instance, in the 5:4 case, the edge has 1x2,
We further study the robustness and flexibility of JellyBean (G4)                               1x4, 2x8 workers, and the cloud has 1x16, 1x48, 3xV100.","Since
                                                                                                there are 9 total workers in the medium setting, we rank them
6.4 Sensitivity Analysis                                                                        according their cost and place those with higher costs on higher
                                                                                                tiers (and vice versa).","Results
with the following sensitivity analysis experiments.",2022-05-10 07:32:32+00:00,Serving and Optimizing Machine Learning Workflows on Heterogeneous Infrastructures,cs.LG,"['cs.LG', 'cs.DB', 'cs.DC']","[arxiv.Result.Author('Yongji Wu'), arxiv.Result.Author('Matthew Lentz'), arxiv.Result.Author('Danyang Zhuo'), arxiv.Result.Author('Yao Lu')]","With the advent of ubiquitous deployment of smart devices and the Internet of
Things, data sources for machine learning inference have increasingly moved to
the edge of the network. Existing machine learning inference platforms
typically assume a homogeneous infrastructure and do not take into account the
more complex and tiered computing infrastructure that includes edge devices,
local hubs, edge datacenters, and cloud datacenters. On the other hand, recent
AutoML efforts have provided viable solutions for model compression, pruning
and quantization for heterogeneous environments; for a machine learning model,
now we may easily find or even generate a series of models with different
tradeoffs between accuracy and efficiency. We design and implement JellyBean, a
system for serving and optimizing machine learning inference workflows on
heterogeneous infrastructures. Given service-level objectives (e.g.,
throughput, accuracy), JellyBean picks the most cost-efficient models that meet
the accuracy target and decides how to deploy them across different tiers of
infrastructures. Evaluations show that JellyBean reduces the total serving cost
of visual question answering by up to 58%, and vehicle tracking from the NVIDIA
AI City Challenge by up to 36% compared with state-of-the-art model selection
and worker assignment solutions. JellyBean also outperforms prior ML serving
systems (e.g., Spark on the cloud) up to 5x in serving costs."
5994,"Finally, Section 5 summarizes
the work and raises some potential points which worth further study for improvement.","Then in Section 4, several experiments based on our OpenITS dataset and METR-LA
dataset are designed and conducted to evaluate the proposed model comprehensively.",2.,2022-05-10 09:19:12+00:00,A spatial-temporal short-term traffic flow prediction model based on dynamical-learning graph convolution mechanism,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhijun Chen'), arxiv.Result.Author('Zhe Lu'), arxiv.Result.Author('Qiushi Chen'), arxiv.Result.Author('Hongliang Zhong'), arxiv.Result.Author('Yishi Zhang'), arxiv.Result.Author('Jie Xue'), arxiv.Result.Author('Chaozhong Wu')]","Short-term traffic flow prediction is a vital branch of the Intelligent
Traffic System (ITS) and plays an important role in traffic management. Graph
convolution network (GCN) is widely used in traffic prediction models to better
deal with the graphical structure data of road networks. However, the influence
weights among different road sections are usually distinct in real life, and
hard to be manually analyzed. Traditional GCN mechanism, relying on
manually-set adjacency matrix, is unable to dynamically learn such spatial
pattern during the training. To deal with this drawback, this paper proposes a
novel location graph convolutional network (Location-GCN). Location-GCN solves
this problem by adding a new learnable matrix into the GCN mechanism, using the
absolute value of this matrix to represent the distinct influence levels among
different nodes. Then, long short-term memory (LSTM) is employed in the
proposed traffic prediction model. Moreover, Trigonometric function encoding is
used in this study to enable the short-term input sequence to convey the
long-term periodical information. Ultimately, the proposed model is compared
with the baseline models and evaluated on two real word traffic flow datasets.
The results show our model is more accurate and robust on both datasets than
other representative traffic prediction models."
5995,"All those
drawbacks worth further study.","And since Location-GCN
is still based on the traditional GCN mechanism, the node sections of the road network can not be changed in both
the training and testing process, limiting the convenience and ﬂexibility of the model in real-life usage.","CRediT authorship contribution statement

    Zhijun Chen: Data curation, Formal analysis, Methodology, Resources, Writing – original draft, Funding ac-
quisition.",2022-05-10 09:19:12+00:00,A spatial-temporal short-term traffic flow prediction model based on dynamical-learning graph convolution mechanism,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhijun Chen'), arxiv.Result.Author('Zhe Lu'), arxiv.Result.Author('Qiushi Chen'), arxiv.Result.Author('Hongliang Zhong'), arxiv.Result.Author('Yishi Zhang'), arxiv.Result.Author('Jie Xue'), arxiv.Result.Author('Chaozhong Wu')]","Short-term traffic flow prediction is a vital branch of the Intelligent
Traffic System (ITS) and plays an important role in traffic management. Graph
convolution network (GCN) is widely used in traffic prediction models to better
deal with the graphical structure data of road networks. However, the influence
weights among different road sections are usually distinct in real life, and
hard to be manually analyzed. Traditional GCN mechanism, relying on
manually-set adjacency matrix, is unable to dynamically learn such spatial
pattern during the training. To deal with this drawback, this paper proposes a
novel location graph convolutional network (Location-GCN). Location-GCN solves
this problem by adding a new learnable matrix into the GCN mechanism, using the
absolute value of this matrix to represent the distinct influence levels among
different nodes. Then, long short-term memory (LSTM) is employed in the
proposed traffic prediction model. Moreover, Trigonometric function encoding is
used in this study to enable the short-term input sequence to convey the
long-term periodical information. Ultimately, the proposed model is compared
with the baseline models and evaluated on two real word traffic flow datasets.
The results show our model is more accurate and robust on both datasets than
other representative traffic prediction models."
6033,"Secondly, the widely known expand
strategy achieves the best performance only for 6 out of 24 methods (Figure 3), suggesting that further research of data
preparation approaches is required.","We also observe a correlation between window length and ALS performance, where the
larger the length used, the less 𝑀𝐴𝑃@10 score is achieved by ALS (Figure 3 (h, i)).","The last interesting insight from this comparison is that it may be possible to avoid
having to provide a model with a full history of user interactions.",2022-05-11 10:30:38+00:00,CVTT: Cross-Validation Through Time,cs.LG,['cs.LG'],"[arxiv.Result.Author('Sergey Kolesnikov'), arxiv.Result.Author('Mikhail Andronov')]","The practical aspects of evaluating recommender systems is an actively
discussed topic in the research community. While many current evaluation
techniques bring performance down to a single-value metric as a straightforward
approach for model comparison, it is based on a strong assumption of the
methods' stable performance over time. In this paper, we argue that leaving out
a method's continuous performance can lead to losing valuable insight into
joint data-method effects. We propose the Cross-Validation Thought Time (CVTT)
technique to perform more detailed evaluations, which focus on model
cross-validation performance over time. Using the proposed technique, we
conduct a detailed analysis of popular RecSys algorithms' performance against
various metrics and datasets. We also compare several data preparation and
evaluation strategies to analyze their impact on model performance. Our results
show that model performance can vary significantly over time, and both data and
evaluation setup can have a marked effect on it."
6045,"The opti-
mal setup for such a framework and how to best utilize these model classes side
by side poses an interesting direction for further research.","While we were able to show superior performance over existing methods, our
results suggest, that a combination of DL approaches with traditional machine
learning and statistical methods could further improve performance.","Our framework makes
use of BO and utilizes multi-ﬁdelity optimization in order to introduce a cost-
aware component and alleviate the costs incurred by the expensive training of
DL models.",2022-05-11 14:03:25+00:00,Efficient Automated Deep Learning for Time Series Forecasting,cs.LG,['cs.LG'],"[arxiv.Result.Author('Difan Deng'), arxiv.Result.Author('Florian Karl'), arxiv.Result.Author('Frank Hutter'), arxiv.Result.Author('Bernd Bischl'), arxiv.Result.Author('Marius Lindauer')]","Recent years have witnessed tremendously improved efficiency of Automated
Machine Learning (AutoML), especially Automated Deep Learning (AutoDL) systems,
but recent work focuses on tabular, image, or NLP tasks. So far, little
attention has been paid to general AutoDL frameworks for time series
forecasting, despite the enormous success in applying different novel
architectures to such tasks. In this paper, we propose an efficient approach
for the joint optimization of neural architecture and hyperparameters of the
entire data processing pipeline for time series forecasting. In contrast to
common NAS search spaces, we designed a novel neural architecture search space
covering various state-of-the-art architectures, allowing for an efficient
macro-search over different DL approaches. To efficiently search in such a
large configuration space, we use Bayesian optimization with multi-fidelity
optimization. We empirically study several different budget types enabling
efficient multi-fidelity optimization on different forecasting datasets.
Furthermore, we compared our resulting system, dubbed Auto-PyTorch-TS, against
several established baselines and show that it significantly outperforms all of
them across several datasets."
6046,"The opti-
mal setup for such a framework and how to best utilize these model classes side
by side poses an interesting direction for further research.","While we were able to show superior performance over existing methods, our
results suggest, that a combination of DL approaches with traditional machine
learning and statistical methods could further improve performance.","Our framework makes
use of BO and utilizes multi-ﬁdelity optimization in order to introduce a cost-
aware component and alleviate the costs incurred by the expensive training of
DL models.",2022-05-11 14:03:25+00:00,Efficient Automated Deep Learning for Time Series Forecasting,cs.LG,['cs.LG'],"[arxiv.Result.Author('Difan Deng'), arxiv.Result.Author('Florian Karl'), arxiv.Result.Author('Frank Hutter'), arxiv.Result.Author('Bernd Bischl'), arxiv.Result.Author('Marius Lindauer')]","Recent years have witnessed tremendously improved efficiency of Automated
Machine Learning (AutoML), especially Automated Deep Learning (AutoDL) systems,
but recent work focuses on tabular, image, or NLP tasks. So far, little
attention has been paid to general AutoDL frameworks for time series
forecasting, despite the enormous success in applying different novel
architectures to such tasks. In this paper, we propose an efficient approach
for the joint optimization of neural architecture and hyperparameters of the
entire data processing pipeline for time series forecasting. In contrast to
common NAS search spaces, we designed a novel neural architecture search space
covering various state-of-the-art architectures, allowing for an efficient
macro-search over different DL approaches. To efficiently search in such a
large configuration space, we use Bayesian optimization with multi-fidelity
optimization. We empirically study several different budget types enabling
efficient multi-fidelity optimization on different forecasting datasets.
Furthermore, we compared our resulting system, dubbed Auto-PyTorch-TS, against
several established baselines and show that it significantly outperforms all of
them across several datasets."
6047,"The optimal
setup for such a framework and how to best utilize these model classes side by
side poses an interesting direction for further research.","While we were able to show superior performance over existing methods, our
results suggest, that a combination of DL approaches with traditional machine
learning and statistical methods could further improve performance.","Our framework makes
use of BO and utilizes multi-ﬁdelity optimization in order to alleviate the costs
incurred by the expensive training of DL models.",2022-05-11 14:03:25+00:00,Efficient Automated Deep Learning for Time Series Forecasting,cs.LG,['cs.LG'],"[arxiv.Result.Author('Difan Deng'), arxiv.Result.Author('Florian Karl'), arxiv.Result.Author('Frank Hutter'), arxiv.Result.Author('Bernd Bischl'), arxiv.Result.Author('Marius Lindauer')]","Recent years have witnessed tremendously improved efficiency of Automated
Machine Learning (AutoML), especially Automated Deep Learning (AutoDL) systems,
but recent work focuses on tabular, image, or NLP tasks. So far, little
attention has been paid to general AutoDL frameworks for time series
forecasting, despite the enormous success in applying different novel
architectures to such tasks. In this paper, we propose an efficient approach
for the joint optimization of neural architecture and hyperparameters of the
entire data processing pipeline for time series forecasting. In contrast to
common NAS search spaces, we designed a novel neural architecture search space
covering various state-of-the-art architectures, allowing for an efficient
macro-search over different DL approaches. To efficiently search in such a
large configuration space, we use Bayesian optimization with multi-fidelity
optimization. We empirically study several different budget types enabling
efficient multi-fidelity optimization on different forecasting datasets.
Furthermore, we compared our resulting system, dubbed \system, against several
established baselines and show that it significantly outperforms all of them
across several datasets."
6107,"Another direction of further research concerns improving the sample complexity bounds
for k-DL in the present paper.","10
    Deriving sample complexity bounds for the robust learnability of halfspaces under the uniform
distribution is perhaps the most natural next step towards resolving the above-mentioned open
problem.","Here we have used a proper PAC-learning algorithm as a black
box in our robust learning procedure (see Corollary 4).",2022-05-12 14:40:18+00:00,Sample Complexity Bounds for Robustly Learning Decision Lists against Evasion Attacks,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Pascale Gourdeau'), arxiv.Result.Author('Varun Kanade'), arxiv.Result.Author('Marta Kwiatkowska'), arxiv.Result.Author('James Worrell')]","A fundamental problem in adversarial machine learning is to quantify how much
training data is needed in the presence of evasion attacks. In this paper we
address this issue within the framework of PAC learning, focusing on the class
of decision lists. Given that distributional assumptions are essential in the
adversarial setting, we work with probability distributions on the input data
that satisfy a Lipschitz condition: nearby points have similar probability. Our
key results illustrate that the adversary's budget (that is, the number of bits
it can perturb on each input) is a fundamental quantity in determining the
sample complexity of robust learning. Our first main result is a
sample-complexity lower bound: the class of monotone conjunctions (essentially
the simplest non-trivial hypothesis class on the Boolean hypercube) and any
superclass has sample complexity at least exponential in the adversary's
budget. Our second main result is a corresponding upper bound: for every fixed
$k$ the class of $k$-decision lists has polynomial sample complexity against a
$\log(n)$-bounded adversary. This sheds further light on the question of
whether an efficient PAC learning algorithm can always be used as an efficient
$\log(n)$-robust learning algorithm under the uniform distribution."
6178,"Lastly, further study could be
directed towards the exploration of diﬀerent RL algorithms instead of DQN and/or diﬀerent hyperparameters
for the neural networks.","We tested with 2, 4, and 10 agents, but it would be interesting to see how the approach scales to larger number
of agents and with diﬀerent threshold values for k. Additionally, there may be other algorithms to benchmark
against that could perform better than the CSMA-style algorithms we employed.","To make it easier for those interested in further investigating or expanding on this work, we provide the open
source software for the Gym environment and RL agents in Appendix A.",2022-05-13 17:53:00+00:00,Distributed Transmission Control for Wireless Networks using Multi-Agent Reinforcement Learning,cs.LG,"['cs.LG', 'cs.NI']","[arxiv.Result.Author('Collin Farquhar'), arxiv.Result.Author('Prem Sagar Pattanshetty Vasanth Kumar'), arxiv.Result.Author('Anu Jagannath'), arxiv.Result.Author('Jithin Jagannath')]","We examine the problem of transmission control, i.e., when to transmit, in
distributed wireless communications networks through the lens of multi-agent
reinforcement learning. Most other works using reinforcement learning to
control or schedule transmissions use some centralized control mechanism,
whereas our approach is fully distributed. Each transmitter node is an
independent reinforcement learning agent and does not have direct knowledge of
the actions taken by other agents. We consider the case where only a subset of
agents can successfully transmit at a time, so each agent must learn to act
cooperatively with other agents. An agent may decide to transmit a certain
number of steps into the future, but this decision is not communicated to the
other agents, so it the task of the individual agents to attempt to transmit at
appropriate times. We achieve this collaborative behavior through studying the
effects of different actions spaces. We are agnostic to the physical layer,
which makes our approach applicable to many types of networks. We submit that
approaches similar to ours may be useful in other domains that use multi-agent
reinforcement learning with independent agents."
6188,"To further study the impact of the diﬀerent neuron number on PIELM and BPIELM, we choose
three scenarios with Nbc = 16, 28, 37.","In a word,
BPIELM outperforms PIELM at the same number of sensors in terms of Max-AEs and MAEs.",The results are shown in Fig.9.,2022-05-14 02:32:48+00:00,Bayesian Physics-Informed Extreme Learning Machine for Forward and Inverse PDE Problems with Noisy Data,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Wen Yao'), arxiv.Result.Author('Wei Peng'), arxiv.Result.Author('Weien Zhou')]","Physics-informed extreme learning machine (PIELM) has recently received
significant attention as a rapid version of physics-informed neural network
(PINN) for solving partial differential equations (PDEs). The key
characteristic is to fix the input layer weights with random values and use
Moore-Penrose generalized inverse for the output layer weights. The framework
is effective, but it easily suffers from overfitting noisy data and lacks
uncertainty quantification for the solution under noise scenarios.To this end,
we develop the Bayesian physics-informed extreme learning machine (BPIELM) to
solve both forward and inverse linear PDE problems with noisy data in a unified
framework. In our framework, a prior probability distribution is introduced in
the output layer for extreme learning machine with physic laws and the Bayesian
method is used to estimate the posterior of parameters. Besides, for inverse
PDE problems, problem parameters considered as new output layer weights are
unified in a framework with forward PDE problems. Finally, we demonstrate
BPIELM considering both forward problems, including Poisson, advection, and
diffusion equations, as well as inverse problems, where unknown problem
parameters are estimated. The results show that, compared with PIELM, BPIELM
quantifies uncertainty arising from noisy data and provides more accurate
predictions. In addition, BPIELM is considerably cheaper than PINN in terms of
the computational cost."
6220,"will promote further study of the generalisation of GNNs and         Therefore, developing a technological ecosystem that supports
guide the design of trustworthy GNNs.","Current           [34], and software platforms [296], [297] that are suitable for
research into the extrapolation of GNNs [74] aims to reveal the      miscellaneous applications will also be required if efforts to
relationship between tasks and aggregation functions, which          develop trustworthy GNNs are to be successfully evaluated.","Therefore, studying            trustworthiness is an essential step in researching and indus-
the proper handling and management of the trustworthiness-           trialising trustworthy GNNs.",2022-05-16 02:21:09+00:00,"Trustworthy Graph Neural Networks: Aspects, Methods and Trends",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('He Zhang'), arxiv.Result.Author('Bang Wu'), arxiv.Result.Author('Xingliang Yuan'), arxiv.Result.Author('Shirui Pan'), arxiv.Result.Author('Hanghang Tong'), arxiv.Result.Author('Jian Pei')]","Graph neural networks (GNNs) have emerged as a series of competent graph
learning methods for diverse real-world scenarios, ranging from daily
applications like recommendation systems and question answering to cutting-edge
technologies such as drug discovery in life sciences and n-body simulation in
astrophysics. However, task performance is not the only requirement for GNNs.
Performance-oriented GNNs have exhibited potential adverse effects like
vulnerability to adversarial attacks, unexplainable discrimination against
disadvantaged groups, or excessive resource consumption in edge computing
environments. To avoid these unintentional harms, it is necessary to build
competent GNNs characterised by trustworthiness. To this end, we propose a
comprehensive roadmap to build trustworthy GNNs from the view of the various
computing technologies involved. In this survey, we introduce basic concepts
and comprehensively summarise existing efforts for trustworthy GNNs from six
aspects, including robustness, explainability, privacy, fairness,
accountability, and environmental well-being. Additionally, we highlight the
intricate cross-aspect relations between the above six aspects of trustworthy
GNNs. Finally, we present a thorough overview of trending directions for
facilitating the research and industrialisation of trustworthy GNNs."
6221,"In the future,
we will further study how to extend this method to the unnormalized OT problems,
which means initial and target densities with diﬀerent total mass.","Generally speaking,
for diﬀerent dimension, our proposed method takes about 1.5s to perform one itera-
tion with our unoptimized PyTorch codes and GPU implementation.","REFERENCES

[1] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, Convolutional
           neural networks for speech recognition, IEEE/ACM Transactions on Audio, Speech, and
           Language Processing, 22 (2014), pp.",2022-05-16 08:56:05+00:00,A scalable deep learning approach for solving high-dimensional dynamic optimal transport,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Wei Wan'), arxiv.Result.Author('Yuejin Zhang'), arxiv.Result.Author('Chenglong Bao'), arxiv.Result.Author('Bin Dong'), arxiv.Result.Author('Zuoqiang Shi')]","The dynamic formulation of optimal transport has attracted growing interests
in scientific computing and machine learning, and its computation requires to
solve a PDE-constrained optimization problem. The classical Eulerian
discretization based approaches suffer from the curse of dimensionality, which
arises from the approximation of high-dimensional velocity field. In this work,
we propose a deep learning based method to solve the dynamic optimal transport
in high dimensional space. Our method contains three main ingredients: a
carefully designed representation of the velocity field, the discretization of
the PDE constraint along the characteristics, and the computation of high
dimensional integral by Monte Carlo method in each time step. Specifically, in
the representation of the velocity field, we apply the classical nodal basis
function in time and the deep neural networks in space domain with the H1-norm
regularization. This technique promotes the regularity of the velocity field in
both time and space such that the discretization along the characteristic
remains to be stable during the training process. Extensive numerical examples
have been conducted to test the proposed method. Compared to other solvers of
optimal transport, our method could give more accurate results in high
dimensional cases and has very good scalability with respect to dimension.
Finally, we extend our method to more complicated cases such as crowd motion
problem."
6260,further research directions in Section 6.,We derive limitations and            millions of devices (cross-device).,"Finally, Section 7
concludes this literature review.",2022-05-07 12:23:22+00:00,Decentral and Incentivized Federated Learning Frameworks: A Systematic Literature Review,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Leon Witt'), arxiv.Result.Author('Mathis Heyer'), arxiv.Result.Author('Kentaroh Toyoda'), arxiv.Result.Author('Wojciech Samek'), arxiv.Result.Author('Dan Li')]","The advent of Federated Learning (FL) has ignited a new paradigm for parallel
and confidential decentralized Machine Learning (ML) with the potential of
utilizing the computational power of a vast number of IoT, mobile and edge
devices without data leaving the respective device, ensuring privacy by design.
Yet, in order to scale this new paradigm beyond small groups of already
entrusted entities towards mass adoption, the Federated Learning Framework
(FLF) has to become (i) truly decentralized and (ii) participants have to be
incentivized. This is the first systematic literature review analyzing holistic
FLFs in the domain of both, decentralized and incentivized federated learning.
\rawresults publications were retrieved, by querying 12 major scientific
databases. Finally, \finalresults articles remained after a systematic review
and filtering process for in-depth examination. Although having massive
potential to direct the future of a more distributed and secure AI, none of the
analyzed FLF is production-ready. The approaches vary heavily in terms of
use-cases, system design, solved issues and thoroughness. We are the first to
provide a systematic approach to classify and quantify differences between FLF,
exposing limitations of current works and derive future directions for research
in this novel domain."
6261,"We derive limitations and                Note that the FL setting can range from a few
further research directions in Section 6.","applications of the FLFs, Blockchain features, incentive
mechanisms and experiments.","Finally, Section 7     collaborating entities (cross-silo) to a federated system of
concludes this literature review.",2022-05-07 12:23:22+00:00,Decentral and Incentivized Federated Learning Frameworks: A Systematic Literature Review,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Leon Witt'), arxiv.Result.Author('Mathis Heyer'), arxiv.Result.Author('Kentaroh Toyoda'), arxiv.Result.Author('Wojciech Samek'), arxiv.Result.Author('Dan Li')]","The advent of Federated Learning (FL) has ignited a new paradigm for parallel
and confidential decentralized Machine Learning (ML) with the potential of
utilizing the computational power of a vast number of IoT, mobile and edge
devices without data leaving the respective device, ensuring privacy by design.
Yet, in order to scale this new paradigm beyond small groups of already
entrusted entities towards mass adoption, the Federated Learning Framework
(FLF) has to become (i) truly decentralized and (ii) participants have to be
incentivized. This is the first systematic literature review analyzing holistic
FLFs in the domain of both, decentralized and incentivized federated learning.
422 publications were retrieved, by querying 12 major scientific databases.
Finally, 40 articles remained after a systematic review and filtering process
for in-depth examination. Although having massive potential to direct the
future of a more distributed and secure AI, none of the analyzed FLF is
production-ready. The approaches vary heavily in terms of use-cases, system
design, solved issues and thoroughness. We are the first to provide a
systematic approach to classify and quantify differences between FLF, exposing
limitations of current works and derive future directions for research in this
novel domain."
6262,"To do a further study, we plot the training curve in Figure 5.",Training process.,"It is shown that
the Query dataset always has a quick start, which indicates that the query method can extract the
features of the distinguishable programs effectively and makes them easier to be synthesized.",2022-05-08 13:53:18+00:00,Neural Program Synthesis with Query,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Di Huang'), arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Xing Hu'), arxiv.Result.Author('Xishan Zhang'), arxiv.Result.Author('Pengwei Jin'), arxiv.Result.Author('Nan Li'), arxiv.Result.Author('Zidong Du'), arxiv.Result.Author('Qi Guo'), arxiv.Result.Author('Yunji Chen')]","Aiming to find a program satisfying the user intent given input-output
examples, program synthesis has attracted increasing interest in the area of
machine learning. Despite the promising performance of existing methods, most
of their success comes from the privileged information of well-designed
input-output examples. However, providing such input-output examples is
unrealistic because it requires the users to have the ability to describe the
underlying program with a few input-output examples under the training
distribution. In this work, we propose a query-based framework that trains a
query neural network to generate informative input-output examples
automatically and interactively from a large query space. The quality of the
query depends on the amount of the mutual information between the query and the
corresponding program, which can guide the optimization of the query framework.
To estimate the mutual information more accurately, we introduce the functional
space (F-space) which models the relevance between the input-output examples
and the programs in a differentiable way. We evaluate the effectiveness and
generalization of the proposed query-based framework on the Karel task and the
list processing task. Experimental results show that the query-based framework
can generate informative input-output examples which achieve and even
outperform well-designed input-output examples."
6274,"Even though the reported results are not comparable with the state-of-the-art
results in reinforcement learning, we demonstrate that such an approach has
the potential to achieve strong performance in the future and is worthwhile for
further research.","Our method is applied to video frames
from Atari games in order to teach an artiﬁcial agent to play those games.","iv
                         Acknowledgements

This dissertation is both the biggest academic challenge I have ever faced and my greatest
achievement so far.",2022-05-16 19:52:45+00:00,Deep Apprenticeship Learning for Playing Games,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'I.2.8; I.2.10; I.2.6; I.5.4']",[arxiv.Result.Author('Dejan Markovikj')],"In the last decade, deep learning has achieved great success in machine
learning tasks where the input data is represented with different levels of
abstractions. Driven by the recent research in reinforcement learning using
deep neural networks, we explore the feasibility of designing a learning model
based on expert behaviour for complex, multidimensional tasks where reward
function is not available. We propose a novel method for apprenticeship
learning based on the previous research on supervised learning techniques in
reinforcement learning. Our method is applied to video frames from Atari games
in order to teach an artificial agent to play those games. Even though the
reported results are not comparable with the state-of-the-art results in
reinforcement learning, we demonstrate that such an approach has the potential
to achieve strong performance in the future and is worthwhile for further
research."
6298,"Some
                                              of the surrogate loss and for both the family of           recent publications (Awasthi et al., 2021a; Bao et al., 2021)
                                              linear functions and neural networks with one              further study H-consistency guarantees for the adversarial
                                              hidden-layer.","Instead, the hypothesis-set dependent notion of
                                              We then present a series of explicit bounds in the         H-consistency should be adopted, as argued by Long &
                                              case of the zero-one loss, with multiple choices           Servedio (2013) (see also (Zhang & Agarwal, 2020)).","We further prove more favorable              loss (Goodfellow et al., 2014; Madry et al., 2017; Tsipras
                                              distribution-dependent guarantees in that case.",2022-05-16 23:13:36+00:00,$\mathscr{H}$-Consistency Estimation Error of Surrogate Loss Minimizers,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Pranjal Awasthi'), arxiv.Result.Author('Anqi Mao'), arxiv.Result.Author('Mehryar Mohri'), arxiv.Result.Author('Yutao Zhong')]","We present a detailed study of estimation errors in terms of surrogate loss
estimation errors. We refer to such guarantees as $\mathscr{H}$-consistency
estimation error bounds, since they account for the hypothesis set
$\mathscr{H}$ adopted. These guarantees are significantly stronger than
$\mathscr{H}$-calibration or $\mathscr{H}$-consistency. They are also more
informative than similar excess error bounds derived in the literature, when
$\mathscr{H}$ is the family of all measurable functions. We prove general
theorems providing such guarantees, for both the distribution-dependent and
distribution-independent settings. We show that our bounds are tight, modulo a
convexity assumption. We also show that previous excess error bounds can be
recovered as special cases of our general results.
  We then present a series of explicit bounds in the case of the zero-one loss,
with multiple choices of the surrogate loss and for both the family of linear
functions and neural networks with one hidden-layer. We further prove more
favorable distribution-dependent guarantees in that case. We also present a
series of explicit bounds in the case of the adversarial loss, with surrogate
losses based on the supremum of the $\rho$-margin, hinge or sigmoid loss and
for the same two general hypothesis sets. Here too, we prove several
enhancements of these guarantees under natural distributional assumptions.
Finally, we report the results of simulations illustrating our bounds and their
tightness."
6303,"Similarly,
                                                                              further study is required at the intersection of efﬁciency

                                                                           9
                                      (a)                                [7] Aditya Golatkar, Alessandro Achille, and Stefano Soatto.","Some possible future work includes devising provable
                                                                              information bounds on the amount of unlearning in order
                                                                              to increase the conﬁdence of all stakeholders.","Eternal sunshine of the spotless net: Selective forgetting in
                                      (b)                                     deep networks.",2022-05-17 05:13:17+00:00,Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher,cs.LG,['cs.LG'],"[arxiv.Result.Author('Vikram S Chundawat'), arxiv.Result.Author('Ayush K Tarun'), arxiv.Result.Author('Murari Mandal'), arxiv.Result.Author('Mohan Kankanhalli')]","Machine unlearning has become an important field of research due to an
increasing focus on addressing the evolving data privacy rules and regulations
into the machine learning (ML) applications. It facilitates the request for
removal of certain set or class of data from the already trained ML model
without retraining from scratch. Recently, several efforts have been made to
perform unlearning in an effective and efficient manner. We propose a novel
machine unlearning method by exploring the utility of competent and incompetent
teachers in a student-teacher framework to induce forgetfulness. The knowledge
from the competent and incompetent teachers is selectively transferred to the
student to obtain a model that doesn't contain any information about the forget
data. We experimentally show that this method is well generalized, fast, and
effective. Furthermore, we introduce a zero retrain forgetting (ZRF) metric to
evaluate the unlearning method. Unlike the existing unlearning metrics, the ZRF
score does not depend on the availability of the expensive retrained model.
This makes it useful for analysis of the unlearned model after deployment as
well. The experiments are conducted for random subset forgetting and class
forgetting on various deep networks and across different application domains. A
use case of forgetting information about the patients' medical records is also
presented."
6322,"These norms rest on a shared belief that recogniz-
ing, exploring, and articulating limitations can foster greater precision in the descriptions of research (making it easier to
reproduce), help ensure appropriate interpretation of research findings, make research claims more credible, and highlight
issues that would benefit from further research [5, 6, 18].","Many scientific fields have well-established norms around disclosing and discussing limitations, which are often recog-
nized as necessary for improving scientific rigor and research integrity.","Although practices necessarily vary by field and by publication
venue [12], the ML research community is notable for not having particularly well-developed norms around disclosing
and discussing limitations.",2022-05-05 15:32:45+00:00,"REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research",cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Jessie J. Smith'), arxiv.Result.Author('Saleema Amershi'), arxiv.Result.Author('Solon Barocas'), arxiv.Result.Author('Hanna Wallach'), arxiv.Result.Author('Jennifer Wortman Vaughan')]","Transparency around limitations can improve the scientific rigor of research,
help ensure appropriate interpretation of research findings, and make research
claims more credible. Despite these benefits, the machine learning (ML)
research community lacks well-developed norms around disclosing and discussing
limitations. To address this gap, we conduct an iterative design process with
30 ML and ML-adjacent researchers to develop and test REAL ML, a set of guided
activities to help ML researchers recognize, explore, and articulate the
limitations of their research. Using a three-stage interview and survey study,
we identify ML researchers' perceptions of limitations, as well as the
challenges they face when recognizing, exploring, and articulating limitations.
We develop REAL ML to address some of these practical challenges, and highlight
additional cultural challenges that will require broader shifts in community
norms to address. We hope our study and REAL ML help move the ML research
community toward more active and appropriate engagement with limitations."
6323,"First, our study analyses a
synchronous NGD algorithm, which ignores asynchronous problems, which should be
the subject of further study.","Finally, we discuss interesting topics for future study.","Second, our theorem suggests that a suﬃciently small

                                                      30
      0.40  Central-Client                      0
            Fixed-Degree                        1
      0.35  Circle-Type                         2
                                                3
      0.30                                      4
                                                5
Mean  0.25                             log(SD)  6
                                                7
      0.20                                      8

      0.15                                        Nu5mbe1r0of 2It0era3t0ions45(×15000)

      0.10

      0.05

      0.00 5 10 20 30 45 50

          Number of Iterations (×100)

            (a) Lenent on MNIST

      0.5   Central-Client                      0
                                                1
      0.4   Fixed-Degree                        2
            Circle-Type                         3
                                                4
Mean  0.3                              log(SD)  5
                                                6
      0.2                                       7
                                                8
      0.1
                                                    4 6Epoc1h0s (×201002)5 30
      0.0 4 6 10 20 25 30

                Epochs (×100)

            (b) MobileNet on CIFAR10

Figure 6: The mean and log(SD) for the Err values obtained by deep learning models.",2022-05-06 02:53:31+00:00,Network Gradient Descent Algorithm for Decentralized Federated Learning,cs.LG,"['cs.LG', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Shuyuan Wu'), arxiv.Result.Author('Danyang Huang'), arxiv.Result.Author('Hansheng Wang')]","We study a fully decentralized federated learning algorithm, which is a novel
gradient descent algorithm executed on a communication-based network. For
convenience, we refer to it as a network gradient descent (NGD) method. In the
NGD method, only statistics (e.g., parameter estimates) need to be
communicated, minimizing the risk of privacy. Meanwhile, different clients
communicate with each other directly according to a carefully designed network
structure without a central master. This greatly enhances the reliability of
the entire algorithm. Those nice properties inspire us to carefully study the
NGD method both theoretically and numerically. Theoretically, we start with a
classical linear regression model. We find that both the learning rate and the
network structure play significant roles in determining the NGD estimator's
statistical efficiency. The resulting NGD estimator can be statistically as
efficient as the global estimator, if the learning rate is sufficiently small
and the network structure is well balanced, even if the data are distributed
heterogeneously. Those interesting findings are then extended to general models
and loss functions. Extensive numerical studies are presented to corroborate
our theoretical findings. Classical deep learning models are also presented for
illustration purpose."
6324,"With respect to all complexity in financial markets, too many other topics,

                                                                                                                         23/26
May 18, 2022  components, and problems have remained unexplored, and further research is required
              to address them.","It avoids
              supposing unrealistic assumptions and tries to suggest solutions to help organizations,
              researchers, or people who would like to earn profit from their financial activities.","This research is the first attempt to adopt a compatible mindset with
              financial markets and define a guideline to interpret these time series data better.",2022-05-11 20:44:08+00:00,"Compatible deep neural network framework with financial time series data, including data preprocessor, neural network model and trading strategy",cs.LG,"['cs.LG', 'cs.AI', 'cs.CE']","[arxiv.Result.Author('Mohammadmahdi Ghahramani'), arxiv.Result.Author('Hamid Esmaeili Najafabadi')]","Experience has shown that trading in stock and cryptocurrency markets has the
potential to be highly profitable. In this light, considerable effort has been
recently devoted to investigate how to apply machine learning and deep learning
to interpret and predict market behavior. This research introduces a new deep
neural network architecture and a novel idea of how to prepare financial data
before feeding them to the model. In the data preparation part, the first step
is to generate many features using technical indicators and then apply the
XGBoost model for feature engineering. Splitting data into three categories and
using separate autoencoders, we extract high-level mixed features at the second
step. This data preprocessing is introduced to predict price movements.
Regarding modeling, different convolutional layers, an long short-term memory
unit, and several fully-connected layers have been designed to perform binary
classification. This research also introduces a trading strategy to exploit the
trained model outputs. Three different datasets are used to evaluate this
method, where results indicate that this framework can provide us with
profitable and robust predictions."
6326,"It can be reused with other similarity met-
      rics or models for further research in the domain.","(ii) We have released the library created for this experiment (https://github.com/
      bonheml/VAE_learning_dynamics).","(iii) During our analysis, we found that (1) the encoder is learned before the decoder; (2) all the
      layers of the encoder, except the mean and variance layers, learn very similar representations
      regardless of the learning objective and regularisation strength used; and (3) linear CKA
      could be an efﬁcient tool to track posterior collapse.",2022-05-17 14:31:57+00:00,How do Variational Autoencoders Learn? Insights from Representational Similarity,cs.LG,"['cs.LG', 'I.2.6; G.3']","[arxiv.Result.Author('Lisa Bonheme'), arxiv.Result.Author('Marek Grzes')]","The ability of Variational Autoencoders (VAEs) to learn disentangled
representations has made them popular for practical applications. However,
their behaviour is not yet fully understood. For example, the questions of when
they can provide disentangled representations, or suffer from posterior
collapse are still areas of active research. Despite this, there are no
layerwise comparisons of the representations learned by VAEs, which would
further our understanding of these models. In this paper, we thus look into the
internal behaviour of VAEs using representational similarity techniques.
Specifically, using the CKA and Procrustes similarities, we found that the
encoders' representations are learned long before the decoders', and this
behaviour is independent of hyperparameters, learning objectives, and datasets.
Moreover, the encoders' representations up to the mean and variance layers are
similar across hyperparameters and learning objectives."
6327,"We believe that further research using dynamics-based metrics, such as ﬁxed-point topology [31],
could provide additional insights into the representations learned by VAEs.","While this gave us compelling insights, these metrics have some limitations,
discussed in Section 2.3, and cannot, for example, reliably compare layers with different architectures.","Acknowledgments and Disclosure of Funding

The authors thank Frances Ding for an insightful discussion on the Procrustes distance, as well as
Théophile Champion and Declan Collins for their helpful comments on the paper.",2022-05-17 14:31:57+00:00,How do Variational Autoencoders Learn? Insights from Representational Similarity,cs.LG,"['cs.LG', 'I.2.6; G.3']","[arxiv.Result.Author('Lisa Bonheme'), arxiv.Result.Author('Marek Grzes')]","The ability of Variational Autoencoders (VAEs) to learn disentangled
representations has made them popular for practical applications. However,
their behaviour is not yet fully understood. For example, the questions of when
they can provide disentangled representations, or suffer from posterior
collapse are still areas of active research. Despite this, there are no
layerwise comparisons of the representations learned by VAEs, which would
further our understanding of these models. In this paper, we thus look into the
internal behaviour of VAEs using representational similarity techniques.
Specifically, using the CKA and Procrustes similarities, we found that the
encoders' representations are learned long before the decoders', and this
behaviour is independent of hyperparameters, learning objectives, and datasets.
Moreover, the encoders' representations up to the mean and variance layers are
similar across hyperparameters and learning objectives."
6328,"It can be reused with other similarity met-
      rics or models for further research in the domain.","(ii) We have released the library created for this experiment (https://github.com/
      bonheml/VAE_learning_dynamics).","(iii) During our analysis, we found that (1) the encoder is learned before the decoder; (2) all the
      layers of the encoder, except the mean and variance layers, learn very similar representations
      regardless of the learning objective and regularisation strength used; and (3) linear CKA
      could be an efﬁcient tool to track posterior collapse.",2022-05-17 14:31:57+00:00,How do Variational Autoencoders Learn? Insights from Representational Similarity,cs.LG,"['cs.LG', 'I.2.6; G.3']","[arxiv.Result.Author('Lisa Bonheme'), arxiv.Result.Author('Marek Grzes')]","The ability of Variational Autoencoders (VAEs) to learn disentangled
representations has made them popular for practical applications. However,
their behaviour is not yet fully understood. For example, the questions of when
they can provide disentangled representations, or suffer from posterior
collapse are still areas of active research. Despite this, there are no
layerwise comparisons of the representations learned by VAEs, which would
further our understanding of these models. In this paper, we thus look into the
internal behaviour of VAEs using representational similarity techniques.
Specifically, using the CKA and Procrustes similarities, we found that the
encoders' representations are learned long before the decoders', and this
behaviour is independent of hyperparameters, learning objectives, and datasets.
Moreover, the encoders' representations up to the mean and variance layers are
similar across hyperparameters and learning objectives."
6329,"We believe that further research using dynamics-based metrics, such as ﬁxed-
point topology [31], could provide additional insights into the representations learned by VAEs.","While this gave us compelling insights, these metrics have some limitations,
discussed in Section 2.3, and may underestimate the similarity between layers with different
architectures [31].","Acknowledgments and Disclosure of Funding

The authors thank Frances Ding for an insightful discussion on the Procrustes distance, as well as
Théophile Champion and Declan Collins for their helpful comments on the paper.",2022-05-17 14:31:57+00:00,How do Variational Autoencoders Learn? Insights from Representational Similarity,cs.LG,"['cs.LG', 'I.2.6; G.3']","[arxiv.Result.Author('Lisa Bonheme'), arxiv.Result.Author('Marek Grzes')]","The ability of Variational Autoencoders (VAEs) to learn disentangled
representations has made them popular for practical applications. However,
their behaviour is not yet fully understood. For example, the questions of when
they can provide disentangled representations, or suffer from posterior
collapse are still areas of active research. Despite this, there are no
layerwise comparisons of the representations learned by VAEs, which would
further our understanding of these models. In this paper, we thus look into the
internal behaviour of VAEs using representational similarity techniques.
Specifically, using the CKA and Procrustes similarities, we found that the
encoders' representations are learned long before the decoders', and this
behaviour is independent of hyperparameters, learning objectives, and datasets.
Moreover, the encoders' representations up to the mean and variance layers are
similar across hyperparameters and learning objectives."
6330,"It can be reused with other similarity met-
      rics or models for further research in the domain.","(ii) We have released the library created for this experiment (https://github.com/
      bonheml/VAE_learning_dynamics).","(iii) During our analysis, we found that (1) the encoder is learned before the decoder; (2) all
      the layers of the encoder, except the mean and variance layers, learn very similar represen-
      tations regardless of the learning objective and regularisation strength used; and (3) linear
      CKA could be an efﬁcient tool to track posterior collapse.",2022-05-17 14:31:57+00:00,How do Variational Autoencoders Learn? Insights from Representational Similarity,cs.LG,"['cs.LG', 'I.2.6; G.3']","[arxiv.Result.Author('Lisa Bonheme'), arxiv.Result.Author('Marek Grzes')]","The ability of Variational Autoencoders (VAEs) to learn disentangled
representations has made them popular for practical applications. However,
their behaviour is not yet fully understood. For example, the questions of when
they can provide disentangled representations, or suffer from posterior
collapse are still areas of active research. Despite this, there are no
layerwise comparisons of the representations learned by VAEs, which would
further our understanding of these models. In this paper, we thus look into the
internal behaviour of VAEs using representational similarity techniques.
Specifically, using the CKA and Procrustes similarities, we found that the
encoders' representations are learned long before the decoders', and this
behaviour is independent of hyperparameters, learning objectives, and datasets.
Moreover, the encoders' representations in all but the mean and variance layers
are similar across hyperparameters and learning objectives."
6331,"We believe that further research using dynamics-based
metrics, such as ﬁxed-point topology (Maheswaranathan et al., 2019), could provide additional in-
sights into the representations learned by VAEs.","While this gave us compelling insights, these metrics have some limitations,
discussed in Section 2.3, and may underestimate the similarity between layers with different archi-
tectures (Maheswaranathan et al., 2019).","ACKNOWLEDGMENTS

The authors thank Frances Ding for an insightful discussion on the Procrustes distance, as well as
The´ophile Champion and Declan Collins for their helpful comments on the paper.",2022-05-17 14:31:57+00:00,How do Variational Autoencoders Learn? Insights from Representational Similarity,cs.LG,"['cs.LG', 'I.2.6; G.3']","[arxiv.Result.Author('Lisa Bonheme'), arxiv.Result.Author('Marek Grzes')]","The ability of Variational Autoencoders (VAEs) to learn disentangled
representations has made them popular for practical applications. However,
their behaviour is not yet fully understood. For example, the questions of when
they can provide disentangled representations, or suffer from posterior
collapse are still areas of active research. Despite this, there are no
layerwise comparisons of the representations learned by VAEs, which would
further our understanding of these models. In this paper, we thus look into the
internal behaviour of VAEs using representational similarity techniques.
Specifically, using the CKA and Procrustes similarities, we found that the
encoders' representations are learned long before the decoders', and this
behaviour is independent of hyperparameters, learning objectives, and datasets.
Moreover, the encoders' representations in all but the mean and variance layers
are similar across hyperparameters and learning objectives."
6362,"Such methods can
   Trial design and analysis methods may mitigate the impact of                     broaden unnecessarily restrictive eligibility criteria and, in future
patient heterogeneity, although further research and adoption is                    work, potentially improve fairness by exposing whether certain
crucial [133].","of eligibility criteria on cancer trial populations, finding they could
                                                                                    broaden criteria without sacrificing trial efficacy.",Subgroup analyses explore whether treatment effects                  sub-populations were made unjustifiably ineligible.,2022-05-18 11:59:22+00:00,Multi-disciplinary fairness considerations in machine learning for clinical trials,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Isabel Chien'), arxiv.Result.Author('Nina Deliu'), arxiv.Result.Author('Richard E. Turner'), arxiv.Result.Author('Adrian Weller'), arxiv.Result.Author('Sofia S. Villar'), arxiv.Result.Author('Niki Kilbertus')]","While interest in the application of machine learning to improve healthcare
has grown tremendously in recent years, a number of barriers prevent deployment
in medical practice. A notable concern is the potential to exacerbate
entrenched biases and existing health disparities in society. The area of
fairness in machine learning seeks to address these issues of equity; however,
appropriate approaches are context-dependent, necessitating domain-specific
consideration. We focus on clinical trials, i.e., research studies conducted on
humans to evaluate medical treatments. Clinical trials are a relatively
under-explored application in machine learning for healthcare, in part due to
complex ethical, legal, and regulatory requirements and high costs. Our aim is
to provide a multi-disciplinary assessment of how fairness for machine learning
fits into the context of clinical trials research and practice. We start by
reviewing the current ethical considerations and guidelines for clinical trials
and examine their relationship with common definitions of fairness in machine
learning. We examine potential sources of unfairness in clinical trials,
providing concrete examples, and discuss the role machine learning might play
in either mitigating potential biases or exacerbating them when applied without
care. Particular focus is given to adaptive clinical trials, which may employ
machine learning. Finally, we highlight concepts that require further
investigation and development, and emphasize new approaches to fairness that
may be relevant to the design of clinical trials."
6363,"methods would benefit from further research on the interplay be-
                                                                         tween representativeness, fairness, and generalizability.","Such
therapeutic obligation to treat patients as optimally as possible.","There has been comparatively sparse research on this concep-
tion of unfairness, which seeks to examine the burdens placed on            Generalization of ML models to new populations/settings and
certain subgroups, evaluating fairness from the perspective of the       dataset drift over time have been highlighted as broad challenges
agents or users that are affected by the model outcomes, rather          facing practical adoption of ML for healthcare [130].",2022-05-18 11:59:22+00:00,Multi-disciplinary fairness considerations in machine learning for clinical trials,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Isabel Chien'), arxiv.Result.Author('Nina Deliu'), arxiv.Result.Author('Richard E. Turner'), arxiv.Result.Author('Adrian Weller'), arxiv.Result.Author('Sofia S. Villar'), arxiv.Result.Author('Niki Kilbertus')]","While interest in the application of machine learning to improve healthcare
has grown tremendously in recent years, a number of barriers prevent deployment
in medical practice. A notable concern is the potential to exacerbate
entrenched biases and existing health disparities in society. The area of
fairness in machine learning seeks to address these issues of equity; however,
appropriate approaches are context-dependent, necessitating domain-specific
consideration. We focus on clinical trials, i.e., research studies conducted on
humans to evaluate medical treatments. Clinical trials are a relatively
under-explored application in machine learning for healthcare, in part due to
complex ethical, legal, and regulatory requirements and high costs. Our aim is
to provide a multi-disciplinary assessment of how fairness for machine learning
fits into the context of clinical trials research and practice. We start by
reviewing the current ethical considerations and guidelines for clinical trials
and examine their relationship with common definitions of fairness in machine
learning. We examine potential sources of unfairness in clinical trials,
providing concrete examples, and discuss the role machine learning might play
in either mitigating potential biases or exacerbating them when applied without
care. Particular focus is given to adaptive clinical trials, which may employ
machine learning. Finally, we highlight concepts that require further
investigation and development, and emphasize new approaches to fairness that
may be relevant to the design of clinical trials."
6374,"We see three areas that need further research to gauge the ability of weak supervision to
produce reliable forecasts.","In this paper, we have scratched the surface of high resolution crop yield forecasting without strong
supervision.","First, the scale differences that can be handled by weak supervision needs
investigation.",2022-05-18 15:52:35+00:00,A weakly supervised framework for high-resolution crop yield forecasts,cs.LG,['cs.LG'],"[arxiv.Result.Author('Dilli R. Paudel'), arxiv.Result.Author('Diego Marcos'), arxiv.Result.Author('Allard de Wit'), arxiv.Result.Author('Hendrik Boogaard'), arxiv.Result.Author('Ioannis N. Athanasiadis')]","Predictor inputs and label data for crop yield forecasting are not always
available at the same spatial resolution. We propose a deep learning framework
that uses high resolution inputs and low resolution labels to produce crop
yield forecasts for both spatial levels. The forecasting model is calibrated by
weak supervision from low resolution crop area and yield statistics. We
evaluated the framework by disaggregating regional yields in Europe from parent
statistical regions to sub-regions for five countries (Germany, Spain, France,
Hungary, Italy) and two crops (soft wheat and potatoes). Performance of weakly
supervised models was compared with linear trend models and Gradient-Boosted
Decision Trees (GBDT). Higher resolution crop yield forecasts are useful to
policymakers and other stakeholders. Weakly supervised deep learning methods
provide a way to produce such forecasts even in the absence of high resolution
yield data."
6376,"3.2 Discussion

It is worthwhile to further study why activation function ABS outperforms GELU when lr is relatively
small.","This indicates all the heads work fairly in coordination to produce the
ﬁnal predicted values.","Here, we provide a possible explanation — the dying ReLU phenomenon [44].",2022-05-17 01:58:34+00:00,POViT: Vision Transformer for Multi-objective Design and Characterization of Nanophotonic Devices,cs.LG,"['cs.LG', 'physics.app-ph', 'physics.optics']","[arxiv.Result.Author('Xinyu Chen'), arxiv.Result.Author('Renjie Li'), arxiv.Result.Author('Yueyao Yu'), arxiv.Result.Author('Yuanwen Shen'), arxiv.Result.Author('Wenye Li'), arxiv.Result.Author('Zhaoyu Zhang'), arxiv.Result.Author('Yin Zhang')]","We solve a fundamental challenge in semiconductor IC design: the fast and
accurate characterization of nanoscale photonic devices. Much like the fusion
between AI and EDA, many efforts have been made to apply DNNs such as
convolutional neural networks (CNN) to prototype and characterize next-gen
optoelectronic devices commonly found in photonic integrated circuits (PIC) and
LiDAR. These prior works generally strive to predict the quality factor (Q) and
modal volume (V) of for instance, photonic crystals, with ultra-high accuracy
and speed. However, state-of-the-art models are still far from being directly
applicable in the real-world: e.g. the correlation coefficient of V
($V_{coeff}$ ) is only about 80%, which is much lower than what it takes to
generate reliable and reproducible nanophotonic designs. Recently,
attention-based transformer models have attracted extensive interests and been
widely used in CV and NLP. In this work, we propose the first-ever Transformer
model (POViT) to efficiently design and simulate semiconductor photonic devices
with multiple objectives. Unlike the standard Vision Transformer (ViT), we
supplied photonic crystals as data input and changed the activation layer from
GELU to an absolute-value function (ABS). Our experiments show that POViT
exceeds results reported by previous models significantly. The correlation
coefficient $V_{coeff}$ increases by over 12% (i.e., to 92.0%) and the
prediction errors of Q is reduced by an order of magnitude, among several other
key metric improvements. Our work has the potential to drive the expansion of
EDA to fully automated photonic design. The complete dataset and code will be
released to aid researchers endeavoring in the interdisciplinary field of
physics and computer science."
6398,"4.3 Future Research

The following four key areas could be identified where further research is necessary to increase
the utility and hence the adoption of DL in business analytics.","Overall, management and practitioners responsible for digital
strategy and transformation should avoid seeing DL as a simple replacement or enhancement
of existing tools for predictive analytics tasks, but as an opportunity to develop new application
areas and use cases for business analytics based on the strength of DL – which are predictions
based on vast amounts of unstructured data.","(1) Future research in business analytics could focus on identifying currently non-existing uses
which are in line with the strength of DL.",2022-05-19 06:28:31+00:00,Deep Learning in Business Analytics: A Clash of Expectations and Reality,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE', 'cs.DB', 'q-fin.RM']",[arxiv.Result.Author('Marc Andreas Schmitt')],"Our fast-paced digital economy shaped by global competition requires
increased data-driven decision-making based on artificial intelligence (AI) and
machine learning (ML). The benefits of deep learning (DL) are manifold, but it
comes with limitations that have - so far - interfered with widespread industry
adoption. This paper explains why DL - despite its popularity - has
difficulties speeding up its adoption within business analytics. It is shown -
by a mixture of content analysis and empirical study - that the adoption of
deep learning is not only affected by computational complexity, lacking big
data architecture, lack of transparency (black-box), and skill shortage, but
also by the fact that DL does not outperform traditional ML models in the case
of structured datasets with fixed-length feature vectors. Deep learning should
be regarded as a powerful addition to the existing body of ML models instead of
a one size fits all solution."
6399,"A simple replacement makes hence no sense unless further research in this
area realizes performance improvements for DL on structured classification tasks.","DL has several advantages over
traditional methods but has in its current capacity difficulties reaching the performance and
accuracy levels of tree-based ensembles as Random Forest and GBM for predictions on
structured data.","Developments as dropout (Srivastava et al., 2014) and the Maxout activation function
(Goodfellow et al., 2013), which were specifically developed to tackle classification problems
are going in this direction, but as shown above, are not enough to reach accuracy levels to
justify the replacement of tree-based ensemble models as RF or GBM.",2022-05-19 06:28:31+00:00,Deep Learning in Business Analytics: A Clash of Expectations and Reality,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE', 'cs.DB', 'q-fin.RM']",[arxiv.Result.Author('Marc Andreas Schmitt')],"Our fast-paced digital economy shaped by global competition requires
increased data-driven decision-making based on artificial intelligence (AI) and
machine learning (ML). The benefits of deep learning (DL) are manifold, but it
comes with limitations that have - so far - interfered with widespread industry
adoption. This paper explains why DL - despite its popularity - has
difficulties speeding up its adoption within business analytics. It is shown -
by a mixture of content analysis and empirical study - that the adoption of
deep learning is not only affected by computational complexity, lacking big
data architecture, lack of transparency (black-box), and skill shortage, but
also by the fact that DL does not outperform traditional ML models in the case
of structured datasets with fixed-length feature vectors. Deep learning should
be regarded as a powerful addition to the existing body of ML models instead of
a one size fits all solution."
6404,"We circumvent this effect for
4th row atoms by increasing the number of intermediate Monte Carlo steps, but further research
into Monte Carlo sampling methods [29, 30] is required to fully address this issue.","Additionally, when increasing
the nuclear charges, the wavefunction becomes increasingly localised, which leads to a reduction
in average Monte Carlo stepsize and potentially correlated samples.","Despite our
improvements for the accuracy of energy differences between different molecules or geometries, DL-
VMC is still outperformed by other, computationally cheaper methods in some cases.",2022-05-19 09:54:41+00:00,Gold-standard solutions to the Schrödinger equation using deep learning: How much physics do we need?,cs.LG,"['cs.LG', 'physics.chem-ph', 'physics.comp-ph']","[arxiv.Result.Author('Leon Gerard'), arxiv.Result.Author('Michael Scherbela'), arxiv.Result.Author('Philipp Marquetand'), arxiv.Result.Author('Philipp Grohs')]","Finding accurate solutions to the Schr\""odinger equation is the key unsolved
challenge of computational chemistry. Given its importance for the development
of new chemical compounds, decades of research have been dedicated to this
problem, but due to the large dimensionality even the best available methods do
not yet reach the desired accuracy. Recently the combination of deep learning
with Monte Carlo methods has emerged as a promising way to obtain highly
accurate energies and moderate scaling of computational cost. In this paper we
significantly contribute towards this goal by introducing a novel deep-learning
architecture that achieves 40-70% lower energy error at 8x lower computational
cost compared to previous approaches. Using our method we establish a new
benchmark by calculating the most accurate variational ground state energies
ever published for a number of different atoms and molecules. We systematically
break down and measure our improvements, focusing in particular on the effect
of increasing physical prior knowledge. We surprisingly find that increasing
the prior knowledge given to the architecture can actually decrease accuracy."
6405,"We circumvent this effect for
4th row atoms by increasing the number of intermediate Monte Carlo steps, but further research
into Monte Carlo sampling methods [30, 31] is required to fully address this issue.","Additionally, when increasing
the nuclear charges, the wavefunction becomes increasingly localised, which leads to a reduction
in average Monte Carlo stepsize and potentially correlated samples.","Despite our
improvements for the accuracy of energy differences between different molecules or geometries, DL-
VMC is still outperformed by other, computationally cheaper methods in some cases.",2022-05-19 09:54:41+00:00,Gold-standard solutions to the Schrödinger equation using deep learning: How much physics do we need?,cs.LG,"['cs.LG', 'physics.chem-ph', 'physics.comp-ph']","[arxiv.Result.Author('Leon Gerard'), arxiv.Result.Author('Michael Scherbela'), arxiv.Result.Author('Philipp Marquetand'), arxiv.Result.Author('Philipp Grohs')]","Finding accurate solutions to the Schr\""odinger equation is the key unsolved
challenge of computational chemistry. Given its importance for the development
of new chemical compounds, decades of research have been dedicated to this
problem, but due to the large dimensionality even the best available methods do
not yet reach the desired accuracy. Recently the combination of deep learning
with Monte Carlo methods has emerged as a promising way to obtain highly
accurate energies and moderate scaling of computational cost. In this paper we
significantly contribute towards this goal by introducing a novel deep-learning
architecture that achieves 40-70% lower energy error at 8x lower computational
cost compared to previous approaches. Using our method we establish a new
benchmark by calculating the most accurate variational ground state energies
ever published for a number of different atoms and molecules. We systematically
break down and measure our improvements, focusing in particular on the effect
of increasing physical prior knowledge. We surprisingly find that increasing
the prior knowledge given to the architecture can actually decrease accuracy."
6406,"We circumvent this effect for
4th row atoms by increasing the number of intermediate Monte Carlo steps, but further research
into Monte Carlo sampling methods [31, 32] is required to fully address this issue.","Additionally, when increasing
the nuclear charges, the wavefunction becomes increasingly localised, which leads to a reduction
in average Monte Carlo stepsize and potentially correlated samples.","Despite our
improvements for the accuracy of energy differences between different molecules or geometries, DL-
VMC is still outperformed by other, computationally cheaper methods in some cases.",2022-05-19 09:54:41+00:00,Gold-standard solutions to the Schrödinger equation using deep learning: How much physics do we need?,cs.LG,"['cs.LG', 'physics.chem-ph', 'physics.comp-ph']","[arxiv.Result.Author('Leon Gerard'), arxiv.Result.Author('Michael Scherbela'), arxiv.Result.Author('Philipp Marquetand'), arxiv.Result.Author('Philipp Grohs')]","Finding accurate solutions to the Schr\""odinger equation is the key unsolved
challenge of computational chemistry. Given its importance for the development
of new chemical compounds, decades of research have been dedicated to this
problem, but due to the large dimensionality even the best available methods do
not yet reach the desired accuracy. Recently the combination of deep learning
with Monte Carlo methods has emerged as a promising way to obtain highly
accurate energies and moderate scaling of computational cost. In this paper we
significantly contribute towards this goal by introducing a novel deep-learning
architecture that achieves 40-70% lower energy error at 6x lower computational
cost compared to previous approaches. Using our method we establish a new
benchmark by calculating the most accurate variational ground state energies
ever published for a number of different atoms and molecules. We systematically
break down and measure our improvements, focusing in particular on the effect
of increasing physical prior knowledge. We surprisingly find that increasing
the prior knowledge given to the architecture can actually decrease accuracy."
6414,"• To our knowledge, this is the ﬁrst work that decently introduces collaborative training techniques
  over cloud clusters, which could inspire further researches to delve into this area.","The contribution of this work can be summarized as follows:

• We introduce a novel framework that well ﬁts distributed deep learning over remote cloud clusters,
  which are connected by low-bandwidth networks;

• We propose a uniﬁed optimization technique in Nebula-I, named Nebula-Optimizer, that can jointly
  optimize the training strategy, parallization and communication;

• Both pre-training and ﬁne-tuning scenarios have been validated on the cloud environment, demon-
  strating the effectiveness of the proposed framework;

• We output a multilingual pre-trained language model, ERNIE-M Extra, which has obtained
  new SoTA results on cross-lingual natural language inference tasks, under the proposed novel
  multilingual learning framework and Nebula-I.","2 The Nebula-I framework

The overview of Nebula-I is shown in Figure 1.",2022-05-19 11:10:14+00:00,Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Yang Xiang'), arxiv.Result.Author('Zhihua Wu'), arxiv.Result.Author('Weibao Gong'), arxiv.Result.Author('Siyu Ding'), arxiv.Result.Author('Xianjie Mo'), arxiv.Result.Author('Yuang Liu'), arxiv.Result.Author('Shuohuan Wang'), arxiv.Result.Author('Peng Liu'), arxiv.Result.Author('Yongshuai Hou'), arxiv.Result.Author('Long Li'), arxiv.Result.Author('Bin Wang'), arxiv.Result.Author('Shaohuai Shi'), arxiv.Result.Author('Yaqian Han'), arxiv.Result.Author('Yue Yu'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Yu Sun'), arxiv.Result.Author('Yanjun Ma'), arxiv.Result.Author('Dianhai Yu')]","The ever-growing model size and scale of compute have attracted increasing
interests in training deep learning models over multiple nodes. However, when
it comes to training on cloud clusters, especially across remote clusters, huge
challenges are faced. In this work, we introduce a general framework, Nebula-I,
for collaboratively training deep learning models over remote heterogeneous
clusters, the connections between which are low-bandwidth wide area networks
(WANs). We took natural language processing (NLP) as an example to show how
Nebula-I works in different training phases that include: a) pre-training a
multilingual language model using two remote clusters; and b) fine-tuning a
machine translation model using knowledge distilled from pre-trained models,
which run through the most popular paradigm of recent deep learning. To balance
the accuracy and communication efficiency, in Nebula-I, parameter-efficient
training strategies, hybrid parallel computing methods and adaptive
communication acceleration techniques are jointly applied. Meanwhile, security
strategies are employed to guarantee the safety, reliability and privacy in
intra-cluster computation and inter-cluster communication. Nebula-I is
implemented with the PaddlePaddle deep learning framework, which can support
collaborative training over heterogeneous hardware, e.g. GPU and NPU.
Experiments demonstrate that the proposed framework could substantially
maximize the training efficiency while preserving satisfactory NLP performance.
By using Nebula-I, users can run large-scale training tasks over cloud clusters
with minimum developments, and the utility of existed large pre-trained models
could be further promoted. We also introduced new state-of-the-art results on
cross-lingual natural language inference tasks, which are generated based upon
a novel learning framework and Nebula-I."
6415,"How to split the model and displace the model to different cloud clusters
by considering the hardware conﬁgurations to optimally utilize the hardware resources is another
direction for further study [58].","Though the straggler problem in distributed training has been
well studied, our proposed solutions (i.e., splitting the model to two parts according to the model
architecture) to enable distributed training in the cross-cloud environment is quite different from the
data-parallel scenario [57].","Security In the current version of Nebula-I, we only provide basic security mechanisms for com-
putation and communication based on the trust between two cross-cloud clusters.",2022-05-19 11:10:14+00:00,Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Yang Xiang'), arxiv.Result.Author('Zhihua Wu'), arxiv.Result.Author('Weibao Gong'), arxiv.Result.Author('Siyu Ding'), arxiv.Result.Author('Xianjie Mo'), arxiv.Result.Author('Yuang Liu'), arxiv.Result.Author('Shuohuan Wang'), arxiv.Result.Author('Peng Liu'), arxiv.Result.Author('Yongshuai Hou'), arxiv.Result.Author('Long Li'), arxiv.Result.Author('Bin Wang'), arxiv.Result.Author('Shaohuai Shi'), arxiv.Result.Author('Yaqian Han'), arxiv.Result.Author('Yue Yu'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Yu Sun'), arxiv.Result.Author('Yanjun Ma'), arxiv.Result.Author('Dianhai Yu')]","The ever-growing model size and scale of compute have attracted increasing
interests in training deep learning models over multiple nodes. However, when
it comes to training on cloud clusters, especially across remote clusters, huge
challenges are faced. In this work, we introduce a general framework, Nebula-I,
for collaboratively training deep learning models over remote heterogeneous
clusters, the connections between which are low-bandwidth wide area networks
(WANs). We took natural language processing (NLP) as an example to show how
Nebula-I works in different training phases that include: a) pre-training a
multilingual language model using two remote clusters; and b) fine-tuning a
machine translation model using knowledge distilled from pre-trained models,
which run through the most popular paradigm of recent deep learning. To balance
the accuracy and communication efficiency, in Nebula-I, parameter-efficient
training strategies, hybrid parallel computing methods and adaptive
communication acceleration techniques are jointly applied. Meanwhile, security
strategies are employed to guarantee the safety, reliability and privacy in
intra-cluster computation and inter-cluster communication. Nebula-I is
implemented with the PaddlePaddle deep learning framework, which can support
collaborative training over heterogeneous hardware, e.g. GPU and NPU.
Experiments demonstrate that the proposed framework could substantially
maximize the training efficiency while preserving satisfactory NLP performance.
By using Nebula-I, users can run large-scale training tasks over cloud clusters
with minimum developments, and the utility of existed large pre-trained models
could be further promoted. We also introduced new state-of-the-art results on
cross-lingual natural language inference tasks, which are generated based upon
a novel learning framework and Nebula-I."
6426,"Some minimal requirements for those mappings are formulated in Appendix A, leaving a thorough
treatment of this important topic for further research.","We have seen that for a valid training of a model in the ﬁctitious world as well as for its evaluation
regarding fairness, methods should be found to create mappings from the real to the ﬁctitious world.","4 Conclusion and Discussion

Despite a rapidly growing body of literature addressing the topic of fairML, there was still a lack
of a uniﬁed theoretical foundation.",2022-05-19 15:37:26+00:00,What Is Fairness? Implications For FairML,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Ludwig Bothmann'), arxiv.Result.Author('Kristina Peters'), arxiv.Result.Author('Bernd Bischl')]","A growing body of literature in fairness-aware ML (fairML) aspires to
mitigate machine learning (ML)-related unfairness in automated decision making
(ADM) by defining metrics that measure fairness of an ML model and by proposing
methods that ensure that trained ML models achieve low values in those
measures. However, the underlying concept of fairness, i.e., the question of
what fairness is, is rarely discussed, leaving a considerable gap between
centuries of philosophical discussion and recent adoption of the concept in the
ML community. In this work, we try to bridge this gap by formalizing a
consistent concept of fairness and by translating the philosophical
considerations into a formal framework for the evaluation of ML models in ADM
systems. We derive that fairness problems can already arise without the
presence of protected attributes, pointing out that fairness and predictive
performance are not irreconcilable counterparts, but rather that the latter is
necessary to achieve the former. Moreover, we argue why and how causal
considerations are necessary when assessing fairness in the presence of
protected attributes. Eventually, we achieve greater linguistic clarity for the
discussion of fairML by clearly assigning responsibilities to stakeholders
inside and outside ML."
6429,"The outcomes of our
                                        analysis are synthesized in a set of insights that help to maximize GNN performance, and a comprehensive list of challenges and
                                        opportunities for further research into efﬁcient GNN computations.","Finally, we
                                        investigate different forms of asynchronicity, navigating the path for future asynchronous parallel GNN pipelines.",Our work will help to advance the design of future GNNs.,2022-05-19 17:11:45+00:00,Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis,cs.LG,"['cs.LG', 'cs.AR', 'cs.DC']","[arxiv.Result.Author('Maciej Besta'), arxiv.Result.Author('Torsten Hoefler')]","Graph neural networks (GNNs) are among the most powerful tools in deep
learning. They routinely solve complex problems on unstructured networks, such
as node classification, graph classification, or link prediction, with high
accuracy. However, both inference and training of GNNs are complex, and they
uniquely combine the features of irregular graph processing with dense and
regular computations. This complexity makes it very challenging to execute GNNs
efficiently on modern massively parallel architectures. To alleviate this, we
first design a taxonomy of parallelism in GNNs, considering data and model
parallelism, and different forms of pipelining. Then, we use this taxonomy to
investigate the amount of parallelism in numerous GNN models, GNN-driven
machine learning tasks, software frameworks, or hardware accelerators. We use
the work-depth model, and we also assess communication volume and
synchronization. We specifically focus on the sparsity/density of the
associated tensors, in order to understand how to effectively apply techniques
such as vectorization. We also formally analyze GNN pipelining, and we
generalize the established Message-Passing class of GNN models to cover
arbitrary pipeline depths, facilitating future optimizations. Finally, we
investigate different forms of asynchronicity, navigating the path for future
asynchronous parallel GNN pipelines. The outcomes of our analysis are
synthesized in a set of insights that help to maximize GNN performance, and a
comprehensive list of challenges and opportunities for further research into
efficient GNN computations. Our work will help to advance the design of future
GNNs."
6430,"The outcomes of our
                                        analysis are synthesized in a set of insights that help to maximize GNN performance, and a comprehensive list of challenges and
                                        opportunities for further research into efﬁcient GNN computations.","Finally, we
                                        investigate different forms of asynchronicity, navigating the path for future asynchronous parallel GNN pipelines.",Our work will help to advance the design of future GNNs.,2022-05-19 17:11:45+00:00,Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis,cs.LG,"['cs.LG', 'cs.AR', 'cs.DC']","[arxiv.Result.Author('Maciej Besta'), arxiv.Result.Author('Torsten Hoefler')]","Graph neural networks (GNNs) are among the most powerful tools in deep
learning. They routinely solve complex problems on unstructured networks, such
as node classification, graph classification, or link prediction, with high
accuracy. However, both inference and training of GNNs are complex, and they
uniquely combine the features of irregular graph processing with dense and
regular computations. This complexity makes it very challenging to execute GNNs
efficiently on modern massively parallel architectures. To alleviate this, we
first design a taxonomy of parallelism in GNNs, considering data and model
parallelism, and different forms of pipelining. Then, we use this taxonomy to
investigate the amount of parallelism in numerous GNN models, GNN-driven
machine learning tasks, software frameworks, or hardware accelerators. We use
the work-depth model, and we also assess communication volume and
synchronization. We specifically focus on the sparsity/density of the
associated tensors, in order to understand how to effectively apply techniques
such as vectorization. We also formally analyze GNN pipelining, and we
generalize the established Message-Passing class of GNN models to cover
arbitrary pipeline depths, facilitating future optimizations. Finally, we
investigate different forms of asynchronicity, navigating the path for future
asynchronous parallel GNN pipelines. The outcomes of our analysis are
synthesized in a set of insights that help to maximize GNN performance, and a
comprehensive list of challenges and opportunities for further research into
efficient GNN computations. Our work will help to advance the design of future
GNNs."
6431,"The outcomes of our
                                        analysis are synthesized in a set of insights that help to maximize GNN performance, and a comprehensive list of challenges and
                                        opportunities for further research into efﬁcient GNN computations.","Finally, we
                                        investigate different forms of asynchronicity, navigating the path for future asynchronous parallel GNN pipelines.",Our work will help to advance the design of future GNNs.,2022-05-19 17:11:45+00:00,Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis,cs.LG,"['cs.LG', 'cs.AR', 'cs.DC']","[arxiv.Result.Author('Maciej Besta'), arxiv.Result.Author('Torsten Hoefler')]","Graph neural networks (GNNs) are among the most powerful tools in deep
learning. They routinely solve complex problems on unstructured networks, such
as node classification, graph classification, or link prediction, with high
accuracy. However, both inference and training of GNNs are complex, and they
uniquely combine the features of irregular graph processing with dense and
regular computations. This complexity makes it very challenging to execute GNNs
efficiently on modern massively parallel architectures. To alleviate this, we
first design a taxonomy of parallelism in GNNs, considering data and model
parallelism, and different forms of pipelining. Then, we use this taxonomy to
investigate the amount of parallelism in numerous GNN models, GNN-driven
machine learning tasks, software frameworks, or hardware accelerators. We use
the work-depth model, and we also assess communication volume and
synchronization. We specifically focus on the sparsity/density of the
associated tensors, in order to understand how to effectively apply techniques
such as vectorization. We also formally analyze GNN pipelining, and we
generalize the established Message-Passing class of GNN models to cover
arbitrary pipeline depths, facilitating future optimizations. Finally, we
investigate different forms of asynchronicity, navigating the path for future
asynchronous parallel GNN pipelines. The outcomes of our analysis are
synthesized in a set of insights that help to maximize GNN performance, and a
comprehensive list of challenges and opportunities for further research into
efficient GNN computations. Our work will help to advance the design of future
GNNs."
6432,"The outcomes of our
                                        analysis are synthesized in a set of insights that help to maximize GNN performance, and a comprehensive list of challenges and
                                        opportunities for further research into efﬁcient GNN computations.","Finally, we
                                        investigate different forms of asynchronicity, navigating the path for future asynchronous parallel GNN pipelines.",Our work will help to advance the design of future GNNs.,2022-05-19 17:11:45+00:00,Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis,cs.LG,"['cs.LG', 'cs.AR', 'cs.DC']","[arxiv.Result.Author('Maciej Besta'), arxiv.Result.Author('Torsten Hoefler')]","Graph neural networks (GNNs) are among the most powerful tools in deep
learning. They routinely solve complex problems on unstructured networks, such
as node classification, graph classification, or link prediction, with high
accuracy. However, both inference and training of GNNs are complex, and they
uniquely combine the features of irregular graph processing with dense and
regular computations. This complexity makes it very challenging to execute GNNs
efficiently on modern massively parallel architectures. To alleviate this, we
first design a taxonomy of parallelism in GNNs, considering data and model
parallelism, and different forms of pipelining. Then, we use this taxonomy to
investigate the amount of parallelism in numerous GNN models, GNN-driven
machine learning tasks, software frameworks, or hardware accelerators. We use
the work-depth model, and we also assess communication volume and
synchronization. We specifically focus on the sparsity/density of the
associated tensors, in order to understand how to effectively apply techniques
such as vectorization. We also formally analyze GNN pipelining, and we
generalize the established Message-Passing class of GNN models to cover
arbitrary pipeline depths, facilitating future optimizations. Finally, we
investigate different forms of asynchronicity, navigating the path for future
asynchronous parallel GNN pipelines. The outcomes of our analysis are
synthesized in a set of insights that help to maximize GNN performance, and a
comprehensive list of challenges and opportunities for further research into
efficient GNN computations. Our work will help to advance the design of future
GNNs."
6437,"To further study spurious
correlation in MTL, in the future, we’d like to construct benchmark MTL datasets with known
confounder changes (or analyze how some key attribute changes lead to spurious correlation problem),
build mathematical model based on it, and also explore and visualize which part of knowledge in
real-world MTL datasets (e.g.","As mitigation,
in analysis part, we create two synthetic datasets, and in experiment part, we adopt train/valid/test
split with several attribution differences to mimic confounder changes.",Taskonomy) could be spuriously correlated to other tasks.,2022-05-19 18:31:54+00:00,Improving Multi-Task Generalization via Regularizing Spurious Correlation,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ziniu Hu'), arxiv.Result.Author('Zhe Zhao'), arxiv.Result.Author('Xinyang Yi'), arxiv.Result.Author('Tiansheng Yao'), arxiv.Result.Author('Lichan Hong'), arxiv.Result.Author('Yizhou Sun'), arxiv.Result.Author('Ed H. Chi')]","Multi-Task Learning (MTL) is a powerful learning paradigm to improve
generalization performance via knowledge sharing. However, existing studies
find that MTL could sometimes hurt generalization, especially when two tasks
are less correlated. One possible reason that hurts generalization is spurious
correlation, i.e., some knowledge is spurious and not causally related to task
labels, but the model could mistakenly utilize them and thus fail when such
correlation changes. In MTL setup, there exist several unique challenges of
spurious correlation. First, the risk of having non-causal knowledge is higher,
as the shared MTL model needs to encode all knowledge from different tasks, and
causal knowledge for one task could be potentially spurious to the other.
Second, the confounder between task labels brings in a different type of
spurious correlation to MTL. We theoretically prove that MTL is more prone to
taking non-causal knowledge from other tasks than single-task learning, and
thus generalize worse. To solve this problem, we propose Multi-Task Causal
Representation Learning framework, aiming to represent multi-task knowledge via
disentangled neural modules, and learn which module is causally related to each
task via MTL-specific invariant regularization. Experiments show that it could
enhance MTL model's performance by 5.5% on average over Multi-MNIST, MovieLens,
Taskonomy, CityScape, and NYUv2, via alleviating spurious correlation problem."
6438,"To further study spurious
correlation in MTL, in the future, we’d like to construct benchmark MTL datasets with known
confounder changes (or analyze how some key attribute changes lead to spurious correlation problem),
build mathematical model based on it, and also explore and visualize which part of knowledge in
real-world MTL datasets (e.g.","As mitigation,
in analysis part, we create two synthetic datasets, and in experiment part, we adopt train/valid/test
split with several attribution differences to mimic confounder changes.",Taskonomy) could be spuriously correlated to other tasks.,2022-05-19 18:31:54+00:00,Improving Multi-Task Generalization via Regularizing Spurious Correlation,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ziniu Hu'), arxiv.Result.Author('Zhe Zhao'), arxiv.Result.Author('Xinyang Yi'), arxiv.Result.Author('Tiansheng Yao'), arxiv.Result.Author('Lichan Hong'), arxiv.Result.Author('Yizhou Sun'), arxiv.Result.Author('Ed H. Chi')]","Multi-Task Learning (MTL) is a powerful learning paradigm to improve
generalization performance via knowledge sharing. However, existing studies
find that MTL could sometimes hurt generalization, especially when two tasks
are less correlated. One possible reason that hurts generalization is spurious
correlation, i.e., some knowledge is spurious and not causally related to task
labels, but the model could mistakenly utilize them and thus fail when such
correlation changes. In MTL setup, there exist several unique challenges of
spurious correlation. First, the risk of having non-causal knowledge is higher,
as the shared MTL model needs to encode all knowledge from different tasks, and
causal knowledge for one task could be potentially spurious to the other.
Second, the confounder between task labels brings in a different type of
spurious correlation to MTL. We theoretically prove that MTL is more prone to
taking non-causal knowledge from other tasks than single-task learning, and
thus generalize worse. To solve this problem, we propose Multi-Task Causal
Representation Learning framework, aiming to represent multi-task knowledge via
disentangled neural modules, and learn which module is causally related to each
task via MTL-specific invariant regularization. Experiments show that it could
enhance MTL model's performance by 5.5% on average over Multi-MNIST, MovieLens,
Taskonomy, CityScape, and NYUv2, via alleviating spurious correlation problem."
6448,"We motivate further study into
enforcing geometric structure to parameter subspace construction and methods of test-time adaptation
towards (joint) distribution shifts.","5 Conclusion

We learn that sampling points in the parameter subspace can return lower-loss mapped parameters if
the space was compressed during training, yield robust accuracy across various perturbation types,
and reduces catastrophic forgetting when adapted into a hypernetwork.","9
References

Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong.",2022-05-19 22:57:55+00:00,Interpolating Compressed Parameter Subspaces,cs.LG,['cs.LG'],"[arxiv.Result.Author('Siddhartha Datta'), arxiv.Result.Author('Nigel Shadbolt')]","Inspired by recent work on neural subspaces and mode connectivity, we revisit
parameter subspace sampling for shifted and/or interpolatable input
distributions (instead of a single, unshifted distribution). We enforce a
compressed geometric structure upon a set of trained parameters mapped to a set
of train-time distributions, denoting the resulting subspaces as Compressed
Parameter Subspaces (CPS). We show the success and failure modes of the types
of shifted distributions whose optimal parameters reside in the CPS. We find
that ensembling point-estimates within a CPS can yield a high average accuracy
across a range of test-time distributions, including backdoor, adversarial,
permutation, stylization and rotation perturbations. We also find that the CPS
can contain low-loss point-estimates for various task shifts (albeit
interpolated, perturbed, unseen or non-identical coarse labels). We further
demonstrate this property in a continual learning setting with CIFAR100."
6461,"[13]
emphasized that information on early predictive factors for particularly severe and fatal
COVID-19 cases is relatively limited and further research is needed.",[12] and Zheng et al.,Huyut et al.,2022-05-20 05:47:29+00:00,A New Feature Selection Method for LogNNet and its Application for Diagnosis and Prognosis of COVID-19 Disease Using Routine Blood Values,cs.LG,"['cs.LG', 'cs.AI', 'physics.med-ph', 'q-bio.QM']","[arxiv.Result.Author('Mehmet Tahir Huyut'), arxiv.Result.Author('Andrei Velichko')]","Since February-2020, the world has embarked on an intense struggle with the
COVID-19 disease, and health systems have come under a tragic pressure as the
disease turned into a pandemic. The aim of this study is to determine the most
effective routine-blood-values (RBV) in the diagnosis/prognosis of COVID-19
using new feature selection method for LogNNet reservoir neural network. First
dataset in this study consists of a total of 5296-patients with a same number
of negative and positive covid test. Second dataset consists of a total of
3899-patients with a diagnosis of COVID-19, who were treated in hospital with
severe-infected (203) and mildly-infected (3696). The most important RBVs that
affect the diagnosis of the disease from the first dataset were
mean-corpuscular-hemoglobin-concentration (MCHC), mean-corpuscular-hemoglobin
(MCH) and activated-partial-prothrombin-time (aPTT). The most effective
features in the prognosis of the disease were erythrocyte-sedimentation-rate
(ESR), neutrophil-count (NEU), C-reactive-protein (CRP). LogNNet-model achieved
an accuracy rate of A46 = 99.5% in the diagnosis of the disease with 46
features and A3 = 99.17% with only MCHC, MCH, and aPTT features. Model reached
an accuracy rate of A48 = 94.4% in determining the prognosis of the disease
with 48 features and A3 = 82.7% with only ESR, NEU, and CRP features. LogNNet
model demonstrated a very high disease diagnosis/prognosis of COVID-19
performance without knowing about the symptoms or history of the patients. The
model is suitable for devices with low resources (3-14 kB of RAM used on the
Arduino microcontroller), and is promising to create mobile health monitoring
systems in the Internet of Things. Our method will reduce the negative
pressures on the health sector and help doctors understand pathogenesis of
COVID-19 through key futures and contribute positively to the treatment
processes."
6462,"[14]
emphasized that information on early predictive factors for particularly severe and fatal
COVID-19 cases is relatively limited and further research is needed.",[13] and Zheng et al.,Huyut et al.,2022-05-20 05:47:29+00:00,Diagnosis and Prognosis of COVID-19 Disease Using Routine Blood Values and LogNNet Neural Network,cs.LG,"['cs.LG', 'cs.AI', 'physics.med-ph', 'q-bio.QM']","[arxiv.Result.Author('Mehmet Tahir Huyut'), arxiv.Result.Author('Andrei Velichko')]","Since February 2020, the world has been engaged in an intense struggle with
the COVID-19 dis-ease, and health systems have come under tragic pressure as
the disease turned into a pandemic. The aim of this study is to obtain the most
effective routine blood values (RBV) in the diagnosis and prognosis of COVID-19
using a backward feature elimination algorithm for the LogNNet reservoir neural
network. The first dataset in the study consists of a total of 5296 patients
with the same number of negative and positive COVID-19 tests. The LogNNet-model
achieved the accuracy rate of 99.5% in the diagnosis of the disease with 46
features and the accuracy of 99.17% with only mean corpuscular hemoglobin
concentration, mean corpuscular hemoglobin, and activated partial prothrombin
time. The second dataset consists of a total of 3899 patients with a diagnosis
of COVID-19 who were treated in hospital, of which 203 were severe patients and
3696 were mild patients. The model reached the accuracy rate of 94.4% in
determining the prognosis of the disease with 48 features and the accuracy of
82.7% with only erythrocyte sedimentation rate, neutrophil count, and C
reactive protein features. Our method will reduce the negative pressures on the
health sector and help doctors to understand the pathogenesis of COVID-19 using
the key features. The method is promising to create mobile health monitoring
systems in the Internet of Things."
6479,"5 show it         to further research on MIMO architectures that will lead to
leads to subnetworks sharing all features: the subnetworks           their deployment smaller mobile and AR/VR devices.",1 and Fig.,are identical.,2022-05-20 12:33:34+00:00,Towards efficient feature sharing in MIMO architectures,cs.LG,['cs.LG'],"[arxiv.Result.Author('Rémy Sun'), arxiv.Result.Author('Alexandre Ramé'), arxiv.Result.Author('Clément Masson'), arxiv.Result.Author('Nicolas Thome'), arxiv.Result.Author('Matthieu Cord')]","Multi-input multi-output architectures propose to train multiple subnetworks
within one base network and then average the subnetwork predictions to benefit
from ensembling for free. Despite some relative success, these architectures
are wasteful in their use of parameters. Indeed, we highlight in this paper
that the learned subnetwork fail to share even generic features which limits
their applicability on smaller mobile and AR/VR devices. We posit this behavior
stems from an ill-posed part of the multi-input multi-output framework. To
solve this issue, we propose a novel unmixing step in MIMO architectures that
allows subnetworks to properly share features. Preliminary experiments on
CIFAR-100 show our adjustments allow feature sharing and improve model
performance for small architectures."
6509,"Finally, we plan further research on HYENA with a focus on predicting regional demand, which could then be
aggregated to create a national demand forecast.","An
additional challenge was the limited computational power at our disposal, which was particularly felt for the SARIMAX
implementation: more resources would mean a longer training set and a model extension which could account for
yearly seasonality.","We expect that this approach would lead to a sharp increase of accuracy,

                                                                      7
Athanasopoulou et al.",2022-05-20 22:13:25+00:00,A Hybrid Model for Forecasting Short-Term Electricity Demand,cs.LG,"['cs.LG', 'cs.AI', 'I.2.6']","[arxiv.Result.Author('Maria Eleni Athanasopoulou'), arxiv.Result.Author('Justina Deveikyte'), arxiv.Result.Author('Alan Mosca'), arxiv.Result.Author('Ilaria Peri'), arxiv.Result.Author('Alessandro Provetti')]","Currently the UK Electric market is guided by load (demand) forecasts
published every thirty minutes by the regulator. A key factor in predicting
demand is weather conditions, with forecasts published every hour. We present
HYENA: a hybrid predictive model that combines feature engineering (selection
of the candidate predictor features), mobile-window predictors and finally LSTM
encoder-decoders to achieve higher accuracy with respect to mainstream models
from the literature. HYENA decreased MAPE loss by 16\% and RMSE loss by 10\%
over the best available benchmark model, thus establishing a new state of the
art for the UK electric load (and price) forecasting."
6510,"We think
ossiﬁcation is an interesting phenomenon that merits further study.","Overall, our
ﬁnding that repetition can induce ossiﬁcation provides medium causal evidence to this hypothesis.","5.5 Limitations

We attempt to discuss limitations throughout the text where appropriate, but for the reader’s convenience, we
enumerate them here.",2022-05-21 02:14:27+00:00,Scaling Laws and Interpretability of Learning from Repeated Data,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Danny Hernandez'), arxiv.Result.Author('Tom Brown'), arxiv.Result.Author('Tom Conerly'), arxiv.Result.Author('Nova DasSarma'), arxiv.Result.Author('Dawn Drain'), arxiv.Result.Author('Sheer El-Showk'), arxiv.Result.Author('Nelson Elhage'), arxiv.Result.Author('Zac Hatfield-Dodds'), arxiv.Result.Author('Tom Henighan'), arxiv.Result.Author('Tristan Hume'), arxiv.Result.Author('Scott Johnston'), arxiv.Result.Author('Ben Mann'), arxiv.Result.Author('Chris Olah'), arxiv.Result.Author('Catherine Olsson'), arxiv.Result.Author('Dario Amodei'), arxiv.Result.Author('Nicholas Joseph'), arxiv.Result.Author('Jared Kaplan'), arxiv.Result.Author('Sam McCandlish')]","Recent large language models have been trained on vast datasets, but also
often on repeated data, either intentionally for the purpose of upweighting
higher quality data, or unintentionally because data deduplication is not
perfect and the model is exposed to repeated data at the sentence, paragraph,
or document level. Some works have reported substantial negative performance
effects of this repeated data. In this paper we attempt to study repeated data
systematically and to understand its effects mechanistically. To do this, we
train a family of models where most of the data is unique but a small fraction
of it is repeated many times. We find a strong double descent phenomenon, in
which repeated data can lead test loss to increase midway through training. A
predictable range of repetition frequency leads to surprisingly severe
degradation in performance. For instance, performance of an 800M parameter
model can be degraded to that of a 2x smaller model (400M params) by repeating
0.1% of the data 100 times, despite the other 90% of the training tokens
remaining unique. We suspect there is a range in the middle where the data can
be memorized and doing so consumes a large fraction of the model's capacity,
and this may be where the peak of degradation occurs. Finally, we connect these
observations to recent mechanistic interpretability work - attempting to
reverse engineer the detailed computations performed by the model - by showing
that data repetition disproportionately damages copying and internal structures
associated with generalization, such as induction heads, providing a possible
mechanism for the shift from generalization to memorization. Taken together,
these results provide a hypothesis for why repeating a relatively small
fraction of data in large language models could lead to disproportionately
large harms to performance."
6515,"Concrete advice on parameter choices in ML is
subject to further research (Ng, 2018).","In case the hyperparameter value is not mentioned in Table 3 or 4 the default value ascribed
by H2O was used during the model training.","3.3 Software

The complete software setup – data preparation, preprocessing, model fitting, and evaluation
– was done in the integrated development environment (IDE) RStudio, which is widely used
for research in data science and machine learning and based on the statistical programming
language R (R Core Team, 2019).",2022-05-21 08:27:55+00:00,Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art machine learning algorithms for credit scoring,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE', 'q-fin.CP', 'q-fin.RM']",[arxiv.Result.Author('Marc Schmitt')],"Artificial intelligence (AI) and machine learning (ML) have become vital to
remain competitive for financial services companies around the globe. The two
models currently competing for the pole position in credit risk management are
deep learning (DL) and gradient boosting machines (GBM). This paper benchmarked
those two algorithms in the context of credit scoring using three distinct
datasets with different features to account for the reality that model
choice/power is often dependent on the underlying characteristics of the
dataset. The experiment has shown that GBM tends to be more powerful than DL
and has also the advantage of speed due to lower computational requirements.
This makes GBM the winner and choice for credit scoring. However, it was also
shown that the outperformance of GBM is not always guaranteed and ultimately
the concrete problem scenario or dataset will determine the final model choice.
Overall, based on this study both algorithms can be considered state-of-the-art
for binary classification tasks on structured datasets, while GBM should be the
go-to solution for most problem scenarios due to easier use, significantly
faster training time, and superior accuracy."
6516,"However, further research could successfully strengthen those findings and
confirm that GBM is the best model available for structured datasets.","To strengthen the above findings additional datasets can be used, but it is unlikely that the
findings will be negatively challenged as results already indicate that there is no guarantee, but
a strong tendency that GBM is the preferred choice for structured datasets in the case of binary
classification.","6 Future Research

The traditional logistic regression does not reach accuracy levels that come close to artificial
neural networks or ensembles such as bagging and boosting.",2022-05-21 08:27:55+00:00,Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art machine learning algorithms for credit scoring,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE', 'q-fin.CP', 'q-fin.RM']",[arxiv.Result.Author('Marc Schmitt')],"Artificial intelligence (AI) and machine learning (ML) have become vital to
remain competitive for financial services companies around the globe. The two
models currently competing for the pole position in credit risk management are
deep learning (DL) and gradient boosting machines (GBM). This paper benchmarked
those two algorithms in the context of credit scoring using three distinct
datasets with different features to account for the reality that model
choice/power is often dependent on the underlying characteristics of the
dataset. The experiment has shown that GBM tends to be more powerful than DL
and has also the advantage of speed due to lower computational requirements.
This makes GBM the winner and choice for credit scoring. However, it was also
shown that the outperformance of GBM is not always guaranteed and ultimately
the concrete problem scenario or dataset will determine the final model choice.
Overall, based on this study both algorithms can be considered state-of-the-art
for binary classification tasks on structured datasets, while GBM should be the
go-to solution for most problem scenarios due to easier use, significantly
faster training time, and superior accuracy."
6517,"Also, given the strong
performance of the AutoML solution created by H2O, it is almost certain that further research
will result in prediction accuracy levels that are on par with models adjusted by ML experts.","The answer to this question is
mainly dependent on the use case at hand, and whether a tiny performance improvement
justifies the additional time required for manual model tuning.","Overall, AutoML is an important first step toward complete end-to-end decision processes.",2022-05-21 08:35:02+00:00,Automated machine learning: AI-driven decision making in business analytics,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE']",[arxiv.Result.Author('Marc Schmitt')],"The realization that AI-driven decision-making is indispensable in todays
fast-paced and ultra-competitive marketplace has raised interest in industrial
machine learning (ML) applications significantly. The current demand for
analytics experts vastly exceeds the supply. One solution to this problem is to
increase the user-friendliness of ML frameworks to make them more accessible
for the non-expert. Automated machine learning (AutoML) is an attempt to solve
the problem of expertise by providing fully automated off-the-shelf solutions
for model choice and hyperparameter tuning. This paper analyzed the potential
of AutoML for applications within business analytics, which could help to
increase the adoption rate of ML across all industries. The H2O AutoML
framework was benchmarked against a manually tuned stacked ML model on three
real-world datasets to test its performance, robustness, and reliability. The
manually tuned ML model could reach a performance advantage in all three case
studies used in the experiment. Nevertheless, the H2O AutoML package proved to
be quite potent. It is fast, easy to use, and delivers reliable results, which
come close to a professionally tuned ML model. The H2O AutoML framework in its
current capacity is a valuable tool to support fast prototyping with the
potential to shorten development and deployment cycles. It can also bridge the
existing gap between supply and demand for ML experts and is a big step towards
fully automated decisions in business analytics."
6518,"(1) Preprocessing              (2) Hyperparameter         (3) Actual Decision
                                  Tuning, Model               (Prescriptive
                                    Choice, and                 Analytics)
                                     Evaluation

• Currently poorely handelled  • AutoML performs already  • AutoML is only concerned
 by AutoML, further reserach    quite good and close to    about predictive analytics,
 necessary                      expert level tuning        further research necessary

Figure 2.",See figure 2.,"This graphic shows the current capabilities of AutoML and points towards further research
necessary to completely automate the predictive analytics workflow to finalize the notion of complete
off-the-shelf ML solutions for data-driven decision-making.",2022-05-21 08:35:02+00:00,Automated machine learning: AI-driven decision making in business analytics,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE']",[arxiv.Result.Author('Marc Schmitt')],"The realization that AI-driven decision-making is indispensable in todays
fast-paced and ultra-competitive marketplace has raised interest in industrial
machine learning (ML) applications significantly. The current demand for
analytics experts vastly exceeds the supply. One solution to this problem is to
increase the user-friendliness of ML frameworks to make them more accessible
for the non-expert. Automated machine learning (AutoML) is an attempt to solve
the problem of expertise by providing fully automated off-the-shelf solutions
for model choice and hyperparameter tuning. This paper analyzed the potential
of AutoML for applications within business analytics, which could help to
increase the adoption rate of ML across all industries. The H2O AutoML
framework was benchmarked against a manually tuned stacked ML model on three
real-world datasets to test its performance, robustness, and reliability. The
manually tuned ML model could reach a performance advantage in all three case
studies used in the experiment. Nevertheless, the H2O AutoML package proved to
be quite potent. It is fast, easy to use, and delivers reliable results, which
come close to a professionally tuned ML model. The H2O AutoML framework in its
current capacity is a valuable tool to support fast prototyping with the
potential to shorten development and deployment cycles. It can also bridge the
existing gap between supply and demand for ML experts and is a big step towards
fully automated decisions in business analytics."
6519,"This graphic shows the current capabilities of AutoML and points towards further research
necessary to completely automate the predictive analytics workflow to finalize the notion of complete
off-the-shelf ML solutions for data-driven decision-making.","(1) Preprocessing              (2) Hyperparameter         (3) Actual Decision
                                  Tuning, Model               (Prescriptive
                                    Choice, and                 Analytics)
                                     Evaluation

• Currently poorely handelled  • AutoML performs already  • AutoML is only concerned
 by AutoML, further reserach    quite good and close to    about predictive analytics,
 necessary                      expert level tuning        further research necessary

Figure 2.","Current research mainly focuses on predictive tasks and results must be interpreted by human
decision-makers.",2022-05-21 08:35:02+00:00,Automated machine learning: AI-driven decision making in business analytics,cs.LG,"['cs.LG', 'cs.AI', 'cs.CE']",[arxiv.Result.Author('Marc Schmitt')],"The realization that AI-driven decision-making is indispensable in todays
fast-paced and ultra-competitive marketplace has raised interest in industrial
machine learning (ML) applications significantly. The current demand for
analytics experts vastly exceeds the supply. One solution to this problem is to
increase the user-friendliness of ML frameworks to make them more accessible
for the non-expert. Automated machine learning (AutoML) is an attempt to solve
the problem of expertise by providing fully automated off-the-shelf solutions
for model choice and hyperparameter tuning. This paper analyzed the potential
of AutoML for applications within business analytics, which could help to
increase the adoption rate of ML across all industries. The H2O AutoML
framework was benchmarked against a manually tuned stacked ML model on three
real-world datasets to test its performance, robustness, and reliability. The
manually tuned ML model could reach a performance advantage in all three case
studies used in the experiment. Nevertheless, the H2O AutoML package proved to
be quite potent. It is fast, easy to use, and delivers reliable results, which
come close to a professionally tuned ML model. The H2O AutoML framework in its
current capacity is a valuable tool to support fast prototyping with the
potential to shorten development and deployment cycles. It can also bridge the
existing gap between supply and demand for ML experts and is a big step towards
fully automated decisions in business analytics."
6531,"We further study model’s
generalization capacities, robustness, and sensibility of outputs in a series of additional experiments.","In an extensive evaluation, we show
that this method works better than strong but unstructured baselines.","Acknowledgments and Disclosure of Funding

This research is supported in part by the DARPA ReMath program under Contract No.",2022-05-21 20:55:57+00:00,NS3: Neuro-Symbolic Semantic Code Search,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shushan Arakelyan'), arxiv.Result.Author('Anna Hakhverdyan'), arxiv.Result.Author('Miltiadis Allamanis'), arxiv.Result.Author('Luis Garcia'), arxiv.Result.Author('Christophe Hauser'), arxiv.Result.Author('Xiang Ren')]","Semantic code search is the task of retrieving a code snippet given a textual
description of its functionality. Recent work has been focused on using
similarity metrics between neural embeddings of text and code. However, current
language models are known to struggle with longer, compositional text, and
multi-step reasoning. To overcome this limitation, we propose supplementing the
query sentence with a layout of its semantic structure. The semantic layout is
used to break down the final reasoning decision into a series of lower-level
decisions. We use a Neural Module Network architecture to implement this idea.
We compare our model - NS3 (Neuro-Symbolic Semantic Search) - to a number of
baselines, including state-of-the-art semantic code retrieval methods, and
evaluate on two datasets - CodeSearchNet and Code Search and Question
Answering. We demonstrate that our approach results in more precise code
retrieval, and we study the effectiveness of our modular design when handling
compositional queries."
6535,"Moreover, the further research may focus                            Signal Processing, vol.","1183–1186, 1989.

allocation, coding/decoding to take advantage of intelligent                     [24] G. J. Gibson, S. Siu, and C. F. N. Cowan, “The application of nonlinear
                                                                                       structures to the reconstruction of binary signals,” IEEE Transactions on
feedback networks.","39, no.",2022-05-22 05:28:43+00:00,Multi-Agent Feedback Enabled Neural Networks for Intelligent Communications,cs.LG,"['cs.LG', 'cs.MA', 'eess.SP']","[arxiv.Result.Author('Fanglei Sun'), arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Ying Wen'), arxiv.Result.Author('Jingchen Hu'), arxiv.Result.Author('Jun Wang'), arxiv.Result.Author('Yang Yang'), arxiv.Result.Author('Kai Li')]","In the intelligent communication field, deep learning (DL) has attracted much
attention due to its strong fitting ability and data-driven learning
capability. Compared with the typical DL feedforward network structures, an
enhancement structure with direct data feedback have been studied and proved to
have better performance than the feedfoward networks. However, due to the above
simple feedback methods lack sufficient analysis and learning ability on the
feedback data, it is inadequate to deal with more complicated nonlinear systems
and therefore the performance is limited for further improvement. In this
paper, a novel multi-agent feedback enabled neural network (MAFENN) framework
is proposed, which make the framework have stronger feedback learning
capabilities and more intelligence on feature abstraction, denoising or
generation, etc. Furthermore, the MAFENN framework is theoretically formulated
into a three-player Feedback Stackelberg game, and the game is proved to
converge to the Feedback Stackelberg equilibrium. The design of MAFENN
framework and algorithm are dedicated to enhance the learning capability of the
feedfoward DL networks or their variations with the simple data feedback. To
verify the MAFENN framework's feasibility in wireless communications, a
multi-agent MAFENN based equalizer (MAFENN-E) is developed for wireless fading
channels with inter-symbol interference (ISI). Experimental results show that
when the quadrature phase-shift keying (QPSK) modulation scheme is adopted, the
SER performance of our proposed method outperforms that of the traditional
equalizers by about 2 dB in linear channels. When in nonlinear channels, the
SER performance of our proposed method outperforms that of either traditional
or DL based equalizers more significantly, which shows the effectiveness and
robustness of our proposal in the complex channel environment."
6585,"egy, LHE learns potential heterophilious edges and remove them
                                                                        from the graph such that message passing along those edges is not
   We further study the impact of heterophilious edges in the train-    allowed.","Instead of devising better feature aggregation strat-
        messaging.","To achieve this, we introduce an edge classifier that is
ing phase.",2022-05-23 14:07:29+00:00,Learning heterophilious edge to drop: A general framework for boosting graph neural networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jincheng Huang'), arxiv.Result.Author('Ping Li'), arxiv.Result.Author('Rui Huang'), arxiv.Result.Author('Chen Na')]","Graph Neural Networks (GNNs) aim at integrating node contents with graph
structure to learn nodes/graph representations. Nevertheless, it is found that
most of existing GNNs do not work well on data with high heterophily level that
accounts for a large proportion of edges between different class labels.
Recently, many efforts to tackle this problem focus on optimizing the way of
feature learning. From another angle, this work aims at mitigating the negative
impacts of heterophily by optimizing graph structure for the first time.
Specifically, on assumption that graph smoothing along heterophilious edges can
hurt prediction performance, we propose a structure learning method called LHE
to identify heterophilious edges to drop. A big advantage of this solution is
that it can boost GNNs without careful modification of feature learning
strategy. Extensive experiments demonstrate the remarkable performance
improvement of GNNs with \emph{LHE} on multiple datasets across full spectrum
of homophily level."
6586,We further study the impact of heterophilious edges on graph learning via the training processes.,"Training and validation loss on Cornell under the heterophily edge deletion rates of 0%, 50%, and 100% configurations.","Figure 3 shows the
variations of training losses of SGC under the configurations where heterophilious edges are randomly removed with
deletion rate of 0%, 50%, and 100%, respectively.",2022-05-23 14:07:29+00:00,Revisiting the role of heterophily in graph representation learning: An edge classification perspective,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jincheng Huang'), arxiv.Result.Author('Ping Li'), arxiv.Result.Author('Rui Huang'), arxiv.Result.Author('Chen Na'), arxiv.Result.Author('Acong Zhang')]","Graph representation learning aim at integrating node contents with graph
structure to learn nodes/graph representations. Nevertheless, it is found that
many existing graph learning methods do not work well on data with high
heterophily level that accounts for a large proportion of edges between
different class labels. Recent efforts to this problem focus on improving the
message passing mechanism. However, it remains unclear whether heterophily
truly does harm to the performance of graph neural networks (GNNs). The key is
to unfold the relationship between a node and its immediate neighbors, e.g.,
are they heterophilous or homophilious? From this perspective, here we study
the role of heterophily in graph representation learning before/after the
relationships between connected nodes are disclosed. In particular, we propose
an end-to-end framework that both learns the type of edges (i.e.,
heterophilous/homophilious) and leverage edge type information to improve the
expressiveness of graph neural networks. We implement this framework in two
different ways. Specifically, to avoid messages passing through heterophilous
edges, we can optimize the graph structure to be homophilious by dropping
heterophilous edges identified by an edge classifier. Alternatively, it is
possible to exploit the information about the presence of heterophilous
neighbors for feature learning, so a hybrid message passing approach is devised
to aggregate homophilious neighbors and diversify heterophilous neighbors based
on edge classification. Extensive experiments demonstrate the remarkable
performance improvement of GNNs with the proposed framework on multiple
datasets across the full spectrum of homophily level."
6587,"With POLTER’s easy implementation
and negligible computational requirements, we hope that it ﬁnds its way into more URL algorithms
and spurs further research in how we can learn general priors for arbitrary tasks.","In future work, this manual tuning could be
automated by means of AutoRL [Parker-Holder et al., 2022].","9
Broader Impact

URL has the potential to improve the sample-efﬁciency of RL agents, which in turn might reduce the
amount of energy and other resources that is required to train them for their respective task.",2022-05-23 14:42:38+00:00,POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning,cs.LG,"['cs.LG', 'cs.RO']","[arxiv.Result.Author('Frederik Schubert'), arxiv.Result.Author('Carolin Benjamins'), arxiv.Result.Author('Sebastian Döhler'), arxiv.Result.Author('Bodo Rosenhahn'), arxiv.Result.Author('Marius Lindauer')]","The goal of Unsupervised Reinforcement Learning (URL) is to find a
reward-agnostic prior policy on a task domain, such that the sample-efficiency
on supervised downstream tasks is improved. Although agents initialized with
such a prior policy can achieve a significantly higher reward with fewer
samples when finetuned on the downstream task, it is still an open question how
an optimal pretrained prior policy can be achieved in practice. In this work,
we present POLTER (Policy Trajectory Ensemble Regularization) - a general
method to regularize the pretraining that can be applied to any URL algorithm
and is especially useful on data- and knowledge-based URL algorithms. It
utilizes an ensemble of policies that are discovered during pretraining and
moves the policy of the URL algorithm closer to its optimal prior. Our method
is theoretically justified, and we analyze its practical effects on a white-box
benchmark, allowing us to study POLTER with full control. In our main
experiments, we evaluate POLTER on the Unsupervised Reinforcement Learning
Benchmark (URLB), which consists of 12 tasks in 3 domains. We demonstrate the
generality of our approach by improving the performance of a diverse set of
data- and knowledge-based URL algorithms by 19% on average and up to 40% in the
best case. Under a fair comparison with tuned baselines and tuned POLTER, we
establish a new the state-of-the-art on the URLB."
6594,"More comprehensive analyses of data dependent binning
algorithms, their parametrization, and their sample complexity, especially for the more expressive
and understudied “full” ECE lens, is an important direction of further research.","That being said,
based on our results, we posit the existence of tighter, or at least more fully characterized, bounds

between the histogram estimated ECE and true ECE when working with standard data contexts
and corresponding values for N and M .","C ADDITIONAL EXPERIMENTAL DETAILS

In order to provide for technical transparency, reproducibility of our results, and further experimenta-
tion based on our work, we describe our experimental setup in further detail below.",2022-05-23 16:45:02+00:00,What is Your Metric Telling You? Evaluating Classifier Calibration under Context-Specific Definitions of Reliability,cs.LG,['cs.LG'],"[arxiv.Result.Author('John Kirchenbauer'), arxiv.Result.Author('Jacob Oaks'), arxiv.Result.Author('Eric Heim')]","Classifier calibration has received recent attention from the machine
learning community due both to its practical utility in facilitating decision
making, as well as the observation that modern neural network classifiers are
poorly calibrated. Much of this focus has been towards the goal of learning
classifiers such that their output with largest magnitude (the ""predicted
class"") is calibrated. However, this narrow interpretation of classifier
outputs does not adequately capture the variety of practical use cases in which
classifiers can aid in decision making. In this work, we argue that more
expressive metrics must be developed that accurately measure calibration error
for the specific context in which a classifier will be deployed. To this end,
we derive a number of different metrics using a generalization of Expected
Calibration Error (ECE) that measure calibration error under different
definitions of reliability. We then provide an extensive empirical evaluation
of commonly used neural network architectures and calibration techniques with
respect to these metrics. We find that: 1) definitions of ECE that focus solely
on the predicted class fail to accurately measure calibration error under a
selection of practically useful definitions of reliability and 2) many common
calibration techniques fail to improve calibration performance uniformly across
ECE metrics derived from these diverse definitions of reliability."
6639,"Our results show the suitability of the SFW algorithm and
highlight the importance of the learning rate rescaling, which we justify theoretically in the hope of
enabling further research.","One strength is the fact that the
proposed methods cover a wide range of compression domains, i.e., unstructured pruning, structured
pruning, as well as matrix decomposition.","2
Related Work.",2022-05-24 09:29:02+00:00,Compression-aware Training of Neural Networks using Frank-Wolfe,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Max Zimmer'), arxiv.Result.Author('Christoph Spiegel'), arxiv.Result.Author('Sebastian Pokutta')]","Many existing Neural Network pruning approaches either rely on retraining to
compensate for pruning-caused performance degradation or they induce strong
biases to converge to a specific sparse solution throughout training. A third
paradigm obtains a wide range of compression ratios from a single dense
training run while also avoiding retraining. Recent work of Pokutta et al.
(2020) and Miao et al. (2022) suggests that the Stochastic Frank-Wolfe (SFW)
algorithm is particularly suited for training state-of-the-art models that are
robust to compression. We propose leveraging $k$-support norm ball constraints
and demonstrate significant improvements over the results of Miao et al. (2022)
in the case of unstructured pruning. We also extend these ideas to the
structured pruning domain and propose novel approaches to both ensure
robustness to the pruning of convolutional filters as well as to low-rank
tensor decompositions of convolutional layers. In the latter case, our approach
performs on-par with nuclear-norm regularization baselines while requiring only
half of the computational resources. Our findings also indicate that the
robustness of SFW-trained models largely depends on the gradient rescaling of
the learning rate and we establish a theoretical foundation for that practice."
6640,"We hope that our ﬁndings regarding
the importance of the learning rate rescaling as well as Theorem 4.1 stimulate further research in the
direction of compression-aware training with SFW.","[50] can be
signiﬁcantly improved and we further propose novel approaches towards compression-aware training
in the structured pruning as well as matrix-decomposition setting.","However, we emphasize that our results hold primarily in the setting that we described, namely that
of compression-aware training, where the training is sparsity-agnostic and retraining is prohibitive.",2022-05-24 09:29:02+00:00,Compression-aware Training of Neural Networks using Frank-Wolfe,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Max Zimmer'), arxiv.Result.Author('Christoph Spiegel'), arxiv.Result.Author('Sebastian Pokutta')]","Many existing Neural Network pruning approaches either rely on retraining to
compensate for pruning-caused performance degradation or they induce strong
biases to converge to a specific sparse solution throughout training. A third
paradigm obtains a wide range of compression ratios from a single dense
training run while also avoiding retraining. Recent work of Pokutta et al.
(2020) and Miao et al. (2022) suggests that the Stochastic Frank-Wolfe (SFW)
algorithm is particularly suited for training state-of-the-art models that are
robust to compression. We propose leveraging $k$-support norm ball constraints
and demonstrate significant improvements over the results of Miao et al. (2022)
in the case of unstructured pruning. We also extend these ideas to the
structured pruning domain and propose novel approaches to both ensure
robustness to the pruning of convolutional filters as well as to low-rank
tensor decompositions of convolutional layers. In the latter case, our approach
performs on-par with nuclear-norm regularization baselines while requiring only
half of the computational resources. Our findings also indicate that the
robustness of SFW-trained models largely depends on the gradient rescaling of
the learning rate and we establish a theoretical foundation for that practice."
6644,"All three of these is-
posed to the general structure of the brain which can vary       sues provide a strong motivation to further study the use of
greatly form subject to subject.","This can be mitigated with
MRI-fMRI dataset for each subject may cause the 3D-CNN           the aforementioned transformer and can allow for the use
to over train of each subjects speciﬁc brain structure as op-    of the full resolution of the dataset.",This general brain struc-       transformers for classifying ADHD via fMRI data.,2022-05-24 11:39:11+00:00,Highly Accurate FMRI ADHD Classification using time distributed multi modal 3D CNNs,cs.LG,"['cs.LG', 'eess.IV']",[arxiv.Result.Author('Christopher Sims')],"This work proposes an algorithm for fMRI data analysis for the classification
of ADHD disorders. There have been several breakthroughs in the analysis of
fMRI via 3D convolutional neural networks (CNNs). With these new techniques it
is possible to preserve the 3D spatial data of fMRI data. Additionally there
have been recent advances in the use of 3D generative adversarial neural
networks (GANs) for the generation of normal MRI data. This work utilizes multi
modal 3D CNNs with data augmentation from 3D GAN for ADHD prediction from fMRI.
By leveraging a 3D-GAN it would be possible to use deepfake data to enhance the
accuracy of 3D CNN classification of brain disorders. A comparison will be made
between a time distributed single modal 3D CNN model for classification and the
modified multi modal model with MRI data as well."
6648,"The trade-       therefore, to transfer the label information from the normal-
off between transferable and damage sensitive features is a      condition, ﬁrst damage location- and third damage location-
topic for further research.","The transfer learning problem is,
normal-condition response and temperature data.","in the source structure to the same location in the target
                                                                 structure.",2022-05-24 13:01:28+00:00,On statistic alignment for domain adaptation in structural health monitoring,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jack Poole'), arxiv.Result.Author('Paul Gardner'), arxiv.Result.Author('Nikolaos Dervilis'), arxiv.Result.Author('Lawrence Bull'), arxiv.Result.Author('Keith Worden')]","The practical application of structural health monitoring (SHM) is often
limited by the availability of labelled data. Transfer learning - specifically
in the form of domain adaptation (DA) - gives rise to the possibility of
leveraging information from a population of physical or numerical structures,
by inferring a mapping that aligns the feature spaces. Typical DA methods rely
on nonparametric distance metrics, which require sufficient data to perform
density estimation. In addition, these methods can be prone to performance
degradation under class imbalance. To address these issues, statistic alignment
(SA) is discussed, with a demonstration of how these methods can be made robust
to class imbalance, including a special case of class imbalance called a
partial DA scenario. SA is demonstrated to facilitate damage localisation with
no target labels in a numerical case study, outperforming other
state-of-the-art DA methods. It is then shown to be capable of aligning the
feature spaces of a real heterogeneous population, the Z24 and KW51 bridges,
with only 220 samples used from the KW51 bridge. Finally, in scenarios where
more complex mappings are required for knowledge transfer, SA is shown to be a
vital pre-processing tool, increasing the performance of established DA
methods."
6678,"Our software enables teams to more easily
calculate the kernels, which we hope will give way for further research and application.","This work is impactful because neural kernels are objects of
interest to the theoretical community, and with PyTorch support we can extend the number
of researchers who have access to compute them.","A key takeaway from our work is that teams should consider the performance needs
to conduct their research and determine whether calculating the explicit derivative of the
network with respect to the parameters is worthwhile.",2022-05-24 21:27:58+00:00,TorchNTK: A Library for Calculation of Neural Tangent Kernels of PyTorch Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andrew Engel'), arxiv.Result.Author('Zhichao Wang'), arxiv.Result.Author('Anand D. Sarwate'), arxiv.Result.Author('Sutanay Choudhury'), arxiv.Result.Author('Tony Chiang')]","We introduce torchNTK, a python library to calculate the empirical neural
tangent kernel (NTK) of neural network models in the PyTorch framework. We
provide an efficient method to calculate the NTK of multilayer perceptrons. We
compare the explicit differentiation implementation against autodifferentiation
implementations, which have the benefit of extending the utility of the library
to any architecture supported by PyTorch, such as convolutional networks. A
feature of the library is that we expose the user to layerwise NTK components,
and show that in some regimes a layerwise calculation is more memory efficient.
We conduct preliminary experiments to demonstrate use cases for the software
and probe the NTK."
6685,"0.0
                                                                                           In future work, we will further research the effectiveness of
                              0       10  20          30      40                50  60  our algorithm in the medical dataset.","So CR-Aug has huge potential
                                                                                        in DL.","It is known that medical
                                                      epochs                            images are harder to augment as compared to natural images
                                                                                        due to the TB-consistent ﬁndings and the ROI spans for a
                                    Figure 2.",2022-05-25 03:15:36+00:00,Augmentation-induced Consistency Regularization for Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jianhan Wu'), arxiv.Result.Author('Shijing Si'), arxiv.Result.Author('Jianzong Wang'), arxiv.Result.Author('Jing Xiao')]","Deep neural networks have become popular in many supervised learning tasks,
but they may suffer from overfitting when the training dataset is limited. To
mitigate this, many researchers use data augmentation, which is a widely used
and effective method for increasing the variety of datasets. However, the
randomness introduced by data augmentation causes inevitable inconsistency
between training and inference, which leads to poor improvement. In this paper,
we propose a consistency regularization framework based on data augmentation,
called CR-Aug, which forces the output distributions of different sub models
generated by data augmentation to be consistent with each other. Specifically,
CR-Aug evaluates the discrepancy between the output distributions of two
augmented versions of each sample, and it utilizes a stop-gradient operation to
minimize the consistency loss. We implement CR-Aug to image and audio
classification tasks and conduct extensive experiments to verify its
effectiveness in improving the generalization ability of classifiers. Our
CR-Aug framework is ready-to-use, it can be easily adapted to many
state-of-the-art network architectures. Our empirical results show that CR-Aug
outperforms baseline methods by a significant margin."
6686,"0.0
                                                                                           In future work, we will further research the effectiveness of
                              0       10  20          30      40                50  60  our algorithm in the medical dataset.","So CR-Aug has huge potential
                                                                                        in DL.","It is known that medical
                                                      epochs                            images are harder to augment as compared to natural images
                                                                                        due to the TB-consistent ﬁndings and the ROI spans for a
                                    Figure 2.",2022-05-25 03:15:36+00:00,Augmentation-induced Consistency Regularization for Classification,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jianhan Wu'), arxiv.Result.Author('Shijing Si'), arxiv.Result.Author('Jianzong Wang'), arxiv.Result.Author('Jing Xiao')]","Deep neural networks have become popular in many supervised learning tasks,
but they may suffer from overfitting when the training dataset is limited. To
mitigate this, many researchers use data augmentation, which is a widely used
and effective method for increasing the variety of datasets. However, the
randomness introduced by data augmentation causes inevitable inconsistency
between training and inference, which leads to poor improvement. In this paper,
we propose a consistency regularization framework based on data augmentation,
called CR-Aug, which forces the output distributions of different sub models
generated by data augmentation to be consistent with each other. Specifically,
CR-Aug evaluates the discrepancy between the output distributions of two
augmented versions of each sample, and it utilizes a stop-gradient operation to
minimize the consistency loss. We implement CR-Aug to image and audio
classification tasks and conduct extensive experiments to verify its
effectiveness in improving the generalization ability of classifiers. Our
CR-Aug framework is ready-to-use, it can be easily adapted to many
state-of-the-art network architectures. Our empirical results show that CR-Aug
outperforms baseline methods by a significant margin."
6706,"We ﬁnish the paper with some general remarks, conjectures,
and directions for further research.2

2 Revisiting AlphaZero and LCZero

AlphaZero, which astonished the chess world, has essentially been replicated by LCZero.","In section 6, we give an overview of the AlphaZero
algorithms and the changes we made tailored to nim and demonstrate the difﬁculty of our AlphaZero style nim algo-
rithm have in becoming an expert agent on large boards.","This section focuses on a
highly trained version of LCZero.",2022-05-25 14:02:02+00:00,Impartial Games: A Challenge for Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bei Zhou'), arxiv.Result.Author('Søren Riis')]","The AlphaZero algorithm and its successor MuZero have revolutionised several
competitive strategy games, including chess, Go, and shogi and video games like
Atari, by learning to play these games better than any human and any
specialised computer program. Aside from knowing the rules, AlphaZero had no
prior knowledge of each game. This dramatically advanced progress on a
long-standing AI challenge to create programs that can learn for themselves
from first principles.
  Theoretically, there are well-known limits to the power of deep learning for
strategy games like chess, Go, and shogi, as they are known to be NEXPTIME
hard. Some papers have argued that the AlphaZero methodology has limitations
and is unsuitable for general AI. However, none of these works has suggested
any specific limits for any particular game.
  In this paper, we provide more powerful bottlenecks than previously
suggested. We present the first concrete example of a game - namely the
(children) game of nim - and other impartial games that seem to be a stumbling
block for AlphaZero and similar reinforcement learning algorithms. We show
experimentally that the bottlenecks apply to both the policy and value
networks. Since solving nim can be done in linear time using logarithmic space
i.e. has very low-complexity, our experimental results supersede known
theoretical limits based on many games' PSPACE (and NEXPTIME) completeness.
  We show that nim can be learned on small boards, but when the board size
increases, AlphaZero style algorithms rapidly fail to improve.
  We quantify the difficulties for various setups, parameter settings and
computational resources. Our results might help expand the AlphaZero self-play
paradigm by allowing it to use meta-actions during training and/or actual game
play like applying abstract transformations, or reading and writing to an
external memory."
6707,"And it is why this paper on impartial games might help navigate further research on expanding
AlphaZero style learning.","This is why we looked carefully and closely at the way the AlphaZero clone
LC0 played chess.",Parity-related problems occur naturally for a large class of combinatorial games.,2022-05-25 14:02:02+00:00,Impartial Games: A Challenge for Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bei Zhou'), arxiv.Result.Author('Søren Riis')]","The AlphaZero algorithm and its successor MuZero have revolutionised several
competitive strategy games, including chess, Go, and shogi and video games like
Atari, by learning to play these games better than any human and any
specialised computer program. Aside from knowing the rules, AlphaZero had no
prior knowledge of each game. This dramatically advanced progress on a
long-standing AI challenge to create programs that can learn for themselves
from first principles.
  Theoretically, there are well-known limits to the power of deep learning for
strategy games like chess, Go, and shogi, as they are known to be NEXPTIME
hard. Some papers have argued that the AlphaZero methodology has limitations
and is unsuitable for general AI. However, none of these works has suggested
any specific limits for any particular game.
  In this paper, we provide more powerful bottlenecks than previously
suggested. We present the first concrete example of a game - namely the
(children) game of nim - and other impartial games that seem to be a stumbling
block for AlphaZero and similar reinforcement learning algorithms. We show
experimentally that the bottlenecks apply to both the policy and value
networks. Since solving nim can be done in linear time using logarithmic space
i.e. has very low-complexity, our experimental results supersede known
theoretical limits based on many games' PSPACE (and NEXPTIME) completeness.
  We show that nim can be learned on small boards, but when the board size
increases, AlphaZero style algorithms rapidly fail to improve.
  We quantify the difficulties for various setups, parameter settings and
computational resources. Our results might help expand the AlphaZero self-play
paradigm by allowing it to use meta-actions during training and/or actual game
play like applying abstract transformations, or reading and writing to an
external memory."
6719,"Of course, this is a very simpliﬁed case of
analysis, where the eﬀect of overﬁtting may occur, so this approach requires further study.","The results show that an intelligent agent
can ﬁnd the an optimal proﬁtable strategy.","The main goal is to show that, using reinforced learning and an environment model based
on historical ﬁnancial data and quantitative characteristics of tweets, it is possible to build
a model in which an intelligent agent can ﬁnd an optimal strategy that optimizes the re-
ward function in episodes of interaction of learning agent with the environment.",2022-05-25 16:51:09+00:00,Analytics of Business Time Series Using Machine Learning and Bayesian Inference,cs.LG,['cs.LG'],[arxiv.Result.Author('Bohdan M. Pavlyshenko')],"In the survey we consider the case studies on sales time series forecasting,
the deep learning approach for forecasting non-stationary time series using
time trend correction, dynamic price and supply optimization using Q-learning,
Bitcoin price modeling, COVID-19 spread impact on stock market, using social
networks signals in analytics. The use of machine learning and Bayesian
inference in predictive analytics has been analyzed."
6720,"Of course, this is a very simpliﬁed case of
analysis, where the eﬀect of overﬁtting may occur, so this approach requires further study.","The results show that an intelligent agent
can ﬁnd the an optimal proﬁtable strategy.","The main goal is to show that, using reinforced learning and an environment model based
on historical ﬁnancial data and quantitative characteristics of tweets, it is possible to build
a model in which an intelligent agent can ﬁnd an optimal strategy that optimizes the re-
ward function in episodes of interaction of learning agent with the environment.",2022-05-25 16:51:09+00:00,Analytics of Business Time Series Using Machine Learning and Bayesian Inference,cs.LG,['cs.LG'],[arxiv.Result.Author('Bohdan M. Pavlyshenko')],"In the survey we consider the case studies on sales time series forecasting,
the deep learning approach for forecasting non-stationary time series using
time trend correction, dynamic price and supply optimization using Q-learning,
Bitcoin price modeling, COVID-19 spread impact on stock market, using social
networks signals in analytics. The use of machine learning and Bayesian
inference in predictive analytics has been analyzed."
6759,"embedding principle in depth uncovers the depth-wise hierar-
   chical structure of deep learning loss landscape, which serves     (referred to as the embedding principle in width in the fol-
   as a solid foundation for the further study about the role of      lowing), we study in this work the relation between loss
   depth for DNNs.","Overall, our discovery of the         the same colored spans of loss value in (a).","landscapes of shallow and deep networks due to the extreme
                                                                      importance of depth for DNNs.",2022-05-26 11:42:44+00:00,Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhiwei Bai'), arxiv.Result.Author('Tao Luo'), arxiv.Result.Author('Zhi-Qin John Xu'), arxiv.Result.Author('Yaoyu Zhang')]","Understanding the relation between deep and shallow neural networks is
extremely important for the theoretical study of deep learning. In this work,
we discover an embedding principle in depth that loss landscape of an NN
""contains"" all critical points of the loss landscapes for shallower NNs. The
key tool for our discovery is the critical lifting operator proposed in this
work that maps any critical point of a network to critical manifolds of any
deeper network while preserving the outputs. This principle provides new
insights to many widely observed behaviors of DNNs. Regarding the easy training
of deep networks, we show that local minimum of an NN can be lifted to strict
saddle points of a deeper NN. Regarding the acceleration effect of batch
normalization, we demonstrate that batch normalization helps avoid the critical
manifolds lifted from shallower NNs by suppressing layer linearization. We also
prove that increasing training data shrinks the lifted critical manifolds,
which can result in acceleration of training as demonstrated in experiments.
Overall, our discovery of the embedding principle in depth uncovers the
depth-wise hierarchical structure of deep learning loss landscape, which serves
as a solid foundation for the further study about the role of depth for DNNs."
6760,"The colored spans of accuracy in (b) correspond to
   as a solid foundation for the further study about the role of      the same colored spans of loss value in (a).","(b)
   embedding principle in depth uncovers the depth-wise hierar-       The training accuracy for NNs of different depths on Iris
   chical structure of deep learning loss landscape, which serves     dataset.",depth for DNNs.,2022-05-26 11:42:44+00:00,Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhiwei Bai'), arxiv.Result.Author('Tao Luo'), arxiv.Result.Author('Zhi-Qin John Xu'), arxiv.Result.Author('Yaoyu Zhang')]","Understanding the relation between deep and shallow neural networks is
extremely important for the theoretical study of deep learning. In this work,
we discover an embedding principle in depth that loss landscape of an NN
""contains"" all critical points of the loss landscapes for shallower NNs. The
key tool for our discovery is the critical lifting operator proposed in this
work that maps any critical point of a network to critical manifolds of any
deeper network while preserving the outputs. This principle provides new
insights to many widely observed behaviors of DNNs. Regarding the easy training
of deep networks, we show that local minimum of an NN can be lifted to strict
saddle points of a deeper NN. Regarding the acceleration effect of batch
normalization, we demonstrate that batch normalization helps avoid the critical
manifolds lifted from shallower NNs by suppressing layer linearization. We also
prove that increasing training data shrinks the lifted critical manifolds,
which can result in acceleration of training as demonstrated in experiments.
Overall, our discovery of the embedding principle in depth uncovers the
depth-wise hierarchical structure of deep learning loss landscape, which serves
as a solid foundation for the further study about the role of depth for DNNs."
6762,"To further study the
expressive power of K-hop message passing on regular graphs, we show the following result:

Theorem 1.","These two examples convincingly demonstrate that the K-hop
message passing with K > 1 can have better expressive power than K = 1.","Consider all pairs of n-sized r-regular graphs, let 3 ≤ r < (2log2n)1/2 and be a
ﬁxed constant.",2022-05-26 13:03:56+00:00,How Powerful are K-hop Message Passing Graph Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jiarui Feng'), arxiv.Result.Author('Yixin Chen'), arxiv.Result.Author('Fuhai Li'), arxiv.Result.Author('Anindya Sarkar'), arxiv.Result.Author('Muhan Zhang')]","The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop
message passing -- aggregating information from 1-hop neighbors repeatedly.
However, the expressive power of 1-hop message passing is bounded by the
Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message
passing to K-hop message passing by aggregating information from K-hop
neighbors of nodes simultaneously. However, there is no work on analyzing the
expressive power of K-hop message passing. In this work, we theoretically
characterize the expressive power of K-hop message passing. Specifically, we
first formally differentiate two different kernels of K-hop message passing
which are often misused in previous works. We then characterize the expressive
power of K-hop message passing by showing that it is more powerful than 1-WL
and can distinguish almost all regular graphs. Despite the higher expressive
power, we show that K-hop message passing still cannot distinguish some simple
regular graphs and its expressive power is bounded by 3-WL. To further enhance
its expressive power, we introduce a KP-GNN framework, which improves K-hop
message passing by leveraging the peripheral subgraph information in each hop.
We show that KP-GNN can distinguish many distance regular graphs which could
not be distinguished by previous distance encoding or 3-WL methods.
Experimental results verify the expressive power and effectiveness of KP-GNN.
KP-GNN achieves competitive results across all benchmark datasets."
6764,"We extend this line of
        the OoD environment(s), which motivates further research       work by proposing combinatorial criteria (e.g.","This involves ex-
     • Despite the varying levels of performance drop in the first     cluding groups based on demographics, splitting features related
        step of OoD candidate generation, we show that there is no     to a dynamic clinical status, or artificially creating OoD groups
        significant performance improvement when using data from       by withholding them during training [20].","both gender and
        on the suitability of this benchmark dataset for evaluating    age).",2022-05-26 14:46:13+00:00,Looking for Out-of-Distribution Environments in Critical Care: A case study with the eICU Database,cs.LG,['cs.LG'],"[arxiv.Result.Author('Dimitris Spathis'), arxiv.Result.Author('Stephanie L. Hyland')]","Generalizing to new populations and domains in machine learning is still an
open problem which has seen increased interest recently. In particular,
clinical models show a significant performance drop when tested in settings not
seen during training, e.g., new hospitals or population demographics. Recently
proposed models for domain generalisation promise to alleviate this problem by
learning invariant characteristics across environments, however, there is still
scepticism about whether they improve over traditional training. In this work,
we take a principled approach to identifying Out of Distribution (OoD)
environments, motivated by the problem of cross-hospital generalization in
critical care. We propose model-based and heuristic approaches to identify OoD
environments and systematically compare models with different levels of
held-out information. In particular, based on the assumption that models with
access to OoD data should outperform other models, we train models across a
range of experimental setups that include leave-one-hospital-out training and
cross-sectional feature splits. We find that access to OoD data does not
translate to increased performance, pointing to inherent limitations in
defining potential OoD environments in the eICU Database potentially due to
data harmonisation and sampling. Echoing similar results with other popular
clinical benchmarks in the literature, new approaches are required to evaluate
robust models in critical care."
6769,"Nevertheless, ﬁnding a family of activation functions that both play
well with Φ and is not a family of radial functions needs further study.","We have managed to ﬁnd a robust learning rule for a
generalized notion of ANNs.","6 Conclusion

In this paper we introduced a general framework for training robust ANN classiﬁers.",2022-05-26 17:16:39+00:00,An Analytic Framework for Robust Training of Artificial Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ramin Barati'), arxiv.Result.Author('Reza Safabakhsh'), arxiv.Result.Author('Mohammad Rahmati')]","The reliability of a learning model is key to the successful deployment of
machine learning in various industries. Creating a robust model, particularly
one unaffected by adversarial attacks, requires a comprehensive understanding
of the adversarial examples phenomenon. However, it is difficult to describe
the phenomenon due to the complicated nature of the problems in machine
learning. Consequently, many studies investigate the phenomenon by proposing a
simplified model of how adversarial examples occur and validate it by
predicting some aspect of the phenomenon. While these studies cover many
different characteristics of the adversarial examples, they have not reached a
holistic approach to the geometric and analytic modeling of the phenomenon.
This paper propose a formal framework to study the phenomenon in learning
theory and make use of complex analysis and holomorphicity to offer a robust
learning rule for artificial neural networks. With the help of complex
analysis, we can effortlessly move between geometric and analytic perspectives
of the phenomenon and offer further insights on the phenomenon by revealing its
connection with harmonic functions. Using our model, we can explain some of the
most intriguing characteristics of adversarial examples, including
transferability of adversarial examples, and pave the way for novel approaches
to mitigate the effects of the phenomenon."
6770,"Though our work has shown very successful domain transfer results, we note that social biases
associated with feature representations require further study to gain an improved understanding of
model behavior.","Also, the ability to work without target labels has the potential to enhance privacy.","9
Acknowledgments

DISTRIBUTION STATEMENT A.",2022-05-26 17:20:08+00:00,Pick up the PACE: Fast and Simple Domain Adaptation via Ensemble Pseudo-Labeling,cs.LG,['cs.LG'],"[arxiv.Result.Author('Christopher Liao'), arxiv.Result.Author('Theodoros Tsiligkaridis'), arxiv.Result.Author('Brian Kulis')]","Domain Adaptation (DA) has received widespread attention from deep learning
researchers in recent years because of its potential to improve test accuracy
with out-of-distribution labeled data. Most state-of-the-art DA algorithms
require an extensive amount of hyperparameter tuning and are computationally
intensive due to the large batch sizes required. In this work, we propose a
fast and simple DA method consisting of three stages: (1) domain alignment by
covariance matching, (2) pseudo-labeling, and (3) ensembling. We call this
method $\textbf{PACE}$, for $\textbf{P}$seudo-labels, $\textbf{A}$lignment of
$\textbf{C}$ovariances, and $\textbf{E}$nsembles. PACE is trained on top of
fixed features extracted from an ensemble of modern pretrained backbones. PACE
exceeds previous state-of-the-art by $\textbf{5 - 10 \%}$ on most benchmark
adaptation tasks without training a neural network. PACE reduces training time
and hyperparameter tuning time by $82\%$ and $97\%$, respectively, when
compared to state-of-the-art DA methods. Code is released here:
https://github.com/Chris210634/PACE-Domain-Adaptation"
6782,"Prelimi-
nary results with FEDADAM are given (Appendix C.4.4), but further study is required.","Adaptive optimization [Reddi et al., 2020] with mixed FL has not been explored adequately.","Application
of adaptivity could positively impact practical convergence experience.",2022-05-26 22:22:15+00:00,Mixed Federated Learning: Joint Decentralized and Centralized Learning,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Sean Augenstein'), arxiv.Result.Author('Andrew Hard'), arxiv.Result.Author('Lin Ning'), arxiv.Result.Author('Karan Singhal'), arxiv.Result.Author('Satyen Kale'), arxiv.Result.Author('Kurt Partridge'), arxiv.Result.Author('Rajiv Mathews')]","Federated learning (FL) enables learning from decentralized privacy-sensitive
data, with computations on raw data confined to take place at edge clients.
This paper introduces mixed FL, which incorporates an additional loss term
calculated at the coordinating server (while maintaining FL's private data
restrictions). There are numerous benefits. For example, additional datacenter
data can be leveraged to jointly learn from centralized (datacenter) and
decentralized (federated) training data and better match an expected inference
data distribution. Mixed FL also enables offloading some intensive computations
(e.g., embedding regularization) to the server, greatly reducing communication
and client computation load. For these and other mixed FL use cases, we present
three algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY
GRADIENT TRANSFER. We state convergence bounds for each, and give intuition on
which are suited to particular mixed FL problems. Finally we perform extensive
experiments on three tasks, demonstrating that mixed FL can blend training data
to achieve an oracle's accuracy on an inference distribution, and can reduce
communication and computation overhead by over 90%. Our experiments confirm
theoretical predictions of how algorithms perform under different mixed FL
problem settings."
6783,"Prelimi-
nary results with FEDADAM are given (Appendix C.4.4), but further study is required.","Adaptive optimization [Reddi et al., 2020] with mixed FL has not been explored adequately.","Application
of adaptivity could positively impact practical convergence experience.",2022-05-26 22:22:15+00:00,Mixed Federated Learning: Joint Decentralized and Centralized Learning,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Sean Augenstein'), arxiv.Result.Author('Andrew Hard'), arxiv.Result.Author('Lin Ning'), arxiv.Result.Author('Karan Singhal'), arxiv.Result.Author('Satyen Kale'), arxiv.Result.Author('Kurt Partridge'), arxiv.Result.Author('Rajiv Mathews')]","Federated learning (FL) enables learning from decentralized privacy-sensitive
data, with computations on raw data confined to take place at edge clients.
This paper introduces mixed FL, which incorporates an additional loss term
calculated at the coordinating server (while maintaining FL's private data
restrictions). There are numerous benefits. For example, additional datacenter
data can be leveraged to jointly learn from centralized (datacenter) and
decentralized (federated) training data and better match an expected inference
data distribution. Mixed FL also enables offloading some intensive computations
(e.g., embedding regularization) to the server, greatly reducing communication
and client computation load. For these and other mixed FL use cases, we present
three algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY
GRADIENT TRANSFER. We state convergence bounds for each, and give intuition on
which are suited to particular mixed FL problems. Finally we perform extensive
experiments on three tasks, demonstrating that mixed FL can blend training data
to achieve an oracle's accuracy on an inference distribution, and can reduce
communication and computation overhead by over 90%. Our experiments confirm
theoretical predictions of how algorithms perform under different mixed FL
problem settings."
6784,"A.3 Layer-wise Study of SeedGNN

In this section, we further study the matching process when we apply SeedGNN.",Fix n = 500.,"As such, we present
the similarity matrix of each layer of SeedGNN and compare with the witness matrix of the iterative
1-hop and 2-hop algorithms at each iteration.",2022-05-26 23:50:42+00:00,SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching,cs.LG,['cs.LG'],"[arxiv.Result.Author('Liren Yu'), arxiv.Result.Author('Jiaming Xu'), arxiv.Result.Author('Xiaojun Lin')]","Recently, there have been significant interests in designing Graph Neural
Networks (GNNs) for seeded graph matching, which aims to match two (unlabeled)
graphs using only topological information and a small set of seeds. However,
most previous GNN architectures for seeded graph matching employ a
semi-supervised approach, which learns from only the seed set in a single pair
of graphs, and therefore does not attempt to learn from many training
examples/graphs to best match future unseen graphs. In contrast, this paper is
the first to propose a supervised approach for seeded graph matching, which had
so far only been used for seedless graph matching. Our proposed SeedGNN
architecture employs a number of novel design choices that are inspired by
theoretical studies of seeded graph matching. First, SeedGNN can easily learn
the capability of counting and using witnesses of different hops, in a way that
can be generalized to graphs with different sizes. Second, SeedGNN can use
easily-matched pairs as new seeds to percolate and match other nodes. We
evaluate SeedGNN on both synthetic and real graphs, and demonstrate significant
performance improvement over both non-learning and learning algorithms in the
existing literature. Further, our experiments confirm that the knowledge
learned by SeedGNN from training graphs can be generalized to test graphs with
different sizes and categories."
6788,We release our implementation to help facilitate further study.,"FMs hallucinate knowledge when uncertain, are predominantly available for
resource-rich languages, and are expensive to pretrain.","Acknowledgements

We thank Sabri Eyuboglu, Neel Guha, Michael Zhang, Laurel Orr, Kawin Ethayarajh, Deepak Narayanan, Mayee Chen,
Maya Varma, Gary Cheng, Rohith Kuditipudi, Xuechen Li, Sidd Karamcheti, and Rishi Bommasani for their helpful
feedback and discussions.",2022-05-27 02:32:26+00:00,Can Foundation Models Help Us Achieve Perfect Secrecy?,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Simran Arora'), arxiv.Result.Author('Christopher Ré')]","A key promise of machine learning is the ability to assist users with
personal tasks. Because the personal context required to make accurate
predictions is often sensitive, we require systems that protect privacy. A gold
standard privacy-preserving system will satisfy perfect secrecy, meaning that
interactions with the system provably reveal no additional private information
to adversaries. This guarantee should hold even as we perform multiple personal
tasks over the same underlying data. However, privacy and quality appear to be
in tension in existing systems for personal tasks. Neural models typically
require lots of training to perform well, while individual users typically hold
a limited scale of data, so the systems propose to learn from the aggregate
data of multiple users. This violates perfect secrecy and instead, in the last
few years, academics have defended these solutions using statistical notions of
privacy -- i.e., the probability of learning private information about a user
should be reasonably low. Given the vulnerabilities of these solutions, we
explore whether the strong perfect secrecy guarantee can be achieved using
recent zero-to-few sample adaptation techniques enabled by foundation models.
In response, we propose FOCUS, a framework for personal tasks. Evaluating on
popular privacy benchmarks, we find the approach, satisfying perfect secrecy,
competes with strong collaborative learning baselines on 6 of 7 tasks. We
empirically analyze the proposal, highlighting the opportunities and
limitations across task types, and model inductive biases and sizes."
6789,"Observation 3 indicates that further research on the MLP structure is needed
to determine widths and depths for PINNs.","Observation 1 and 2 suggest that it is possible to decouple the activation function and changing point
from the search space.","Observation 4 means that overﬁtting is not a problem
for PINNs.",2022-05-27 03:24:31+00:00,Auto-PINN: Understanding and Optimizing Physics-Informed Neural Architecture,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yicheng Wang'), arxiv.Result.Author('Xiaotian Han'), arxiv.Result.Author('Chia-Yuan Chang'), arxiv.Result.Author('Daochen Zha'), arxiv.Result.Author('Ulisses Braga-Neto'), arxiv.Result.Author('Xia Hu')]","Physics-informed neural networks (PINNs) are revolutionizing science and
engineering practice by bringing together the power of deep learning to bear on
scientific computation. In forward modeling problems, PINNs are meshless
partial differential equation (PDE) solvers that can handle irregular,
high-dimensional physical domains. Naturally, the neural architecture
hyperparameters have a large impact on the efficiency and accuracy of the PINN
solver. However, this remains an open and challenging problem because of the
large search space and the difficulty of identifying a proper search objective
for PDEs. Here, we propose Auto-PINN, the first systematic, automated
hyperparameter optimization approach for PINNs, which employs Neural
Architecture Search (NAS) techniques to PINN design. Auto-PINN avoids manually
or exhaustively searching the hyperparameter space associated with PINNs. A
comprehensive set of pre-experiments using standard PDE benchmarks allows us to
probe the structure-performance relationship in PINNs. We find that the
different hyperparameters can be decoupled, and that the training loss function
of PINNs is a good search objective. Comparison experiments with baseline
methods demonstrate that Auto-PINN produces neural architectures with superior
stability and accuracy over alternative baselines."
6800,"cannot take an action during the turn of another player,
                                        which does not conform to a regular Markov Decision Process             We further study the integration of historic information to
                                        (MDP).",Another challenge is apparent when the agent           optimize both bidding and playing.,"Therefore, the agent has to operate on two different          restore the violated Markov property.",2022-05-27 08:59:42+00:00,Improving Bidding and Playing Strategies in the Trick-Taking game Wizard using Deep Q-Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jonas Schumacher'), arxiv.Result.Author('Marco Pleines')]","In this work, the trick-taking game Wizard with a separate bidding and
playing phase is modeled by two interleaved partially observable Markov
decision processes (POMDP). Deep Q-Networks (DQN) are used to empower
self-improving agents, which are capable of tackling the challenges of a highly
non-stationary environment. To compare algorithms between each other, the
accuracy between bid and trick count is monitored, which strongly correlates
with the actual rewards and provides a well-defined upper and lower performance
bound. The trained DQN agents achieve accuracies between 66% and 87% in
self-play, leaving behind both a random baseline and a rule-based heuristic.
The conducted analysis also reveals a strong information asymmetry concerning
player positions during bidding. To overcome the missing Markov property of
imperfect-information games, a long short-term memory (LSTM) network is
implemented to integrate historic information into the decision-making process.
Additionally, a forward-directed tree search is conducted by sampling a state
of the environment and thereby turning the game into a perfect information
setting. To our surprise, both approaches do not surpass the performance of the
basic DQN agent."
6820,"We believe our results will spur further research into exploring
automating auxiliary learning across a variety of settings.","Without introducing any external data or architectural modiﬁcations, AANG yields
1.1% improvement on average over our strongest baseline [20] and 4.2% over standard ﬁne-tuning
of RoBERTa across the chosen tasks.","Notably, while we focus on NLP when
discussing the space of auxiliary objectives (Section 3) and in our empirical evaluation (Section 6),
our theoretical results (Section 4) and AANG itself are domain-agnostic.",2022-05-27 16:32:28+00:00,AANG: Automating Auxiliary Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Lucio M. Dery'), arxiv.Result.Author('Paul Michel'), arxiv.Result.Author('Mikhail Khodak'), arxiv.Result.Author('Graham Neubig'), arxiv.Result.Author('Ameet Talwalkar')]","When faced with data-starved or highly complex end-tasks, it is commonplace
for machine learning practitioners to introduce auxiliary objectives as
supplementary learning signals. Whilst much work has been done to formulate
useful auxiliary objectives, their construction is still an art which proceeds
by slow and tedious hand-design. Intuitions about how and when these objectives
improve end-task performance have also had limited theoretical backing. In this
work, we present an approach for automatically generating a suite of auxiliary
objectives. We achieve this by deconstructing existing objectives within a
novel unified taxonomy, identifying connections between them, and generating
new ones based on the uncovered structure. Next, we theoretically formalize
widely-held intuitions about how auxiliary learning improves generalization of
the end-task. This leads us to a principled and efficient algorithm for
searching the space of generated objectives to find those most useful to a
specified end-task. With natural language processing (NLP) as our domain of
study, we empirically verify that our automated auxiliary learning pipeline
leads to strong improvements over competitive baselines across continued
training experiments on a pre-trained model on 5 NLP end-tasks."
6825,Algorithmic improvements are a possible direction for further research.,"Full comparison to the numerous non-ML based approaches to CO problems is
beyond the scope of this paper (though extended baselines are presented in SM Section E), but it is
clear that ECORD represents a signiﬁcant step forward in the scalability of learnt heuristics.","An adaptive (or learnt)
temperature schedule could better trade-off stochastic exploration and deterministic solution improve-
ment.",2022-05-27 17:13:10+00:00,Learning to Solve Combinatorial Graph Partitioning Problems via Efficient Exploration,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thomas D. Barrett'), arxiv.Result.Author('Christopher W. F. Parsonson'), arxiv.Result.Author('Alexandre Laterre')]","From logistics to the natural sciences, combinatorial optimisation on graphs
underpins numerous real-world applications. Reinforcement learning (RL) has
shown particular promise in this setting as it can adapt to specific problem
structures and does not require pre-solved instances for these, often NP-hard,
problems. However, state-of-the-art (SOTA) approaches typically suffer from
severe scalability issues, primarily due to their reliance on expensive graph
neural networks (GNNs) at each decision step. We introduce ECORD; a novel RL
algorithm that alleviates this expense by restricting the GNN to a single
pre-processing step, before entering a fast-acting exploratory phase directed
by a recurrent unit. Experimentally, ECORD achieves a new SOTA for RL
algorithms on the Maximum Cut problem, whilst also providing orders of
magnitude improvement in speed and scalability. Compared to the nearest
competitor, ECORD reduces the optimality gap by up to 73% on 500 vertex graphs
with a decreased wall-clock time. Moreover, ECORD retains strong performance
when generalising to larger graphs with up to 10000 vertices."
6826,"To further study the behavior of BRGCL, we show the number of robust prototypes estimated by
BPL in Table 10.",B.6 Number of Conﬁdent Prototypes.,"It can be observed from the results that the estimated number of robust prototypes
is usually very close to the ground truth number of classes for different datasets, justifying the
effectiveness of BPL.",2022-05-27 17:21:17+00:00,Bayesian Robust Graph Contrastive Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yancheng Wang'), arxiv.Result.Author('Yingzhen Yang')]","Graph Neural Networks (GNNs) have been widely used to learn node
representations and with outstanding performance on various tasks such as node
classification. However, noise, which inevitably exists in real-world graph
data, would considerably degrade the performance of GNNs as the noise is easily
propagated via the graph structure. In this work, we propose a novel and robust
method, Bayesian Robust Graph Contrastive Learning (BRGCL), which trains a GNN
encoder to learn robust node representations. The BRGCL encoder is a completely
unsupervised encoder. Two steps are iteratively executed at each epoch of
training the BRGCL encoder: (1) estimating confident nodes and computing robust
cluster prototypes of node representations through a novel Bayesian
nonparametric method; (2) prototypical contrastive learning between the node
representations and the robust cluster prototypes. Experiments on public and
large-scale benchmarks demonstrate the superior performance of BRGCL and the
robustness of the learned node representations. The code of BRGCL is available
at \url{https://github.com/BRGCL-code/BRGCL-code}."
6827,"To further study the behavior of BRGCL, we show the number of robust prototypes estimated by
BPL in Table 10.",B.6 Number of Conﬁdent Prototypes.,"It can be observed from the results that the estimated number of robust prototypes
is usually very close to the ground truth number of classes for different datasets, justifying the
effectiveness of BPL.",2022-05-27 17:21:17+00:00,Bayesian Robust Graph Contrastive Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yancheng Wang'), arxiv.Result.Author('Yingzhen Yang')]","Graph Neural Networks (GNNs) have been widely used to learn node
representations and with outstanding performance on various tasks such as node
classification. However, noise, which inevitably exists in real-world graph
data, would considerably degrade the performance of GNNs as the noise is easily
propagated via the graph structure. In this work, we propose a novel and robust
method, Bayesian Robust Graph Contrastive Learning (BRGCL), which trains a GNN
encoder to learn robust node representations. The BRGCL encoder is a completely
unsupervised encoder. Two steps are iteratively executed at each epoch of
training the BRGCL encoder: (1) estimating confident nodes and computing robust
cluster prototypes of node representations through a novel Bayesian
nonparametric method; (2) prototypical contrastive learning between the node
representations and the robust cluster prototypes. Experiments on public and
large-scale benchmarks demonstrate the superior performance of BRGCL and the
robustness of the learned node representations. The code of BRGCL is available
at \url{https://github.com/BRGCL-code/BRGCL-code}."
6828,"To further study the behavior of BRGCL, we show the number of robust prototypes estimated by
BPL in Table 10.",B.6 Number of Conﬁdent Prototypes.,"It can be observed from the results that the estimated number of robust prototypes
is usually very close to the ground truth number of classes for different datasets, justifying the
effectiveness of BPL.",2022-05-27 17:21:17+00:00,Bayesian Robust Graph Contrastive Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yancheng Wang'), arxiv.Result.Author('Yingzhen Yang')]","Graph Neural Networks (GNNs) have been widely used to learn node
representations and with outstanding performance on various tasks such as node
classification. However, noise, which inevitably exists in real-world graph
data, would considerably degrade the performance of GNNs as the noise is easily
propagated via the graph structure. In this work, we propose a novel and robust
method, Bayesian Robust Graph Contrastive Learning (BRGCL), which trains a GNN
encoder to learn robust node representations. The BRGCL encoder is a completely
unsupervised encoder. Two steps are iteratively executed at each epoch of
training the BRGCL encoder: (1) estimating confident nodes and computing robust
cluster prototypes of node representations through a novel Bayesian
nonparametric method; (2) prototypical contrastive learning between the node
representations and the robust cluster prototypes. Experiments on public and
large-scale benchmarks demonstrate the superior performance of BRGCL and the
robustness of the learned node representations. The code of BRGCL is available
at \url{https://github.com/BRGCL-code/BRGCL-code}."
6848,"A comprehensive review of branch-and-
   bound algorithms: Guidelines and directions for further research on the ﬂowshop scheduling
   problem.",Caio Paziani Tomazella and Marcelo Seido Nagano.,"Expert Systems with Applications, 158:113556, 2020.",2022-05-28 06:08:07+00:00,Reinforcement Learning for Branch-and-Bound Optimisation using Retrospective Trajectories,cs.LG,['cs.LG'],"[arxiv.Result.Author('Christopher W. F. Parsonson'), arxiv.Result.Author('Alexandre Laterre'), arxiv.Result.Author('Thomas D. Barrett')]","Combinatorial optimisation problems framed as mixed integer linear programmes
(MILPs) are ubiquitous across a range of real-world applications. The canonical
branch-and-bound (B&B) algorithm seeks to exactly solve MILPs by constructing a
search tree of increasingly constrained sub-problems. In practice, its solving
time performance is dependent on heuristics, such as the choice of the next
variable to constrain ('branching'). Recently, machine learning (ML) has
emerged as a promising paradigm for branching. However, prior works have
struggled to apply reinforcement learning (RL), citing sparse rewards,
difficult exploration, and partial observability as significant challenges.
Instead, leading ML methodologies resort to approximating high quality
handcrafted heuristics with imitation learning (IL), which precludes the
discovery of novel policies and requires expensive data labelling. In this
work, we propose retro branching; a simple yet effective approach to RL for
branching. By retrospectively deconstructing the search tree into multiple
paths each contained within a sub-tree, we enable the agent to learn from
shorter trajectories with more predictable next states. In experiments on four
combinatorial tasks, our approach enables learning-to-branch without any expert
guidance or pre-training. We outperform the current state-of-the-art RL
branching algorithm by 3-5x and come within 20% of the best IL method's
performance on MILPs with 500 constraints and 1000 variables, with ablations
verifying that our retrospectively constructed trajectories are essential to
achieving these results."
6849,"PySCIPOpt: Mathemat-             directions for further research on the ﬂowshop scheduling
ical Programming in Python with the SCIP Optimization               problem.","A comprehen-
Maher, S.; Miltenberger, M.; Pedroso, J. P.; Rehfeldt, D.;          sive review of Branch-and-Bound algorithms: Guidelines and
Schwarz, R.; and Serrano, F. 2016.","Expert Systems with Applications, 158: 113556.",2022-05-28 06:08:07+00:00,Reinforcement Learning for Branch-and-Bound Optimisation using Retrospective Trajectories,cs.LG,['cs.LG'],"[arxiv.Result.Author('Christopher W. F. Parsonson'), arxiv.Result.Author('Alexandre Laterre'), arxiv.Result.Author('Thomas D. Barrett')]","Combinatorial optimisation problems framed as mixed integer linear programmes
(MILPs) are ubiquitous across a range of real-world applications. The canonical
branch-and-bound algorithm seeks to exactly solve MILPs by constructing a
search tree of increasingly constrained sub-problems. In practice, its solving
time performance is dependent on heuristics, such as the choice of the next
variable to constrain ('branching'). Recently, machine learning (ML) has
emerged as a promising paradigm for branching. However, prior works have
struggled to apply reinforcement learning (RL), citing sparse rewards,
difficult exploration, and partial observability as significant challenges.
Instead, leading ML methodologies resort to approximating high quality
handcrafted heuristics with imitation learning (IL), which precludes the
discovery of novel policies and requires expensive data labelling. In this
work, we propose retro branching; a simple yet effective approach to RL for
branching. By retrospectively deconstructing the search tree into multiple
paths each contained within a sub-tree, we enable the agent to learn from
shorter trajectories with more predictable next states. In experiments on four
combinatorial tasks, our approach enables learning-to-branch without any expert
guidance or pre-training. We outperform the current state-of-the-art RL
branching algorithm by 3-5x and come within 20% of the best IL method's
performance on MILPs with 500 constraints and 1000 variables, with ablations
verifying that our retrospectively constructed trajectories are essential to
achieving these results."
6850,"directions for further research on the ﬂowshop scheduling
Springer International Publishing.","In Mathematical Software – ICMS 2016, 301–307.",problem.,2022-05-28 06:08:07+00:00,Reinforcement Learning for Branch-and-Bound Optimisation using Retrospective Trajectories,cs.LG,['cs.LG'],"[arxiv.Result.Author('Christopher W. F. Parsonson'), arxiv.Result.Author('Alexandre Laterre'), arxiv.Result.Author('Thomas D. Barrett')]","Combinatorial optimisation problems framed as mixed integer linear programmes
(MILPs) are ubiquitous across a range of real-world applications. The canonical
branch-and-bound algorithm seeks to exactly solve MILPs by constructing a
search tree of increasingly constrained sub-problems. In practice, its solving
time performance is dependent on heuristics, such as the choice of the next
variable to constrain ('branching'). Recently, machine learning (ML) has
emerged as a promising paradigm for branching. However, prior works have
struggled to apply reinforcement learning (RL), citing sparse rewards,
difficult exploration, and partial observability as significant challenges.
Instead, leading ML methodologies resort to approximating high quality
handcrafted heuristics with imitation learning (IL), which precludes the
discovery of novel policies and requires expensive data labelling. In this
work, we propose retro branching; a simple yet effective approach to RL for
branching. By retrospectively deconstructing the search tree into multiple
paths each contained within a sub-tree, we enable the agent to learn from
shorter trajectories with more predictable next states. In experiments on four
combinatorial tasks, our approach enables learning-to-branch without any expert
guidance or pre-training. We outperform the current state-of-the-art RL
branching algorithm by 3-5x and come within 20% of the best IL method's
performance on MILPs with 500 constraints and 1000 variables, with ablations
verifying that our retrospectively constructed trajectories are essential to
achieving these results."
6861,"However, new challenges and
                                                                              opportunities have emerged that require further research.",work [1] [2] [4] are solved.,"mHealth: TinyML opens up a broad spectrum of real-time
and low-footprint eHealth applications, some of which                         Combining Data and Knowledge: Real-world IoT
are summarized in Table X.",2022-05-29 00:59:38+00:00,Machine Learning for Microcontroller-Class Hardware -- A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Swapnil Sayan Saha'), arxiv.Result.Author('Sandeep Singh Sandha'), arxiv.Result.Author('Mani Srivastava')]","The advancements in machine learning opened a new opportunity to bring
intelligence to the low-end Internet-of-Things nodes such as microcontrollers.
Conventional machine learning deployment has high memory and compute footprint
hindering their direct deployment on ultra resource-constrained microcontroller
nodes. This paper highlights the unique challenges of enabling onboard machine
learning for microcontroller class devices. Recently, researchers have used a
specialized model development cycle for resource-limited applications to ensure
the compute and latency budget is within the limits while still maintaining the
desired accuracy. We introduce a closed-loop widely applicable workflow of
machine learning model development for microcontroller class devices and show
that several classes of applications adopt a specific instance of it. We
present both qualitative and numerical insights into different stages of model
development by showcasing several applications. Finally, we identify the open
research challenges and unsolved questions demanding careful considerations
moving forward."
6862,require further research.,"However, the following new challenges are emerging that
   4) Which training frameworks can I use?",5) Do I need support for intermittent computing?,2022-05-29 00:59:38+00:00,Machine Learning for Microcontroller-Class Hardware -- A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Swapnil Sayan Saha'), arxiv.Result.Author('Sandeep Singh Sandha'), arxiv.Result.Author('Mani Srivastava')]","The advancements in machine learning opened a new opportunity to bring
intelligence to the low-end Internet-of-Things nodes such as microcontrollers.
Conventional machine learning deployment has high memory and compute footprint
hindering their direct deployment on ultra resource-constrained
microcontrollers. This paper highlights the unique requirements of enabling
onboard machine learning for microcontroller class devices. Researchers use a
specialized model development workflow for resource-limited applications to
ensure the compute and latency budget is within the device limits while still
maintaining the desired performance. We characterize a closed-loop widely
applicable workflow of machine learning model development for microcontroller
class devices and show that several classes of applications adopt a specific
instance of it. We present both qualitative and numerical insights into
different stages of model development by showcasing several use cases. Finally,
we identify the open research challenges and unsolved questions demanding
careful considerations moving forward."
6863,require further research.,"However, the following new challenges are emerging that
   4) Which training frameworks can I use?",5) Do I need support for intermittent computing?,2022-05-29 00:59:38+00:00,Machine Learning for Microcontroller-Class Hardware -- A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Swapnil Sayan Saha'), arxiv.Result.Author('Sandeep Singh Sandha'), arxiv.Result.Author('Mani Srivastava')]","The advancements in machine learning opened a new opportunity to bring
intelligence to the low-end Internet-of-Things nodes such as microcontrollers.
Conventional machine learning deployment has high memory and compute footprint
hindering their direct deployment on ultra resource-constrained
microcontrollers. This paper highlights the unique requirements of enabling
onboard machine learning for microcontroller class devices. Researchers use a
specialized model development workflow for resource-limited applications to
ensure the compute and latency budget is within the device limits while still
maintaining the desired performance. We characterize a closed-loop widely
applicable workflow of machine learning model development for microcontroller
class devices and show that several classes of applications adopt a specific
instance of it. We present both qualitative and numerical insights into
different stages of model development by showcasing several use cases. Finally,
we identify the open research challenges and unsolved questions demanding
careful considerations moving forward."
6864,require further research.,"However, the following new challenges are emerging that
   4) Which training frameworks can I use?",5) Do I need support for intermittent computing?,2022-05-29 00:59:38+00:00,Machine Learning for Microcontroller-Class Hardware: A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Swapnil Sayan Saha'), arxiv.Result.Author('Sandeep Singh Sandha'), arxiv.Result.Author('Mani Srivastava')]","The advancements in machine learning opened a new opportunity to bring
intelligence to the low-end Internet-of-Things nodes such as microcontrollers.
Conventional machine learning deployment has high memory and compute footprint
hindering their direct deployment on ultra resource-constrained
microcontrollers. This paper highlights the unique requirements of enabling
onboard machine learning for microcontroller class devices. Researchers use a
specialized model development workflow for resource-limited applications to
ensure the compute and latency budget is within the device limits while still
maintaining the desired performance. We characterize a closed-loop widely
applicable workflow of machine learning model development for microcontroller
class devices and show that several classes of applications adopt a specific
instance of it. We present both qualitative and numerical insights into
different stages of model development by showcasing several use cases. Finally,
we identify the open research challenges and unsolved questions demanding
careful considerations moving forward."
6879,"It
       emphasizes the relevance of pursuing further research towards scalable graph autoencoders.","several popular scalable unsu-
       pervised node embedding methods, such as node2vec and DeepWalk from Section 2.2.3.",This chapter is organized as follows.,2022-05-29 13:14:53+00:00,Contributions to Representation Learning with Graph Autoencoders and Applications to Music Recommendation,cs.LG,"['cs.LG', 'cs.IR', 'cs.SI']",[arxiv.Result.Author('Guillaume Salha-Galvan')],"Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as
two powerful groups of unsupervised node embedding methods, with various
applications to graph-based machine learning problems such as link prediction
and community detection. Nonetheless, at the beginning of this Ph.D. project,
GAE and VGAE models were also suffering from key limitations, preventing them
from being adopted in the industry. In this thesis, we present several
contributions to improve these models, with the general aim of facilitating
their use to address industrial-level problems involving graph representations.
Firstly, we propose two strategies to overcome the scalability issues of
previous GAE and VGAE models, permitting to effectively train these models on
large graphs with millions of nodes and edges. These strategies leverage graph
degeneracy and stochastic subgraph decoding techniques, respectively. Besides,
we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of
these models for directed graphs, that are ubiquitous in industrial
applications. We also consider extensions of GAE and VGAE models for dynamic
graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily
complex, and we propose to simplify them by leveraging linear encoders. Lastly,
we introduce Modularity-Aware GAE and VGAE to improve community detection on
graphs, while jointly preserving good performances on link prediction. In the
last part of this thesis, we evaluate our methods on several graphs extracted
from the music streaming service Deezer. We put the emphasis on graph-based
music recommendation problems. In particular, we show that our methods can
improve the detection of communities of similar musical items to recommend to
users, that they can effectively rank similar artists in a cold start setting,
and that they permit modeling the music genre perception across cultures."
6880,"This emphasizes
the relevance of pursuing further research towards more scalable GAE and VGAE models.","several popular scalable node embedding
methods such as node2vec and DeepWalk, in a majority of our experiments.","Simultaneously, in our experiments, we also identiﬁed several limitations of our GAE and VGAE
models.",2022-05-29 13:14:53+00:00,Contributions to Representation Learning with Graph Autoencoders and Applications to Music Recommendation,cs.LG,"['cs.LG', 'cs.IR', 'cs.SI']",[arxiv.Result.Author('Guillaume Salha-Galvan')],"Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as
two powerful groups of unsupervised node embedding methods, with various
applications to graph-based machine learning problems such as link prediction
and community detection. Nonetheless, at the beginning of this Ph.D. project,
GAE and VGAE models were also suffering from key limitations, preventing them
from being adopted in the industry. In this thesis, we present several
contributions to improve these models, with the general aim of facilitating
their use to address industrial-level problems involving graph representations.
Firstly, we propose two strategies to overcome the scalability issues of
previous GAE and VGAE models, permitting to effectively train these models on
large graphs with millions of nodes and edges. These strategies leverage graph
degeneracy and stochastic subgraph decoding techniques, respectively. Besides,
we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of
these models for directed graphs, that are ubiquitous in industrial
applications. We also consider extensions of GAE and VGAE models for dynamic
graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily
complex, and we propose to simplify them by leveraging linear encoders. Lastly,
we introduce Modularity-Aware GAE and VGAE to improve community detection on
graphs, while jointly preserving good performances on link prediction. In the
last part of this thesis, we evaluate our methods on several graphs extracted
from the music streaming service Deezer. We put the emphasis on graph-based
music recommendation problems. In particular, we show that our methods can
improve the detection of communities of similar musical items to recommend to
users, that they can effectively rank similar artists in a cold start setting,
and that they permit modeling the music genre perception across cultures."
6881,"It
       emphasizes the relevance of pursuing further research towards scalable graph autoencoders.","several popular scalable unsu-
       pervised node embedding methods, such as node2vec and DeepWalk from Section 2.2.3.",This chapter is organized as follows.,2022-05-29 13:14:53+00:00,Contributions to Representation Learning with Graph Autoencoders and Applications to Music Recommendation,cs.LG,"['cs.LG', 'cs.IR', 'cs.SI']",[arxiv.Result.Author('Guillaume Salha-Galvan')],"Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as
two powerful groups of unsupervised node embedding methods, with various
applications to graph-based machine learning problems such as link prediction
and community detection. Nonetheless, at the beginning of this Ph.D. project,
GAE and VGAE models were also suffering from key limitations, preventing them
from being adopted in the industry. In this thesis, we present several
contributions to improve these models, with the general aim of facilitating
their use to address industrial-level problems involving graph representations.
Firstly, we propose two strategies to overcome the scalability issues of
previous GAE and VGAE models, permitting to effectively train these models on
large graphs with millions of nodes and edges. These strategies leverage graph
degeneracy and stochastic subgraph decoding techniques, respectively. Besides,
we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of
these models for directed graphs, that are ubiquitous in industrial
applications. We also consider extensions of GAE and VGAE models for dynamic
graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily
complex, and we propose to simplify them by leveraging linear encoders. Lastly,
we introduce Modularity-Aware GAE and VGAE to improve community detection on
graphs, while jointly preserving good performances on link prediction. In the
last part of this thesis, we evaluate our methods on several graphs extracted
from the music streaming service Deezer. We put the emphasis on graph-based
music recommendation problems. In particular, we show that our methods can
improve the detection of communities of similar musical items to recommend to
users, that they can effectively rank similar artists in a cold start setting,
and that they permit modeling the music genre perception across cultures."
6882,"This emphasizes
the relevance of pursuing further research towards more scalable GAE and VGAE models.","several popular scalable node embedding
methods such as node2vec and DeepWalk, in a majority of our experiments.","Simultaneously, in our experiments, we also identiﬁed several limitations of our GAE and VGAE
models.",2022-05-29 13:14:53+00:00,Contributions to Representation Learning with Graph Autoencoders and Applications to Music Recommendation,cs.LG,"['cs.LG', 'cs.IR', 'cs.SI']",[arxiv.Result.Author('Guillaume Salha-Galvan')],"Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as
two powerful groups of unsupervised node embedding methods, with various
applications to graph-based machine learning problems such as link prediction
and community detection. Nonetheless, at the beginning of this Ph.D. project,
GAE and VGAE models were also suffering from key limitations, preventing them
from being adopted in the industry. In this thesis, we present several
contributions to improve these models, with the general aim of facilitating
their use to address industrial-level problems involving graph representations.
Firstly, we propose two strategies to overcome the scalability issues of
previous GAE and VGAE models, permitting to effectively train these models on
large graphs with millions of nodes and edges. These strategies leverage graph
degeneracy and stochastic subgraph decoding techniques, respectively. Besides,
we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of
these models for directed graphs, that are ubiquitous in industrial
applications. We also consider extensions of GAE and VGAE models for dynamic
graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily
complex, and we propose to simplify them by leveraging linear encoders. Lastly,
we introduce Modularity-Aware GAE and VGAE to improve community detection on
graphs, while jointly preserving good performances on link prediction. In the
last part of this thesis, we evaluate our methods on several graphs extracted
from the music streaming service Deezer. We put the emphasis on graph-based
music recommendation problems. In particular, we show that our methods can
improve the detection of communities of similar musical items to recommend to
users, that they can effectively rank similar artists in a cold start setting,
and that they permit modeling the music genre perception across cultures."
6902,"We further study the effect
                                                                                  of multi-formation factor (i.e., upsampling factor).",Analysis                                                                     On Multi-Formation Factor.,"Table 7
Ablation Study.",2022-05-30 09:55:31+00:00,Dataset Condensation via Efficient Synthetic-Data Parameterization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jang-Hyun Kim'), arxiv.Result.Author('Jinuk Kim'), arxiv.Result.Author('Seong Joon Oh'), arxiv.Result.Author('Sangdoo Yun'), arxiv.Result.Author('Hwanjun Song'), arxiv.Result.Author('Joonhyun Jeong'), arxiv.Result.Author('Jung-Woo Ha'), arxiv.Result.Author('Hyun Oh Song')]","The great success of machine learning with massive amounts of data comes at a
price of huge computation costs and storage for training and tuning. Recent
studies on dataset condensation attempt to reduce the dependence on such
massive data by synthesizing a compact training dataset. However, the existing
approaches have fundamental limitations in optimization due to the limited
representability of synthetic datasets without considering any data regularity
characteristics. To this end, we propose a novel condensation framework that
generates multiple synthetic data with a limited storage budget via efficient
parameterization considering data regularity. We further analyze the
shortcomings of the existing gradient matching-based condensation methods and
develop an effective optimization technique for improving the condensation of
training data information. We propose a unified algorithm that drastically
improves the quality of condensed data against the current state-of-the-art on
CIFAR-10, ImageNet, and Speech Commands."
6903,"We further study the effect
                                                                                  of multi-formation factor (i.e., upsampling factor).",Analysis                                                                     On Multi-Formation Factor.,"Table 7
Ablation Study.",2022-05-30 09:55:31+00:00,Dataset Condensation via Efficient Synthetic-Data Parameterization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jang-Hyun Kim'), arxiv.Result.Author('Jinuk Kim'), arxiv.Result.Author('Seong Joon Oh'), arxiv.Result.Author('Sangdoo Yun'), arxiv.Result.Author('Hwanjun Song'), arxiv.Result.Author('Joonhyun Jeong'), arxiv.Result.Author('Jung-Woo Ha'), arxiv.Result.Author('Hyun Oh Song')]","The great success of machine learning with massive amounts of data comes at a
price of huge computation costs and storage for training and tuning. Recent
studies on dataset condensation attempt to reduce the dependence on such
massive data by synthesizing a compact training dataset. However, the existing
approaches have fundamental limitations in optimization due to the limited
representability of synthetic datasets without considering any data regularity
characteristics. To this end, we propose a novel condensation framework that
generates multiple synthetic data with a limited storage budget via efficient
parameterization considering data regularity. We further analyze the
shortcomings of the existing gradient matching-based condensation methods and
develop an effective optimization technique for improving the condensation of
training data information. We propose a unified algorithm that drastically
improves the quality of condensed data against the current state-of-the-art on
CIFAR-10, ImageNet, and Speech Commands."
6914,"4: Impact of p on prediction errors

We further study the effectiveness our temporal correlation      When p < 12, we do not have LPGCN output Zti for
module (Section 3.3).","1.6                                                    2
                                                             1.4                                                    1.8
                                                             1.2                                                    1.6
                                                                                                                    1.4
                                                               1                                                    1.2
                                                             0.8 15 15-GAMCN
                                                             0.6 30 30-GAMCN
                                                             00..24 60 60-GAMCN

                                                               0
                                                                  1 3 6 9 12
                                                             MAE                                                    1
                                                                                                               MAE  0.8
                                                                                                                    0.6  15 15-GAMCN

                                                                                                                    0.4 30 30-GAMCN
                                                                                                                    0.2 60 60-GAMCN
                                                                                                                    0

                                                                                                                         1 3 6 9 12

                                                                               p                                         p

                                                             (a) PEMS04                                                  (b) PEMS08

4.4 Effectiveness of the Temporal Correlation Module                           Fig.",We compare GAMCN with three vari-      every prediction time tp+i.,2022-05-30 16:24:43+00:00,A Graph and Attentive Multi-Path Convolutional Network for Traffic Prediction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jianzhong Qi'), arxiv.Result.Author('Zhuowei Zhao'), arxiv.Result.Author('Egemen Tanin'), arxiv.Result.Author('Tingru Cui'), arxiv.Result.Author('Neema Nassir'), arxiv.Result.Author('Majid Sarvi')]","Traffic prediction is an important and yet highly challenging problem due to
the complexity and constantly changing nature of traffic systems. To address
the challenges, we propose a graph and attentive multi-path convolutional
network (GAMCN) model to predict traffic conditions such as traffic speed
across a given road network into the future. Our model focuses on the spatial
and temporal factors that impact traffic conditions. To model the spatial
factors, we propose a variant of the graph convolutional network (GCN) named
LPGCN to embed road network graph vertices into a latent space, where vertices
with correlated traffic conditions are close to each other. To model the
temporal factors, we use a multi-path convolutional neural network (CNN) to
learn the joint impact of different combinations of past traffic conditions on
the future traffic conditions. Such a joint impact is further modulated by an
attention} generated from an embedding of the prediction time, which encodes
the periodic patterns of traffic conditions. We evaluate our model on
real-world road networks and traffic data. The experimental results show that
our model outperforms state-of-art traffic prediction models by up to 18.9% in
terms of prediction errors and 23.4% in terms of prediction efficiency."
6917,"In addition,  Engilberge, M., Chevallier, L., Pe´rez, P., and Cord, M.
we plan to further study the generalization of the RankSim          Sodeep: A sorting deep net to learn ranking loss sur-
regularizer to representation learning more broadly.",We leave this for a future extension.,rogates.,2022-05-30 16:51:25+00:00,RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Yu Gong'), arxiv.Result.Author('Greg Mori'), arxiv.Result.Author('Frederick Tung')]","Data imbalance, in which a plurality of the data samples come from a small
proportion of labels, poses a challenge in training deep neural networks.
Unlike classification, in regression the labels are continuous, potentially
boundless, and form a natural ordering. These distinct features of regression
call for new techniques that leverage the additional information encoded in
label-space relationships. This paper presents the RankSim (ranking similarity)
regularizer for deep imbalanced regression, which encodes an inductive bias
that samples that are closer in label space should also be closer in feature
space. In contrast to recent distribution smoothing based approaches, RankSim
captures both nearby and distant relationships: for a given data sample,
RankSim encourages the sorted list of its neighbors in label space to match the
sorted list of its neighbors in feature space. RankSim is complementary to
conventional imbalanced learning techniques, including re-weighting, two-stage
training, and distribution smoothing, and lifts the state-of-the-art
performance on three imbalanced regression benchmarks: IMDB-WIKI-DIR,
AgeDB-DIR, and STS-B-DIR."
6918,"In addition,  Engilberge, M., Chevallier, L., Pe´rez, P., and Cord, M.
we plan to further study the generalization of the RankSim          Sodeep: A sorting deep net to learn ranking loss sur-
regularizer to representation learning more broadly.",We leave this for a future extension.,rogates.,2022-05-30 16:51:25+00:00,RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Yu Gong'), arxiv.Result.Author('Greg Mori'), arxiv.Result.Author('Frederick Tung')]","Data imbalance, in which a plurality of the data samples come from a small
proportion of labels, poses a challenge in training deep neural networks.
Unlike classification, in regression the labels are continuous, potentially
boundless, and form a natural ordering. These distinct features of regression
call for new techniques that leverage the additional information encoded in
label-space relationships. This paper presents the RankSim (ranking similarity)
regularizer for deep imbalanced regression, which encodes an inductive bias
that samples that are closer in label space should also be closer in feature
space. In contrast to recent distribution smoothing based approaches, RankSim
captures both nearby and distant relationships: for a given data sample,
RankSim encourages the sorted list of its neighbors in label space to match the
sorted list of its neighbors in feature space. RankSim is complementary to
conventional imbalanced learning techniques, including re-weighting, two-stage
training, and distribution smoothing, and lifts the state-of-the-art
performance on three imbalanced regression benchmarks: IMDB-WIKI-DIR,
AgeDB-DIR, and STS-B-DIR."
6923,"We hope this paper spark further research
beyond the realms of well-designed model structures.","5 Conclusions and Limitations

This paper proposes a novel paradigm of shifting a model’s priors into an optimizer and present an
implementation via Gradient Re-parameterization (GR).","However, though the methodology is empir-
ically validated, we believe further investigations (e.g., a mathematically provable bound) will be
useful, which may require a much deeper understanding of the black box of deep neural networks.",2022-05-30 16:55:59+00:00,Re-parameterizing Your Optimizers rather than Architectures,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Xiaohan Ding'), arxiv.Result.Author('Honghao Chen'), arxiv.Result.Author('Xiangyu Zhang'), arxiv.Result.Author('Kaiqi Huang'), arxiv.Result.Author('Jungong Han'), arxiv.Result.Author('Guiguang Ding')]","The well-designed structures in neural networks reflect the prior knowledge
incorporated into the models. However, though different models have various
priors, we are used to training them with model-agnostic optimizers (e.g.,
SGD). In this paper, we propose a novel paradigm of incorporating
model-specific prior knowledge into optimizers and using them to train generic
(simple) models. As an implementation, we propose a novel methodology to add
prior knowledge by modifying the gradients according to a set of model-specific
hyper-parameters, which is referred to as Gradient Re-parameterization, and the
optimizers are named RepOptimizers. For the extreme simplicity of model
structure, we focus on a VGG-style plain model and showcase that such a simple
model trained with a RepOptimizer, which is referred to as RepOpt-VGG, performs
on par with the recent well-designed models. From a practical perspective,
RepOpt-VGG is a favorable base model because of its simple structure, high
inference speed and training efficiency. Compared to Structural
Re-parameterization, which adds priors into models via constructing extra
training-time structures, RepOptimizers require no extra forward/backward
computations and solve the problem of quantization. The code and models are
publicly available at https://github.com/DingXiaoH/RepOptimizers."
6924,"We hope to spark further research beyond
                                                   the realms of model structure design.","Compared to Structural
                                                   Re-parameterization, which adds priors into models via constructing extra training-
                                                   time structures, RepOptimizers require no extra forward/backward computations
                                                   and solve the problem of quantization.","The code and models are publicly available
                                                   at https://github.com/DingXiaoH/RepOptimizers.",2022-05-30 16:55:59+00:00,Re-parameterizing Your Optimizers rather than Architectures,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Xiaohan Ding'), arxiv.Result.Author('Honghao Chen'), arxiv.Result.Author('Xiangyu Zhang'), arxiv.Result.Author('Kaiqi Huang'), arxiv.Result.Author('Jungong Han'), arxiv.Result.Author('Guiguang Ding')]","The well-designed structures in neural networks reflect the prior knowledge
incorporated into the models. However, though different models have various
priors, we are used to training them with model-agnostic optimizers such as
SGD. In this paper, we propose to incorporate model-specific prior knowledge
into optimizers by modifying the gradients according to a set of model-specific
hyper-parameters. Such a methodology is referred to as Gradient
Re-parameterization, and the optimizers are named RepOptimizers. For the
extreme simplicity of model structure, we focus on a VGG-style plain model and
showcase that such a simple model trained with a RepOptimizer, which is
referred to as RepOpt-VGG, performs on par with or better than the recent
well-designed models. From a practical perspective, RepOpt-VGG is a favorable
base model because of its simple structure, high inference speed and training
efficiency. Compared to Structural Re-parameterization, which adds priors into
models via constructing extra training-time structures, RepOptimizers require
no extra forward/backward computations and solve the problem of quantization.
We hope to spark further research beyond the realms of model structure design.
The code and models are publicly available at
https://github.com/DingXiaoH/RepOptimizers."
6933,"However, I don’t
pursue that line of argumentation here since I tend to think that any problems with nonstationarity could be avoided if the agent had
enough data and computation since it would then be able to predict the nonstationarity (like Laplace’s demon in footnote 15 below);
but this is admittedly a complex and controversial point that needs further research.","It could
be argued that nonstationarity is as important as scarcity of the data, and an independent root cause of suffering.","8However, the Buddhist impermanence has also aspects that cannot be considered to be forms of uncertainty.",2022-05-27 07:32:33+00:00,Painful intelligence: What AI can tell us about human suffering,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']",[arxiv.Result.Author('Aapo Hyvärinen')],"This book uses the modern theory of artificial intelligence (AI) to
understand human suffering or mental pain. Both humans and sophisticated AI
agents process information about the world in order to achieve goals and obtain
rewards, which is why AI can be used as a model of the human brain and mind.
This book intends to make the theory accessible to a relatively general
audience, requiring only some relevant scientific background. The book starts
with the assumption that suffering is mainly caused by frustration. Frustration
means the failure of an agent (whether AI or human) to achieve a goal or a
reward it wanted or expected. Frustration is inevitable because of the
overwhelming complexity of the world, limited computational resources, and
scarcity of good data. In particular, such limitations imply that an agent
acting in the real world must cope with uncontrollability, unpredictability,
and uncertainty, which all lead to frustration. Fundamental in such modelling
is the idea of learning, or adaptation to the environment. While AI uses
machine learning, humans and animals adapt by a combination of evolutionary
mechanisms and ordinary learning. Even frustration is fundamentally an error
signal that the system uses for learning. This book explores various aspects
and limitations of learning algorithms and their implications regarding
suffering. At the end of the book, the computational theory is used to derive
various interventions or training methods that will reduce suffering in humans.
The amount of frustration is expressed by a simple equation which indicates how
it can be reduced. The ensuing interventions are very similar to those proposed
by Buddhist and Stoic philosophy, and include mindfulness meditation.
Therefore, this book can be interpreted as an exposition of a computational
theory justifying why such philosophies and meditation reduce human suffering."
6934,"The theory in this book will hopefully be comple-
mented by further research; I think this is just the very beginning of a long-term scientiﬁc endeavour.","The interventions I proposed were mostly identical to what existing philosophical systems propose, while
I showed how to motivate them using current AI theories.","I hope
it will lead to more and more efﬁcient interventions in the future, including completely new kinds of interven-
tions.",2022-05-27 07:32:33+00:00,Painful intelligence: What AI can tell us about human suffering,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']",[arxiv.Result.Author('Aapo Hyvärinen')],"This book uses the modern theory of artificial intelligence (AI) to
understand human suffering or mental pain. Both humans and sophisticated AI
agents process information about the world in order to achieve goals and obtain
rewards, which is why AI can be used as a model of the human brain and mind.
This book intends to make the theory accessible to a relatively general
audience, requiring only some relevant scientific background. The book starts
with the assumption that suffering is mainly caused by frustration. Frustration
means the failure of an agent (whether AI or human) to achieve a goal or a
reward it wanted or expected. Frustration is inevitable because of the
overwhelming complexity of the world, limited computational resources, and
scarcity of good data. In particular, such limitations imply that an agent
acting in the real world must cope with uncontrollability, unpredictability,
and uncertainty, which all lead to frustration. Fundamental in such modelling
is the idea of learning, or adaptation to the environment. While AI uses
machine learning, humans and animals adapt by a combination of evolutionary
mechanisms and ordinary learning. Even frustration is fundamentally an error
signal that the system uses for learning. This book explores various aspects
and limitations of learning algorithms and their implications regarding
suffering. At the end of the book, the computational theory is used to derive
various interventions or training methods that will reduce suffering in humans.
The amount of frustration is expressed by a simple equation which indicates how
it can be reduced. The ensuing interventions are very similar to those proposed
by Buddhist and Stoic philosophy, and include mindfulness meditation.
Therefore, this book can be interpreted as an exposition of a computational
theory justifying why such philosophies and meditation reduce human suffering."
6958,"Acknowledgements
This invites further research in algorithmic reasoners that
can support such computation.","on tasks requiring long-range rollouts (such as DFS), or
recursive reasoning (such as Quicksort and Quickselect).","It is further revealed that        CLRS-30 was developed over a long time-frame, with many
more specialised inductive biases and training regimes may       useful contributions, which we kindly acknowledge here.",2022-05-31 09:56:44+00:00,The CLRS Algorithmic Reasoning Benchmark,cs.LG,"['cs.LG', 'cs.DS', 'stat.ML']","[arxiv.Result.Author('Petar Veličković'), arxiv.Result.Author('Adrià Puigdomènech Badia'), arxiv.Result.Author('David Budden'), arxiv.Result.Author('Razvan Pascanu'), arxiv.Result.Author('Andrea Banino'), arxiv.Result.Author('Misha Dashevskiy'), arxiv.Result.Author('Raia Hadsell'), arxiv.Result.Author('Charles Blundell')]","Learning representations of algorithms is an emerging area of machine
learning, seeking to bridge concepts from neural networks with classical
algorithms. Several important works have investigated whether neural networks
can effectively reason like algorithms, typically by learning to execute them.
The common trend in the area, however, is to generate targeted kinds of
algorithmic data to evaluate specific hypotheses, making results hard to
transfer across publications, and increasing the barrier of entry. To
consolidate progress and work towards unified evaluation, we propose the CLRS
Algorithmic Reasoning Benchmark, covering classical algorithms from the
Introduction to Algorithms textbook. Our benchmark spans a variety of
algorithmic reasoning procedures, including sorting, searching, dynamic
programming, graph algorithms, string algorithms and geometric algorithms. We
perform extensive experiments to demonstrate how several popular algorithmic
reasoning baselines perform on these tasks, and consequently, highlight links
to several open challenges. Our library is readily available at
https://github.com/deepmind/clrs."
6959,"Acknowledgements
This invites further research in algorithmic reasoners that
can support such computation.","on tasks requiring long-range rollouts (such as DFS), or
recursive reasoning (such as Quicksort and Quickselect).","It is further revealed that        CLRS-30 was developed over a long time-frame, with many
more specialised inductive biases and training regimes may       useful contributions, which we kindly acknowledge here.",2022-05-31 09:56:44+00:00,The CLRS Algorithmic Reasoning Benchmark,cs.LG,"['cs.LG', 'cs.DS', 'stat.ML']","[arxiv.Result.Author('Petar Veličković'), arxiv.Result.Author('Adrià Puigdomènech Badia'), arxiv.Result.Author('David Budden'), arxiv.Result.Author('Razvan Pascanu'), arxiv.Result.Author('Andrea Banino'), arxiv.Result.Author('Misha Dashevskiy'), arxiv.Result.Author('Raia Hadsell'), arxiv.Result.Author('Charles Blundell')]","Learning representations of algorithms is an emerging area of machine
learning, seeking to bridge concepts from neural networks with classical
algorithms. Several important works have investigated whether neural networks
can effectively reason like algorithms, typically by learning to execute them.
The common trend in the area, however, is to generate targeted kinds of
algorithmic data to evaluate specific hypotheses, making results hard to
transfer across publications, and increasing the barrier of entry. To
consolidate progress and work towards unified evaluation, we propose the CLRS
Algorithmic Reasoning Benchmark, covering classical algorithms from the
Introduction to Algorithms textbook. Our benchmark spans a variety of
algorithmic reasoning procedures, including sorting, searching, dynamic
programming, graph algorithms, string algorithms and geometric algorithms. We
perform extensive experiments to demonstrate how several popular algorithmic
reasoning baselines perform on these tasks, and consequently, highlight links
to several open challenges. Our library is readily available at
https://github.com/deepmind/clrs."
6966,"Directions for further research include investigation to whether additions like

                                                           9
the soft-cluster assignment, cluster hardening loss or a Gaussian latent prior - which have shown to
improve the SOM-VAE model [19] - improve SOM-CPC performance as well.","We believe that the SOM-CPC model opens up new research directions for interpretable representation
learning of time series.","References

 [1] R. B. Berry, R. Brooks, C. E. Gamaldo, S. M. Harding, C. Marcus, B. V. Vaughn, et al.",2022-05-31 15:21:21+00:00,SOM-CPC: Unsupervised Contrastive Learning with Self-Organizing Maps for Structured Representations of High-Rate Time Series,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Author('Iris A. M. Huijben'), arxiv.Result.Author('Arthur A. Nijdam'), arxiv.Result.Author('Sebastiaan Overeem'), arxiv.Result.Author('Merel M. van Gilst'), arxiv.Result.Author('Ruud J. G. van Sloun')]","Continuous monitoring with an ever-increasing number of sensors has become
ubiquitous across many application domains. Acquired data are typically
high-dimensional and difficult to interpret, but they are also hypothesized to
lie on a lower-dimensional manifold. Many deep learning (DL) models aim to
identify this manifold, but do not promote structure nor interpretability. We
propose the SOM-CPC model, which jointly optimizes Contrastive Predictive
Coding (CPC), and a Self-Organizing Map (SOM) to find such an organized
manifold. We address a largely unexplored and challenging set of scenarios
comprising high-rate time series, and show on synthetic and real-life medical
and audio data that SOM-CPC outperforms strong baseline models that combine DL
with SOMs. SOM-CPC has great potential to expose latent patterns in high-rate
data streams, and may therefore contribute to a better understanding of many
different processes and systems."
6979,"We further show that DDGMs, and
DAED especially, generalize well to unseen data, what opens new possibilities for further research in
terms of transfer or continual learning of DDGMs.","On
the other hand, with increasing noise processed by DAE, DAED smoothens the generations resulting
in lower performance when training with the simpliﬁed objective.","8 Acknowledgements

This research was funded by National Science Centre, Poland (grant no 2018/31/N/ST6/02374 and
2020/39/B/ST6/01511) and the Hybrid Intelligence Center, a 10-year programme funded by the
Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientiﬁc
Research.",2022-05-31 19:29:27+00:00,On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kamil Deja'), arxiv.Result.Author('Anna Kuzina'), arxiv.Result.Author('Tomasz Trzciński'), arxiv.Result.Author('Jakub M. Tomczak')]","Diffusion-based Deep Generative Models (DDGMs) offer state-of-the-art
performance in generative modeling. Their main strength comes from their unique
setup in which a model (the backward diffusion process) is trained to reverse
the forward diffusion process, which gradually adds noise to the input signal.
Although DDGMs are well studied, it is still unclear how the small amount of
noise is transformed during the backward diffusion process. Here, we focus on
analyzing this problem to gain more insight into the behavior of DDGMs and
their denoising and generative capabilities. We observe a fluid transition
point that changes the functionality of the backward diffusion process from
generating a (corrupted) image from noise to denoising the corrupted image to
the final sample. Based on this observation, we postulate to divide a DDGM into
two parts: a denoiser and a generator. The denoiser could be parameterized by a
denoising auto-encoder, while the generator is a diffusion-based model with its
own set of parameters. We experimentally validate our proposition, showing its
pros and cons."
6986,We further study what condition permits even faster rate than O (1/ ).,4.,"Leveraging our lower bound

mentioned above, we propose a new condition, uniform optimal policy coverage, which posits that the

visitation probabilities of the behavior policy are uniformly lower bounded by P for states where an

optimal policy’s visitation probabilities are positive.",2022-06-01 01:44:12+00:00,On Gap-dependent Bounds for Offline Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Xinqi Wang'), arxiv.Result.Author('Qiwen Cui'), arxiv.Result.Author('Simon S. Du')]","This paper presents a systematic study on gap-dependent sample complexity in
offline reinforcement learning. Prior work showed when the density ratio
between an optimal policy and the behavior policy is upper bounded (the optimal
policy coverage assumption), then the agent can achieve an
$O\left(\frac{1}{\epsilon^2}\right)$ rate, which is also minimax optimal. We
show under the optimal policy coverage assumption, the rate can be improved to
$O\left(\frac{1}{\epsilon}\right)$ when there is a positive sub-optimality gap
in the optimal $Q$-function. Furthermore, we show when the visitation
probabilities of the behavior policy are uniformly lower bounded for states
where an optimal policy's visitation probabilities are positive (the uniform
optimal policy coverage assumption), the sample complexity of identifying an
optimal policy is independent of $\frac{1}{\epsilon}$. Lastly, we present
nearly-matching lower bounds to complement our gap-dependent upper bounds."
6987,We further study what condition permits even faster rate than O (1/ ).,4.,"Leveraging our lower bound

mentioned above, we propose a new condition, uniform optimal policy coverage, which posits that the

visitation probabilities of the behavior policy are uniformly lower bounded by P for states where an

optimal policy’s visitation probabilities are positive.",2022-06-01 01:44:12+00:00,On Gap-dependent Bounds for Offline Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Xinqi Wang'), arxiv.Result.Author('Qiwen Cui'), arxiv.Result.Author('Simon S. Du')]","This paper presents a systematic study on gap-dependent sample complexity in
offline reinforcement learning. Prior work showed when the density ratio
between an optimal policy and the behavior policy is upper bounded (the optimal
policy coverage assumption), then the agent can achieve an
$O\left(\frac{1}{\epsilon^2}\right)$ rate, which is also minimax optimal. We
show under the optimal policy coverage assumption, the rate can be improved to
$O\left(\frac{1}{\epsilon}\right)$ when there is a positive sub-optimality gap
in the optimal $Q$-function. Furthermore, we show when the visitation
probabilities of the behavior policy are uniformly lower bounded for states
where an optimal policy's visitation probabilities are positive (the uniform
optimal policy coverage assumption), the sample complexity of identifying an
optimal policy is independent of $\frac{1}{\epsilon}$. Lastly, we present
nearly-matching lower bounds to complement our gap-dependent upper bounds."
6994,"Such an observation nonetheless
guides further study of the coupling relationship between the search and the estimation procedures.","This shows that there is a
possibility that the search result of CONSOLE might not be optimal (i.e., an extra consine term exists
but is close to 1), but the learned equation is still highly accurate.","DSR doesn’t perform well when the underlying equation is relatively complex, e.g., y1 in Syn1 and y1
and y2 in Syn2.",2022-06-01 06:38:03+00:00,CoNSoLe: Convex Neural Symbolic Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Yang Weng'), arxiv.Result.Author('Hanghang Tong')]","Learning the underlying equation from data is a fundamental problem in many
disciplines. Recent advances rely on Neural Networks (NNs) but do not provide
theoretical guarantees in obtaining the exact equations owing to the
non-convexity of NNs. In this paper, we propose Convex Neural Symbolic Learning
(CoNSoLe) to seek convexity under mild conditions. The main idea is to
decompose the recovering process into two steps and convexify each step. In the
first step of searching for right symbols, we convexify the deep Q-learning.
The key is to maintain double convexity for both the negative Q-function and
the negative reward function in each iteration, leading to provable convexity
of the negative optimal Q function to learn the true symbol connections.
Conditioned on the exact searching result, we construct a Locally Convex
equation Learner (LoCaL) neural network to convexify the estimation of symbol
coefficients. With such a design, we quantify a large region with strict
convexity in the loss surface of LoCaL for commonly used physical functions.
Finally, we demonstrate the superior performance of the CoNSoLe framework over
the state-of-the-art on a diverse set of datasets."
6995,"Such an observation nonetheless

guides further study of the coupling relationship between the search and the estimation procedures.","This shows that there is a
possibility that the search result of CONSOLE might not be optimal (i.e., an extra consine term exists
but is close to 1), but the learned equation is still highly accurate.",(a) The average percentage error Ec(%) of coefﬁcients.,2022-06-01 06:38:03+00:00,CoNSoLe: Convex Neural Symbolic Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Haoran Li'), arxiv.Result.Author('Yang Weng'), arxiv.Result.Author('Hanghang Tong')]","Learning the underlying equation from data is a fundamental problem in many
disciplines. Recent advances rely on Neural Networks (NNs) but do not provide
theoretical guarantees in obtaining the exact equations owing to the
non-convexity of NNs. In this paper, we propose Convex Neural Symbolic Learning
(CoNSoLe) to seek convexity under mild conditions. The main idea is to
decompose the recovering process into two steps and convexify each step. In the
first step of searching for right symbols, we convexify the deep Q-learning.
The key is to maintain double convexity for both the negative Q-function and
the negative reward function in each iteration, leading to provable convexity
of the negative optimal Q function to learn the true symbol connections.
Conditioned on the exact searching result, we construct a Locally Convex
equation Learner (LoCaL) neural network to convexify the estimation of symbol
coefficients. With such a design, we quantify a large region with strict
convexity in the loss surface of LoCaL for commonly used physical functions.
Finally, we demonstrate the superior performance of the CoNSoLe framework over
the state-of-the-art on a diverse set of datasets."
7005,"For each family
                                       of problems, we present some strong baseline algorithms and their complexities,
                                       which will motivate further research for improving the existing results.","We
                                       formulate DXO into three special families of non-convex optimization problems
                                       belonging to non-convex min-max optimization, non-convex compositional op-
                                       timization, and non-convex bilevel optimization, respectively.","Discus-
                                       sions about the presented results and future studies are given at the end.",2022-06-01 12:22:56+00:00,Algorithmic Foundation of Deep X-Risk Optimization,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",[arxiv.Result.Author('Tianbao Yang')],"X-risk is a term introduced to represent a family of compositional measures
or objectives, in which each data point is compared with a set of data points
explicitly or implicitly for defining a risk function. It includes many widely
used measures or objectives, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP,
top-$K$ NDCG, top-$K$ MAP, listwise losses, p-norm push, top push,
precision/recall at top $K$ positions, precision at a certain recall level,
contrastive objectives, etc. While these measures/objectives and their
optimization algorithms have been studied in the literature of machine
learning, computer vision, information retrieval, and etc, optimizing these
measures/objectives has encountered some unique challenges for deep learning.
In this technical report, we survey our recent rigorous efforts for deep X-risk
optimization (DXO) by focusing on its algorithmic foundation. We introduce a
class of techniques for optimizing X-risk for deep learning. We formulate DXO
into three special families of non-convex optimization problems belonging to
non-convex min-max optimization, non-convex compositional optimization, and
non-convex bilevel optimization, respectively. For each family of problems, we
present some strong baseline algorithms and their complexities, which will
motivate further research for improving the existing results. Discussions about
the presented results and future studies are given at the end."
7006,"For each family
                                       of problems, we present some strong baseline algorithms and their complexities,
                                       which will motivate further research for improving the existing results.","We
                                       formulate DXO into three special families of non-convex optimization problems
                                       belonging to non-convex min-max optimization, non-convex compositional op-
                                       timization, and non-convex bilevel optimization, respectively.","Discus-
                                       sions about the presented results and future studies are given at the end.",2022-06-01 12:22:56+00:00,Algorithmic Foundation of Deep X-Risk Optimization,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",[arxiv.Result.Author('Tianbao Yang')],"X-risk is a term introduced to represent a family of compositional measures
or objectives, in which each data point is compared with a set of data points
explicitly or implicitly for defining a risk function. It includes many widely
used measures or objectives, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP,
top-$K$ NDCG, top-$K$ MAP, listwise losses, p-norm push, top push,
precision/recall at top $K$ positions, precision at a certain recall level,
contrastive objectives, etc. While these measures/objectives and their
optimization algorithms have been studied in the literature of machine
learning, computer vision, information retrieval, and etc, optimizing these
measures/objectives has encountered some unique challenges for deep learning.
In this technical report, we survey our recent rigorous efforts for deep X-risk
optimization (DXO) by focusing on its algorithmic foundation. We introduce a
class of techniques for optimizing X-risk for deep learning. We formulate DXO
into three special families of non-convex optimization problems belonging to
non-convex min-max optimization, non-convex compositional optimization, and
non-convex bilevel optimization, respectively. For each family of problems, we
present some strong baseline algorithms and their complexities, which will
motivate further research for improving the existing results. Discussions about
the presented results and future studies are given at the end. Efficient
algorithms for optimizing a variety of X-risks are implemented in the LibAUC
library at www.libauc.org."
7007,"For
                                        each family of problems, we present some strong baseline algorithms and their
                                        complexities, which will motivate further research for improving the existing re-
                                        sults.","We formulate DXO into three special families of non-convex optimiza-
                                        tion problems belonging to non-convex min-max optimization, non-convex com-
                                        positional optimization, and non-convex bilevel optimization, respectively.","Discussions about the presented results and future studies are given at the
                                        end.",2022-06-01 12:22:56+00:00,Algorithmic Foundation of Deep X-Risk Optimization,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",[arxiv.Result.Author('Tianbao Yang')],"X-risk is a term introduced to represent a family of compositional measures
or objectives, in which each data point is compared with a set of data points
explicitly or implicitly for defining a risk function. It includes many widely
used measures or objectives, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP,
top-$K$ NDCG, top-$K$ MAP, listwise losses, p-norm push, top push,
precision/recall at top $K$ positions, precision at a certain recall level,
contrastive objectives, etc. While these measures/objectives and their
optimization algorithms have been studied in the literature of machine
learning, computer vision, information retrieval, and etc, optimizing these
measures/objectives has encountered some unique challenges for deep learning.
In this technical report, we survey our recent rigorous efforts for deep X-risk
optimization (DXO) by focusing on its algorithmic foundation. We introduce a
class of techniques for optimizing X-risk for deep learning. We formulate DXO
into three special families of non-convex optimization problems belonging to
non-convex min-max optimization, non-convex compositional optimization, and
non-convex bilevel optimization, respectively. For each family of problems, we
present some strong baseline algorithms and their complexities, which will
motivate further research for improving the existing results. Discussions about
the presented results and future studies are given at the end. Efficient
algorithms for optimizing a variety of X-risks are implemented in the LibAUC
library at www.libauc.org."
7008,"For each family
                                        of problems, we present some strong baseline algorithms and their complexities,
                                        which will motivate further research for improving the existing results.","We formulate DXO into three special families of non-convex optimization prob-
                                        lems belonging to non-convex min-max optimization, non-convex compositional
                                        optimization, and non-convex bilevel optimization, respectively.","Discus-
                                        sions about the presented results and future studies are given at the end.",2022-06-01 12:22:56+00:00,Algorithmic Foundation of Deep X-Risk Optimization,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",[arxiv.Result.Author('Tianbao Yang')],"X-risk is a term introduced to represent a family of compositional measures
or objectives, in which each data point is compared with a large number of
items explicitly or implicitly for defining a risk function. It includes many
widely used measures or objectives, e.g., AUROC, AUPRC, partial AUROC, NDCG,
MAP, top-$K$ NDCG, top-$K$ MAP, listwise losses, p-norm push, top push,
precision/recall at top $K$ positions, precision at a certain recall level,
contrastive objectives, etc. While these non-decomposable measures/objectives
and their optimization algorithms have been studied in the literature of
machine learning, computer vision, information retrieval, and etc, optimizing
these measures/objectives has encountered some unique challenges for deep
learning. In this paper, we survey recent rigorous efforts for deep X-risk
optimization (DXO) by focusing on its algorithmic foundation. We introduce a
class of techniques for optimizing X-risks for deep learning. We formulate DXO
into three special families of non-convex optimization problems belonging to
non-convex min-max optimization, non-convex compositional optimization, and
non-convex bilevel optimization, respectively. For each family of problems, we
present some strong baseline algorithms and their complexities, which will
motivate further research for improving the existing results. Discussions about
the presented results and future studies are given at the end. Efficient
algorithms for optimizing a variety of X-risks are implemented in the LibAUC
library at www.libauc.org."
7009,"For each family
                                        of problems, we present some strong baseline algorithms and their complexities,
                                        which will motivate further research for improving the existing results.","We formulate DXO into three special families of non-convex optimization prob-
                                        lems belonging to non-convex min-max optimization, non-convex compositional
                                        optimization, and non-convex bilevel optimization, respectively.","Discus-
                                        sions about the presented results and future studies are given at the end.",2022-06-01 12:22:56+00:00,Algorithmic Foundation of Deep X-Risk Optimization,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",[arxiv.Result.Author('Tianbao Yang')],"X-risk is a term introduced to represent a family of compositional measures
or objectives, in which each data point is compared with a large number of
items explicitly or implicitly for defining a risk function. It includes many
widely used measures or objectives, e.g., AUROC, AUPRC, partial AUROC, NDCG,
MAP, top-$K$ NDCG, top-$K$ MAP, listwise losses, p-norm push, top push,
precision/recall at top $K$ positions, precision at a certain recall level,
contrastive objectives, etc. While these non-decomposable measures/objectives
and their optimization algorithms have been studied in the literature of
machine learning, computer vision, information retrieval, and etc, optimizing
these measures/objectives has encountered some unique challenges for deep
learning. In this paper, we survey recent rigorous efforts for deep X-risk
optimization (DXO) by focusing on its algorithmic foundation. We introduce a
class of techniques for optimizing X-risks for deep learning. We formulate DXO
into three special families of non-convex optimization problems belonging to
non-convex min-max optimization, non-convex compositional optimization, and
non-convex bilevel optimization, respectively. For each family of problems, we
present some strong baseline algorithms and their complexities, which will
motivate further research for improving the existing results. Discussions about
the presented results and future studies are given at the end. Efficient
algorithms for optimizing a variety of X-risks are implemented in the LibAUC
library at www.libauc.org."
7014,"Therefore, it would be insufﬁcient to consider only the interpolators under
the noiseless regimes, and further studying on the early stopping classiﬁer is necessary.","Furthermore, Theorem 3.1.3 shows that the overﬁtting is avoidable
through early stopping.","One may doubt the fast convergence rate in Statement One and Statement Three, which seems too
good to be true.",2022-06-01 14:00:37+00:00,Realistic Deep Learning May Not Fit Benignly,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Kaiyue Wen'), arxiv.Result.Author('Jiaye Teng'), arxiv.Result.Author('Jingzhao Zhang')]","Studies on benign overfitting provide insights for the success of
overparameterized deep learning models. In this work, we examine the benign
overfitting phenomena in real-world settings. We found that for tasks such as
training a ResNet model on ImageNet dataset, the model does not fit benignly.
To understand why benign overfitting fails in the ImageNet experiment, we
analyze previous benign overfitting models under a more restrictive setup where
the number of parameters is not significantly larger than the number of data
points. Under this mild overparameterization setup, our analysis identifies a
phase change: unlike in the heavy overparameterization setting, benign
overfitting can now fail in the presence of label noise. Our study explains our
empirical observations, and naturally leads to a simple technique known as
self-training that can boost the model's generalization performances.
Furthermore, our work highlights the importance of understanding implicit bias
in underfitting regimes as a future direction."
7030,"We expect them to be extendable to an
n-class setting but leave this to further research.","4 Limitations and Outlook

Our theoretical results are so far only formulated for two classes.","It remains to be seen whether our proposed training setup
can be extended to more complicated datasets while retaining stability.",2022-06-01 20:48:24+00:00,Merlin-Arthur Classifiers: Formal Interpretability with Interactive Black Boxes,cs.LG,"['cs.LG', 'cs.AI', '68T01, 91A06', 'I.2.0']","[arxiv.Result.Author('Stephan Wäldchen'), arxiv.Result.Author('Kartikey Sharma'), arxiv.Result.Author('Max Zimmer'), arxiv.Result.Author('Sebastian Pokutta')]","We present a new theoretical framework for making black box classifiers such
as Neural Networks interpretable, basing our work on clear assumptions and
guarantees. In our setting, which is inspired by the Merlin-Arthur protocol
from Interactive Proof Systems, two functions cooperate to achieve a
classification together: the \emph{prover} selects a small set of features as a
certificate and presents it to the \emph{classifier}. Including a second,
adversarial prover allows us to connect a game-theoretic equilibrium to
information-theoretic guarantees on the exchanged features. We define notions
of completeness and soundness that enable us to lower bound the mutual
information between features and class. To demonstrate good agreement between
theory and practice, we support our framework by providing numerical
experiments for Neural Network classifiers, explicitly calculating the mutual
information of features with respect to the class."
7031,"We hope that further research will
determine if this realisation can be trained in a stable manner.","One possibility might be
that since each UNet both cooperates with Arthur and wants to fool him, they are more sensitive when Arthur
changes his focus between achieving good soundness and completeness.","Merlin: Opt                                      Merlin: Opt, Morgana: Opt

1.2                                  Pe
1.0                                  Icoop
0.8                                  H(C(Y )|Y ∈ M (X))
0.6
0.4     2 4 8 16 32 1 2 4 8 16 32
0.2
0.0     Merlin: Net                                      Merlin: Net, Morgana: Opt

     1     Pe                                                   Pe
           Icoop                                                Icoop
1.2        H(C(Y)|Y M(X))                                       H(C(Y)|Y M(X))
1.0
0.8     2 4 8 16 32 1 2 4 8 16 32
0.6
0.4
0.2
0.0

     1

        k                                                    k

Figure 13: This ﬁgure depicts the mean and standard deviation over 10 training runs for the error probability, cooperative
information and the class entropy.",2022-06-01 20:48:24+00:00,Merlin-Arthur Classifiers: Formal Interpretability with Interactive Black Boxes,cs.LG,"['cs.LG', 'cs.AI', '68T01, 91A06', 'I.2.0']","[arxiv.Result.Author('Stephan Wäldchen'), arxiv.Result.Author('Kartikey Sharma'), arxiv.Result.Author('Max Zimmer'), arxiv.Result.Author('Sebastian Pokutta')]","We present a new theoretical framework for making black box classifiers such
as Neural Networks interpretable, basing our work on clear assumptions and
guarantees. In our setting, which is inspired by the Merlin-Arthur protocol
from Interactive Proof Systems, two functions cooperate to achieve a
classification together: the \emph{prover} selects a small set of features as a
certificate and presents it to the \emph{classifier}. Including a second,
adversarial prover allows us to connect a game-theoretic equilibrium to
information-theoretic guarantees on the exchanged features. We define notions
of completeness and soundness that enable us to lower bound the mutual
information between features and class. To demonstrate good agreement between
theory and practice, we support our framework by providing numerical
experiments for Neural Network classifiers, explicitly calculating the mutual
information of features with respect to the class."
7036,"We further study the independent impact of
We adopt a batch size of 128 for the ﬁrst two devices, fol-    the above perspective (2) in the Appendix.",of parallelism.,"C.
lowing (Ding et al., 2021), and 64 for the last device, and
Frame-Per-Second (FPS) as the efﬁciency metric.",2022-06-02 02:32:47+00:00,DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Yonggan Fu'), arxiv.Result.Author('Haichuan Yang'), arxiv.Result.Author('Jiayi Yuan'), arxiv.Result.Author('Meng Li'), arxiv.Result.Author('Cheng Wan'), arxiv.Result.Author('Raghuraman Krishnamoorthi'), arxiv.Result.Author('Vikas Chandra'), arxiv.Result.Author('Yingyan Lin')]","Efficient deep neural network (DNN) models equipped with compact operators
(e.g., depthwise convolutions) have shown great potential in reducing DNNs'
theoretical complexity (e.g., the total number of weights/operations) while
maintaining a decent model accuracy. However, existing efficient DNNs are still
limited in fulfilling their promise in boosting real-hardware efficiency, due
to their commonly adopted compact operators' low hardware utilization. In this
work, we open up a new compression paradigm for developing real-hardware
efficient DNNs, leading to boosted hardware efficiency while maintaining model
accuracy. Interestingly, we observe that while some DNN layers' activation
functions help DNNs' training optimization and achievable accuracy, they can be
properly removed after training without compromising the model accuracy.
Inspired by this observation, we propose a framework dubbed DepthShrinker,
which develops hardware-friendly compact networks via shrinking the basic
building blocks of existing efficient DNNs that feature irregular computation
patterns into dense ones with much improved hardware utilization and thus
real-hardware efficiency. Excitingly, our DepthShrinker framework delivers
hardware-friendly compact networks that outperform both state-of-the-art
efficient DNNs and compression techniques, e.g., a 3.06\% higher accuracy and
1.53$\times$ throughput on Tesla V100 over SOTA channel-wise pruning method
MetaPruning. Our codes are available at:
https://github.com/RICE-EIC/DepthShrinker."
7037,"We further study the independent impact of
We adopt a batch size of 128 for the ﬁrst two devices, fol-    the above perspective (2) in the Appendix.",of parallelism.,"C.
lowing (Ding et al., 2021), and 64 for the last device, and
Frame-Per-Second (FPS) as the efﬁciency metric.",2022-06-02 02:32:47+00:00,DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Yonggan Fu'), arxiv.Result.Author('Haichuan Yang'), arxiv.Result.Author('Jiayi Yuan'), arxiv.Result.Author('Meng Li'), arxiv.Result.Author('Cheng Wan'), arxiv.Result.Author('Raghuraman Krishnamoorthi'), arxiv.Result.Author('Vikas Chandra'), arxiv.Result.Author('Yingyan Lin')]","Efficient deep neural network (DNN) models equipped with compact operators
(e.g., depthwise convolutions) have shown great potential in reducing DNNs'
theoretical complexity (e.g., the total number of weights/operations) while
maintaining a decent model accuracy. However, existing efficient DNNs are still
limited in fulfilling their promise in boosting real-hardware efficiency, due
to their commonly adopted compact operators' low hardware utilization. In this
work, we open up a new compression paradigm for developing real-hardware
efficient DNNs, leading to boosted hardware efficiency while maintaining model
accuracy. Interestingly, we observe that while some DNN layers' activation
functions help DNNs' training optimization and achievable accuracy, they can be
properly removed after training without compromising the model accuracy.
Inspired by this observation, we propose a framework dubbed DepthShrinker,
which develops hardware-friendly compact networks via shrinking the basic
building blocks of existing efficient DNNs that feature irregular computation
patterns into dense ones with much improved hardware utilization and thus
real-hardware efficiency. Excitingly, our DepthShrinker framework delivers
hardware-friendly compact networks that outperform both state-of-the-art
efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and
1.53$\times$ throughput on Tesla V100 over SOTA channel-wise pruning method
MetaPruning. Our codes are available at:
https://github.com/facebookresearch/DepthShrinker."
7063,"While our work makes several fundamental contributions, there is scope for further research along
these lines.","According to the LFA framework, this disagreement occurs because different methods approximate
the black box model over different neighborhoods using different loss functions.","First, we analyzed seven popular post hoc explanation methods and this analysis could be
extended to other methods.",2022-06-02 19:09:30+00:00,Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tessa Han'), arxiv.Result.Author('Suraj Srinivas'), arxiv.Result.Author('Himabindu Lakkaraju')]","Despite the plethora of post hoc model explanation methods, the basic
properties and behavior of these methods and the conditions under which each
one is effective are not well understood. In this work, we bridge these gaps
and address a fundamental question: Which explanation method should one use in
a given situation? To this end, we adopt a function approximation perspective
and formalize the local function approximation (LFA) framework. We show that
popular explanation methods are instances of this framework, performing
function approximations of the underlying model in different neighborhoods
using different loss functions. We introduce a no free lunch theorem for
explanation methods which demonstrates that no single method can perform
optimally across all neighbourhoods and calls for choosing among methods. To
choose among methods, we set forth a guiding principle based on the function
approximation perspective, considering a method to be effective if it recovers
the underlying model when the model is a member of the explanation function
class. Then, we analyze the conditions under which popular explanation methods
are effective and provide recommendations for choosing among explanation
methods and creating new ones. Lastly, we empirically validate our theoretical
results using various real world datasets, model classes, and prediction tasks.
By providing a principled mathematical framework which unifies diverse
explanation methods, our work characterizes the behaviour of these methods and
their relation to one another, guides the choice of explanation methods, and
paves the way for the creation of new ones."
7080,"To check if our intuition also generalizes to
deeper networks we further study RCAD’s behaviour in a non-linear setting using a diﬀerent toy example in
Appendix D. Furthermore, while our analysis is restricted to linear classiﬁers, linear models can provide a
rough proxy for neural network learning dynamics via the neural tangent kernel (NTK) approximation [23].","The analysis above explains why RCAD improves test performance in the linear setting and it is inline with
our intuition of RCAD’s ability to unlearn spurious features.","This strategy has been used in a number of prior works [2, 39, 57] and extending the analysis to the NTK
setting may be possible in future work.",2022-06-03 02:26:24+00:00,Adversarial Unlearning: Reducing Confidence Along Adversarial Directions,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Amrith Setlur'), arxiv.Result.Author('Benjamin Eysenbach'), arxiv.Result.Author('Virginia Smith'), arxiv.Result.Author('Sergey Levine')]","Supervised learning methods trained with maximum likelihood objectives often
overfit on training data. Most regularizers that prevent overfitting look to
increase confidence on additional examples (e.g., data augmentation,
adversarial training), or reduce it on training data (e.g., label smoothing).
In this work we propose a complementary regularization strategy that reduces
confidence on self-generated examples. The method, which we call RCAD (Reducing
Confidence along Adversarial Directions), aims to reduce confidence on
out-of-distribution examples lying along directions adversarially chosen to
increase training loss. In contrast to adversarial training, RCAD does not try
to robustify the model to output the original label, but rather regularizes it
to have reduced confidence on points generated using much larger perturbations
than in conventional adversarial training. RCAD can be easily integrated into
training pipelines with a few lines of code. Despite its simplicity, we find on
many classification benchmarks that RCAD can be added to existing techniques
(e.g., label smoothing, MixUp training) to increase test accuracy by 1-3% in
absolute value, with more significant gains in the low data regime. We also
provide a theoretical analysis that helps to explain these benefits in
simplified settings, showing that RCAD can provably help the model unlearn
spurious features in the training data."
7092,"Appl., 31(11):8069–8085, 2019.
for eﬃcient study of the underlying problem and making
further research decisions.",Neural Comput.,We have demonstrated the in-                   [16] P. O. Hoyer.,2022-06-03 10:20:46+00:00,Finding Rule-Interpretable Non-Negative Data Representation,cs.LG,['cs.LG'],"[arxiv.Result.Author('Matej Mihelčić'), arxiv.Result.Author('Pauli Miettinen')]","Non-negative Matrix Factorization (NMF) is an intensively used technique for
obtaining parts-based, lower dimensional and non-negative representation of
non-negative data. It is a popular method in different research fields.
Scientists performing research in the fields of biology, medicine and pharmacy
often prefer NMF over other dimensionality reduction approaches (such as PCA)
because the non-negativity of the approach naturally fits the characteristics
of the domain problem and its result is easier to analyze and understand.
Despite these advantages, it still can be hard to get exact characterization
and interpretation of the NMF's resulting latent factors due to their numerical
nature. On the other hand, rule-based approaches are often considered more
interpretable but lack the parts-based interpretation. In this work, we present
a version of the NMF approach that merges rule-based descriptions with
advantages of part-based representation offered by the NMF approach. Given the
numerical input data with non-negative entries and a set of rules with high
entity coverage, the approach creates the lower-dimensional non-negative
representation of the input data in such a way that its factors are described
by the appropriate subset of the input rules. In addition to revealing
important attributes for latent factors, it allows analyzing relations between
these attributes and provides the exact numerical intervals or categorical
values they take. The proposed approach provides numerous advantages in tasks
such as focused embedding or performing supervised multi-label NMF."
7094,Draw up further research directions based on current developments in the literature.,"Illustrate potentials as well as current bottlenecks and pitfalls when applying these techniques
         in a realistic setting with time-varying rainfall inputs and complex pipe flow dynamics

    3.","2 Material and methods

2.1 Principal concepts

Our aim is to construct a fast surrogate model for 1D hydrodynamics in pipes that enables the
simulation of both water levels and flows in all nodes and links of an UDS, much like existing HiFi
models.",2022-05-24 19:44:46+00:00,Accelerating hydrodynamic simulations of urban drainage systems with physics-guided machine learning,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Rocco Palmitessa'), arxiv.Result.Author('Morten Grum'), arxiv.Result.Author('Allan Peter Engsig-Karup'), arxiv.Result.Author('Roland Löwe')]","We propose and demonstrate a new approach for fast and accurate surrogate
modelling of urban drainage system hydraulics based on physics-guided machine
learning. The surrogates are trained against a limited set of simulation
results from a hydrodynamic (HiFi) model. Our approach reduces simulation times
by one to two orders of magnitude compared to a HiFi model. It is thus slower
than e.g. conceptual hydrological models, but it enables simulations of water
levels, flows and surcharges in all nodes and links of a drainage network and
thus largely preserves the level of detail provided by HiFi models. Comparing
time series simulated by the surrogate and the HiFi model, R2 values in the
order of 0.9 are achieved. Surrogate training times are currently in the order
of one hour. However, they can likely be reduced through the application of
transfer learning and graph neural networks. Our surrogate approach will be
useful for interactive workshops in initial design phases of urban drainage
systems, as well as for real time applications. In addition, our model
formulation is generic and future research should investigate its application
for simulating other water systems."
7095,"While these results are promising, our work has several clear limitations that will need
to be addressed in further research to make the approach applicable in practice:

    1.","The surrogates enable
long time series simulations (in our case 8,200 rain events extracted from 40 years of rainfall
observations) of water levels, flows and surcharges in all nodes and pipes of a drainage system within
few minutes.","While our test case is an example of a typical drainage network, it does not include any
         structures apart from overflow and surcharge weirs, and all pipes were circular.",2022-05-24 19:44:46+00:00,Accelerating hydrodynamic simulations of urban drainage systems with physics-guided machine learning,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Rocco Palmitessa'), arxiv.Result.Author('Morten Grum'), arxiv.Result.Author('Allan Peter Engsig-Karup'), arxiv.Result.Author('Roland Löwe')]","We propose and demonstrate a new approach for fast and accurate surrogate
modelling of urban drainage system hydraulics based on physics-guided machine
learning. The surrogates are trained against a limited set of simulation
results from a hydrodynamic (HiFi) model. Our approach reduces simulation times
by one to two orders of magnitude compared to a HiFi model. It is thus slower
than e.g. conceptual hydrological models, but it enables simulations of water
levels, flows and surcharges in all nodes and links of a drainage network and
thus largely preserves the level of detail provided by HiFi models. Comparing
time series simulated by the surrogate and the HiFi model, R2 values in the
order of 0.9 are achieved. Surrogate training times are currently in the order
of one hour. However, they can likely be reduced through the application of
transfer learning and graph neural networks. Our surrogate approach will be
useful for interactive workshops in initial design phases of urban drainage
systems, as well as for real time applications. In addition, our model
formulation is generic and future research should investigate its application
for simulating other water systems."
7096,Draw up further research directions based on current developments in the literature.,"Illustrate potentials as well as current bottlenecks and pitfalls when applying these techniques
         in a realistic setting with time-varying rainfall inputs and complex pipe flow dynamics

    3.","2 Material and methods

2.1 Principal concepts

Our aim is to construct a fast surrogate model for 1D hydrodynamics in pipes that enables the
simulation of both water levels and flows in all nodes and links of an UDS, much like existing HiFi
models.",2022-05-24 19:44:46+00:00,Accelerating hydrodynamic simulations of urban drainage systems with physics-guided machine learning,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Rocco Palmitessa'), arxiv.Result.Author('Morten Grum'), arxiv.Result.Author('Allan Peter Engsig-Karup'), arxiv.Result.Author('Roland Löwe')]","We propose and demonstrate a new approach for fast and accurate surrogate
modelling of urban drainage system hydraulics based on physics-guided machine
learning. The surrogates are trained against a limited set of simulation
results from a hydrodynamic (HiFi) model. Our approach reduces simulation times
by one to two orders of magnitude compared to a HiFi model. It is thus slower
than e.g. conceptual hydrological models, but it enables simulations of water
levels, flows and surcharges in all nodes and links of a drainage network and
thus largely preserves the level of detail provided by HiFi models. Comparing
time series simulated by the surrogate and the HiFi model, R2 values in the
order of 0.9 are achieved. Surrogate training times are currently in the order
of one hour. However, they can likely be reduced through the application of
transfer learning and graph neural networks. Our surrogate approach will be
useful for interactive workshops in initial design phases of urban drainage
systems, as well as for real time applications. In addition, our model
formulation is generic and future research should investigate its application
for simulating other water systems."
7097,"While these results are promising, our work has several clear limitations that will need
to be addressed in further research to make the approach applicable in practice:

    1.","The surrogates enable
long time series simulations (in our case 8,200 rain events extracted from 40 years of rainfall
observations) of water levels, flows and surcharges in all nodes and pipes of a drainage system within
few minutes.","While our test case is an example of a typical drainage network, it does not include any
         structures apart from overflow and surcharge weirs, and all pipes were circular.",2022-05-24 19:44:46+00:00,Accelerating hydrodynamic simulations of urban drainage systems with physics-guided machine learning,cs.LG,"['cs.LG', 'cs.CE']","[arxiv.Result.Author('Rocco Palmitessa'), arxiv.Result.Author('Morten Grum'), arxiv.Result.Author('Allan Peter Engsig-Karup'), arxiv.Result.Author('Roland Löwe')]","We propose and demonstrate a new approach for fast and accurate surrogate
modelling of urban drainage system hydraulics based on physics-guided machine
learning. The surrogates are trained against a limited set of simulation
results from a hydrodynamic (HiFi) model. Our approach reduces simulation times
by one to two orders of magnitude compared to a HiFi model. It is thus slower
than e.g. conceptual hydrological models, but it enables simulations of water
levels, flows and surcharges in all nodes and links of a drainage network and
thus largely preserves the level of detail provided by HiFi models. Comparing
time series simulated by the surrogate and the HiFi model, R2 values in the
order of 0.9 are achieved. Surrogate training times are currently in the order
of one hour. However, they can likely be reduced through the application of
transfer learning and graph neural networks. Our surrogate approach will be
useful for interactive workshops in initial design phases of urban drainage
systems, as well as for real time applications. In addition, our model
formulation is generic and future research should investigate its application
for simulating other water systems."
7100,"We observe that for this dataset as         We further study the compute and storage efﬁciency
well, the pruned models consistently outperform the base-
line model, except for very low budgets of less than 10%,
where minor drops in performance are observed.","other datasets, we also report results for mini-ImageNet in
Tables 7, 8, 9 and 10.","Similar
to CIFAR-fs, performance gains of up to 2% in accuracy
are observed for this dataset as well.",2022-06-03 17:09:26+00:00,Dynamic Kernel Selection for Improved Generalization and Memory Efficiency in Meta-learning,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Arnav Chavan'), arxiv.Result.Author('Rishabh Tiwari'), arxiv.Result.Author('Udbhav Bamba'), arxiv.Result.Author('Deepak K. Gupta')]","Gradient based meta-learning methods are prone to overfit on the
meta-training set, and this behaviour is more prominent with large and complex
networks. Moreover, large networks restrict the application of meta-learning
models on low-power edge devices. While choosing smaller networks avoid these
issues to a certain extent, it affects the overall generalization leading to
reduced performance. Clearly, there is an approximately optimal choice of
network architecture that is best suited for every meta-learning problem,
however, identifying it beforehand is not straightforward. In this paper, we
present MetaDOCK, a task-specific dynamic kernel selection strategy for
designing compressed CNN models that generalize well on unseen tasks in
meta-learning. Our method is based on the hypothesis that for a given set of
similar tasks, not all kernels of the network are needed by each individual
task. Rather, each task uses only a fraction of the kernels, and the selection
of the kernels per task can be learnt dynamically as a part of the inner update
steps. MetaDOCK compresses the meta-model as well as the task-specific inner
models, thus providing significant reduction in model size for each task, and
through constraining the number of active kernels for every task, it implicitly
mitigates the issue of meta-overfitting. We show that for the same inference
budget, pruned versions of large CNN models obtained using our approach
consistently outperform the conventional choices of CNN models. MetaDOCK
couples well with popular meta-learning approaches such as iMAML. The efficacy
of our method is validated on CIFAR-fs and mini-ImageNet datasets, and we have
observed that our approach can provide improvements in model accuracy of up to
2% on standard meta-learning benchmark, while reducing the model size by more
than 75%."
7101,"This result fosters further research on classiﬁer-speciﬁc certiﬁcation and demonstrates
                                               that randomized smoothing is still worth investigating.","Since this process is executed at
                                               certiﬁcation time rather than at test time, it entails no loss in natural accuracy while enhancing the quality
                                               of the certiﬁcates.","Although classiﬁer-speciﬁc certiﬁcation may
                                               induce more computational cost, we also provide some theoretical insight on how to mitigate it.",2022-06-03 17:48:54+00:00,Towards Evading the Limits of Randomized Smoothing: A Theoretical Analysis,cs.LG,['cs.LG'],"[arxiv.Result.Author('Raphael Ettedgui'), arxiv.Result.Author('Alexandre Araujo'), arxiv.Result.Author('Rafael Pinot'), arxiv.Result.Author('Yann Chevaleyre'), arxiv.Result.Author('Jamal Atif')]","Randomized smoothing is the dominant standard for provable defenses against
adversarial examples. Nevertheless, this method has recently been proven to
suffer from important information theoretic limitations. In this paper, we
argue that these limitations are not intrinsic, but merely a byproduct of
current certification methods. We first show that these certificates use too
little information about the classifier, and are in particular blind to the
local curvature of the decision boundary. This leads to severely sub-optimal
robustness guarantees as the dimension of the problem increases. We then show
that it is theoretically possible to bypass this issue by collecting more
information about the classifier. More precisely, we show that it is possible
to approximate the optimal certificate with arbitrary precision, by probing the
decision boundary with several noise distributions. Since this process is
executed at certification time rather than at test time, it entails no loss in
natural accuracy while enhancing the quality of the certificates. This result
fosters further research on classifier-specific certification and demonstrates
that randomized smoothing is still worth investigating. Although
classifier-specific certification may induce more computational cost, we also
provide some theoretical insight on how to mitigate it."
7115,"We
Saliency Attack: Towards Imperceptible Black-box Adversarial Attack                                                                                                                                       111:11

further study the change of their MAD scores in Appendix A.4 and find most AEs can be improved
after restricting in salient region, which also validates the effectiveness of our method.","Instead, Saliency Attack conservatively selects small regions to perturb, hence its
query efficiency is a little lower than some baselines at the beginning but soon exceeds them.","SR-query convergence plot under MAD <= 20   SR-query convergence plot under MAD <= 30                                                                           SR-query convergence plot under MAD <= 40
0.7                                           1                                                                                                                   1

                         Boundary                                      Boundary                                                                                                            Boundary

0.6                      TVDBA                0.8                      TVDBA                                                                                      0.8                      TVDBA

                         Square                                        Square                                                                                                              Square

0.5                      Square-sal                                    Square-sal                                                                                                          Square-sal

                         Parsimonious         0.6                      Parsimonious                                                                               0.6                      Parsimonious

0.4                      Parsimonious-sal                              Parsimonious-sal                                                                                                    Parsimonious-sal
SR
                                                                               SR
                                                                                                                                                              SR
                         Saliency                                      Saliency                                                                                                            Saliency

0.3                                           0.4                                                                                                                 0.4

0.2

0.1                                           0.2                                                                                                                 0.2

0                                             0                                                                                                                   0

     0  0.5  1  1.5      2         2.5     3       0  0.5  1     1.5   2         2.5     3                                                                             0  0.5  1  1.5      2         2.5     3

                # query                 104                   # query                 104                                                                                         # query                 104

                Fig.",2022-06-04 03:56:07+00:00,Saliency Attack: Towards Imperceptible Black-box Adversarial Attack,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Zeyu Dai'), arxiv.Result.Author('Shengcai Liu'), arxiv.Result.Author('Ke Tang'), arxiv.Result.Author('Qing Li')]","Deep neural networks are vulnerable to adversarial examples, even in the
black-box setting where the attacker is only accessible to the model output.
Recent studies have devised effective black-box attacks with high query
efficiency. However, such performance is often accompanied by compromises in
attack imperceptibility, hindering the practical use of these approaches. In
this paper, we propose to restrict the perturbations to a small salient region
to generate adversarial examples that can hardly be perceived. This approach is
readily compatible with many existing black-box attacks and can significantly
improve their imperceptibility with little degradation in attack success rate.
Further, we propose the Saliency Attack, a new black-box attack aiming to
refine the perturbations in the salient region to achieve even better
imperceptibility. Extensive experiments show that compared to the
state-of-the-art black-box attacks, our approach achieves much better
imperceptibility scores, including most apparent distortion (MAD), $L_0$ and
$L_2$ distances, and also obtains significantly higher success rates judged by
a human-like threshold on MAD. Importantly, the perturbations generated by our
approach are interpretable to some extent. Finally, it is also demonstrated to
be robust to different detection-based defenses."
7126,"We further study a special model, the linear model.","Then with probability at least 1 − δ, for all t ≥ 1, it
                                                                                  In this paper, we propose the combinatorial causal bandit
holds that θ lies in the conﬁdence region                                         (CCB) framework, and provide a solution for CCB under the
                                                                                  BGLM.","θ ∈ [0, 1]d : θ − θˆt ≤                                                    1√     We show that our algorithm would work for models with
                                                d log(1 + td) + 2 log + d .",2022-06-04 14:14:58+00:00,Combinatorial Causal Bandits,cs.LG,"['cs.LG', 'cs.SI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Shi Feng'), arxiv.Result.Author('Wei Chen')]","In combinatorial causal bandits (CCB), the learning agent chooses at most $K$
variables in each round to intervene, collects feedback from the observed
variables, with the goal of minimizing expected regret on the target variable
$Y$. Different from all prior studies on causal bandits, CCB needs to deal with
exponentially large action space. We study under the context of binary
generalized linear models (BGLMs) with a succinct parametric representation of
the causal models. We present the algorithm BGLM-OFU for Markovian BGLMs (i.e.
no hidden variables) based on the maximum likelihood estimation method, and
show that it achieves $O(\sqrt{T}\log T)$ regret, where $T$ is the time
horizon. For the special case of linear models with hidden variables, we apply
causal inference techniques such as the do-calculus to convert the original
model into a Markovian model, and then show that our BGLM-OFU algorithm and
another algorithm based on the linear regression both solve such linear models
with hidden variables. Our novelty includes (a) considering the combinatorial
intervention action space, (b) considering general causal models including ones
with hidden variables, (c) integrating and adapting techniques from diverse
studies such as generalized linear bandits and online influence maximization,
and (d) not relying on unrealistic assumptions such as knowing the joint
distribution of the parents of $Y$ under all interventions used in some prior
studies."
7127,"We further study a special model, the linear model.","Then with probability at least 1 − δ, for all t ≥ 1, it
                                                                                     In this paper, we propose the combinatorial causal bandit
holds that θ lies in the conﬁdence region                                            (CCB) framework, and provide a solution for CCB under the
                                                                                     BGLM.","θ ∈ [0, 1]d : θ − θˆt ≤                                                    1√        We show that our algorithm would work for models with
                                                d log(1 + td) + 2 log + d .",2022-06-04 14:14:58+00:00,Combinatorial Causal Bandits,cs.LG,"['cs.LG', 'cs.SI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Shi Feng'), arxiv.Result.Author('Wei Chen')]","In combinatorial causal bandits (CCB), the learning agent chooses at most $K$
variables in each round to intervene, collects feedback from the observed
variables, with the goal of minimizing expected regret on the target variable
$Y$. Different from all prior studies on causal bandits, CCB needs to deal with
exponentially large action space. We study under the context of binary
generalized linear models (BGLMs) with a succinct parametric representation of
the causal models. We present the algorithm BGLM-OFU for Markovian BGLMs (i.e.
no hidden variables) based on the maximum likelihood estimation method, and
show that it achieves $O(\sqrt{T}\log T)$ regret, where $T$ is the time
horizon. For the special case of linear models with hidden variables, we apply
causal inference techniques such as the do-calculus to convert the original
model into a Markovian model, and then show that our BGLM-OFU algorithm and
another algorithm based on the linear regression both solve such linear models
with hidden variables. Our novelty includes (a) considering the combinatorial
intervention action space, (b) considering general causal models including ones
with hidden variables, (c) integrating and adapting techniques from diverse
studies such as generalized linear bandits and online influence maximization,
and (d) not relying on unrealistic assumptions such as knowing the joint
distribution of the parents of $Y$ under all interventions used in some prior
studies."
7128,"We further study a special model, the linear model.","Then with probability at least 1 − δ, for all t ≥ 1, it
                                                                                     In this paper, we propose the combinatorial causal bandit
holds that θ lies in the conﬁdence region                                            (CCB) framework, and provide a solution for CCB under the
                                                                                     BGLM.","θ ∈ [0, 1]d : θ − θˆt ≤                                                    1√        We show that our algorithm would work for models with
                                                d log(1 + td) + 2 log + d .",2022-06-04 14:14:58+00:00,Combinatorial Causal Bandits,cs.LG,"['cs.LG', 'cs.SI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Shi Feng'), arxiv.Result.Author('Wei Chen')]","In combinatorial causal bandits (CCB), the learning agent chooses at most $K$
variables in each round to intervene, collects feedback from the observed
variables, with the goal of minimizing expected regret on the target variable
$Y$. Different from all prior studies on causal bandits, CCB needs to deal with
exponentially large action space. We study under the context of binary
generalized linear models (BGLMs) with a succinct parametric representation of
the causal models. We present the algorithm BGLM-OFU for Markovian BGLMs (i.e.
no hidden variables) based on the maximum likelihood estimation method, and
show that it achieves $O(\sqrt{T}\log T)$ regret, where $T$ is the time
horizon. For the special case of linear models with hidden variables, we apply
causal inference techniques such as the do-calculus to convert the original
model into a Markovian model, and then show that our BGLM-OFU algorithm and
another algorithm based on the linear regression both solve such linear models
with hidden variables. Our novelty includes (a) considering the combinatorial
intervention action space and the general causal models including ones with
hidden variables, (b) integrating and adapting techniques from diverse studies
such as generalized linear bandits and online influence maximization, and (c)
not relying on unrealistic assumptions such as knowing the joint distribution
of the parents of $Y$ under all interventions used in some prior studies."
7129,"We further study a special model, the linear model.",We can remove the initialization phase and the de-                  BGLM.,"pendency on Assumption 3, by noticing that our MLE based                    We show that our algorithm would work for models with
on pseudo log-likelihood maximization is equivalent to lin-                 many types of hidden variables.",2022-06-04 14:14:58+00:00,Combinatorial Causal Bandits,cs.LG,"['cs.LG', 'cs.SI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Shi Feng'), arxiv.Result.Author('Wei Chen')]","In combinatorial causal bandits (CCB), the learning agent chooses at most $K$
variables in each round to intervene, collects feedback from the observed
variables, with the goal of minimizing expected regret on the target variable
$Y$. We study under the context of binary generalized linear models (BGLMs)
with a succinct parametric representation of the causal models. We present the
algorithm BGLM-OFU for Markovian BGLMs (i.e. no hidden variables) based on the
maximum likelihood estimation method, and show that it achieves $O(\sqrt{T}\log
T)$ regret, where $T$ is the time horizon. For the special case of linear
models with hidden variables, we apply causal inference techniques such as the
do-calculus to convert the original model into a Markovian model, and then show
that our BGLM-OFU algorithm and another algorithm based on the linear
regression both solve such linear models with hidden variables. Our novelty
includes (a) considering the combinatorial intervention action space and the
general causal models including ones with hidden variables, (b) integrating and
adapting techniques from diverse studies such as generalized linear bandits and
online influence maximization, and (c) avoiding unrealistic assumptions (such
as knowing the joint distribution of the parents of $Y$ under all
interventions) and regret factors exponential to causal graph size in prior
studies."
7130,"We further study a special model, the linear model.",BGLM.,"We rewrite it as Lemma 11 in Appendix E.                                    We show that our algorithm would work for models with
                                                                            many types of hidden variables.",2022-06-04 14:14:58+00:00,Combinatorial Causal Bandits,cs.LG,"['cs.LG', 'cs.SI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Shi Feng'), arxiv.Result.Author('Wei Chen')]","In combinatorial causal bandits (CCB), the learning agent chooses at most $K$
variables in each round to intervene, collects feedback from the observed
variables, with the goal of minimizing expected regret on the target variable
$Y$. We study under the context of binary generalized linear models (BGLMs)
with a succinct parametric representation of the causal models. We present the
algorithm BGLM-OFU for Markovian BGLMs (i.e. no hidden variables) based on the
maximum likelihood estimation method, and show that it achieves $O(\sqrt{T}\log
T)$ regret, where $T$ is the time horizon. For the special case of linear
models with hidden variables, we apply causal inference techniques such as the
do-calculus to convert the original model into a Markovian model, and then show
that our BGLM-OFU algorithm and another algorithm based on the linear
regression both solve such linear models with hidden variables. Our novelty
includes (a) considering the combinatorial intervention action space and the
general causal models including ones with hidden variables, (b) integrating and
adapting techniques from diverse studies such as generalized linear bandits and
online influence maximization, and (c) avoiding unrealistic assumptions (such
as knowing the joint distribution of the parents of $Y$ under all
interventions) and regret factors exponential to causal graph size in prior
studies."
7132,"We hope
                                                  that HVE could shed some light on further research on reinforcement learning from ﬁxed data.","OPHVE outperforms other off-policy evaluation
                                                  methods in all three metrics measuring the estimation effectiveness, while MOHVE achieves better
                                                  or comparable performance with state-of-the-art ofﬂine reinforcement learning algorithms.","1 Introduction

                                       Reinforcement learning (RL) [1, 2, 3, 4] has demonstrated great success in various sequential decision making problems,
                                       e.g., sequential recommendation systems [5, 6] and robotic locomotion skill learning [7, 8].",2022-06-04 14:32:41+00:00,Hybrid Value Estimation for Off-policy Evaluation and Offline Reinforcement Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xue-Kun Jin'), arxiv.Result.Author('Xu-Hui Liu'), arxiv.Result.Author('Shengyi Jiang'), arxiv.Result.Author('Yang Yu')]","Value function estimation is an indispensable subroutine in reinforcement
learning, which becomes more challenging in the offline setting. In this paper,
we propose Hybrid Value Estimation (HVE) to reduce value estimation error,
which trades off bias and variance by balancing between the value estimation
from offline data and the learned model. Theoretical analysis discloses that
HVE enjoys a better error bound than the direct methods. HVE can be leveraged
in both off-policy evaluation and offline reinforcement learning settings. We,
therefore, provide two concrete algorithms Off-policy HVE (OPHVE) and
Model-based Offline HVE (MOHVE), respectively. Empirical evaluations on MuJoCo
tasks corroborate the theoretical claim. OPHVE outperforms other off-policy
evaluation methods in all three metrics measuring the estimation effectiveness,
while MOHVE achieves better or comparable performance with state-of-the-art
offline reinforcement learning algorithms. We hope that HVE could shed some
light on further research on reinforcement learning from fixed data."
7140,"We further study multi-task learning as a way to enhance the generalization
ability of the model.",Multi-Task Learning.,"In our benchmark, the sizes of the training sets on different tasks could
be signiﬁcantly different.",2022-06-05 05:21:56+00:00,PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding,cs.LG,['cs.LG'],"[arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Jiarui Lu'), arxiv.Result.Author('Zhaocheng Zhu'), arxiv.Result.Author('Yangtian Zhang'), arxiv.Result.Author('Chang Ma'), arxiv.Result.Author('Runcheng Liu'), arxiv.Result.Author('Jian Tang')]","We are now witnessing significant progress of deep learning methods in a
variety of tasks (or datasets) of proteins. However, there is a lack of a
standard benchmark to evaluate the performance of different methods, which
hinders the progress of deep learning in this field. In this paper, we propose
such a benchmark called PEER, a comprehensive and multi-task benchmark for
Protein sEquence undERstanding. PEER provides a set of diverse protein
understanding tasks including protein function prediction, protein localization
prediction, protein structure prediction, protein-protein interaction
prediction, and protein-ligand interaction prediction. We evaluate different
types of sequence-based methods for each task including traditional feature
engineering approaches, different sequence encoding methods as well as
large-scale pre-trained protein language models. In addition, we also
investigate the performance of these methods under the multi-task learning
setting. Experimental results show that large-scale pre-trained protein
language models achieve the best performance for most individual tasks, and
jointly training multiple tasks further boosts the performance. The datasets
and source codes of this benchmark will be open-sourced soon."
7141,"We further study multi-task learning as a way to enhance the generalization
ability of the model.",Multi-Task Learning.,"In our benchmark, the sizes of the training sets on different tasks could
be signiﬁcantly different.",2022-06-05 05:21:56+00:00,PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding,cs.LG,['cs.LG'],"[arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Zuobai Zhang'), arxiv.Result.Author('Jiarui Lu'), arxiv.Result.Author('Zhaocheng Zhu'), arxiv.Result.Author('Yangtian Zhang'), arxiv.Result.Author('Chang Ma'), arxiv.Result.Author('Runcheng Liu'), arxiv.Result.Author('Jian Tang')]","We are now witnessing significant progress of deep learning methods in a
variety of tasks (or datasets) of proteins. However, there is a lack of a
standard benchmark to evaluate the performance of different methods, which
hinders the progress of deep learning in this field. In this paper, we propose
such a benchmark called PEER, a comprehensive and multi-task benchmark for
Protein sEquence undERstanding. PEER provides a set of diverse protein
understanding tasks including protein function prediction, protein localization
prediction, protein structure prediction, protein-protein interaction
prediction, and protein-ligand interaction prediction. We evaluate different
types of sequence-based methods for each task including traditional feature
engineering approaches, different sequence encoding methods as well as
large-scale pre-trained protein language models. In addition, we also
investigate the performance of these methods under the multi-task learning
setting. Experimental results show that large-scale pre-trained protein
language models achieve the best performance for most individual tasks, and
jointly training multiple tasks further boosts the performance. The datasets
and source codes of this benchmark are all available at
https://github.com/DeepGraphLearning/PEER_Benchmark"
7144,"Based on these early results, we hope that further research can be conducted into the effects of
pretraining on model performance with more data points.","We discuss these effects in greater detail in Appendix H. Note, that although
our small sample comparing pretraining on ImageNet21k to vanilla training suggests pretraining may
be harmful to AUROC, the size of the sample prevents us from making any deﬁnitive conclusions.","Evaluations of the simple post-training calibration method of temperature scaling (TS) [31], which
is widely known to improve ECE without changing the model’s accuracy, also revealed several
interesting facts: (1) TS consistently and greatly improves AUROC and selective performance (see
Figure 3)—meaning not only does TS calibrate the probabilistic estimation for each individual
instance, but it also improves the partial order of all instances induced by those improved estimations.",2022-06-05 11:15:35+00:00,Which models are innately best at uncertainty estimation?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ido Galil'), arxiv.Result.Author('Mohammed Dabbah'), arxiv.Result.Author('Ran El-Yaniv')]","Deep neural networks must be equipped with an uncertainty estimation
mechanism when deployed for risk-sensitive tasks. This paper studies the
relationship between deep architectures and their training regimes with their
corresponding selective prediction and uncertainty estimation performance. We
consider both in-distribution uncertainties and class-out-of-distribution ones.
Moreover, we consider some of the most popular estimation performance metrics
previously proposed including AUROC, ECE, AURC, and coverage for selective
accuracy constraint. We present a novel and comprehensive study of selective
prediction and the uncertainty estimation performance of 484 existing
pretrained deep ImageNet classifiers that are available at popular
repositories. We identify numerous and previously unknown factors that affect
uncertainty estimation and examine the relationships between the different
metrics. We find that distillation-based training regimes consistently yield
better uncertainty estimations than other training schemes such as vanilla
training, pretraining on a larger dataset and adversarial training. We also
provide strong empirical evidence showing that ViT is by far the most superior
architecture in terms of uncertainty estimation performance, judging by any
aspect, in both in-distribution and class-out-of-distribution scenarios."
7153,"Regarding the human side of the problem of reward learning from preferences, further research could
provide several improvements.","However, other research has addressed this problem through active learning
[14][9][25], and it may be possible to simply swap our Algorithm 1 into these active learning methods,
combining the improved sample efﬁciency of Pregret with that of these active learning methods.","First, we are conﬁdent that humans can be inﬂuenced by their training
and by the preference elicitation interface, which is a particularly rich direction for follow-up study.",2022-06-05 17:58:02+00:00,Models of human preference for learning reward functions,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'I.2.6; I.2.8']","[arxiv.Result.Author('W. Bradley Knox'), arxiv.Result.Author('Stephane Hatgis-Kessell'), arxiv.Result.Author('Serena Booth'), arxiv.Result.Author('Scott Niekum'), arxiv.Result.Author('Peter Stone'), arxiv.Result.Author('Alessandro Allievi')]","The utility of reinforcement learning is limited by the alignment of reward
functions with the interests of human stakeholders. One promising method for
alignment is to learn the reward function from human-generated preferences
between pairs of trajectory segments. These human preferences are typically
assumed to be informed solely by partial return, the sum of rewards along each
segment. We find this assumption to be flawed and propose modeling preferences
instead as arising from a different statistic: each segment's regret, a measure
of a segment's deviation from optimal decision-making. Given infinitely many
preferences generated according to regret, we prove that we can identify a
reward function equivalent to the reward function that generated those
preferences. We also prove that the previous partial return model lacks this
identifiability property without preference noise that reveals rewards'
relative proportions, and we empirically show that our proposed regret
preference model outperforms it with finite training data in otherwise the same
setting. Additionally, our proposed regret preference model better predicts
real human preferences and also learns reward functions from these preferences
that lead to policies that are better human-aligned. Overall, this work
establishes that the choice of preference model is impactful, and our proposed
regret preference model provides an improvement upon a core assumption of
recent research."
7157,"The complementariness of our method with domain-speciﬁc augmentation is not clear at this
moment and deserves further study.","However, when the training dataset size is very small, the
improvement is only comparable to state-of-the-art domain-speciﬁc augmentation methods.","Moreover, current experiments are focused on image
generation tasks with either the StyleGAN2 or ProjectedGAN framework.",2022-06-05 20:45:01+00:00,Diffusion-GAN: Training GANs with Diffusion,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Zhendong Wang'), arxiv.Result.Author('Huangjie Zheng'), arxiv.Result.Author('Pengcheng He'), arxiv.Result.Author('Weizhu Chen'), arxiv.Result.Author('Mingyuan Zhou')]","For stable training of generative adversarial networks (GANs), injecting
instance noise into the input of the discriminator is considered as a
theoretically sound solution, which, however, has not yet delivered on its
promise in practice. This paper introduces Diffusion-GAN that employs a
Gaussian mixture distribution, defined over all the diffusion steps of a
forward diffusion chain, to inject instance noise. A random sample from the
mixture, which is diffused from an observed or generated data, is fed as the
input to the discriminator. The generator is updated by backpropagating its
gradient through the forward diffusion chain, whose length is adaptively
adjusted to control the maximum noise-to-data ratio allowed at each training
step. Theoretical analysis verifies the soundness of the proposed
Diffusion-GAN, which provides model- and domain-agnostic differentiable
augmentation. A rich set of experiments on diverse datasets show that
Diffusion-GAN can provide stable and data-efficient GAN training, bringing
consistent performance improvement over strong GAN baselines for synthesizing
photo-realistic images."
7158,"The complementariness of our method with domain-speciﬁc augmentation is not clear at this
moment and deserves further study.","However, when the training dataset size is very small, the
improvement is only comparable to state-of-the-art domain-speciﬁc augmentation methods.","Moreover, current experiments are focused on image
generation tasks with either the StyleGAN2 or ProjectedGAN framework.",2022-06-05 20:45:01+00:00,Diffusion-GAN: Training GANs with Diffusion,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Zhendong Wang'), arxiv.Result.Author('Huangjie Zheng'), arxiv.Result.Author('Pengcheng He'), arxiv.Result.Author('Weizhu Chen'), arxiv.Result.Author('Mingyuan Zhou')]","For stable training of generative adversarial networks (GANs), injecting
instance noise into the input of the discriminator is considered as a
theoretically sound solution, which, however, has not yet delivered on its
promise in practice. This paper introduces Diffusion-GAN that employs a
Gaussian mixture distribution, defined over all the diffusion steps of a
forward diffusion chain, to inject instance noise. A random sample from the
mixture, which is diffused from an observed or generated data, is fed as the
input to the discriminator. The generator is updated by backpropagating its
gradient through the forward diffusion chain, whose length is adaptively
adjusted to control the maximum noise-to-data ratio allowed at each training
step. Theoretical analysis verifies the soundness of the proposed
Diffusion-GAN, which provides model- and domain-agnostic differentiable
augmentation. A rich set of experiments on diverse datasets show that
Diffusion-GAN can provide stable and data-efficient GAN training, bringing
consistent performance improvement over strong GAN baselines for synthesizing
photo-realistic images."
7175,"Other Facets of VHL                                                                Round

In this section, we dive into VHL deeper by further studying                 (a) Test Accuracy                                                   (b) Model Divergence
other facets related to it.","Round
                                                                             200 400 600 800 1000
5.3.","In addition, we conduct experi-
ments on CIFAR to investigate the intriguing property of               Figure 3.",2022-06-06 10:02:21+00:00,Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Zhenheng Tang'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Shaohuai Shi'), arxiv.Result.Author('Xin He'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Xiaowen Chu')]","In federated learning (FL), model performance typically suffers from client
drift induced by data heterogeneity, and mainstream works focus on correcting
client drift. We propose a different approach named virtual homogeneity
learning (VHL) to directly ""rectify"" the data heterogeneity. In particular, VHL
conducts FL with a virtual homogeneous dataset crafted to satisfy two
conditions: containing no private information and being separable. The virtual
dataset can be generated from pure noise shared across clients, aiming to
calibrate the features from the heterogeneous clients. Theoretically, we prove
that VHL can achieve provable generalization performance on the natural
distribution. Empirically, we demonstrate that VHL endows FL with drastically
improved convergence speed and generalization performance. VHL is the first
attempt towards using a virtual dataset to address data heterogeneity, offering
new and effective means to FL."
7176,Experiment results of further study of VHL on CIFAR10.,"To verify this
we conduct feature calibration based on these shared fea-
                 Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning

Table 6.",best performance.,2022-06-06 10:02:21+00:00,Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Zhenheng Tang'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Shaohuai Shi'), arxiv.Result.Author('Xin He'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Xiaowen Chu')]","In federated learning (FL), model performance typically suffers from client
drift induced by data heterogeneity, and mainstream works focus on correcting
client drift. We propose a different approach named virtual homogeneity
learning (VHL) to directly ""rectify"" the data heterogeneity. In particular, VHL
conducts FL with a virtual homogeneous dataset crafted to satisfy two
conditions: containing no private information and being separable. The virtual
dataset can be generated from pure noise shared across clients, aiming to
calibrate the features from the heterogeneous clients. Theoretically, we prove
that VHL can achieve provable generalization performance on the natural
distribution. Empirically, we demonstrate that VHL endows FL with drastically
improved convergence speed and generalization performance. VHL is the first
attempt towards using a virtual dataset to address data heterogeneity, offering
new and effective means to FL."
7177,"Other Facets of VHL                                                                Round

In this section, we dive into VHL deeper by further studying                 (a) Test Accuracy                                                   (b) Model Divergence
other facets related to it.","Round
                                                                             200 400 600 800 1000
5.3.","In addition, we conduct experi-
ments on CIFAR to investigate the intriguing property of               Figure 3.",2022-06-06 10:02:21+00:00,Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Zhenheng Tang'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Shaohuai Shi'), arxiv.Result.Author('Xin He'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Xiaowen Chu')]","In federated learning (FL), model performance typically suffers from client
drift induced by data heterogeneity, and mainstream works focus on correcting
client drift. We propose a different approach named virtual homogeneity
learning (VHL) to directly ""rectify"" the data heterogeneity. In particular, VHL
conducts FL with a virtual homogeneous dataset crafted to satisfy two
conditions: containing no private information and being separable. The virtual
dataset can be generated from pure noise shared across clients, aiming to
calibrate the features from the heterogeneous clients. Theoretically, we prove
that VHL can achieve provable generalization performance on the natural
distribution. Empirically, we demonstrate that VHL endows FL with drastically
improved convergence speed and generalization performance. VHL is the first
attempt towards using a virtual dataset to address data heterogeneity, offering
new and effective means to FL."
7178,Experiment results of further study of VHL on CIFAR10.,"To verify this
we conduct feature calibration based on these shared fea-
                 Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning

Table 6.",best performance.,2022-06-06 10:02:21+00:00,Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Zhenheng Tang'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Shaohuai Shi'), arxiv.Result.Author('Xin He'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Xiaowen Chu')]","In federated learning (FL), model performance typically suffers from client
drift induced by data heterogeneity, and mainstream works focus on correcting
client drift. We propose a different approach named virtual homogeneity
learning (VHL) to directly ""rectify"" the data heterogeneity. In particular, VHL
conducts FL with a virtual homogeneous dataset crafted to satisfy two
conditions: containing no private information and being separable. The virtual
dataset can be generated from pure noise shared across clients, aiming to
calibrate the features from the heterogeneous clients. Theoretically, we prove
that VHL can achieve provable generalization performance on the natural
distribution. Empirically, we demonstrate that VHL endows FL with drastically
improved convergence speed and generalization performance. VHL is the first
attempt towards using a virtual dataset to address data heterogeneity, offering
new and effective means to FL."
7188,"We further study popular methods and introduce variations of them,
                                                 allowing us to relate this theoretical result to current practices and show the inﬂu-
                                                 ence (or lack thereof) of design choices on downstream performance.","By designing contrastive
                                                 and covariance based non-contrastive criteria that can be related algebraically and
                                                 shown to be equivalent under limited assumptions, we show how close those fam-
                                                 ilies can be.","Motivated
                                                 by our equivalence result, we investigate the low performance of SimCLR and
                                                 show how it can match VICReg’s with careful hyperparameter tuning, improv-
                                                 ing signiﬁcantly over known baselines.",2022-06-03 08:04:12+00:00,On the duality between contrastive and non-contrastive self-supervised learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Quentin Garrido'), arxiv.Result.Author('Yubei Chen'), arxiv.Result.Author('Adrien Bardes'), arxiv.Result.Author('Laurent Najman'), arxiv.Result.Author('Yann Lecun')]","Recent approaches in self-supervised learning of image representations can be
categorized into different families of methods and, in particular, can be
divided into contrastive and non-contrastive approaches. While differences
between the two families have been thoroughly discussed to motivate new
approaches, we focus more on the theoretical similarities between them. By
designing contrastive and covariance based non-contrastive criteria that can be
related algebraically and shown to be equivalent under limited assumptions, we
show how close those families can be. We further study popular methods and
introduce variations of them, allowing us to relate this theoretical result to
current practices and show the influence (or lack thereof) of design choices on
downstream performance. Motivated by our equivalence result, we investigate the
low performance of SimCLR and show how it can match VICReg's with careful
hyperparameter tuning, improving significantly over known baselines. We also
challenge the popular assumptions that contrastive and non-contrastive methods,
respectively, need large batch sizes and output dimensions. Our theoretical and
quantitative results suggest that the numerical gaps between contrastive and
non-contrastive methods in certain regimes can be closed given better network
design choices and hyperparameter tuning. The evidence shows that unifying
different SOTA methods is an important direction to build a better
understanding of self-supervised learning."
7189,"To address this limitation, we want to further study the calibration of 𝛿 from logged data.","In Section 5.2 we mention what 𝛿 they suggest, e.g., for CM 1 − 𝛿 for each model parameter to hold jointly
with probability 1 − 𝛿 |E |.","ACKNOWLEDGMENTS

This research was partially supported by TAILOR, a project funded by EU Horizon 2020 research and innovation
programme under GA No.",2022-06-06 12:58:28+00:00,Pessimistic Off-Policy Optimization for Learning to Rank,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR']","[arxiv.Result.Author('Matej Cief'), arxiv.Result.Author('Branislav Kveton'), arxiv.Result.Author('Michal Kompan')]","Off-policy learning is a framework for optimizing policies without deploying
them, using data collected by another policy. In recommender systems, this is
especially challenging due to the imbalance in logged data: some items are
recommended and thus logged much more frequently than others. This is further
perpetuated when recommending a list of items, as the action space is
combinatorial. To address this challenge, we study pessimistic off-policy
optimization for learning to rank. The key idea is to compute lower confidence
bounds on parameters of click models and then return the list with the highest
pessimistic estimate of its value. This approach is computationally efficient
and we analyze it. We study its Bayesian and frequentist variants, and overcome
the limitation of unknown prior by incorporating empirical Bayes. To show the
empirical effectiveness of our approach, we compare it to off-policy optimizers
that use inverse propensity scores or neglect uncertainty. Our approach
outperforms all baselines, is robust, and is also general."
7190,"achieved with bootstrapping, although this method presents
computational challenges, and further research is needed so                                          Chuklin, A.; Markov, I.; and Rijke, M. d. 2015.","This can be generally                                             edge management, 621–630.","Click models
the optimization is tractable.",2022-06-06 12:58:28+00:00,Pessimistic Off-Policy Optimization for Learning to Rank,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR']","[arxiv.Result.Author('Matej Cief'), arxiv.Result.Author('Branislav Kveton'), arxiv.Result.Author('Michal Kompan')]","Off-policy learning is a framework for optimizing policies without deploying
them, using data collected by another policy. In recommender systems, this is
especially challenging due to the imbalance in logged data: some items are
recommended and thus logged more frequently than others. This is further
perpetuated when recommending a list of items, as the action space is
combinatorial. To address this challenge, we study pessimistic off-policy
optimization for learning to rank. The key idea is to compute lower confidence
bounds on parameters of click models and then return the list with the highest
pessimistic estimate of its value. This approach is computationally efficient
and we analyze it. We study its Bayesian and frequentist variants, and overcome
the limitation of unknown prior by incorporating empirical Bayes. To show the
empirical effectiveness of our approach, we compare it to off-policy optimizers
that use inverse propensity scores or neglect uncertainty. Our approach
outperforms all baselines, is robust, and is also general."
7197,"More broadly, we hope our work inspires
further study on the interplay between second-order gradients and generalization.","Second, it would be interesting to apply
Hessians to understand generalization in other black-box models.","Acknowledgement

Thanks to the anonymous referees for their feedback.",2022-06-06 14:52:46+00:00,Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees,cs.LG,"['cs.LG', 'cs.CV', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Haotian Ju'), arxiv.Result.Author('Dongyue Li'), arxiv.Result.Author('Hongyang R. Zhang')]","We consider transfer learning approaches that fine-tune a pretrained deep
neural network on a target task. We study generalization properties of
fine-tuning to understand the problem of overfitting, which commonly occurs in
practice. Previous works have shown that constraining the distance from the
initialization of fine-tuning improves generalization. Using a PAC-Bayesian
analysis, we observe that besides distance from initialization, Hessians affect
generalization through the noise stability of deep neural networks against
noise injections. Motivated by the observation, we develop Hessian
distance-based generalization bounds for a wide range of fine-tuning methods.
Additionally, we study the robustness of fine-tuning in the presence of noisy
labels. Motivated by our theory, we design an algorithm that incorporates
consistent losses and distance-based regularization for fine-tuning, along with
a generalization error guarantee under class conditional independent noise in
the training set labels. We perform a detailed empirical study of our algorithm
on various noisy environments and architectures. On six image classification
tasks whose training labels are generated with programmatic labeling, we find a
3.26% accuracy gain over prior fine-tuning methods. Meanwhile, the Hessian
distance measure of the fine-tuned model decreases by six times more than
existing approaches."
7198,"More broadly, we hope our work
inspires further study on the interplay between second-order gradients and generalization.","Second, it would be interesting to
apply Hessians to understand generalization in other black-box models.","12
Acknowledgement

Thanks to the anonymous referees for their constructive feedback.",2022-06-06 14:52:46+00:00,Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees,cs.LG,"['cs.LG', 'cs.CV', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Haotian Ju'), arxiv.Result.Author('Dongyue Li'), arxiv.Result.Author('Hongyang R. Zhang')]","We consider transfer learning approaches that fine-tune a pretrained deep
neural network on a target task. We study generalization properties of
fine-tuning to understand the problem of overfitting, which commonly occurs in
practice. Previous works have shown that constraining the distance from the
initialization of fine-tuning improves generalization. Using a PAC-Bayesian
analysis, we observe that besides distance from initialization, Hessians affect
generalization through the noise stability of deep neural networks against
noise injections. Motivated by the observation, we develop Hessian
distance-based generalization bounds for a wide range of fine-tuning methods.
Additionally, we study the robustness of fine-tuning in the presence of noisy
labels. Motivated by our theory, we design an algorithm that incorporates
consistent losses and distance-based regularization for fine-tuning, along with
a generalization error guarantee under class conditional independent noise in
the training set labels. We perform a detailed empirical study of our algorithm
on various noisy environments and architectures. On six image classification
tasks whose training labels are generated with programmatic labeling, we find a
3.26% accuracy gain over prior fine-tuning methods. Meanwhile, the Hessian
distance measure of the fine-tuned model decreases by six times more than
existing approaches."
7200,"Lower bounds for both metrics are established to demonstrate the optimality of our
algorithms with respect to the number of episodes K.

We further study an interesting limiting case of Iterated CVaR RL when α approaches 0, called Worst
Path RL, where the decision maker is extremely risk-adverse and concerns the worst case (e.g., in
autonomous driving Wen et al.","We also develop novel analytical techniques, to bound the
change of CVaR due to the value function shift and decompose the regret via a distorted visitation
distribution.",[2020] and clinical treatment planning Coronato et al.,2022-06-06 15:24:06+00:00,Risk-Sensitive Reinforcement Learning: Iterated CVaR and the Worst Path,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yihan Du'), arxiv.Result.Author('Siwei Wang'), arxiv.Result.Author('Longbo Huang')]","In this paper, we study a novel episodic risk-sensitive Reinforcement
Learning (RL) problem, named Iterated CVaR RL, where the objective is to
maximize the tail of the reward-to-go at each step. Different from existing
risk-aware RL formulations, Iterated CVaR RL focuses on safety-at-all-time, by
enabling the agent to tightly control the risk of getting into catastrophic
situations at each stage, and is applicable to important risk-sensitive tasks
that demand strong safety guarantees throughout the decision process, such as
autonomous driving, clinical treatment planning and robotics. We investigate
Iterated CVaR RL with two performance metrics, i.e., Regret Minimization and
Best Policy Identification. For both metrics, we design efficient algorithms
ICVaR-RM and ICVaR-BPI, respectively, and provide matching upper and lower
bounds with respect to the number of episodes $K$. We also investigate an
interesting limiting case of Iterated CVaR RL, called Worst Path RL, where the
objective becomes to maximize the minimum possible cumulative reward, and
propose an efficient algorithm with constant upper and lower bounds. Finally,
the techniques we develop for bounding the change of CVaR due to the value
function shift and decomposing the regret via a distorted visitation
distribution are novel and can find applications in other risk-sensitive online
learning problems."
7207,"We further study the
impact of the OOD loss and ood on the performance and the value estimation.","In the above analysis, we know the OOD loss is a key component in RORL.","As shown in Figure 7
(a), when ood = 0, the performance of RORL drops signiﬁcantly, which illustrates the effectiveness
of underestimating values of OOD states since the smoothness of RORL may overestimates these
values.",2022-06-06 18:07:41+00:00,RORL: Robust Offline Reinforcement Learning via Conservative Smoothing,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Rui Yang'), arxiv.Result.Author('Chenjia Bai'), arxiv.Result.Author('Xiaoteng Ma'), arxiv.Result.Author('Zhaoran Wang'), arxiv.Result.Author('Chongjie Zhang'), arxiv.Result.Author('Lei Han')]","Offline reinforcement learning (RL) provides a promising direction to exploit
the massive amount of offline data for complex decision-making tasks. Due to
the distribution shift issue, current offline RL algorithms are generally
designed to be conservative for value estimation and action selection. However,
such conservatism impairs the robustness of learned policies, leading to a
significant change even for a small perturbation on observations. To trade off
robustness and conservatism, we propose Robust Offline Reinforcement Learning
(RORL) with a novel conservative smoothing technique. In RORL, we explicitly
introduce regularization on the policy and the value function for states near
the dataset and additional conservative value estimation on these OOD states.
Theoretically, we show RORL enjoys a tighter suboptimality bound than recent
theoretical results in linear MDPs. We demonstrate that RORL can achieve the
state-of-the-art performance on the general offline RL benchmark and is
considerably robust to adversarial observation perturbation."
7208,"We further study the
impact of the OOD loss and ood on the performance and the value estimation.","In the above analysis, we know that the OOD loss is a key component in RORL.","As shown in Figure 9
(a), when ood = 0, the performance of RORL drops signiﬁcantly, which illustrates the effectiveness
of underestimating values of OOD states since the smoothness of RORL may overestimate these
values.",2022-06-06 18:07:41+00:00,RORL: Robust Offline Reinforcement Learning via Conservative Smoothing,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Rui Yang'), arxiv.Result.Author('Chenjia Bai'), arxiv.Result.Author('Xiaoteng Ma'), arxiv.Result.Author('Zhaoran Wang'), arxiv.Result.Author('Chongjie Zhang'), arxiv.Result.Author('Lei Han')]","Offline reinforcement learning (RL) provides a promising direction to exploit
the massive amount of offline data for complex decision-making tasks. Due to
the distribution shift issue, current offline RL algorithms are generally
designed to be conservative in value estimation and action selection. However,
such conservatism can impair the robustness of learned policies when
encountering observation deviation under realistic conditions, such as sensor
errors and adversarial attacks. To trade off robustness and conservatism, we
propose Robust Offline Reinforcement Learning (RORL) with a novel conservative
smoothing technique. In RORL, we explicitly introduce regularization on the
policy and the value function for states near the dataset, as well as
additional conservative value estimation on these OOD states. Theoretically, we
show RORL enjoys a tighter suboptimality bound than recent theoretical results
in linear MDPs. We demonstrate that RORL can achieve state-of-the-art
performance on the general offline RL benchmark and is considerably robust to
adversarial observation perturbations."
7209,"We further study the
impact of the OOD loss and ood on the performance and the value estimation.","In the above analysis, we know that the OOD loss is a key component in RORL.","As shown in Figure 9
(a), when ood = 0, the performance of RORL drops signiﬁcantly, which illustrates the effectiveness
of underestimating values of OOD states since the smoothness of RORL may overestimate these
values.",2022-06-06 18:07:41+00:00,RORL: Robust Offline Reinforcement Learning via Conservative Smoothing,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Rui Yang'), arxiv.Result.Author('Chenjia Bai'), arxiv.Result.Author('Xiaoteng Ma'), arxiv.Result.Author('Zhaoran Wang'), arxiv.Result.Author('Chongjie Zhang'), arxiv.Result.Author('Lei Han')]","Offline reinforcement learning (RL) provides a promising direction to exploit
massive amount of offline data for complex decision-making tasks. Due to the
distribution shift issue, current offline RL algorithms are generally designed
to be conservative in value estimation and action selection. However, such
conservatism can impair the robustness of learned policies when encountering
observation deviation under realistic conditions, such as sensor errors and
adversarial attacks. To trade off robustness and conservatism, we propose
Robust Offline Reinforcement Learning (RORL) with a novel conservative
smoothing technique. In RORL, we explicitly introduce regularization on the
policy and the value function for states near the dataset, as well as
additional conservative value estimation on these states. Theoretically, we
show RORL enjoys a tighter suboptimality bound than recent theoretical results
in linear MDPs. We demonstrate that RORL can achieve state-of-the-art
performance on the general offline RL benchmark and is considerably robust to
adversarial observation perturbations."
7210,"The slot-
      will help further research in this direction.1 2                   attention module [Locatello et al., 2020] can learn represen-
                                                                         tations from sets of different cardinality.",number of set elements).,"It is important to
    1All the code used to produce the results and conﬁg ﬁles are         note that in principle the size of the input set does not need
available at anonymous.4open.science/r/spg-experiments/                  to be known in advance, but in practice only ﬁxed-size prob-
                                                                         lems have utilized this algorithm (for computational and im-
    2For videos of the agents performing in the different scenar-        plementation reasons).",2022-06-06 19:02:39+00:00,Efficient entity-based reinforcement learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Vince Jankovics'), arxiv.Result.Author('Michael Garcia Ortiz'), arxiv.Result.Author('Eduardo Alonso')]","Recent deep reinforcement learning (DRL) successes rely on end-to-end
learning from fixed-size observational inputs (e.g. image, state-variables).
However, many challenging and interesting problems in decision making involve
observations or intermediary representations which are best described as a set
of entities: either the image-based approach would miss small but important
details in the observations (e.g. ojects on a radar, vehicles on satellite
images, etc.), the number of sensed objects is not fixed (e.g. robotic
manipulation), or the problem simply cannot be represented in a meaningful way
as an image (e.g. power grid control, or logistics). This type of structured
representations is not directly compatible with current DRL architectures,
however, there has been an increase in machine learning techniques directly
targeting structured information, potentially addressing this issue. We propose
to combine recent advances in set representations with slot attention and graph
neural networks to process structured data, broadening the range of
applications of DRL algorithms. This approach allows to address entity-based
problems in an efficient and scalable way. We show that it can improve training
time and robustness significantly, and demonstrate their potential to handle
structured as well as purely visual domains, on multiple environments from the
Atari Learning Environment and Simple Playgrounds."
7222,"In total, 62 locations were thereby iden-
Accepted: date                           tified as suitable locations with characteristic soundscapes for further research utilizing the ISO
Published: date                          12913-2 perceptual quadrants.",Received: date                           Weights hence acted as proxies for participant confidence.,"Audio-visual recordings and acoustic characterization of the sound-
                                         scapes will be made in a future study.",2022-06-07 08:45:17+00:00,Singapore Soundscape Site Selection Survey (S5): Identification of Characteristic Soundscapes of Singapore via Weighted k-means Clustering,cs.LG,"['cs.LG', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Kenneth Ooi'), arxiv.Result.Author('Bhan Lam'), arxiv.Result.Author('Joo Young Hong'), arxiv.Result.Author('Karn N. Watcharasupat'), arxiv.Result.Author('Zhen-Ting Ong'), arxiv.Result.Author('Woon-Seng Gan')]","The ecological validity of soundscape studies usually rests on a choice of
soundscapes that are representative of the perceptual space under
investigation. For example, a soundscape pleasantness study might investigate
locations with soundscapes ranging from ""pleasant"" to ""annoying"". The choice of
soundscapes is typically researcher-led, but a participant-led process can
reduce selection bias and improve result reliability. Hence, we propose a
robust participant-led method to pinpoint characteristic soundscapes possessing
arbitrary perceptual attributes. We validate our method by identifying
Singaporean soundscapes spanning the perceptual quadrants generated from the
""Pleasantness"" and ""Eventfulness"" axes of the ISO 12913-2 circumplex model of
soundscape perception, as perceived by local experts. From memory and
experience, 67 participants first selected locations corresponding to each
perceptual quadrant in each major planning region of Singapore. We then
performed weighted k-means clustering on the selected locations, with weights
for each location derived from previous frequencies and durations spent in each
location by each participant. Weights hence acted as proxies for participant
confidence. In total, 62 locations were thereby identified as suitable
locations with characteristic soundscapes for further research utilizing the
ISO 12913-2 perceptual quadrants. Audio-visual recordings and acoustic
characterization of the soundscapes will be made in a future study."
7230,"Nonetheless, other factors might further accentuate the
difference between these two algorithms during training, leaving the door open for further research
regarding the beneﬁts of adaptive optimization methods with Transformers.","Finally, we gave preliminary evidence that one of the factors contributing to the higher efﬁcacy
of Adam compared to SGD in training Transformers arises from the disproportionate magnitude
of gradients as postulated by our theory.","Acknowledgements

We thank our colleague Jonas Kohler who provided insights and expertise that greatly assisted the
research.",2022-06-07 09:07:24+00:00,Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse,cs.LG,['cs.LG'],"[arxiv.Result.Author('Lorenzo Noci'), arxiv.Result.Author('Sotiris Anagnostidis'), arxiv.Result.Author('Luca Biggio'), arxiv.Result.Author('Antonio Orvieto'), arxiv.Result.Author('Sidak Pal Singh'), arxiv.Result.Author('Aurelien Lucchi')]","Transformers have achieved remarkable success in several domains, ranging
from natural language processing to computer vision. Nevertheless, it has been
recently shown that stacking self-attention layers - the distinctive
architectural component of Transformers - can result in rank collapse of the
tokens' representations at initialization. The question of if and how rank
collapse affects training is still largely unanswered, and its investigation is
necessary for a more comprehensive understanding of this architecture. In this
work, we shed new light on the causes and the effects of this phenomenon.
First, we show that rank collapse of the tokens' representations hinders
training by causing the gradients of the queries and keys to vanish at
initialization. Furthermore, we provide a thorough description of the origin of
rank collapse and discuss how to prevent it via an appropriate depth-dependent
scaling of the residual branches. Finally, our analysis unveils that specific
architectural hyperparameters affect the gradients of queries and values
differently, leading to disproportionate gradient norms. This suggests an
explanation for the widespread use of adaptive methods for Transformers'
optimization."
7238,"This points to the need for further research
on the temporal aspects of cardiac behaviour and how this may be exploited
in cardiac event prediction.","Moreover, most work to date has focused on VT-VF
detection rather than prediction.","ICDs can send their data wirelessly to a central server or could-based
infrastructure, making it easier to gather a large database to have different
patients with whole different type of arrhythmias.",2022-06-07 12:14:05+00:00,Improved Cardiac Arrhythmia Prediction Based on Heart Rate Variability Analysis,cs.LG,"['cs.LG', 'eess.SP']",[arxiv.Result.Author('Ashkan Parsi')],"Many types of ventricular and atrial cardiac arrhythmias have been discovered
in clinical practice in the past 100 years, and these arrhythmias are a major
contributor to sudden cardiac death. Ventricular tachycardia, ventricular
fibrillation, and paroxysmal atrial fibrillation are the most
commonly-occurring and dangerous arrhythmias, therefore early detection is
crucial to prevent any further complications and reduce fatalities. Implantable
devices such as pacemakers are commonly used in patients at high risk of sudden
cardiac death. While great advances have been made in medical technology, there
remain significant challenges in effective management of common arrhythmias.
This thesis proposes novel arrhythmia detection and prediction methods to
differentiate cardiac arrhythmias from non-life-threatening cardiac events, to
increase the likelihood of detecting events that may lead to mortality, as well
as reduce the incidence of unnecessary therapeutic intervention. The methods
are based on detailed analysis of Heart Rate Variability (HRV) information. The
results of the work show good performance of the proposed methods and support
the potential for their deployment in resource-constrained devices for
ventricular and atrial arrhythmia prediction, such as implantable pacemakers
and defibrillators."
7246,The second group typically trains        future directions that are worth further research efforts.,"The first group aims to re-balance the data distribu-     cluding its motivation, strengths and limitations, and as well as the
tion before training the model.","Recent Advances in Bayesian Optimization                                                                                               Conference’17, July 2017, Washington, DC, USA

REFERENCES                                                                                  [25] Ji Cheng, Ping Jiang, Qi Zhou, Jiexiang Hu, and Leshi Shu.",2022-06-07 13:47:00+00:00,Recent Advances in Bayesian Optimization,cs.LG,"['cs.LG', 'cs.DC', 'cs.NE', 'math.OC']","[arxiv.Result.Author('Xilu Wang'), arxiv.Result.Author('Yaochu Jin'), arxiv.Result.Author('Sebastian Schmitt'), arxiv.Result.Author('Markus Olhofer')]","Bayesian optimization has emerged at the forefront of expensive black-box
optimization due to its data efficiency. Recent years have witnessed a
proliferation of studies on the development of new Bayesian optimization
algorithms and their applications. Hence, this paper attempts to provide a
comprehensive and updated survey of recent advances in Bayesian optimization
and identify interesting open problems. We categorize the existing work on
Bayesian optimization into nine main groups according to the motivations and
focus of the proposed algorithms. For each category, we present the main
advances with respect to the construction of surrogate models and adaptation of
the acquisition functions. Finally, we discuss the open questions and suggest
promising future research directions, in particular with regard to
heterogeneity, privacy preservation, and fairness in distributed and federated
optimization systems."
7247,future directions that are worth further research efforts.,"Advances in Neural Information
cluding its motivation, strengths and limitations, and as well as the                   Processing Systems 24 (2011).","[19] James Bergstra, Dan Yamins, David D Cox, et al.",2022-06-07 13:47:00+00:00,Recent Advances in Bayesian Optimization,cs.LG,"['cs.LG', 'cs.DC', 'cs.NE', 'math.OC']","[arxiv.Result.Author('Xilu Wang'), arxiv.Result.Author('Yaochu Jin'), arxiv.Result.Author('Sebastian Schmitt'), arxiv.Result.Author('Markus Olhofer')]","Bayesian optimization has emerged at the forefront of expensive black-box
optimization due to its data efficiency. Recent years have witnessed a
proliferation of studies on the development of new Bayesian optimization
algorithms and their applications. Hence, this paper attempts to provide a
comprehensive and updated survey of recent advances in Bayesian optimization
and identify interesting open problems. We categorize the existing work on
Bayesian optimization into nine main groups according to the motivations and
focus of the proposed algorithms. For each category, we present the main
advances with respect to the construction of surrogate models and adaptation of
the acquisition functions. Finally, we discuss the open questions and suggest
promising future research directions, in particular with regard to
heterogeneity, privacy preservation, and fairness in distributed and federated
optimization systems."
7262,"These results merit
further study of bias-robustness, and suggest that our technique may be valuable for ML practitioners
wishing to explore limitations of their models.","We evaluated our approaches on three commonly-used
benchmark datasets, showing that (i) our approaches can certify bias robustness and (ii) there are
commonly gaps in bias-robustness, particularly for smaller demographic groups.",Limitations.,2022-06-07 20:47:07+00:00,Certifying Data-Bias Robustness in Linear Regression,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anna P. Meyer'), arxiv.Result.Author('Aws Albarghouthi'), arxiv.Result.Author(""Loris D'Antoni"")]","Datasets typically contain inaccuracies due to human error and societal
biases, and these inaccuracies can affect the outcomes of models trained on
such datasets. We present a technique for certifying whether linear regression
models are pointwise-robust to label bias in the training dataset, i.e.,
whether bounded perturbations to the labels of a training dataset result in
models that change the prediction of test points. We show how to solve this
problem exactly for individual test points, and provide an approximate but more
scalable method that does not require advance knowledge of the test point. We
extensively evaluate both techniques and find that linear models -- both
regression- and classification-based -- often display high levels of
bias-robustness. However, we also unearth gaps in bias-robustness, such as high
levels of non-robustness for certain bias assumptions on some datasets.
Overall, our approach can serve as a guide for when to trust, or question, a
model's output."
7268,"Across the
three datasets, the σs on SST-2 are much larger (at dozen-scales) than those on the image dataset
(6.33 ∼ 11.02), which are larger than those on the graph dataset (3.06 ∼ 6.26), leaving room for
further research to understand this difference and improve the fairness in various application domains.","An exception is Ditto-based methods on
SST-2, as the parameter regularization in Ditto may fail for the complex BERT model.","8
Table 3: Fairness results on FEMNIST, SST-2, PUBMED datasets.",2022-06-08 02:51:59+00:00,pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Daoyuan Chen'), arxiv.Result.Author('Dawei Gao'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding')]","Personalized Federated Learning (pFL), which utilizes and deploys distinct
local models, has gained increasing attention in recent years due to its
success in handling the statistical heterogeneity of FL clients. However,
standardized evaluation and systematical analysis of diverse pFL methods remain
a challenge. Firstly, the highly varied datasets, FL simulation settings and
pFL implementations prevent fast and fair comparisons of pFL methods. Secondly,
the effectiveness and robustness of pFL methods are under-explored in various
practical scenarios, such as new clients generalization and resource-limited
clients participation. Finally, the current pFL literature diverges in the
adopted evaluation and ablation protocols. To tackle these challenges, we
propose the first comprehensive pFL benchmark, pFL-Bench, for facilitating
rapid, reproducible, standardized and thorough pFL evaluation. The proposed
benchmark contains more than 10 datasets in diverse application domains with
unified data partition and realistic heterogeneous settings; a modular and
easy-to-extend pFL codebase with more than 20 competitive pFL baseline
implementations; and systematic evaluations under containerized environments in
terms of generalization, fairness, system overhead, and convergence. We
highlight the benefits and potential of state-of-the-art pFL methods and hope
pFL-Bench enables further pFL research and broad applications that would
otherwise be difficult owing to the absence of a dedicated benchmark. The code
is released at
https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench."
7269,"Across the
three datasets, the σs on SST-2 are much larger (at dozen-scales) than those on the image dataset
(6.33 ∼ 11.02), which are larger than those on the graph dataset (3.06 ∼ 6.26), leaving room for
further research to understand this difference and improve the fairness in various application domains.","An exception is Ditto-based methods on
SST-2, as the parameter regularization in Ditto may fail for the complex BERT model.","8
Table 3: Fairness results on FEMNIST, SST-2, PUBMED datasets.",2022-06-08 02:51:59+00:00,pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Daoyuan Chen'), arxiv.Result.Author('Dawei Gao'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding')]","Personalized Federated Learning (pFL), which utilizes and deploys distinct
local models, has gained increasing attention in recent years due to its
success in handling the statistical heterogeneity of FL clients. However,
standardized evaluation and systematical analysis of diverse pFL methods remain
a challenge. Firstly, the highly varied datasets, FL simulation settings and
pFL implementations prevent fast and fair comparisons of pFL methods. Secondly,
the effectiveness and robustness of pFL methods are under-explored in various
practical scenarios, such as new clients generalization and resource-limited
clients participation. Finally, the current pFL literature diverges in the
adopted evaluation and ablation protocols. To tackle these challenges, we
propose the first comprehensive pFL benchmark, pFL-Bench, for facilitating
rapid, reproducible, standardized and thorough pFL evaluation. The proposed
benchmark contains more than 10 datasets in diverse application domains with
unified data partition and realistic heterogeneous settings; a modular and
easy-to-extend pFL codebase with more than 20 competitive pFL baseline
implementations; and systematic evaluations under containerized environments in
terms of generalization, fairness, system overhead, and convergence. We
highlight the benefits and potential of state-of-the-art pFL methods and hope
pFL-Bench enables further pFL research and broad applications that would
otherwise be difficult owing to the absence of a dedicated benchmark. The code
is released at
https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench."
7270,"Across the three datasets,
the σs on SST-2 are much larger (at dozen-scales) than those on the image dataset (6.33 ∼ 11.02),
which are larger than those on the graph dataset (3.06 ∼ 6.26), leaving room for further research to
understand this difference and improve the fairness in various application domains.","We ﬁnd that Acc is usually smaller than the one weighted by local data
size (Acc in Table 2), indicating the client bias in existing pFL evaluation.","Interestingly,
compared with FedAvg, pFL methods can effectively improve bottom accuracy, while they may
gain larger standard deviations.",2022-06-08 02:51:59+00:00,pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Daoyuan Chen'), arxiv.Result.Author('Dawei Gao'), arxiv.Result.Author('Weirui Kuang'), arxiv.Result.Author('Yaliang Li'), arxiv.Result.Author('Bolin Ding')]","Personalized Federated Learning (pFL), which utilizes and deploys distinct
local models, has gained increasing attention in recent years due to its
success in handling the statistical heterogeneity of FL clients. However,
standardized evaluation and systematical analysis of diverse pFL methods remain
a challenge. Firstly, the highly varied datasets, FL simulation settings and
pFL implementations prevent easy and fair comparisons of pFL methods. Secondly,
the current pFL literature diverges in the adopted evaluation and ablation
protocols. Finally, the effectiveness and robustness of pFL methods are
under-explored in various practical scenarios, such as the generalization to
new clients and the participation of resource-limited clients. To tackle these
challenges, we propose the first comprehensive pFL benchmark, pFL-Bench, for
facilitating rapid, reproducible, standardized and thorough pFL evaluation. The
proposed benchmark contains more than 10 dataset variants in various
application domains with a unified data partition and realistic heterogeneous
settings; a modularized and easy-to-extend pFL codebase with more than 20
competitive pFL method implementations; and systematic evaluations under
containerized environments in terms of generalization, fairness, system
overhead, and convergence. We highlight the benefits and potential of
state-of-the-art pFL methods and hope the pFL-Bench enables further pFL
research and broad applications that would otherwise be difficult owing to the
absence of a dedicated benchmark. The code is released at
https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench."
7276,"Overall, the proposed approach is simple, scal-
                                                  able, and likely to be beneﬁted from further research on submodular optimization.","We show it is possible to write the induced objective function for the subproblem as
                                                  a difference of two submodular (DS) functions to make it approximately solvable
                                                  by DS optimization algorithms.",Experiments on real datasets demonstrate the effectiveness of our method.,2022-06-08 07:41:47+00:00,Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Fan Yang'), arxiv.Result.Author('Kai He'), arxiv.Result.Author('Linxiao Yang'), arxiv.Result.Author('Hongxia Du'), arxiv.Result.Author('Jingbang Yang'), arxiv.Result.Author('Bo Yang'), arxiv.Result.Author('Liang Sun')]","Rule sets are highly interpretable logical models in which the predicates for
decision are expressed in disjunctive normal form (DNF, OR-of-ANDs), or,
equivalently, the overall model comprises an unordered collection of if-then
decision rules. In this paper, we consider a submodular optimization based
approach for learning rule sets. The learning problem is framed as a subset
selection task in which a subset of all possible rules needs to be selected to
form an accurate and interpretable rule set. We employ an objective function
that exhibits submodularity and thus is amenable to submodular optimization
techniques. To overcome the difficulty arose from dealing with the
exponential-sized ground set of rules, the subproblem of searching a rule is
casted as another subset selection task that asks for a subset of features. We
show it is possible to write the induced objective function for the subproblem
as a difference of two submodular (DS) functions to make it approximately
solvable by DS optimization algorithms. Overall, the proposed approach is
simple, scalable, and likely to be benefited from further research on
submodular optimization. Experiments on real datasets demonstrate the
effectiveness of our method."
7279,"The paper
concludes with ﬁnal remarks and suggestions for further research in Section 7.",Results are presented in Section 6.,"2 Related Work and Contribution

The analysis and improvement of ED operations has been addressed by many researchers in the past decades.",2022-06-08 08:56:52+00:00,Machine learning-based patient selection in an emergency department,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Nikolaus Furian'), arxiv.Result.Author(""Michael O'Sullivan""), arxiv.Result.Author('Cameron Walker'), arxiv.Result.Author('Melanie Reuter-Oppermann')]","The performance of Emergency Departments (EDs) is of great importance for any
health care system, as they serve as the entry point for many patients.
However, among other factors, the variability of patient acuity levels and
corresponding treatment requirements of patients visiting EDs imposes
significant challenges on decision makers. Balancing waiting times of patients
to be first seen by a physician with the overall length of stay over all acuity
levels is crucial to maintain an acceptable level of operational performance
for all patients. To address those requirements when assigning idle resources
to patients, several methods have been proposed in the past, including the
Accumulated Priority Queuing (APQ) method. The APQ method linearly assigns
priority scores to patients with respect to their time in the system and acuity
level. Hence, selection decisions are based on a simple system representation
that is used as an input for a selection function. This paper investigates the
potential of an Machine Learning (ML) based patient selection method. It
assumes that for a large set of training data, including a multitude of
different system states, (near) optimal assignments can be computed by a
(heuristic) optimizer, with respect to a chosen performance metric, and aims to
imitate such optimal behavior when applied to new situations. Thereby, it
incorporates a comprehensive state representation of the system and a complex
non-linear selection function. The motivation for the proposed approach is that
high quality selection decisions may depend on a variety of factors describing
the current state of the ED, not limited to waiting times, which can be
captured and utilized by the ML model. Results show that the proposed method
significantly outperforms the APQ method for a majority of evaluated settings"
7296,"In
particular, an interesting direction of further research is to investigate where these minority subpopulations
that are worse-aﬀected by private training intersects with the subpopulations that are relevant for the speciﬁc
domain.","This indicates that, irrespective of sizes,
private training hurts fairness disproportionately more for certain subpopulations compared to others.","While most past works [7, 14] have also used sizes of subpopulations to diﬀerentiate between
disparately impacted subpopulations, this suggests that that is not always the case.",2022-06-08 16:03:44+00:00,How unfair is private learning ?,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Amartya Sanyal'), arxiv.Result.Author('Yaxi Hu'), arxiv.Result.Author('Fanny Yang')]","As machine learning algorithms are deployed on sensitive data in critical
decision making processes, it is becoming increasingly important that they are
also private and fair. In this paper, we show that, when the data has a
long-tailed structure, it is not possible to build accurate learning algorithms
that are both private and results in higher accuracy on minority
subpopulations. We further show that relaxing overall accuracy can lead to good
fairness even with strict privacy requirements. To corroborate our theoretical
results in practice, we provide an extensive set of experimental results using
a variety of synthetic, vision~(\cifar and CelebA), and tabular~(Law School)
datasets and learning algorithms."
7297,This begs further research to develop fair and private algorithms that are closer to the pareto optimal frontier.,"It is possible
that in some real-world datasets there are fair and private algorithms that achieve a more optimistic trade-oﬀ.","13
5 Acknowledgements

AS is partially supported by the ETH AI Center postdoctoral fellowship.",2022-06-08 16:03:44+00:00,How unfair is private learning ?,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Amartya Sanyal'), arxiv.Result.Author('Yaxi Hu'), arxiv.Result.Author('Fanny Yang')]","As machine learning algorithms are deployed on sensitive data in critical
decision making processes, it is becoming increasingly important that they are
also private and fair. In this paper, we show that, when the data has a
long-tailed structure, it is not possible to build accurate learning algorithms
that are both private and results in higher accuracy on minority
subpopulations. We further show that relaxing overall accuracy can lead to good
fairness even with strict privacy requirements. To corroborate our theoretical
results in practice, we provide an extensive set of experimental results using
a variety of synthetic, vision~(\cifar and CelebA), and tabular~(Law School)
datasets and learning algorithms."
7298,"In particular, an interesting direction of further research is to investigate where these
minority subpopulations that are worse-aﬀected by private training intersects with the subpopulations that
are relevant for the speciﬁc domain.","This indicates
that, irrespective of sizes, private training hurts fairness disproportionately more for certain subpopulations
compared to others.","While most past works [5, 12] have also used sizes of subpopulations to
diﬀerentiate between disparately impacted subpopulations, this suggests that that is not always the case.",2022-06-08 16:03:44+00:00,How unfair is private learning ?,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Amartya Sanyal'), arxiv.Result.Author('Yaxi Hu'), arxiv.Result.Author('Fanny Yang')]","As machine learning algorithms are deployed on sensitive data in critical
decision making processes, it is becoming increasingly important that they are
also private and fair. In this paper, we show that, when the data has a
long-tailed structure, it is not possible to build accurate learning algorithms
that are both private and results in higher accuracy on minority
subpopulations. We further show that relaxing overall accuracy can lead to good
fairness even with strict privacy requirements. To corroborate our theoretical
results in practice, we provide an extensive set of experimental results using
a variety of synthetic, vision (CIFAR10 and CelebA), and tabular (Law School)
datasets and learning algorithms."
7299,This begs further research to develop fair and private algorithms that are closer to the pareto optimal frontier.,"It is possible
that in some real-world datasets there are fair and private algorithms that achieve a more optimistic trade-oﬀ.","5 Acknowledgements

AS is supported by the ETH AI Center and Hasler Stiftung.",2022-06-08 16:03:44+00:00,How unfair is private learning ?,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Amartya Sanyal'), arxiv.Result.Author('Yaxi Hu'), arxiv.Result.Author('Fanny Yang')]","As machine learning algorithms are deployed on sensitive data in critical
decision making processes, it is becoming increasingly important that they are
also private and fair. In this paper, we show that, when the data has a
long-tailed structure, it is not possible to build accurate learning algorithms
that are both private and results in higher accuracy on minority
subpopulations. We further show that relaxing overall accuracy can lead to good
fairness even with strict privacy requirements. To corroborate our theoretical
results in practice, we provide an extensive set of experimental results using
a variety of synthetic, vision (CIFAR10 and CelebA), and tabular (Law School)
datasets and learning algorithms."
7304,"Finally, we conclude by discussing the avenues
                                             for further research and propose potential research problems.","We hope that our analysis presents a
                                             multifaceted perspective on modelling NC and aids in forming connections with the gen-
                                             eralization capabilities of neural networks.","Keywords: Neural Collapse, Neural Networks, Simplex ETF, Local Elasticity, Stochastic
                                             Diﬀerential Equations, Generalization, Optimization, Transfer Learning.",2022-06-08 17:55:28+00:00,Neural Collapse: A Review on Modelling Principles and Generalization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Vignesh Kothapalli'), arxiv.Result.Author('Ebrahim Rasromani'), arxiv.Result.Author('Vasudev Awatramani')]","With a recent observation of the ""Neural Collapse (NC)"" phenomena by Papyan
et al., various efforts have been made to model it and analyse the
implications. Neural collapse describes that in deep classifier networks, the
class features of the final hidden layer associated with training data tend to
collapse to the respective class feature means. Thus, simplifying the behaviour
of the last layer classifier to that of a nearest-class center decision rule.
In this work, we analyse the principles which aid in modelling such a phenomena
from the ground up and show how they can build a common understanding of the
recently proposed models that try to explain NC. We hope that our analysis
presents a multifaceted perspective on modelling NC and aids in forming
connections with the generalization capabilities of neural networks. Finally,
we conclude by discussing the avenues for further research and propose
potential research problems."
7328,"There are limited studies on both data transmission
forwarded to the hospital database through technologies                    accuracy and efficiency, which needs further research.","Health data are              medical image processing from gathered medical data [2 -
collected from patients’ wearable sensor devices and                       11].","supporting cellular networks, and then transmitted to cloud                Author, James Kang’s studies [12 - 16] have focused on
storage systems.",2022-06-09 02:57:42+00:00,Enhancement of Healthcare Data Transmission using the Levenberg-Marquardt Algorithm,cs.LG,"['cs.LG', 'eess.SP', 'J.3']","[arxiv.Result.Author('Angela An'), arxiv.Result.Author('James Jin Kang')]","In the healthcare system, patients are required to use wearable devices for
the remote data collection and real-time monitoring of health data and the
status of health conditions. This adoption of wearables results in a
significant increase in the volume of data that is collected and transmitted.
As the devices are run by small battery power, they can be quickly diminished
due to the high processing requirements of the device for data collection and
transmission. Given the importance attached to medical data, it is imperative
that all transmitted data adhere to strict integrity and availability
requirements. Reducing the volume of healthcare data and the frequency of
transmission will improve the device battery life via using inference
algorithm. There is an issue of improving transmission metrics with accuracy
and efficiency, which trade-off each other such as increasing accuracy reduces
the efficiency. This paper demonstrates that machine learning can be used to
analyze complex health data metrics such as the accuracy and efficiency of data
transmission to overcome the trade-off problem using the Levenberg-Marquardt
algorithm to enhance both metrics by taking fewer samples to transmit whilst
maintaining the accuracy. The algorithm is tested with a standard heart rate
dataset to compare the metrics. The result shows that the LMA has best
performed with an efficiency of 3.33 times for reduced sample data size and
accuracy of 79.17%, which has the similar accuracies in 7 different sampling
cases adopted for testing but demonstrates improved efficiency. These proposed
methods significantly improved both metrics using machine learning without
sacrificing a metric over the other compared to the existing methods with high
efficiency."
7329,"We think that such a distributional perspective
of adversarial noises calls for further study to understand the diﬃculty of adversarial learning or to improve
the current adversarial training algorithms.","This may partially answer why adversarial training is not that eﬃcient for learning original features, which
usually leads to deteriorated performance on clean test data.","References

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.",2022-06-09 07:26:46+00:00,Adversarial Noises Are Linearly Separable for (Nearly) Random Neural Networks,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Huishuai Zhang'), arxiv.Result.Author('Da Yu'), arxiv.Result.Author('Yiping Lu'), arxiv.Result.Author('Di He')]","Adversarial examples, which are usually generated for specific inputs with a
specific model, are ubiquitous for neural networks. In this paper we unveil a
surprising property of adversarial noises when they are put together, i.e.,
adversarial noises crafted by one-step gradient methods are linearly separable
if equipped with the corresponding labels. We theoretically prove this property
for a two-layer network with randomly initialized entries and the neural
tangent kernel setup where the parameters are not far from initialization. The
proof idea is to show the label information can be efficiently backpropagated
to the input while keeping the linear separability. Our theory and experimental
evidence further show that the linear classifier trained with the adversarial
noises of the training data can well classify the adversarial noises of the
test data, indicating that adversarial noises actually inject a distributional
perturbation to the original data distribution. Furthermore, we empirically
demonstrate that the adversarial noises may become less linearly separable when
the above conditions are compromised while they are still much easier to
classify than original features."
7332,"To further study the impact of statistical heterogeneity on the performance, we follow prior
works [Zhu et al., 2021] and simulate Non-IID data on MNIST [LeCun et al., 1998] dataset via
Dirichlet sampling Dir(α), where a smaller value of α denotes greater heterogeneity (see Figure 5
in Appendix B).","The HAR dataset consists of sensor
data (ﬂattened into an 1152-valued vector) generated by users performing six possible actions (i.e.,
classes).","We employ the VGG9 and multilayer perceptron (MLP) for the image classiﬁcation
and activity recognition tasks, respectively, with model conﬁgurations (see Table 5 in Appendix C).",2022-06-09 09:55:31+00:00,HideNseek: Federated Lottery Ticket via Server-side Pruning and Sign Supermask,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anish K. Vallapuram'), arxiv.Result.Author('Pengyuan Zhou'), arxiv.Result.Author('Young D. Kwon'), arxiv.Result.Author('Lik Hang Lee'), arxiv.Result.Author('Hengwei Xu'), arxiv.Result.Author('Pan Hui')]","Federated learning alleviates the privacy risk in distributed learning by
transmitting only the local model updates to the central server. However, it
faces challenges including statistical heterogeneity of clients' datasets and
resource constraints of client devices, which severely impact the training
performance and user experience. Prior works have tackled these challenges by
combining personalization with model compression schemes including quantization
and pruning. However, the pruning is data-dependent and thus must be done on
the client side which requires considerable computation cost. Moreover, the
pruning normally trains a binary supermask $\in \{0, 1\}$ which significantly
limits the model capacity yet with no computation benefit. Consequently, the
training requires high computation cost and a long time to converge while the
model performance does not pay off. In this work, we propose HideNseek which
employs one-shot data-agnostic pruning at initialization to get a subnetwork
based on weights' synaptic saliency. Each client then optimizes a sign
supermask $\in \{-1, +1\}$ multiplied by the unpruned weights to allow faster
convergence with the same compression rates as state-of-the-art. Empirical
results from three datasets demonstrate that compared to state-of-the-art,
HideNseek improves inferences accuracies by up to 40.6\% while reducing the
communication cost and training time by up to 39.7\% and 46.8\% respectively."
7403,"We believe that while this is only a ﬁrst step, Swan will
enable further research in this domain and enable future researchers and practitioners to build on top
its toolchain.","By scavenging available resources, Swan ensure faster training times and
lower energy usage across a variety of training tasks, which leads to improvement for distributed
training scenarios such as federated learning.","Societal Impacts and Limitations We expect Swan to be a standardized mobile execution engine
for ML model deployment, which can facilitate today’s ML research and industry.",2022-06-09 05:23:07+00:00,Swan: A Neural Engine for Efficient DNN Training on Smartphone SoCs,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Sanjay Sri Vallabh Singapuram'), arxiv.Result.Author('Fan Lai'), arxiv.Result.Author('Chuheng Hu'), arxiv.Result.Author('Mosharaf Chowdhury')]","The need to train DNN models on end-user devices (e.g., smartphones) is
increasing with the need to improve data privacy and reduce communication
overheads. Unlike datacenter servers with powerful CPUs and GPUs, modern
smartphones consist of a diverse collection of specialized cores following a
system-on-a-chip (SoC) architecture that together perform a variety of tasks.
We observe that training DNNs on a smartphone SoC without carefully considering
its resource constraints can not only lead to suboptimal training performance
but significantly affect user experience as well. In this paper, we present
Swan, a neural engine to optimize DNN training on smartphone SoCs without
hurting user experience. Extensive large-scale evaluations show that Swan can
improve performance by 1.2 - 23.3x over the state-of-the-art."
7410,"Our core contributions are: 1) we
present a novel approach to accurately learn Bregman measures using input convex neural networks
(§2); 2) we show that our method is superior on existing Bregman divergence tasks, including
regression, ranking, and clustering (§4); 3) we further study the performance of our method on
asymmetric tasks where the underlying metric is not known to be Bregman.",In this work we describe Neural Bregman Divergences (NBD).,"Our method performs
reasonably on such tasks (§5).",2022-06-09 20:53:15+00:00,Neural Bregman Divergences for Distance Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Fred Lu'), arxiv.Result.Author('Edward Raff'), arxiv.Result.Author('Francis Ferraro')]","Many metric learning tasks, such as triplet learning, nearest neighbor
retrieval, and visualization, are treated primarily as embedding tasks where
the ultimate metric is some variant of the Euclidean distance (e.g., cosine or
Mahalanobis), and the algorithm must learn to embed points into the pre-chosen
space. The study of non-Euclidean geometries or appropriateness is often not
explored, which we believe is due to a lack of tools for learning non-Euclidean
measures of distance. Under the belief that the use of asymmetric methods in
particular have lacked sufficient study, we propose a new approach to learning
arbitrary Bergman divergences in a differentiable manner via input convex
neural networks. Over a set of both new and previously studied tasks, including
asymmetric regression, ranking, and clustering, we demonstrate that our method
more faithfully learns divergences than prior Bregman learning approaches. In
doing so we obtain the first method for learning neural Bregman divergences and
with it inherit the many nice mathematical properties of Bregman divergences,
providing the foundation and tooling for better developing and studying
asymmetric distance learning."
7411,"[13] discuss,
imposing the triangle inequality on other applications, such as language processing, is not obvious
and needs further study.",As Pitis et al.,"We note that prior works are also computationally expensive, with Deepnorm requiring an O(n2)
loop to compute pairwise distances, and Deep-div requiring a further O(k) loop over max-afﬁne com-
ponents.",2022-06-09 20:53:15+00:00,Neural Bregman Divergences for Distance Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Fred Lu'), arxiv.Result.Author('Edward Raff'), arxiv.Result.Author('Francis Ferraro')]","Many metric learning tasks, such as triplet learning, nearest neighbor
retrieval, and visualization, are treated primarily as embedding tasks where
the ultimate metric is some variant of the Euclidean distance (e.g., cosine or
Mahalanobis), and the algorithm must learn to embed points into the pre-chosen
space. The study of non-Euclidean geometries or appropriateness is often not
explored, which we believe is due to a lack of tools for learning non-Euclidean
measures of distance. Under the belief that the use of asymmetric methods in
particular have lacked sufficient study, we propose a new approach to learning
arbitrary Bergman divergences in a differentiable manner via input convex
neural networks. Over a set of both new and previously studied tasks, including
asymmetric regression, ranking, and clustering, we demonstrate that our method
more faithfully learns divergences than prior Bregman learning approaches. In
doing so we obtain the first method for learning neural Bregman divergences and
with it inherit the many nice mathematical properties of Bregman divergences,
providing the foundation and tooling for better developing and studying
asymmetric distance learning."
7412,"Nevertheless, our study provides signiﬁcant insights for the integration
of human ethical judgments in the development of AI technology for automated driving and offers several starting
points for further research in this ﬁeld.","Thus, the results of our study should
be interpreted in light of these limitations.",VI.,2022-06-09 21:55:22+00:00,What should AI see? Using the Public's Opinion to Determine the Perception of an AI,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.HC']","[arxiv.Result.Author('Robin Chan'), arxiv.Result.Author('Radin Dardashti'), arxiv.Result.Author('Meike Osinski'), arxiv.Result.Author('Matthias Rottmann'), arxiv.Result.Author('Dominik Brüggemann'), arxiv.Result.Author('Cilia Rücker'), arxiv.Result.Author('Peter Schlicht'), arxiv.Result.Author('Fabian Hüger'), arxiv.Result.Author('Nikol Rummel'), arxiv.Result.Author('Hanno Gottschalk')]","Deep neural networks (DNN) have made impressive progress in the
interpretation of image data, so that it is conceivable and to some degree
realistic to use them in safety critical applications like automated driving.
From an ethical standpoint, the AI algorithm should take into account the
vulnerability of objects or subjects on the street that ranges from ""not at
all"", e.g. the road itself, to ""high vulnerability"" of pedestrians. One way to
take this into account is to define the cost of confusion of one semantic
category with another and use cost-based decision rules for the interpretation
of probabilities, which are the output of DNNs. However, it is an open problem
how to define the cost structure, who should be in charge to do that, and
thereby define what AI-algorithms will actually ""see"". As one possible answer,
we follow a participatory approach and set up an online survey to ask the
public to define the cost structure. We present the survey design and the data
acquired along with an evaluation that also distinguishes between perspective
(car passenger vs. external traffic participant) and gender. Using simulation
based $F$-tests, we find highly significant differences between the groups.
These differences have consequences on the reliable detection of pedestrians in
a safety critical distance to the self-driving car. We discuss the ethical
problems that are related to this approach and also discuss the problems
emerging from human-machine interaction through the survey from a psychological
point of view. Finally, we include comments from industry leaders in the field
of AI safety on the applicability of survey based elements in the design of AI
functionalities in automated driving."
7415,"In this section, we present important
desiderata that highlight this, and conclude each with an open problem that requires further research.2

                       100  Original Env Return  100  Shifted Train Return  100  Shifted Test Return

Normalized Return      75                        75                         75

   DrQ+BC Offline DV2  50                        50                         50

                       25                        25                         25

                       00 25 50 75 100 00 25 50 75 100 00 25 50 75 100

                       100                       100                        100

                       75                        75                         75

                       50                        50                         50

                       25                        25                         25

                       00 25 50 75 100 00 25 50 75 100 00 25 50 75 100

                                                   % Data Shifted

Figure 3: Both DrQ+BC and Ofﬂine DV2 readily support training datasets with different distractions (mixture

of original and shifted train).","However, combining rich
visual datasets with RL presents its own unique challenges.","Ofﬂine DV2 additionally shows the ability to generalize to unseen distractions

(shifted test) whereas DrQ+BC is more brittle.",2022-06-09 22:08:47+00:00,Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Cong Lu'), arxiv.Result.Author('Philip J. Ball'), arxiv.Result.Author('Tim G. J. Rudner'), arxiv.Result.Author('Jack Parker-Holder'), arxiv.Result.Author('Michael A. Osborne'), arxiv.Result.Author('Yee Whye Teh')]","Offline reinforcement learning has shown great promise in leveraging large
pre-collected datasets for policy learning, allowing agents to forgo
often-expensive online data collection. However, to date, offline reinforcement
learning from has been relatively under-explored, and there is a lack of
understanding of where the remaining challenges lie. In this paper, we seek to
establish simple baselines for continuous control in the visual domain. We show
that simple modifications to two state-of-the-art vision-based online
reinforcement learning algorithms, DreamerV2 and DrQ-v2, suffice to outperform
prior work and establish a competitive baseline. We rigorously evaluate these
algorithms on both existing offline datasets and a new testbed for offline
reinforcement learning from visual observations that better represents the data
distributions present in real-world offline reinforcement learning problems,
and open-source our code and data to facilitate progress in this important
domain. Finally, we present and analyze several key desiderata unique to
offline RL from visual observations, including visual distractions and visually
identifiable changes in dynamics."
7416,and represents clear opportunities for further research.,Extrap.,"DrQ+BC      walker         90.8       91.4        65.1
                                                                          BC          cheetah
                                                                          Ofﬂine DV2  walker         71.6       65.1        43.2
                                                                                      cheetah
                                                                                      walker         61.2       61.4        47.2
                                                                                      cheetah
                                                                                                     69.7       61.3        39.6

                                                                                                     23.2       16.5        19.8

                                                                                                     8.2        7.2         9.6

Ofﬂine DV2 displays similar trends (results on random datasets are in Appendix E.3).",2022-06-09 22:08:47+00:00,Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Cong Lu'), arxiv.Result.Author('Philip J. Ball'), arxiv.Result.Author('Tim G. J. Rudner'), arxiv.Result.Author('Jack Parker-Holder'), arxiv.Result.Author('Michael A. Osborne'), arxiv.Result.Author('Yee Whye Teh')]","Offline reinforcement learning has shown great promise in leveraging large
pre-collected datasets for policy learning, allowing agents to forgo
often-expensive online data collection. However, to date, offline reinforcement
learning from has been relatively under-explored, and there is a lack of
understanding of where the remaining challenges lie. In this paper, we seek to
establish simple baselines for continuous control in the visual domain. We show
that simple modifications to two state-of-the-art vision-based online
reinforcement learning algorithms, DreamerV2 and DrQ-v2, suffice to outperform
prior work and establish a competitive baseline. We rigorously evaluate these
algorithms on both existing offline datasets and a new testbed for offline
reinforcement learning from visual observations that better represents the data
distributions present in real-world offline reinforcement learning problems,
and open-source our code and data to facilitate progress in this important
domain. Finally, we present and analyze several key desiderata unique to
offline RL from visual observations, including visual distractions and visually
identifiable changes in dynamics."
7417,"In this section, we present important
desiderata that highlight this, and conclude each with an open problem that requires further research.2

                       100  Original Env Return  100  Shifted Train Return  100  Shifted Test Return

Normalized Return      75                        75                         75

   DrQ+BC Offline DV2  50                        50                         50

                       25                        25                         25

                       00 25 50 75 100 00 25 50 75 100 00 25 50 75 100

                       100                       100                        100

                       75                        75                         75

                       50                        50                         50

                       25                        25                         25

                       00 25 50 75 100 00 25 50 75 100 00 25 50 75 100

                                                   % Data Shifted

Figure 3: Both DrQ+BC and Ofﬂine DV2 readily support training datasets with different distractions (mixture

of original and shifted train).","However, combining rich
visual datasets with RL presents its own unique challenges.","Ofﬂine DV2 additionally shows the ability to generalize to unseen distractions

(shifted test) whereas DrQ+BC is more brittle.",2022-06-09 22:08:47+00:00,Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Cong Lu'), arxiv.Result.Author('Philip J. Ball'), arxiv.Result.Author('Tim G. J. Rudner'), arxiv.Result.Author('Jack Parker-Holder'), arxiv.Result.Author('Michael A. Osborne'), arxiv.Result.Author('Yee Whye Teh')]","Offline reinforcement learning has shown great promise in leveraging large
pre-collected datasets for policy learning, allowing agents to forgo
often-expensive online data collection. However, to date, offline reinforcement
learning from visual observations with continuous action spaces has been
relatively under-explored, and there is a lack of understanding of where the
remaining challenges lie. In this paper, we seek to establish simple baselines
for continuous control in the visual domain. We show that simple modifications
to two state-of-the-art vision-based online reinforcement learning algorithms,
DreamerV2 and DrQ-v2, suffice to outperform prior work and establish a
competitive baseline. We rigorously evaluate these algorithms on both existing
offline datasets and a new testbed for offline reinforcement learning from
visual observations that better represents the data distributions present in
real-world offline RL problems, and open-source our code and data to facilitate
progress in this important domain. Finally, we present and analyze several key
desiderata unique to offline RL from visual observations, including visual
distractions and visually identifiable changes in dynamics."
7418,and represents clear opportunities for further research.,Extrap.,"DrQ+BC      walker         90.8       91.4        65.1
                                                                          BC          cheetah
                                                                          Ofﬂine DV2  walker         71.6       65.1        43.2
                                                                                      cheetah
                                                                                      walker         61.2       61.4        47.2
                                                                                      cheetah
                                                                                                     69.7       61.3        39.6

                                                                                                     23.2       16.5        19.8

                                                                                                     8.2        7.2         9.6

Ofﬂine DV2 displays similar trends (results on random datasets are in Appendix E.3).",2022-06-09 22:08:47+00:00,Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Cong Lu'), arxiv.Result.Author('Philip J. Ball'), arxiv.Result.Author('Tim G. J. Rudner'), arxiv.Result.Author('Jack Parker-Holder'), arxiv.Result.Author('Michael A. Osborne'), arxiv.Result.Author('Yee Whye Teh')]","Offline reinforcement learning has shown great promise in leveraging large
pre-collected datasets for policy learning, allowing agents to forgo
often-expensive online data collection. However, to date, offline reinforcement
learning from visual observations with continuous action spaces has been
relatively under-explored, and there is a lack of understanding of where the
remaining challenges lie. In this paper, we seek to establish simple baselines
for continuous control in the visual domain. We show that simple modifications
to two state-of-the-art vision-based online reinforcement learning algorithms,
DreamerV2 and DrQ-v2, suffice to outperform prior work and establish a
competitive baseline. We rigorously evaluate these algorithms on both existing
offline datasets and a new testbed for offline reinforcement learning from
visual observations that better represents the data distributions present in
real-world offline RL problems, and open-source our code and data to facilitate
progress in this important domain. Finally, we present and analyze several key
desiderata unique to offline RL from visual observations, including visual
distractions and visually identifiable changes in dynamics."
7422,"These effects often promote generalization
in ways that differ from non-adaptive optimizers like SGD, and warrant further study to be able
to harness efﬁciently.","These results in their pure form absent explicit regularization, reveal an
intriguing inductive bias of adaptive gradient optimizers that becomes salient in the TPT, characterized
by cyclic stepwise effects on the optimization trajectory.","There are open question remaining to be answered, for instance 1) What’s
the causal factor of the plateau of weight norm growth?",2022-06-10 00:04:21+00:00,The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the \emph{Grokking Phenomenon},cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Vimal Thilak'), arxiv.Result.Author('Etai Littwin'), arxiv.Result.Author('Shuangfei Zhai'), arxiv.Result.Author('Omid Saremi'), arxiv.Result.Author('Roni Paiss'), arxiv.Result.Author('Joshua Susskind')]","The \emph{grokking phenomenon} as reported by Power et
al.~\cite{power2021grokking} refers to a regime where a long period of
overfitting is followed by a seemingly sudden transition to perfect
generalization. In this paper, we attempt to reveal the underpinnings of
Grokking via a series of empirical studies. Specifically, we uncover an
optimization anomaly plaguing adaptive optimizers at extremely late stages of
training, referred to as the \emph{Slingshot Mechanism}. A prominent artifact
of the Slingshot Mechanism can be measured by the cyclic phase transitions
between stable and unstable training regimes, and can be easily monitored by
the cyclic behavior of the norm of the last layers weights. We empirically
observe that without explicit regularization, Grokking as reported in
\cite{power2021grokking} almost exclusively happens at the onset of
\emph{Slingshots}, and is absent without it. While common and easily reproduced
in more general settings, the Slingshot Mechanism does not follow from any
known optimization theories that we are aware of, and can be easily overlooked
without an in depth examination. Our work points to a surprising and useful
inductive bias of adaptive gradient optimizers at late stages of training,
calling for a revised theoretical analysis of their origin."
7423,"These effects often promote generalization
in ways that differ from non-adaptive optimizers like SGD, and warrant further study to be able
to harness efﬁciently.","These results in their pure form absent explicit regularization, reveal an
intriguing inductive bias of adaptive gradient optimizers that becomes salient in the TPT, characterized
by cyclic stepwise effects on the optimization trajectory.","There are open question remaining to be answered, for instance 1) What’s
the causal factor of the plateau of weight norm growth?",2022-06-10 00:04:21+00:00,The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Vimal Thilak'), arxiv.Result.Author('Etai Littwin'), arxiv.Result.Author('Shuangfei Zhai'), arxiv.Result.Author('Omid Saremi'), arxiv.Result.Author('Roni Paiss'), arxiv.Result.Author('Joshua Susskind')]","The grokking phenomenon as reported by Power et al. ( arXiv:2201.02177 )
refers to a regime where a long period of overfitting is followed by a
seemingly sudden transition to perfect generalization. In this paper, we
attempt to reveal the underpinnings of Grokking via a series of empirical
studies. Specifically, we uncover an optimization anomaly plaguing adaptive
optimizers at extremely late stages of training, referred to as the Slingshot
Mechanism. A prominent artifact of the Slingshot Mechanism can be measured by
the cyclic phase transitions between stable and unstable training regimes, and
can be easily monitored by the cyclic behavior of the norm of the last layers
weights. We empirically observe that without explicit regularization, Grokking
as reported in ( arXiv:2201.02177 ) almost exclusively happens at the onset of
Slingshots, and is absent without it. While common and easily reproduced in
more general settings, the Slingshot Mechanism does not follow from any known
optimization theories that we are aware of, and can be easily overlooked
without an in depth examination. Our work points to a surprising and useful
inductive bias of adaptive gradient optimizers at late stages of training,
calling for a revised theoretical analysis of their origin."
7424,"Finally, our exponential family-based theory of HMoGs opens two directions
for further research.","As
such, the basic ﬁtting algorithm we provided could be further enhanced with
regularization or sparsity techniques [8, 25] or specialized EM algorithms for
large datasets [26].","On one hand, the theory of conjugation we developed
provides a toolbox for designing tractable hierarchical models out of arbitrary
exponential families.",2022-06-10 02:03:18+00:00,Hierarchical mixtures of Gaussians for combined dimensionality reduction and clustering,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Sacha Sokoloski'), arxiv.Result.Author('Philipp Berens')]","To avoid the curse of dimensionality, a common approach to clustering
high-dimensional data is to first project the data into a space of reduced
dimension, and then cluster the projected data. Although effective, this
two-stage approach prevents joint optimization of the dimensionality-reduction
and clustering models, and obscures how well the complete model describes the
data. Here, we show how a family of such two-stage models can be combined into
a single, hierarchical model that we call a hierarchical mixture of Gaussians
(HMoG). An HMoG simultaneously captures both dimensionality-reduction and
clustering, and its performance is quantified in closed-form by the likelihood
function. By formulating and extending existing models with exponential family
theory, we show how to maximize the likelihood of HMoGs with
expectation-maximization. We apply HMoGs to synthetic data and RNA sequencing
data, and demonstrate how they exceed the limitations of two-stage models.
Ultimately, HMoGs are a rigorous generalization of a common statistical
framework, and provide researchers with a method to improve model performance
when clustering high-dimensional data."
7425,"We expect that our study
will pave the way for further research into GNN for the HAR domain.",will enable us to establish new benchmarks for HAR domain performance.,"References

 [1] Alireza Abedin, Mahsa Ehsanpour, Qinfeng Shi, Hamid Rezatoﬁghi, and Damith C. Ranasinghe.",2022-06-10 03:04:23+00:00,Beyond the Gates of Euclidean Space: Temporal-Discrimination-Fusions and Attention-based Graph Neural Network for Human Activity Recognition,cs.LG,"['cs.LG', 'eess.SP']","[arxiv.Result.Author('Nafees Ahmad'), arxiv.Result.Author('Savio Ho-Chit Chow'), arxiv.Result.Author('Ho-fung Leung')]","Human activity recognition (HAR) through wearable devices has received much
interest due to its numerous applications in fitness tracking, wellness
screening, and supported living. As a result, we have seen a great deal of work
in this field. Traditional deep learning (DL) has set a state of the art
performance for HAR domain. However, it ignores the data's structure and the
association between consecutive time stamps. To address this constraint, we
offer an approach based on Graph Neural Networks (GNNs) for structuring the
input representation and exploiting the relations among the samples. However,
even when using a simple graph convolution network to eliminate this shortage,
there are still several limiting factors, such as inter-class activities
issues, skewed class distribution, and a lack of consideration for sensor data
priority, all of which harm the HAR model's performance. To improve the current
HAR model's performance, we investigate novel possibilities within the
framework of graph structure to achieve highly discriminated and rich activity
features. We propose a model for (1) time-series-graph module that converts raw
data from HAR dataset into graphs; (2) Graph Convolutional Neural Networks
(GCNs) to discover local dependencies and correlations between neighboring
nodes; and (3) self-attention GNN encoder to identify sensors interactions and
data priorities. To the best of our knowledge, this is the first work for HAR,
which introduces a GNN-based approach that incorporates both the GCN and the
attention mechanism. By employing a uniform evaluation method, our framework
significantly improves the performance on hospital patient's activities dataset
comparatively considered other state of the art baseline methods."
7440,"These results can serve as a
basis for further research on formal veriﬁcation of GNN but their extendability depends on several
parameters.","We also showed that veriﬁcation of node-classiﬁer GNN from the
same setting is decidable if the degree of valid input graphs is bounded.","Dependency on the GNN model We restricted our investigations to GNN from the MPNN model,
which is a blueprint for spatial-based GNN [24].",2022-06-10 13:04:14+00:00,We Cannot Guarantee Safety: The Undecidability of Graph Neural Network Verification,cs.LG,"['cs.LG', 'cs.LO']","[arxiv.Result.Author('Marco Sälzer'), arxiv.Result.Author('Martin Lange')]","Graph Neural Networks (GNN) are commonly used for two tasks: (whole) graph
classification and node classification. We formally introduce generically
formulated decision problems for both tasks, corresponding to the following
pattern: given a GNN, some specification of valid inputs, and some
specification of valid outputs, decide whether there is a valid input
satisfying the output specification. We then prove that graph classifier
verification is undecidable in general, implying that there cannot be an
algorithm surely guaranteeing the absence of misclassification of any kind.
Additionally, we show that verification in the node classification case becomes
decidable as soon as we restrict the degree of the considered graphs.
Furthermore, we discuss possible changes to these results depending on the
considered GNN model and specifications."
7441,"These results can serve as a basis for further research on formal
veriﬁcation of GNN but their extendability depends on several parameters.","We also showed that formal
veriﬁcation of ORP and ARP of node-classiﬁer MPNN is possible, as soon as the degree of the
considered input graphs is bounded.",Dependency on the GNN model.,2022-06-10 13:04:14+00:00,Fundamental Limits in Formal Verification of Message-Passing Neural Networks,cs.LG,"['cs.LG', 'cs.LO']","[arxiv.Result.Author('Marco Sälzer'), arxiv.Result.Author('Martin Lange')]","Output reachability and adversarial robustness are among the most relevant
safety properties of neural networks. We show that in the context of Message
Passing Neural Networks (MPNN), a common Graph Neural Network (GNN) model,
formal verification is impossible. In particular, we show that output
reachability of graph-classifier MPNN, working over graphs of unbounded size,
non-trivial degree and sufficiently expressive node labels, cannot be verified
formally: there is no algorithm that answers correctly (with yes or no), given
an MPNN, whether there exists some valid input to the MPNN such that the
corresponding output satisfies a given specification. However, we also show
that output reachability and adversarial robustness of node-classifier MPNN can
be verified formally when a limit on the degree of input graphs is given a
priori. We discuss the implications of these results, for the purpose of
obtaining a complete picture of the principle possibility to formally verify
GNN, depending on the expressiveness of the involved GNN models and
input-output specifications."
7444,"The speciﬁc nature of the datasets that makes
MCC performs well (or worse) remains an interesting avenue for further research.","As compared with the best available methods, the performance of MCC falls short in certain
datasets such as the CIFAR-10 and Tiny-Imagenet.",We also provide the t-SNE plots on the STL-10 dataset in Fig 5.,2022-06-10 13:37:15+00:00,Federated Momentum Contrastive Clustering,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Runxuan Miao'), arxiv.Result.Author('Erdem Koyuncu')]","We present federated momentum contrastive clustering (FedMCC), a learning
framework that can not only extract discriminative representations over
distributed local data but also perform data clustering. In FedMCC, a
transformed data pair passes through both the online and target networks,
resulting in four representations over which the losses are determined. The
resulting high-quality representations generated by FedMCC can outperform
several existing self-supervised learning methods for linear evaluation and
semi-supervised learning tasks. FedMCC can easily be adapted to ordinary
centralized clustering through what we call momentum contrastive clustering
(MCC). We show that MCC achieves state-of-the-art clustering accuracy results
in certain datasets such as STL-10 and ImageNet-10. We also present a method to
reduce the memory footprint of our clustering schemes."
7445,"The
performance becomes noticeably worse, suggesting a strong dependence of the performance on
m. Optimization of the decay rate for BYOL-type algorithms in the federated setting remains
a very interesting avenue for further research.","Here, we show
                                                                                                                                           13

                                                                   TABLE VII
                                                    UPDATING ONLINE & TARGET NETS

                                              Method                              Update Both
                                              Dataset                         NMI ACC ARI

                                              CIFAR-10 (IID)                  53.2 58.3 43.5
                                              CIFAR-10 (Non-IID)              34.9 40.4 22.8
                                              CIFAR-100 (IID)                 33.0 18.3 7.6
                                              CIFAR-100 (Non-IID)             30.9 15.5 6.4
                                              MNIST (IID)                     62.6 62.9 47.7
                                              MNIST (Non-IID)                 24.5 37.3 14.9

results for the choice m = 0.996 in Table IX, considered in the original BYOL paper [17].","We have generally not performed decay rate
optimization for fair comparison with existing studies (most previous work such as FedU also
consider m = 0.99).",2022-06-10 13:37:15+00:00,Federated Momentum Contrastive Clustering,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Runxuan Miao'), arxiv.Result.Author('Erdem Koyuncu')]","We present federated momentum contrastive clustering (FedMCC), a learning
framework that can not only extract discriminative representations over
distributed local data but also perform data clustering. In FedMCC, a
transformed data pair passes through both the online and target networks,
resulting in four representations over which the losses are determined. The
resulting high-quality representations generated by FedMCC can outperform
several existing self-supervised learning methods for linear evaluation and
semi-supervised learning tasks. FedMCC can easily be adapted to ordinary
centralized clustering through what we call momentum contrastive clustering
(MCC). We show that MCC achieves state-of-the-art clustering accuracy results
in certain datasets such as STL-10 and ImageNet-10. We also present a method to
reduce the memory footprint of our clustering schemes."
7454,"We leave such
algorithms to future work and hope our tools can inspire further research into that type of scheduling.","The dynamic nature of the Pause and Resume optimizations
suggests that well-designed scheduling algorithms should be able to get rather close to the upper-bound.","Moreover, it is
likely that carbon intensity forecasting will improve over the years and eventually extend beyond 24 hours, allowing
time-shifting decisions to become increasingly accurate.",2022-06-10 17:04:04+00:00,Measuring the Carbon Intensity of AI in Cloud Instances,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jesse Dodge'), arxiv.Result.Author('Taylor Prewitt'), arxiv.Result.Author('Remi Tachet Des Combes'), arxiv.Result.Author('Erika Odmark'), arxiv.Result.Author('Roy Schwartz'), arxiv.Result.Author('Emma Strubell'), arxiv.Result.Author('Alexandra Sasha Luccioni'), arxiv.Result.Author('Noah A. Smith'), arxiv.Result.Author('Nicole DeCario'), arxiv.Result.Author('Will Buchanan')]","By providing unprecedented access to computational resources, cloud computing
has enabled rapid growth in technologies such as machine learning, the
computational demands of which incur a high energy cost and a commensurate
carbon footprint. As a result, recent scholarship has called for better
estimates of the greenhouse gas impact of AI: data scientists today do not have
easy or reliable access to measurements of this information, precluding
development of actionable tactics. Cloud providers presenting information about
software carbon intensity to users is a fundamental stepping stone towards
minimizing emissions. In this paper, we provide a framework for measuring
software carbon intensity, and propose to measure operational carbon emissions
by using location-based and time-specific marginal emissions data per energy
unit. We provide measurements of operational software carbon intensity for a
set of modern models for natural language processing and computer vision, and a
wide range of model sizes, including pretraining of a 6.1 billion parameter
language model. We then evaluate a suite of approaches for reducing emissions
on the Microsoft Azure cloud compute platform: using cloud instances in
different geographic regions, using cloud instances at different times of day,
and dynamically pausing cloud instances when the marginal carbon intensity is
above a certain threshold. We confirm previous results that the geographic
region of the data center plays a significant role in the carbon intensity for
a given cloud instance, and find that choosing an appropriate region can have
the largest operational emissions reduction impact. We also show that the time
of day has notable impact on operational software carbon intensity. Finally, we
conclude with recommendations for how machine learning practitioners can use
software carbon intensity information to reduce environmental impact."
7501,"As AB  and BA have the same spectra,
              N                       N
we further study the spectrum of KN .","Consider the following operators:
                                           A : F → Rn, f → f (xN ),

                                        B : Rn → F , v → Kp(·, xN )v.

Then,  Σp  =  1  BA  and  KN       =  1  Kp  (xN  ,  xN  )  =   N1 AB.","19
But KN   =   1  Kp  (xN  ,  xN  )  =  V  1  kν  (xN    ,  xN  )p(dν  )  and  λmax(KN )  ≤  1  follows  from  lemma
             N                           N

G.6.",2022-06-11 20:16:24+00:00,Gradient Boosting Performs Gaussian Process Inference,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Aleksei Ustimenko'), arxiv.Result.Author('Artem Beliakov'), arxiv.Result.Author('Liudmila Prokhorenkova')]","This paper shows that gradient boosting based on symmetric decision trees can
be equivalently reformulated as a kernel method that converges to the solution
of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence
to a Gaussian Process' posterior mean, which, in turn, allows us to easily
transform gradient boosting into a sampler from the posterior to provide better
knowledge uncertainty estimates through Monte-Carlo estimation of the posterior
variance. We show that the proposed sampler allows for better knowledge
uncertainty estimates leading to improved out-of-domain detection."
7512,"9
We hope that our work will spark further research into the near-convergence regime.","This is the case, for example, when training with exponential-type
loss functions and Weight Normalization (Salimans & Kingma, 2016).","For instance,
it may provide an interesting algorithmic approach to the long-standing problem of developing
low-rank regularizers during optimization.",2022-06-12 17:06:35+00:00,SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Tomer Galanti'), arxiv.Result.Author('Zachary S. Siegel'), arxiv.Result.Author('Aparna Gupte'), arxiv.Result.Author('Tomaso Poggio')]","We analyze deep ReLU neural networks trained with mini-batch Stochastic
Gradient Descent (SGD) and weight decay. We show, both theoretically and
empirically, that when training a neural network using SGD with weight decay
and small batch size, the resulting weight matrices tend to be of small rank.
Our analysis relies on a minimal set of assumptions; the neural networks may be
arbitrarily wide or deep and may include residual connections, as well as
convolutional layers. The same analysis implies the inherent presence of SGD
""noise"", defined as the inability of SGD to converge to a stationary point. In
particular, we prove that SGD noise must always be present, even
asymptotically, as long as we incorporate weight decay and the batch size is
smaller than the total number of training samples."
7520,"Prospects for further research are to investigate a way
                                                                        of a more robust automatic pattern selection scheme for an
                                                                        arbitrary training dataset and network conﬁguration.","The improvement of adaptation time of the widespread
                                                                        MAML algorithm will enable its applicability on less pow-
                                                                        erful devices and will in general decrease the time needed
                                                                        for the algorithm to adapt to new tasks.","Funding

                                                                            The work is supported by the state budget scientiﬁc research
                                                                        project of Dnipro University of Technology “Development of New
                                                                        Mobile Information Technologies for Person Identiﬁcation and
                                                                        Object Classiﬁcation in the Surrounding Environment” (state reg-
                                                                        istration number 0121U109787).",2022-06-13 06:57:17+00:00,Faster Optimization-Based Meta-Learning Adaptation Phase,cs.LG,"['cs.LG', 'cs.CV']",[arxiv.Result.Author('Kostiantyn Khabarlak')],"Neural networks require a large amount of annotated data to learn.
Meta-learning algorithms propose a way to decrease the number of training
samples to only a few. One of the most prominent optimization-based
meta-learning algorithms is Model-Agnostic Meta-Learning (MAML). However, the
key procedure of adaptation to new tasks in MAML is quite slow. In this work we
propose an improvement to MAML meta-learning algorithm. We introduce Lambda
patterns by which we restrict which weight are updated in the network during
the adaptation phase. This makes it possible to skip certain gradient
computations. The fastest pattern is selected given an allowed quality
degradation threshold parameter. In certain cases, quality improvement is
possible by a careful pattern selection. The experiments conducted have shown
that via Lambda adaptation pattern selection, it is possible to significantly
improve the MAML method in the following areas: adaptation time has been
decreased by a factor of 3 with minimal accuracy loss; accuracy for one-step
adaptation has been substantially improved."
7523,"Hence, further research should show how sensitive the
   performance of EmProx is towards the training size.","Additionally, it has to be noted that
   EmProx is inherently dependent on the training data, since it bases its predictions directly on the
   performance of training architectures.","To further show robustness of the method,
   experiments with resource limits could be conducted, as has been done extensively by White et al.",2022-06-13 08:35:52+00:00,EmProx: Neural Network Performance Estimation For Neural Architecture Search,cs.LG,['cs.LG'],"[arxiv.Result.Author('G. G. H. Franken'), arxiv.Result.Author('P. Singh'), arxiv.Result.Author('J. Vanschoren')]","Common Neural Architecture Search methods generate large amounts of candidate
architectures that need training in order to assess their performance and find
an optimal architecture. To minimize the search time we use different
performance estimation strategies. The effectiveness of such strategies varies
in terms of accuracy and fit and query time. This study proposes a new method,
EmProx Score (Embedding Proximity Score). Similar to Neural Architecture
Optimization (NAO), this method maps candidate architectures to a continuous
embedding space using an encoder-decoder framework. The performance of
candidates is then estimated using weighted kNN based on the embedding vectors
of architectures of which the performance is known. Performance estimations of
this method are comparable to the MLP performance predictor used in NAO in
terms of accuracy, while being nearly nine times faster to train compared to
NAO. Benchmarking against other performance estimation strategies currently
used shows similar to better accuracy, while being five up to eighty times
faster."
7549,"This suggests that there is a       This work makes use of a convolutional autoencoder as a
need for further research in this area.","AUTOENCODER AND CLUSTERING METHODS USED IN
applying unsupervised machine learning algorithms to galaxy                                     THIS WORK
morphology classiﬁcation when compared to the work done
on supervised classiﬁcation [6].","Notable works applying    feature extractor to reduce the dimensionality of the optical-
unsupervised learning to the domain of galaxy morphology          spectrum galaxy image data.",2022-06-13 13:52:07+00:00,The Classification of Optical Galaxy Morphology Using Unsupervised Learning Techniques,cs.LG,"['cs.LG', 'astro-ph.GA', 'astro-ph.IM']","[arxiv.Result.Author('Ezra Fielding'), arxiv.Result.Author('Clement N. Nyirenda'), arxiv.Result.Author('Mattia Vaccari')]","The advent of large scale, data intensive astronomical surveys has caused the
viability of human-based galaxy morphology classification methods to come into
question. Put simply, too much astronomical data is being produced for
scientists to visually label. Attempts have been made to crowd-source this work
by recruiting volunteers from the general public. However, even these efforts
will soon fail to keep up with data produced by modern surveys. Unsupervised
learning techniques do not require existing labels to classify data and could
pave the way to unplanned discoveries. Therefore, this paper aims to implement
unsupervised learning algorithms to classify the Galaxy Zoo DECaLS dataset
without human supervision. First, a convolutional autoencoder was implemented
as a feature extractor. The extracted features were then clustered via k-means,
fuzzy c-means and agglomerative clustering to provide classifications. The
results were compared to the volunteer classifications of the Galaxy Zoo DECaLS
dataset. Agglomerative clustering generally produced the best results, however,
the performance gain over k-means clustering was not significant. With the
appropriate optimizations, this approach could be used to provide
classifications for the better performing Galaxy Zoo DECaLS decision tree
questions. Ultimately, this unsupervised learning approach provided valuable
insights and results that were useful to scientists."
7550,"This suggests that there is a       This work makes use of a convolutional autoencoder as a
need for further research in this area.","AUTOENCODER AND CLUSTERING METHODS USED IN
applying unsupervised machine learning algorithms to galaxy                                     THIS WORK
morphology classiﬁcation when compared to the work done
on supervised classiﬁcation [6].","Notable works applying    feature extractor to reduce the dimensionality of the optical-
unsupervised learning to the domain of galaxy morphology          spectrum galaxy image data.",2022-06-13 13:52:07+00:00,The Classification of Optical Galaxy Morphology Using Unsupervised Learning Techniques,cs.LG,"['cs.LG', 'astro-ph.GA', 'astro-ph.IM']","[arxiv.Result.Author('Ezra Fielding'), arxiv.Result.Author('Clement N. Nyirenda'), arxiv.Result.Author('Mattia Vaccari')]","In recent years, large scale data intensive astronomical surveys have
resulted in more detailed images being produced than scientists can manually
classify. Even attempts to crowd-source this work will soon be outpaced by the
large amount of data generated by modern surveys. This has brought into
question the viability of human-based methods for classifying galaxy
morphology. While supervised learning methods require datasets with existing
labels, unsupervised learning techniques do not. Therefore, this paper
implements unsupervised learning techniques to classify the Galaxy Zoo DECaLS
dataset. A convolutional autoencoder feature extractor was trained and
implemented. The resulting features were then clustered via k-means, fuzzy
c-means and agglomerative clustering. These clusters were compared against the
true volunteer classifications provided by the Galaxy Zoo DECaLS project. The
best results, in general, were produced by the agglomerate clustering method.
However, the increase in performance compared to k-means clustering was not
significant considering the increase in clustering time. After undergoing the
appropriate clustering algorithm optimizations, this approach could prove
useful for classifying the better performing questions and could serve as the
basis for a novel approach to generating more ""human-like"" galaxy morphology
classifications from unsupervised techniques."
7559,"Hence
   Hierarchical Correlation Analysis (HCR) [8] approach                          regularization can still be improved, what will require further research.","3: (2, 3, 4, 7, 8, 9, 12, 15, 19, 20, 22).","means working on (mixed) moment-like parameters chosen to
allow to reconstruct the (joint) distribution from them.",2022-06-13 14:28:53+00:00,Predicting conditional probability distributions of redshifts of Active Galactic Nuclei using Hierarchical Correlation Reconstruction,cs.LG,"['cs.LG', 'astro-ph.GA', 'astro-ph.HE']",[arxiv.Result.Author('Jarek Duda')],"While there is a general focus on prediction of values, real data often only
allows to predict conditional probability distributions, with capabilities
bounded by conditional entropy $H(Y|X)$. If additionally estimating
uncertainty, we can treat a predicted value as the center of Gaussian of
Laplace distribution - idealization which can be far from complex conditional
distributions of real data. This article applies Hierarchical Correlation
Reconstruction (HCR) approach to inexpensively predict quite complex
conditional probability distributions (e.g. multimodal): by independent MSE
estimation of multiple moment-like parameters, which allow to reconstruct the
conditional distribution. Using linear regression for this purpose, we get
interpretable models: with coefficients describing contributions of features to
conditional moments. This article extends on the original approach especially
by using Canonical Correlation Analysis (CCA) for feature optimization and l1
""lasso"" regularization, focusing on practical problem of prediction of redshift
of Active Galactic Nuclei (AGN) based on Fourth Fermi-LAT Data Release 2 (4LAC)
dataset."
7560,for pairs (or larger numbers) of variables like       optimization problem requiring further research.,"This is a difﬁcult
directly e.g.","(fi1 (xj1 )fi2 (xj2 )) - it allows to exploit also multivariate
dependencies.",2022-06-13 14:28:53+00:00,Predicting conditional probability distributions of redshifts of Active Galactic Nuclei using Hierarchical Correlation Reconstruction,cs.LG,"['cs.LG', 'astro-ph.GA', 'astro-ph.HE']",[arxiv.Result.Author('Jarek Duda')],"While there is a general focus on prediction of values, real data often only
allows to predict conditional probability distributions, with capabilities
bounded by conditional entropy $H(Y|X)$. If additionally estimating
uncertainty, we can treat a predicted value as the center of Gaussian of
Laplace distribution - idealization which can be far from complex conditional
distributions of real data. This article applies Hierarchical Correlation
Reconstruction (HCR) approach to inexpensively predict quite complex
conditional probability distributions (e.g. multimodal): by independent MSE
estimation of multiple moment-like parameters, which allow to reconstruct the
conditional distribution. Using linear regression for this purpose, we get
interpretable models: with coefficients describing contributions of features to
conditional moments. This article extends on the original approach especially
by using Canonical Correlation Analysis (CCA) for feature optimization and l1
""lasso"" regularization, focusing on practical problem of prediction of redshift
of Active Galactic Nuclei (AGN) based on Fourth Fermi-LAT Data Release 2 (4LAC)
dataset."
7561,"ing to the original variables, we get

   This feature selection + regularization is very difﬁcult to      a is an eigenvector of CX−1X CXY CY−Y1 CY X
do right, will require further research, maybe using different
techniques.","based on         Performing singular value decomposition (SVD), and return-
hypothesis testing.",In Fig.,2022-06-13 14:28:53+00:00,Predicting conditional probability distributions of redshifts of Active Galactic Nuclei using Hierarchical Correlation Reconstruction,cs.LG,"['cs.LG', 'astro-ph.GA', 'astro-ph.HE']",[arxiv.Result.Author('Jarek Duda')],"While there is a general focus on prediction of values, real data often only
allows to predict conditional probability distributions, with capabilities
bounded by conditional entropy $H(Y|X)$. If additionally estimating
uncertainty, we can treat a predicted value as the center of Gaussian of
Laplace distribution - idealization which can be far from complex conditional
distributions of real data. This article applies Hierarchical Correlation
Reconstruction (HCR) approach to inexpensively predict quite complex
conditional probability distributions (e.g. multimodal): by independent MSE
estimation of multiple moment-like parameters, which allow to reconstruct the
conditional distribution. Using linear regression for this purpose, we get
interpretable models: with coefficients describing contributions of features to
conditional moments. This article extends on the original approach especially
by using Canonical Correlation Analysis (CCA) for feature optimization and l1
""lasso"" regularization, focusing on practical problem of prediction of redshift
of Active Galactic Nuclei (AGN) based on Fourth Fermi-LAT Data Release 2 (4LAC)
dataset."
7567,"We further study the proper-           been derived (Dziugaite & Roy, 2018).","We prove that SAM always chooses a              ing loss, i.e., how quickly it changes in some neighborhood
                                              solution that enjoys better generalization proper-        around the parameters of the model, correlates well with the
                                              ties than standard gradient descent for a certain         generalization error (Keskar et al., 2016; Jiang et al., 2019),
                                              class of problems, and this effect is ampliﬁed by         and generalization bounds related to the sharpness have
                                              using m-sharpness.","The idea of minimiz-
                                              ties of the implicit bias on non-linear networks          ing the sharpness to improve generalization has motivated
                                              empirically, where we show that ﬁne-tuning a              recent works of Foret et al.",2022-06-13 15:07:32+00:00,Towards Understanding Sharpness-Aware Minimization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Maksym Andriushchenko'), arxiv.Result.Author('Nicolas Flammarion')]","Sharpness-Aware Minimization (SAM) is a recent training method that relies on
worst-case weight perturbations which significantly improves generalization in
various settings. We argue that the existing justifications for the success of
SAM which are based on a PAC-Bayes generalization bound and the idea of
convergence to flat minima are incomplete. Moreover, there are no explanations
for the success of using $m$-sharpness in SAM which has been shown as essential
for generalization. To better understand this aspect of SAM, we theoretically
analyze its implicit bias for diagonal linear networks. We prove that SAM
always chooses a solution that enjoys better generalization properties than
standard gradient descent for a certain class of problems, and this effect is
amplified by using $m$-sharpness. We further study the properties of the
implicit bias on non-linear networks empirically, where we show that
fine-tuning a standard model with SAM can lead to significant generalization
improvements. Finally, we provide convergence results of SAM for non-convex
objectives when used with stochastic gradients. We illustrate these results
empirically for deep networks and discuss their relation to the generalization
behavior of SAM. The code of our experiments is available at
https://github.com/tml-epfl/understanding-sam."
7576,Several topics in this emerging ﬁeld merit further research.,"We hope that the uncertainty techniques presented in this paper will allow
more data to be extracted from information systems in the near future.","We used dropout to
implement variational inference.",2022-06-13 17:05:27+00:00,Learning Uncertainty with Artificial Neural Networks for Improved Predictive Process Monitoring,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Hans Weytjens'), arxiv.Result.Author('Jochen De Weerdt')]","The inability of artificial neural networks to assess the uncertainty of
their predictions is an impediment to their widespread use. We distinguish two
types of learnable uncertainty: model uncertainty due to a lack of training
data and noise-induced observational uncertainty. Bayesian neural networks use
solid mathematical foundations to learn the model uncertainties of their
predictions. The observational uncertainty can be calculated by adding one
layer to these networks and augmenting their loss functions. Our contribution
is to apply these uncertainty concepts to predictive process monitoring tasks
to train uncertainty-based models to predict the remaining time and outcomes.
Our experiments show that uncertainty estimates allow more and less accurate
predictions to be differentiated and confidence intervals to be constructed in
both regression and classification tasks. These conclusions remain true even in
early stages of running processes. Moreover, the deployed techniques are fast
and produce more accurate predictions. The learned uncertainty could increase
users' confidence in their process prediction systems, promote better
cooperation between humans and these systems, and enable earlier
implementations with smaller datasets."
7591,"4.3 Ablation study

4.3.1 Impact of the label usages

We further study the usage of labels of PET.","The results show that PET
performs substantially better than the strong baselines in all comparisons.",The results are summarized in Table 4.,2022-06-14 04:24:52+00:00,Learning Enhanced Representations for Tabular Data via Neighborhood Propagation,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kounianhua Du'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Ruiwen Zhou'), arxiv.Result.Author('Yangkun Wang'), arxiv.Result.Author('Xilong Zhao'), arxiv.Result.Author('Jiarui Jin'), arxiv.Result.Author('Quan Gan'), arxiv.Result.Author('Zheng Zhang'), arxiv.Result.Author('David Wipf')]","Prediction over tabular data is an essential and fundamental problem in many
important downstream tasks. However, existing methods either take a data
instance of the table independently as input or do not fully utilize the
multi-rows features and labels to directly change and enhance the target data
representations. In this paper, we propose to 1) construct a hypergraph from
relevant data instance retrieval to model the cross-row and cross-column
patterns of those instances, and 2) perform message Propagation to Enhance the
target data instance representation for Tabular prediction tasks. Specifically,
our specially-designed message propagation step benefits from 1) fusion of
label and features during propagation, and 2) locality-aware high-order feature
interactions. Experiments on two important tabular data prediction tasks
validate the superiority of the proposed PET model against other baselines.
Additionally, we demonstrate the effectiveness of the model components and the
feature enhancement ability of PET via various ablation studies and
visualizations. The code is included in https://github.com/KounianhuaDu/PET."
7592,"A.3 Impact of Different Retrieval Sizes
We further study the impact of different retrieval sizes.","From the visualization results, we can see that PET yields more informative embeddings, i.e., the
representation of tabular data, that separate positive data and negative data better.",Results are displayed in Figure 7.,2022-06-14 04:24:52+00:00,Learning Enhanced Representations for Tabular Data via Neighborhood Propagation,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kounianhua Du'), arxiv.Result.Author('Weinan Zhang'), arxiv.Result.Author('Ruiwen Zhou'), arxiv.Result.Author('Yangkun Wang'), arxiv.Result.Author('Xilong Zhao'), arxiv.Result.Author('Jiarui Jin'), arxiv.Result.Author('Quan Gan'), arxiv.Result.Author('Zheng Zhang'), arxiv.Result.Author('David Wipf')]","Prediction over tabular data is an essential and fundamental problem in many
important downstream tasks. However, existing methods either take a data
instance of the table independently as input or do not fully utilize the
multi-rows features and labels to directly change and enhance the target data
representations. In this paper, we propose to 1) construct a hypergraph from
relevant data instance retrieval to model the cross-row and cross-column
patterns of those instances, and 2) perform message Propagation to Enhance the
target data instance representation for Tabular prediction tasks. Specifically,
our specially-designed message propagation step benefits from 1) fusion of
label and features during propagation, and 2) locality-aware high-order feature
interactions. Experiments on two important tabular data prediction tasks
validate the superiority of the proposed PET model against other baselines.
Additionally, we demonstrate the effectiveness of the model components and the
feature enhancement ability of PET via various ablation studies and
visualizations. The code is included in https://github.com/KounianhuaDu/PET."
7594,"In a nutshell,
further research about the ensemble of current successful                             DIF constructs an ensemble of representations derived from
deep anomaly detection models.","Given that the burgeoning of deep learning has fuelled                                                                                                                                     4
a plethora of deep anomaly detectors that achieve contin-
uously improved performance, our work may also foster                                 the DEEP ISOLATION FOREST (DIF) method.","deep neural networks, and simple axis-parallel isolation
                                                                                      is operated upon new data spaces to build iTrees in the
3 PRELIMINARIES                                                                       forest.",2022-06-14 05:47:07+00:00,Deep Isolation Forest for Anomaly Detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Hongzuo Xu'), arxiv.Result.Author('Guansong Pang'), arxiv.Result.Author('Yijie Wang'), arxiv.Result.Author('Yongjun Wang')]","Isolation forest (iForest) has been emerging as arguably the most popular
anomaly detector in recent years due to its general effectiveness across
different benchmarks and strong scalability. Nevertheless, its linear
axis-parallel isolation method often leads to (i) failure in detecting hard
anomalies that are difficult to isolate in
high-dimensional/non-linear-separable data space, and (ii) notorious
algorithmic bias that assigns unexpectedly lower anomaly scores to artefact
regions. These issues contribute to high false negative errors. Several iForest
extensions are introduced, but they essentially still employ shallow, linear
data partition, restricting their power in isolating true anomalies. Therefore,
this paper proposes deep isolation forest. We introduce a new representation
scheme that utilises casually initialised neural networks to map original data
into random representation ensembles, where random axis-parallel cuts are
subsequently applied to perform the data partition. This representation scheme
facilitates high freedom of the partition in the original data space
(equivalent to non-linear partition on subspaces of varying sizes), encouraging
a unique synergy between random representations and random partition-based
isolation. Extensive experiments show that our model achieves significant
improvement over state-of-the-art isolation-based methods and deep detectors on
tabular, graph and time series datasets; our model also inherits desired
scalability from iForest."
7642,"In the future, we plan     [8] A. I. Hsu and E. A. Yttri, “B-soid, an open-source unsu-
to test our model on additional datasets to further study how          pervised algorithm for identiﬁcation and fast prediction
behavior patterns are exhibited at different frequencies.","This separation allows us to better discover          5
variations at different timescales and across different agents,
helping us to develop better insights.","of behaviors,” Nature communications, vol.",2022-06-14 17:57:55+00:00,Learning Behavior Representations Through Multi-Timescale Bootstrapping,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mehdi Azabou'), arxiv.Result.Author('Michael Mendelson'), arxiv.Result.Author('Maks Sorokin'), arxiv.Result.Author('Shantanu Thakoor'), arxiv.Result.Author('Nauman Ahad'), arxiv.Result.Author('Carolina Urzay'), arxiv.Result.Author('Eva L. Dyer')]","Natural behavior consists of dynamics that are both unpredictable, can switch
suddenly, and unfold over many different timescales. While some success has
been found in building representations of behavior under constrained or
simplified task-based conditions, many of these models cannot be applied to
free and naturalistic settings due to the fact that they assume a single scale
of temporal dynamics. In this work, we introduce Bootstrap Across Multiple
Scales (BAMS), a multi-scale representation learning model for behavior: we
combine a pooling module that aggregates features extracted over encoders with
different temporal receptive fields, and design a set of latent objectives to
bootstrap the representations in each respective space to encourage
disentanglement across different timescales. We first apply our method on a
dataset of quadrupeds navigating in different terrain types, and show that our
model captures the temporal complexity of behavior. We then apply our method to
the MABe 2022 Multi-agent behavior challenge, where our model ranks 3rd overall
and 1st on two subtasks, and show the importance of incorporating
multi-timescales when analyzing behavior."
7650,"Analyzing this
and generalization to new BPs is a very interesting direction for further research.","Also, extending the dataset included in the BP-RL environment to more BPs, for
which there are almost 400 of (Mitchell, 2019), may reveal another layer of complexity by showing
how much of a communication problem BPs are, since we are switching between different creators
(communicators) of BPs, to which an agent may overﬁt on the communication style.","Conclusively, this paper provided many initial steps for which further research can be pursued
individually for each, with the main intention being to once again draw attention to the challenge
of solving BPs in a time where problems like these get overshadowed by advances in designing
highly complex model architecture and computationally intensively trained models achieving very
impressive results.",2022-06-14 22:53:36+00:00,Towards a Solution to Bongard Problems: A Causal Approach,cs.LG,['cs.LG'],"[arxiv.Result.Author('Salahedine Youssef'), arxiv.Result.Author('Matej Zečević'), arxiv.Result.Author('Devendra Singh Dhami'), arxiv.Result.Author('Kristian Kersting')]","Even though AI has advanced rapidly in recent years displaying success in
solving highly complex problems, the class of Bongard Problems (BPs) yet remain
largely unsolved by modern ML techniques. In this paper, we propose a new
approach in an attempt to not only solve BPs but also extract meaning out of
learned representations. This includes the reformulation of the classical BP
into a reinforcement learning (RL) setting which will allow the model to gain
access to counterfactuals to guide its decisions but also explain its
decisions. Since learning meaningful representations in BPs is an essential
sub-problem, we further make use of contrastive learning for the extraction of
low level features from pixel data. Several experiments have been conducted for
analyzing the general BP-RL setup, feature extraction methods and using the
best combination for the feature space analysis and its interpretation."
7651,"Conclusively, this paper provided many initial steps for which further research can be pursued
individually for each, with the main intention being to once again draw attention to the challenge
of solving BPs in a time where problems like these get overshadowed by advances in designing
highly complex model architecture and computationally intensively trained models achieving very
impressive results.","Analyzing this
and generalization to new BPs is a very interesting direction for further research.","12
Acknowledgements

The authors acknowledge the support of the German Science Foundation (DFG) project “Causality,
Argumentation, and Machine Learning” (CAML2, KE 1686/3-2) of the SPP 1999 “Robust Argu-
mentation Machines” (RATIO).",2022-06-14 22:53:36+00:00,Towards a Solution to Bongard Problems: A Causal Approach,cs.LG,['cs.LG'],"[arxiv.Result.Author('Salahedine Youssef'), arxiv.Result.Author('Matej Zečević'), arxiv.Result.Author('Devendra Singh Dhami'), arxiv.Result.Author('Kristian Kersting')]","Even though AI has advanced rapidly in recent years displaying success in
solving highly complex problems, the class of Bongard Problems (BPs) yet remain
largely unsolved by modern ML techniques. In this paper, we propose a new
approach in an attempt to not only solve BPs but also extract meaning out of
learned representations. This includes the reformulation of the classical BP
into a reinforcement learning (RL) setting which will allow the model to gain
access to counterfactuals to guide its decisions but also explain its
decisions. Since learning meaningful representations in BPs is an essential
sub-problem, we further make use of contrastive learning for the extraction of
low level features from pixel data. Several experiments have been conducted for
analyzing the general BP-RL setup, feature extraction methods and using the
best combination for the feature space analysis and its interpretation."
7660,"In addition, such privacy leakage can be mitigated by in-
                                              Finally, we discuss some promising directions and           creasing the local iterations or batch sizes [Wei et al., 2020b;
                                              open problems for further research.",els.,"Huang et al., 2021] during model training.",2022-06-15 03:52:51+00:00,"A Survey on Gradient Inversion: Attacks, Defenses and Future Directions",cs.LG,['cs.LG'],"[arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Song Guo'), arxiv.Result.Author('Junxiao Wang'), arxiv.Result.Author('Xin Xie'), arxiv.Result.Author('Dacheng Tao')]","Recent studies have shown that the training samples can be recovered from
gradients, which are called Gradient Inversion (GradInv) attacks. However,
there remains a lack of extensive surveys covering recent advances and thorough
analysis of this issue. In this paper, we present a comprehensive survey on
GradInv, aiming to summarize the cutting-edge research and broaden the horizons
for different domains. Firstly, we propose a taxonomy of GradInv attacks by
characterizing existing attacks into two paradigms: iteration- and
recursion-based attacks. In particular, we dig out some critical ingredients
from the iteration-based attacks, including data initialization, model training
and gradient matching. Second, we summarize emerging defense strategies against
GradInv attacks. We find these approaches focus on three perspectives covering
data obscuration, model improvement and gradient protection. Finally, we
discuss some promising directions and open problems for further research."
7666,"In general, proper MOHPO evaluation is understudied an
open challenge for further research.","They can
be used to either visualize the set of all returned fronts evaluated on the validation sets, or, as
described above, on the outer test sets.","Multi-Objective Hyperparameter Optimization – An Overview  21

4.6 Relevant benchmarks and results
In general, relevant benchmarks comparing different multi-objective optimizers for HPO mostly
were conducted in the context of new optimizers being proposed (see, e.g., Guerrero-Viu et al.",2022-06-15 10:23:19+00:00,Multi-Objective Hyperparameter Optimization -- An Overview,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Florian Karl'), arxiv.Result.Author('Tobias Pielok'), arxiv.Result.Author('Julia Moosbauer'), arxiv.Result.Author('Florian Pfisterer'), arxiv.Result.Author('Stefan Coors'), arxiv.Result.Author('Martin Binder'), arxiv.Result.Author('Lennart Schneider'), arxiv.Result.Author('Janek Thomas'), arxiv.Result.Author('Jakob Richter'), arxiv.Result.Author('Michel Lang'), arxiv.Result.Author('Eduardo C. Garrido-Merchán'), arxiv.Result.Author('Juergen Branke'), arxiv.Result.Author('Bernd Bischl')]","Hyperparameter optimization constitutes a large part of typical modern
machine learning workflows. This arises from the fact that machine learning
methods and corresponding preprocessing steps often only yield optimal
performance when hyperparameters are properly tuned. But in many applications,
we are not only interested in optimizing ML pipelines solely for predictive
accuracy; additional metrics or constraints must be considered when determining
an optimal configuration, resulting in a multi-objective optimization problem.
This is often neglected in practice, due to a lack of knowledge and readily
available software implementations for multi-objective hyperparameter
optimization. In this work, we introduce the reader to the basics of multi-
objective hyperparameter optimization and motivate its usefulness in applied
ML. Furthermore, we provide an extensive survey of existing optimization
strategies, both from the domain of evolutionary algorithms and Bayesian
optimization. We illustrate the utility of MOO in several specific ML
applications, considering objectives such as operating conditions, prediction
time, sparseness, fairness, interpretability and robustness."
7667,"In general, proper MOHPO evaluation is understudied and an open
challenge for further research.","They can be
used to either visualize the set of all returned fronts evaluated on the validation sets, or, as described
above, on the outer test sets.","Multi-Objective Hyperparameter Optimization – An Overview  21

4.6 Relevant benchmarks and results
In general, relevant benchmarks comparing different multi-objective optimizers for HPO mostly
were conducted in the context of new optimizers being proposed (see, e.g., Guerrero-Viu et al.",2022-06-15 10:23:19+00:00,Multi-Objective Hyperparameter Optimization -- An Overview,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Florian Karl'), arxiv.Result.Author('Tobias Pielok'), arxiv.Result.Author('Julia Moosbauer'), arxiv.Result.Author('Florian Pfisterer'), arxiv.Result.Author('Stefan Coors'), arxiv.Result.Author('Martin Binder'), arxiv.Result.Author('Lennart Schneider'), arxiv.Result.Author('Janek Thomas'), arxiv.Result.Author('Jakob Richter'), arxiv.Result.Author('Michel Lang'), arxiv.Result.Author('Eduardo C. Garrido-Merchán'), arxiv.Result.Author('Juergen Branke'), arxiv.Result.Author('Bernd Bischl')]","Hyperparameter optimization constitutes a large part of typical modern
machine learning workflows. This arises from the fact that machine learning
methods and corresponding preprocessing steps often only yield optimal
performance when hyperparameters are properly tuned. But in many applications,
we are not only interested in optimizing ML pipelines solely for predictive
accuracy; additional metrics or constraints must be considered when determining
an optimal configuration, resulting in a multi-objective optimization problem.
This is often neglected in practice, due to a lack of knowledge and readily
available software implementations for multi-objective hyperparameter
optimization. In this work, we introduce the reader to the basics of
multi-objective hyperparameter optimization and motivate its usefulness in
applied ML. Furthermore, we provide an extensive survey of existing
optimization strategies, both from the domain of evolutionary algorithms and
Bayesian optimization. We illustrate the utility of MOO in several specific ML
applications, considering objectives such as operating conditions, prediction
time, sparseness, fairness, interpretability and robustness."
7675,"rizes the most important ﬁndings and concludes with some
recommendations for further research.","Finally, Section 6 summa-     body with hand-crafted features.","We note that virtually all work on this data set develops
                                                                classiﬁers that use as input both hand-crafted (external)
2.",2022-06-15 13:42:55+00:00,BaIT: Barometer for Information Trustworthiness,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Oisín Nolan'), arxiv.Result.Author('Jeroen van Mourik'), arxiv.Result.Author('Callum Tilbury')]","This paper presents a new approach to the FNC-1 fake news classification task
which involves employing pre-trained encoder models from similar NLP tasks,
namely sentence similarity and natural language inference, and two neural
network architectures using this approach are proposed. Methods in data
augmentation are explored as a means of tackling class imbalance in the
dataset, employing common pre-existing methods and proposing a method for
sample generation in the under-represented class using a novel sentence
negation algorithm. Comparable overall performance with existing baselines is
achieved, while significantly increasing accuracy on an under-represented but
nonetheless important class for FNC-1."
7692,"the generalization performance in Table VII, domain-level
                                                                  regularization tends to have higher accuracy, possibly because
   Effect of design choices for regularizer: To further study     it allows greater diversity of representations in each domain.","Per-         plied on the features z, logits g or soft labels s. Comparing
formance increases further when both strategies are applied.","the regularizer in isolation, we apply the proposed method with
                                                                     Logits vs Soft labels vs Features: Regularizing on logits
                                        TABLE VII                             Theorem 1: [61] Let H be a hypothesis class with VC
BEARINGS: ACCURACY USING DIFFERENT REGULARIZATION FUNCTIONS.",2022-06-16 01:57:35+00:00,Domain Generalization via Selective Consistency Regularization for Time Series Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Wenyu Zhang'), arxiv.Result.Author('Mohamed Ragab'), arxiv.Result.Author('Chuan-Sheng Foo')]","Domain generalization methods aim to learn models robust to domain shift with
data from a limited number of source domains and without access to target
domain samples during training. Popular domain alignment methods for domain
generalization seek to extract domain-invariant features by minimizing the
discrepancy between feature distributions across all domains, disregarding
inter-domain relationships. In this paper, we instead propose a novel
representation learning methodology that selectively enforces prediction
consistency between source domains estimated to be closely-related.
Specifically, we hypothesize that domains share different class-informative
representations, so instead of aligning all domains which can cause negative
transfer, we only regularize the discrepancy between closely-related domains.
We apply our method to time-series classification tasks and conduct
comprehensive experiments on three public real-world datasets. Our method
significantly improves over the baseline and achieves better or competitive
performance in comparison with state-of-the-art methods in terms of both
accuracy and model calibration."
7695,"We further study (1) the extensibility of MR-MTL as a
strong baseline method to handle clustering structures of silo data distributions and (2) its ﬂexibility to handle
varying heterogeneity levels, by manually introducing two layers of heterogeneity to the MNIST dataset [52].",MR-MTL under Structured Heterogeneity.,"The ﬁrst layer is 4-way rotations: train/test images are evenly split into 4 groups of 10 silos, with each group
applying {0◦, 90◦, 180◦, 270◦} of rotation to their images.",2022-06-16 03:26:48+00:00,On Privacy and Personalization in Cross-Silo Federated Learning,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Ziyu Liu'), arxiv.Result.Author('Shengyuan Hu'), arxiv.Result.Author('Zhiwei Steven Wu'), arxiv.Result.Author('Virginia Smith')]","While the application of differential privacy (DP) has been well-studied in
cross-device federated learning (FL), there is a lack of work considering DP
for cross-silo FL, a setting characterized by a limited number of clients each
containing many data subjects. In cross-silo FL, usual notions of client-level
privacy are less suitable as real-world privacy regulations typically concern
in-silo data subjects rather than the silos themselves. In this work, we
instead consider the more realistic notion of silo-specific item-level privacy,
where silos set their own privacy targets for their local examples. Under this
setting, we reconsider the roles of personalization in federated learning. In
particular, we show that mean-regularized multi-task learning (MR-MTL), a
simple personalization framework, is a strong baseline for cross-silo FL: under
stronger privacy, silos are further incentivized to ""federate"" with each other
to mitigate DP noise, resulting in consistent improvements relative to standard
baseline methods. We provide a thorough empirical study of competing methods as
well as a theoretical characterization of MR-MTL for a mean estimation problem,
highlighting the interplay between privacy and cross-silo data heterogeneity.
Our work serves to establish baselines for private cross-silo FL as well as
identify key directions of future work in this area."
7696,"We further study (1) the extensibility of MR-MTL as a
strong baseline method to handle clustering structures of silo data distributions and (2) its ﬂexibility
to handle varying heterogeneity levels, by manually introducing two layers of heterogeneity to the
MNIST dataset [54].",MR-MTL under structured heterogeneity.,"The ﬁrst layer is 4-way rotations: train/test images are evenly split into 4 groups
of 10 silos, with each group applying {0◦, 90◦, 180◦, 270◦} of rotation to their images.",2022-06-16 03:26:48+00:00,On Privacy and Personalization in Cross-Silo Federated Learning,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Ziyu Liu'), arxiv.Result.Author('Shengyuan Hu'), arxiv.Result.Author('Zhiwei Steven Wu'), arxiv.Result.Author('Virginia Smith')]","While the application of differential privacy (DP) has been well-studied in
cross-device federated learning (FL), there is a lack of work considering DP
and its implications for cross-silo FL, a setting characterized by a limited
number of clients each containing many data subjects. In cross-silo FL, usual
notions of client-level DP are less suitable as real-world privacy regulations
typically concern the in-silo data subjects rather than the silos themselves.
In this work, we instead consider an alternative notion of silo-specific
sample-level DP, where silos set their own privacy targets for their local
examples. Under this setting, we reconsider the roles of personalization in
federated learning. In particular, we show that mean-regularized multi-task
learning (MR-MTL), a simple personalization framework, is a strong baseline for
cross-silo FL: under stronger privacy requirements, silos are incentivized to
federate more with each other to mitigate DP noise, resulting in consistent
improvements relative to standard baseline methods. We provide an empirical
study of competing methods as well as a theoretical characterization of MR-MTL
for mean estimation, highlighting the interplay between privacy and cross-silo
data heterogeneity. Our work serves to establish baselines for private
cross-silo FL as well as identify key directions of future work in this area."
7699,"further research is needed to explore the rich literature of
                                                                    constrained dynamical systems and other strategies for dealing
B. Explainability of models                                         with hard constraints.","Therefore,
a promising direction for research in DRL.","A substantial amount of explainable Artiﬁcial Intelligence       E. Robustness against data/environment
(XAI) literature is emerging on feature relevance techniques           From the perspective of practitioners, it is critical that
to explain the predictions of a deep neural network (DNN) or
explaining models that consume image source data.",2022-06-16 04:52:22+00:00,Challenges and Opportunities in Deep Reinforcement Learning with Graph Neural Networks: A Comprehensive review of Algorithms and Applications,cs.LG,['cs.LG'],"[arxiv.Result.Author('Sai Munikoti'), arxiv.Result.Author('Deepesh Agarwal'), arxiv.Result.Author('Laya Das'), arxiv.Result.Author('Mahantesh Halappanavar'), arxiv.Result.Author('Balasubramaniam Natarajan')]","Deep reinforcement learning (DRL) has empowered a variety of artificial
intelligence fields, including pattern recognition, robotics,
recommendation-systems, and gaming. Similarly, graph neural networks (GNN) have
also demonstrated their superior performance in supervised learning for
graph-structured data. In recent times, the fusion of GNN with DRL for
graph-structured environments has attracted a lot of attention. This paper
provides a comprehensive review of these hybrid works. These works can be
classified into two categories: (1) algorithmic enhancement, where DRL and GNN
complement each other for better utility; (2) application-specific enhancement,
where DRL and GNN support each other. This fusion effectively addresses various
complex problems in engineering and life sciences. Based on the review, we
further analyze the applicability and benefits of fusing these two domains,
especially in terms of increasing generalizability and reducing computational
complexity. Finally, the key challenges in integrating DRL and GNN, and
potential future research directions are highlighted, which will be of interest
to the broader machine learning community."
7700,"Therefore, further research is needed to explore
enables DRL agent to adapt by learning invariant features           the rich literature of constrained dynamical systems and other
across varied and noisy environments.",This      to improve.,We believe that al-           strategies for dealing with hard constraints.,2022-06-16 04:52:22+00:00,Challenges and Opportunities in Deep Reinforcement Learning with Graph Neural Networks: A Comprehensive review of Algorithms and Applications,cs.LG,['cs.LG'],"[arxiv.Result.Author('Sai Munikoti'), arxiv.Result.Author('Deepesh Agarwal'), arxiv.Result.Author('Laya Das'), arxiv.Result.Author('Mahantesh Halappanavar'), arxiv.Result.Author('Balasubramaniam Natarajan')]","Deep reinforcement learning (DRL) has empowered a variety of artificial
intelligence fields, including pattern recognition, robotics,
recommendation-systems, and gaming. Similarly, graph neural networks (GNN) have
also demonstrated their superior performance in supervised learning for
graph-structured data. In recent times, the fusion of GNN with DRL for
graph-structured environments has attracted a lot of attention. This paper
provides a comprehensive review of these hybrid works. These works can be
classified into two categories: (1) algorithmic enhancement, where DRL and GNN
complement each other for better utility; (2) application-specific enhancement,
where DRL and GNN support each other. This fusion effectively addresses various
complex problems in engineering and life sciences. Based on the review, we
further analyze the applicability and benefits of fusing these two domains,
especially in terms of increasing generalizability and reducing computational
complexity. Finally, the key challenges in integrating DRL and GNN, and
potential future research directions are highlighted, which will be of interest
to the broader machine learning community."
7706,"We show that by applying interval arithmetic to opera-
that would require further research to be addressed.","We
Although reformulating continual learning as a sequential          constrain the parameter search within the hyperrectangle
contraction of the model’s parameter space creates a viable        representing the region of valid solutions for the previous
theoretical framework, InterContiNet has several limitations       task.","tions present in standard neural networks in this setting we
                                                                   can derive a tractable and differentiable upper bound of the
To begin with, interval arithmetic cannot be easily combined       CL objective.",2022-06-16 08:28:37+00:00,Continual Learning with Guarantees via Weight Interval Constraints,cs.LG,['cs.LG'],"[arxiv.Result.Author('Maciej Wołczyk'), arxiv.Result.Author('Karol J. Piczak'), arxiv.Result.Author('Bartosz Wójcik'), arxiv.Result.Author('Łukasz Pustelnik'), arxiv.Result.Author('Paweł Morawiecki'), arxiv.Result.Author('Jacek Tabor'), arxiv.Result.Author('Tomasz Trzciński'), arxiv.Result.Author('Przemysław Spurek')]","We introduce a new training paradigm that enforces interval constraints on
neural network parameter space to control forgetting. Contemporary Continual
Learning (CL) methods focus on training neural networks efficiently from a
stream of data, while reducing the negative impact of catastrophic forgetting,
yet they do not provide any firm guarantees that network performance will not
deteriorate uncontrollably over time. In this work, we show how to put bounds
on forgetting by reformulating continual learning of a model as a continual
contraction of its parameter space. To that end, we propose Hyperrectangle
Training, a new training methodology where each task is represented by a
hyperrectangle in the parameter space, fully contained in the hyperrectangles
of the previous tasks. This formulation reduces the NP-hard CL problem back to
polynomial time while providing full resilience against forgetting. We validate
our claim by developing InterContiNet (Interval Continual Learning) algorithm
which leverages interval arithmetic to effectively model parameter regions as
hyperrectangles. Through experimental results, we show that our approach
performs well in a continual learning setup without storing data from previous
tasks."
7715,"They
are thus more suitable for evaluating GNNs with deeper architectures, increased receptive ﬁelds, as
well as long-range modeling, for which we provide a further study in the following analysis.","This points towards the proposed benchmarks being different from several classical benchmarks such
as Cora or Citeseer, inter alia [11], where shallow GCNs [31] fared better than deeper GCNs.",Transformers operating on fully-connected graph show the best performance.,2022-06-16 13:33:22+00:00,Long Range Graph Benchmark,cs.LG,['cs.LG'],"[arxiv.Result.Author('Vijay Prakash Dwivedi'), arxiv.Result.Author('Ladislav Rampášek'), arxiv.Result.Author('Mikhail Galkin'), arxiv.Result.Author('Ali Parviz'), arxiv.Result.Author('Guy Wolf'), arxiv.Result.Author('Anh Tuan Luu'), arxiv.Result.Author('Dominique Beaini')]","Graph Neural Networks (GNNs) that are based on the message passing (MP)
paradigm generally exchange information between 1-hop neighbors to build node
representations at each layer. In principle, such networks are not able to
capture long-range interactions (LRI) that may be desired or necessary for
learning a given task on graphs. Recently, there has been an increasing
interest in development of Transformer-based methods for graphs that can
consider full node connectivity beyond the original sparse structure, thus
enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop
message passing often fare better in several existing graph benchmarks when
combined with positional feature representations, among other innovations,
hence limiting the perceived utility and ranking of Transformer-like
architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5
graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and
Peptides-struct that arguably require LRI reasoning to achieve strong
performance in a given task. We benchmark both baseline GNNs and Graph
Transformer networks to verify that the models which capture long-range
dependencies perform significantly better on these tasks. Therefore, these
datasets are suitable for benchmarking and exploration of MP-GNNs and Graph
Transformer architectures that are intended to capture LRI."
7722,"We hope that this work will inspire further research on settings like Dixit, and help to enhance the
capabilities and adaptability of grounded language models and communicative agents more broadly.","To conclude, our work takes a step towards agents that play Dixit, and opens exciting future directions.","Acknowledgements

The authors would like to thank Frederic Besse, Denis Teplyashin, and Maria Tsimpoukelli for advice
on the engineering aspects of the work.",2022-06-16 17:52:08+00:00,Know your audience: specializing grounded language models with the game of Dixit,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Aaditya K. Singh'), arxiv.Result.Author('David Ding'), arxiv.Result.Author('Andrew Saxe'), arxiv.Result.Author('Felix Hill'), arxiv.Result.Author('Andrew K. Lampinen')]","Effective communication requires adapting to the idiosyncratic common ground
shared with each communicative partner. We study a particularly challenging
instantiation of this problem: the popular game Dixit. We formulate a round of
Dixit as a multi-agent image reference game where a (trained) speaker model is
rewarded for describing a target image such that one (pretrained) listener
model can correctly identify it from a pool of distractors, but another
listener cannot. To adapt to this setting, the speaker must exploit differences
in the common ground it shares with the different listeners. We show that
finetuning an attention-based adapter between a CLIP vision encoder and a large
language model in this contrastive, multi-agent setting gives rise to
context-dependent natural language specialization from rewards only, without
direct supervision. In a series of controlled experiments, we show that the
speaker can adapt according to the idiosyncratic strengths and weaknesses of
various pairs of different listeners. Furthermore, we show zero-shot transfer
of the speaker's specialization to unseen real-world data. Our experiments
offer a step towards adaptive communication in complex multi-partner settings
and highlight the interesting research challenges posed by games like Dixit. We
hope that our work will inspire creative new approaches to adapting pretrained
models."
7730,"connections to adaptive step size methods (Dabney, 2014)
is an interesting direction for further research.",URL.,"Applying        Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Re-
the proposed parametric updates in applied settings where            inforcement learning with deep energy-based policies.",2022-06-17 01:28:38+00:00,A Parametric Class of Approximate Gradient Updates for Policy Optimization,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ramki Gummadi'), arxiv.Result.Author('Saurabh Kumar'), arxiv.Result.Author('Junfeng Wen'), arxiv.Result.Author('Dale Schuurmans')]","Approaches to policy optimization have been motivated from diverse
principles, based on how the parametric model is interpreted (e.g. value versus
policy representation) or how the learning objective is formulated, yet they
share a common goal of maximizing expected return. To better capture the
commonalities and identify key differences between policy optimization methods,
we develop a unified perspective that re-expresses the underlying updates in
terms of a limited choice of gradient form and scaling function. In particular,
we identify a parameterized space of approximate gradient updates for policy
optimization that is highly structured, yet covers both classical and recent
examples, including PPO. As a result, we obtain novel yet well motivated
updates that generalize existing algorithms in a way that can deliver benefits
both in terms of convergence speed and final result quality. An experimental
investigation demonstrates that the additional degrees of freedom provided in
the parameterized family of updates can be leveraged to obtain non-trivial
improvements both in synthetic domains and on popular deep RL benchmarks."
7731,"2.1.2 Attack Scenarios

The above discussion reveals that current attack algorithms are developed under ambiguous settings,
and they are not categorized clearly, deeply hindering fair comparisons and further research.","adjust loss functions or optimization strategies) to inject backdoor and
train models simultaneously [15, 27, 17], or adopt two disjoint training functions, which place the
backdoor and train the victim model separately [33, 43, 38, 40, 44].","To this
end, we recommend developing, discussing, and evaluating attack algorithms under certain real-world
scenarios, where (1) the capabilities of attackers are pre-deﬁned; (2) the evaluation metrics and
models to compare with are reasonable [18].",2022-06-17 02:29:23+00:00,A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,cs.LG,"['cs.LG', 'cs.CL', 'cs.CR']","[arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Lifan Yuan'), arxiv.Result.Author('Bingxiang He'), arxiv.Result.Author('Yangyi Chen'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun')]","Textual backdoor attacks are a kind of practical threat to NLP systems. By
injecting a backdoor in the training phase, the adversary could control model
predictions via predefined triggers. As various attack and defense models have
been proposed, it is of great significance to perform rigorous evaluations.
However, we highlight two issues in previous backdoor learning evaluations: (1)
The differences between real-world scenarios (e.g. releasing poisoned datasets
or models) are neglected, and we argue that each scenario has its own
constraints and concerns, thus requires specific evaluation protocols; (2) The
evaluation metrics only consider whether the attacks could flip the models'
predictions on poisoned samples and retain performances on benign samples, but
ignore that poisoned samples should also be stealthy and semantic-preserving.
To address these issues, we categorize existing works into three practical
scenarios in which attackers release datasets, pre-trained models, and
fine-tuned models respectively, then discuss their unique evaluation
methodologies. On metrics, to completely evaluate poisoned samples, we use
grammar error increase and perplexity difference for stealthiness, along with
text similarity for validity. After formalizing the frameworks, we develop an
open-source toolkit OpenBackdoor to foster the implementations and evaluations
of textual backdoor learning. With this toolkit, we perform extensive
experiments to benchmark attack and defense models under the suggested
paradigm. To facilitate the underexplored defenses against poisoned datasets,
we further propose CUBE, a simple yet strong clustering-based defense baseline.
We hope that our frameworks and benchmarks could serve as the cornerstones for
future model development and evaluations."
7732,"2.1.2 Attack Scenarios

The above discussion reveals that current attack algorithms are developed under ambiguous settings,
and they are not categorized clearly, deeply hindering fair comparisons and further research.","adjust loss functions or optimization strategies) to inject backdoor and
train models simultaneously [25, 45, 27], or adopt two disjoint training functions, which place the
backdoor and train the victim model separately [53, 70, 64, 66, 72].","To this
end, we recommend developing, discussing, and evaluating attack algorithms under certain real-world
scenarios, where (1) the capabilities of attackers are pre-deﬁned; (2) the evaluation metrics and

                              3
models to compare with are reasonable [30].",2022-06-17 02:29:23+00:00,A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,cs.LG,"['cs.LG', 'cs.CL', 'cs.CR']","[arxiv.Result.Author('Ganqu Cui'), arxiv.Result.Author('Lifan Yuan'), arxiv.Result.Author('Bingxiang He'), arxiv.Result.Author('Yangyi Chen'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Maosong Sun')]","Textual backdoor attacks are a kind of practical threat to NLP systems. By
injecting a backdoor in the training phase, the adversary could control model
predictions via predefined triggers. As various attack and defense models have
been proposed, it is of great significance to perform rigorous evaluations.
However, we highlight two issues in previous backdoor learning evaluations: (1)
The differences between real-world scenarios (e.g. releasing poisoned datasets
or models) are neglected, and we argue that each scenario has its own
constraints and concerns, thus requires specific evaluation protocols; (2) The
evaluation metrics only consider whether the attacks could flip the models'
predictions on poisoned samples and retain performances on benign samples, but
ignore that poisoned samples should also be stealthy and semantic-preserving.
To address these issues, we categorize existing works into three practical
scenarios in which attackers release datasets, pre-trained models, and
fine-tuned models respectively, then discuss their unique evaluation
methodologies. On metrics, to completely evaluate poisoned samples, we use
grammar error increase and perplexity difference for stealthiness, along with
text similarity for validity. After formalizing the frameworks, we develop an
open-source toolkit OpenBackdoor to foster the implementations and evaluations
of textual backdoor learning. With this toolkit, we perform extensive
experiments to benchmark attack and defense models under the suggested
paradigm. To facilitate the underexplored defenses against poisoned datasets,
we further propose CUBE, a simple yet strong clustering-based defense baseline.
We hope that our frameworks and benchmarks could serve as the cornerstones for
future model development and evaluations."
7737,"Despite those ongoing efforts, a uniﬁed
                                              named Exact Penalty Optimization (EPO) and suf-            benchmark is of great relevance to facilitate further research
                                              ﬁciently demonstrate its capability in safe AD.","In addition to existing ap-         tonomous driving (AD) (Isele et al., 2018; Chen et al., 2021;
                                              proaches, we propose a novel ﬁrst-order method             Li et al., 2022).",All        on safe AD.,2022-06-17 03:23:51+00:00,SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe Autonomous Driving,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Linrui Zhang'), arxiv.Result.Author('Qin Zhang'), arxiv.Result.Author('Li Shen'), arxiv.Result.Author('Bo Yuan'), arxiv.Result.Author('Xueqian Wang')]","Safe reinforcement learning (RL) has achieved significant success on
risk-sensitive tasks and shown promise in autonomous driving (AD) as well.
Considering the distinctiveness of this community, efficient and reproducible
baselines are still lacking for safe AD. In this paper, we release SafeRL-Kit
to benchmark safe RL methods for AD-oriented tasks. Concretely, SafeRL-Kit
contains several latest algorithms specific to zero-constraint-violation tasks,
including Safety Layer, Recovery RL, off-policy Lagrangian method, and Feasible
Actor-Critic. In addition to existing approaches, we propose a novel
first-order method named Exact Penalty Optimization (EPO) and sufficiently
demonstrate its capability in safe AD. All algorithms in SafeRL-Kit are
implemented (i) under the off-policy setting, which improves sample efficiency
and can better leverage past logs; (ii) with a unified learning framework,
providing off-the-shelf interfaces for researchers to incorporate their
domain-specific knowledge into fundamental safe RL methods. Conclusively, we
conduct a comparative evaluation of the above algorithms in SafeRL-Kit and shed
light on their efficacy for safe autonomous driving. The source code is
available at \href{ https://github.com/zlr20/saferl_kit}{this https URL}."
7738,We leave the further study of these aspects of strategic representation for future work.,"In this sense, the choice of k2 also has implications on
fairness.","From a purely utilitarian point of view, it is tempting to conclude that systems should always set
k2 to be low.",2022-06-17 04:20:57+00:00,Strategic Representation,cs.LG,"['cs.LG', 'cs.GT']","[arxiv.Result.Author('Vineet Nair'), arxiv.Result.Author('Ganesh Ghalme'), arxiv.Result.Author('Inbal Talgam-Cohen'), arxiv.Result.Author('Nir Rosenfeld')]","Humans have come to rely on machines for reducing excessive information to
manageable representations. But this reliance can be abused -- strategic
machines might craft representations that manipulate their users. How can a
user make good choices based on strategic representations? We formalize this as
a learning problem, and pursue algorithms for decision-making that are robust
to manipulation. In our main setting of interest, the system represents
attributes of an item to the user, who then decides whether or not to consume.
We model this interaction through the lens of strategic classification (Hardt
et al. 2016), reversed: the user, who learns, plays first; and the system,
which responds, plays second. The system must respond with representations that
reveal `nothing but the truth' but need not reveal the entire truth. Thus, the
user faces the problem of learning set functions under strategic subset
selection, which presents distinct algorithmic and statistical challenges. Our
main result is a learning algorithm that minimizes error despite strategic
representations, and our theoretical analysis sheds light on the trade-off
between learning effort and susceptibility to manipulation."
7742,"We intend to conduct further research to
strengthen our hypothesis that HMC modeling leads to robust feature representation in future work.","However, CHAMP can use hierarchical relationships to learn richer representations
that are not over-reliant on the co-occurrence of labels.","In the initial explorations, we provide insights into CHAMP’s better performance in scenarios like
less training data and adding noise to images.",2022-06-17 09:32:48+00:00,All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label Predictions (CHAMP),cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Ashwin Vaswani'), arxiv.Result.Author('Gaurav Aggarwal'), arxiv.Result.Author('Praneeth Netrapalli'), arxiv.Result.Author('Narayan G Hegde')]","This paper considers the problem of Hierarchical Multi-Label Classification
(HMC), where (i) several labels can be present for each example, and (ii)
labels are related via a domain-specific hierarchy tree. Guided by the
intuition that all mistakes are not equal, we present Comprehensive Hierarchy
Aware Multi-label Predictions (CHAMP), a framework that penalizes a
misprediction depending on its severity as per the hierarchy tree. While there
have been works that apply such an idea to single-label classification, to the
best of our knowledge, there are limited such works for multilabel
classification focusing on the severity of mistakes. The key reason is that
there is no clear way of quantifying the severity of a misprediction a priori
in the multilabel setting. In this work, we propose a simple but effective
metric to quantify the severity of a mistake in HMC, naturally leading to
CHAMP. Extensive experiments on six public HMC datasets across modalities
(image, audio, and text) demonstrate that incorporating hierarchical
information leads to substantial gains as CHAMP improves both AUPRC (2.6%
median percentage improvement) and hierarchical metrics (2.85% median
percentage improvement), over stand-alone hierarchical or multilabel
classification methods. Compared to standard multilabel baselines, CHAMP
provides improved AUPRC in both robustness (8.87% mean percentage improvement )
and less data regimes. Further, our method provides a framework to enhance
existing multilabel classification algorithms with better mistakes (18.1% mean
percentage increment)."
7745,"We believe that this work is a promising
attempt at connecting ideas from algebraic topology and         each layer is learnt from the data through back-propagation,
differential geometry with machine learning, and hope that
it will spark further research at their intersection.","The weight matrix W(l) at
homophily levels.",by minimising some loss function (e.g.,2022-06-17 11:39:52+00:00,Sheaf Neural Networks with Connection Laplacians,cs.LG,"['cs.LG', 'math.AT', 'math.DG']","[arxiv.Result.Author('Federico Barbero'), arxiv.Result.Author('Cristian Bodnar'), arxiv.Result.Author('Haitz Sáez de Ocáriz Borde'), arxiv.Result.Author('Michael Bronstein'), arxiv.Result.Author('Petar Veličković'), arxiv.Result.Author('Pietro Liò')]","A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that
operates on a sheaf, an object that equips a graph with vector spaces over its
nodes and edges and linear maps between these spaces. SNNs have been shown to
have useful theoretical properties that help tackle issues arising from
heterophily and over-smoothing. One complication intrinsic to these models is
finding a good sheaf for the task to be solved. Previous works proposed two
diametrically opposed approaches: manually constructing the sheaf based on
domain knowledge and learning the sheaf end-to-end using gradient-based
methods. However, domain knowledge is often insufficient, while learning a
sheaf could lead to overfitting and significant computational overhead. In this
work, we propose a novel way of computing sheaves drawing inspiration from
Riemannian geometry: we leverage the manifold assumption to compute
manifold-and-graph-aware orthogonal maps, which optimally align the tangent
spaces of neighbouring data points. We show that this approach achieves
promising results with less computational overhead when compared to previous
SNN models. Overall, this work provides an interesting connection between
algebraic topology and differential geometry, and we hope that it will spark
future research in this direction."
7746,"Furthermore, we are excited by the
differential geometry work that constructs orthogonal maps                 prospect of further research tying intuition stemming from
that optimally align tangent spaces between points, relying                the ﬁelds of algebraic topology and differential geometry
on the manifold assumption.",This was done by leveraging existing                    the downstream task.,We crucially adapted this                      to machine learning.,2022-06-17 11:39:52+00:00,Sheaf Neural Networks with Connection Laplacians,cs.LG,"['cs.LG', 'math.AT', 'math.DG']","[arxiv.Result.Author('Federico Barbero'), arxiv.Result.Author('Cristian Bodnar'), arxiv.Result.Author('Haitz Sáez de Ocáriz Borde'), arxiv.Result.Author('Michael Bronstein'), arxiv.Result.Author('Petar Veličković'), arxiv.Result.Author('Pietro Liò')]","A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that
operates on a sheaf, an object that equips a graph with vector spaces over its
nodes and edges and linear maps between these spaces. SNNs have been shown to
have useful theoretical properties that help tackle issues arising from
heterophily and over-smoothing. One complication intrinsic to these models is
finding a good sheaf for the task to be solved. Previous works proposed two
diametrically opposed approaches: manually constructing the sheaf based on
domain knowledge and learning the sheaf end-to-end using gradient-based
methods. However, domain knowledge is often insufficient, while learning a
sheaf could lead to overfitting and significant computational overhead. In this
work, we propose a novel way of computing sheaves drawing inspiration from
Riemannian geometry: we leverage the manifold assumption to compute
manifold-and-graph-aware orthogonal maps, which optimally align the tangent
spaces of neighbouring data points. We show that this approach achieves
promising results with less computational overhead when compared to previous
SNN models. Overall, this work provides an interesting connection between
algebraic topology and differential geometry, and we hope that it will spark
future research in this direction."
7751,"can lay the ground for further study on these met-
                                              rics’ use in deployed machine learning systems to          • We empirically demonstrate the capacity of the met-
                                              monitor for possible attacks by adversarial exam-             ric using standard and real-world datasets subjected to
                                              ples or related pathologies such as dataset shift.",We believe that his work                samples in batches of images.,"both white- and black-box attacks aligned to recom-
                                                                                                            mendations in (Carlini & Wagner, 2017).",2022-06-17 12:52:43+00:00,Detecting Adversarial Examples in Batches -- a geometrical approach,cs.LG,['cs.LG'],"[arxiv.Result.Author('Danush Kumar Venkatesh'), arxiv.Result.Author('Peter Steinbach')]","Many deep learning methods have successfully solved complex tasks in computer
vision and speech recognition applications. Nonetheless, the robustness of
these models has been found to be vulnerable to perturbed inputs or adversarial
examples, which are imperceptible to the human eye, but lead the model to
erroneous output decisions. In this study, we adapt and introduce two geometric
metrics, density and coverage, and evaluate their use in detecting adversarial
samples in batches of unseen data. We empirically study these metrics using
MNIST and two real-world biomedical datasets from MedMNIST, subjected to two
different adversarial attacks. Our experiments show promising results for both
metrics to detect adversarial examples. We believe that his work can lay the
ground for further study on these metrics' use in deployed machine learning
systems to monitor for possible attacks by adversarial examples or related
pathologies such as dataset shift."
7752,a deviation away from the reference               believe that this can and should be subject of further study.,"We
benign samples (i.e.","metric), to that of the batch of AEs for the attack.",2022-06-17 12:52:43+00:00,Detecting Adversarial Examples in Batches -- a geometrical approach,cs.LG,['cs.LG'],"[arxiv.Result.Author('Danush Kumar Venkatesh'), arxiv.Result.Author('Peter Steinbach')]","Many deep learning methods have successfully solved complex tasks in computer
vision and speech recognition applications. Nonetheless, the robustness of
these models has been found to be vulnerable to perturbed inputs or adversarial
examples, which are imperceptible to the human eye, but lead the model to
erroneous output decisions. In this study, we adapt and introduce two geometric
metrics, density and coverage, and evaluate their use in detecting adversarial
samples in batches of unseen data. We empirically study these metrics using
MNIST and two real-world biomedical datasets from MedMNIST, subjected to two
different adversarial attacks. Our experiments show promising results for both
metrics to detect adversarial examples. We believe that his work can lay the
ground for further study on these metrics' use in deployed machine learning
systems to monitor for possible attacks by adversarial examples or related
pathologies such as dataset shift."
7765,"Experiments already show us that directly applying existing algorithms might not be a
solution, so further research on how to sample safe actions from empirical results given only historical
data might be a good starting point.","We will continue to work on the existing environments to ﬁnd a more stable, efﬁcient and productive
algorithm.","The overall good performance of PLAS hints that digging into
shaping the latent action space might be a good idea.",2022-06-17 15:51:35+00:00,SMPL: Simulated Industrial Manufacturing and Process Control Learning Environments,cs.LG,"['cs.LG', 'I.2.6']","[arxiv.Result.Author('Mohan Zhang'), arxiv.Result.Author('Xiaozhou Wang'), arxiv.Result.Author('Benjamin Decardi-Nelson'), arxiv.Result.Author('Bo Song'), arxiv.Result.Author('An Zhang'), arxiv.Result.Author('Jinfeng Liu'), arxiv.Result.Author('Sile Tao'), arxiv.Result.Author('Jiayi Cheng'), arxiv.Result.Author('Xiaohong Liu'), arxiv.Result.Author('DengDeng Yu'), arxiv.Result.Author('Matthew Poon'), arxiv.Result.Author('Animesh Garg')]","Traditional biological and pharmaceutical manufacturing plants are controlled
by human workers or pre-defined thresholds. Modernized factories have advanced
process control algorithms such as model predictive control (MPC). However,
there is little exploration of applying deep reinforcement learning to control
manufacturing plants. One of the reasons is the lack of high fidelity
simulations and standard APIs for benchmarking. To bridge this gap, we develop
an easy-to-use library that includes five high-fidelity simulation
environments: BeerFMTEnv, ReactorEnv, AtropineEnv, PenSimEnv and mAbEnv, which
cover a wide range of manufacturing processes. We build these environments on
published dynamics models. Furthermore, we benchmark online and offline,
model-based and model-free reinforcement learning algorithms for comparisons of
follow-up research."
7776,"We expect
that with further research on DDPMs for which inference procedures converge in fewer steps [35, 42],
plug-and-play use of DDPMs will become more appealing in various applications.","A notable limitation of DDPMs, which is inherited by our algorithms, is the high cost of inference,
requiring a large number of passes through the denoising network to generate a sample.","Finally, our results on the traveling salesman problem illustrate the ability of DDPMs to reason over
uncertain hypotheses in a manner that can mimic human ‘puzzle-solving’ behavior.",2022-06-17 21:11:36+00:00,Diffusion models as plug-and-play priors,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Alexandros Graikos'), arxiv.Result.Author('Nikolay Malkin'), arxiv.Result.Author('Nebojsa Jojic'), arxiv.Result.Author('Dimitris Samaras')]","We consider the problem of inferring high-dimensional data $\mathbf{x}$ in a
model that consists of a prior $p(\mathbf{x})$ and an auxiliary constraint
$c(\mathbf{x},\mathbf{y})$. In this paper, the prior is an independently
trained denoising diffusion generative model. The auxiliary constraint is
expected to have a differentiable form, but can come from diverse sources. The
possibility of such inference turns diffusion models into plug-and-play
modules, thereby allowing a range of potential applications in adapting models
to new domains and tasks, such as conditional generation or image segmentation.
The structure of diffusion models allows us to perform approximate inference by
iterating differentiation through the fixed denoising network enriched with
different amounts of noise at each step. Considering many noised versions of
$\mathbf{x}$ in evaluation of its fitness is a novel search mechanism that may
lead to new algorithms for solving combinatorial optimization problems."
7777,"We expect
that with further research on DDPMs for which inference procedures converge in fewer steps [37, 45],
plug-and-play use of DDPMs will become more appealing in various applications.","A notable limitation of DDPMs, which is inherited by our algorithms, is the high cost of inference,
requiring a large number of passes through the denoising network to generate a sample.","Finally, our results on the traveling salesman problem illustrate the ability of DDPMs to reason over
uncertain hypotheses in a manner that can mimic human ‘puzzle-solving’ behavior.",2022-06-17 21:11:36+00:00,Diffusion models as plug-and-play priors,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Alexandros Graikos'), arxiv.Result.Author('Nikolay Malkin'), arxiv.Result.Author('Nebojsa Jojic'), arxiv.Result.Author('Dimitris Samaras')]","We consider the problem of inferring high-dimensional data $\mathbf{x}$ in a
model that consists of a prior $p(\mathbf{x})$ and an auxiliary differentiable
constraint $c(\mathbf{x},\mathbf{y})$ on $x$ given some additional information
$\mathbf{y}$. In this paper, the prior is an independently trained denoising
diffusion generative model. The auxiliary constraint is expected to have a
differentiable form, but can come from diverse sources. The possibility of such
inference turns diffusion models into plug-and-play modules, thereby allowing a
range of potential applications in adapting models to new domains and tasks,
such as conditional generation or image segmentation. The structure of
diffusion models allows us to perform approximate inference by iterating
differentiation through the fixed denoising network enriched with different
amounts of noise at each step. Considering many noised versions of $\mathbf{x}$
in evaluation of its fitness is a novel search mechanism that may lead to new
algorithms for solving combinatorial optimization problems."
7779,"As such, further research and evaluation will be required prior to deployment
of this and related behavioral analysis approaches to human-facing domains.","For example, such an approach could be used to
prevent certain undesirable behaviors by agents that interact with humans (e.g, self-driving cars), but
potentially also used by adversaries to predict and exploit other behaviors (e.g., exploiting certain
human preferences for harm), or even inadvertently cause harm due to misinterpretation of certain
behavioral modes.","Nonetheless, as agent
capabilities continue to grow, our view is that behavioral analysis of multiagent systems will become
increasingly important and should complement traditional reward-based performance monitoring.",2022-06-17 23:07:33+00:00,Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis,cs.LG,"['cs.LG', 'cs.MA']","[arxiv.Result.Author('Shayegan Omidshafiei'), arxiv.Result.Author('Andrei Kapishnikov'), arxiv.Result.Author('Yannick Assogba'), arxiv.Result.Author('Lucas Dixon'), arxiv.Result.Author('Been Kim')]","Each year, expert-level performance is attained in increasingly-complex
multiagent domains, notable examples including Go, Poker, and StarCraft II.
This rapid progression is accompanied by a commensurate need to better
understand how such agents attain this performance, to enable their safe
deployment, identify limitations, and reveal potential means of improving them.
In this paper we take a step back from performance-focused multiagent learning,
and instead turn our attention towards agent behavior analysis. We introduce a
model-agnostic method for discovery of behavior clusters in multiagent domains,
using variational inference to learn a hierarchy of behaviors at the joint and
local agent levels. Our framework makes no assumption about agents' underlying
learning algorithms, does not require access to their latent states or models,
and can be trained using entirely offline observational data. We illustrate the
effectiveness of our method for enabling the coupled understanding of behaviors
at the joint and local agent level, detection of behavior changepoints
throughout training, discovery of core behavioral concepts (e.g., those that
facilitate higher returns), and demonstrate the approach's scalability to a
high-dimensional multiagent MuJoCo control domain."
7780,"As such, further research and evaluation will be required prior to deployment

                                 9
of this and related behavioral analysis approaches to human-facing domains.","For example, such an approach could be used to
prevent certain undesirable behaviors by agents that interact with humans (e.g, self-driving cars), but
potentially also used by adversaries to predict and exploit other behaviors (e.g., exploiting certain
human preferences for harm), or even inadvertently cause harm due to misinterpretation of certain
behavioral modes.","Nonetheless, as agent
capabilities continue to grow, our view is that behavioral analysis of multiagent systems will become
increasingly important and should complement traditional reward-based performance monitoring.",2022-06-17 23:07:33+00:00,Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis,cs.LG,"['cs.LG', 'cs.MA']","[arxiv.Result.Author('Shayegan Omidshafiei'), arxiv.Result.Author('Andrei Kapishnikov'), arxiv.Result.Author('Yannick Assogba'), arxiv.Result.Author('Lucas Dixon'), arxiv.Result.Author('Been Kim')]","Each year, expert-level performance is attained in increasingly-complex
multiagent domains, notable examples including Go, Poker, and StarCraft II.
This rapid progression is accompanied by a commensurate need to better
understand how such agents attain this performance, to enable their safe
deployment, identify limitations, and reveal potential means of improving them.
In this paper we take a step back from performance-focused multiagent learning,
and instead turn our attention towards agent behavior analysis. We introduce a
model-agnostic method for discovery of behavior clusters in multiagent domains,
using variational inference to learn a hierarchy of behaviors at the joint and
local agent levels. Our framework makes no assumption about agents' underlying
learning algorithms, does not require access to their latent states or
policies, and is trained using only offline observational data. We illustrate
the effectiveness of our method for enabling the coupled understanding of
behaviors at the joint and local agent level, detection of behavior
changepoints throughout training, discovery of core behavioral concepts,
demonstrate the approach's scalability to a high-dimensional multiagent MuJoCo
control domain, and also illustrate that the approach can disentangle
previously-trained policies in OpenAI's hide-and-seek domain."
7782,"Effectively, we combine ILO and Score-Based models                facilitate further research on this area.","We open-source all our code and pre-trained models to
tor.",into a single framework for inverse problems.,2022-06-18 03:47:37+00:00,Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problem,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Giannis Daras'), arxiv.Result.Author('Yuval Dagan'), arxiv.Result.Author('Alexandros G. Dimakis'), arxiv.Result.Author('Constantinos Daskalakis')]","We prove fast mixing and characterize the stationary distribution of the
Langevin Algorithm for inverting random weighted DNN generators. This result
extends the work of Hand and Voroninski from efficient inversion to efficient
posterior sampling. In practice, to allow for increased expressivity, we
propose to do posterior sampling in the latent space of a pre-trained
generative model. To achieve that, we train a score-based model in the latent
space of a StyleGAN-2 and we use it to solve inverse problems. Our framework,
Score-Guided Intermediate Layer Optimization (SGILO), extends prior work by
replacing the sparsity regularization with a generative prior in the
intermediate layer. Experimentally, we obtain significant improvements over the
previous state-of-the-art, especially in the low measurement regime."
7783,"Effectively, we combine ILO and Score-Based models                facilitate further research on this area.","We open-source all our code and pre-trained models to
tor.",into a single framework for inverse problems.,2022-06-18 03:47:37+00:00,Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Giannis Daras'), arxiv.Result.Author('Yuval Dagan'), arxiv.Result.Author('Alexandros G. Dimakis'), arxiv.Result.Author('Constantinos Daskalakis')]","We prove fast mixing and characterize the stationary distribution of the
Langevin Algorithm for inverting random weighted DNN generators. This result
extends the work of Hand and Voroninski from efficient inversion to efficient
posterior sampling. In practice, to allow for increased expressivity, we
propose to do posterior sampling in the latent space of a pre-trained
generative model. To achieve that, we train a score-based model in the latent
space of a StyleGAN-2 and we use it to solve inverse problems. Our framework,
Score-Guided Intermediate Layer Optimization (SGILO), extends prior work by
replacing the sparsity regularization with a generative prior in the
intermediate layer. Experimentally, we obtain significant improvements over the
previous state-of-the-art, especially in the low measurement regime."
7784,"To further study the magnitude of the error bound in Theorem B.1, we introduce the next
deﬁnition and Lemma B.2 following the discussion in Yan and Bien (2021).","In the case of a “slim” tree where the maximum number of child nodes is
a relatively small value, this framework is expected to show good performance in prediction.","Deﬁnition Denote V (T) as the set of nodes in tree T. The set of nodes B ⊆ V (T) is an aggre-
gation set with respect to tree T if {L(u) : u ∈ B} forms a partition of L(T).",2022-06-18 03:52:43+00:00,Tree-Guided Rare Feature Selection and Logic Aggregation with Electronic Health Records Data,cs.LG,"['cs.LG', 'stat.AP', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Jianmin Chen'), arxiv.Result.Author('Robert H. Aseltine'), arxiv.Result.Author('Fei Wang'), arxiv.Result.Author('Kun Chen')]","Statistical learning with a large number of rare binary features is commonly
encountered in analyzing electronic health records (EHR) data, especially in
the modeling of disease onset with prior medical diagnoses and procedures.
Dealing with the resulting highly sparse and large-scale binary feature matrix
is notoriously challenging as conventional methods may suffer from a lack of
power in testing and inconsistency in model fitting while machine learning
methods may suffer from the inability of producing interpretable results or
clinically-meaningful risk factors. To improve EHR-based modeling and utilize
the natural hierarchical structure of disease classification, we propose a
tree-guided feature selection and logic aggregation approach for large-scale
regression with rare binary features, in which dimension reduction is achieved
through not only a sparsity pursuit but also an aggregation promoter with the
logic operator of ``or''. We convert the combinatorial problem into a convex
linearly-constrained regularized estimation, which enables scalable computation
with theoretical guarantees. In a suicide risk study with EHR data, our
approach is able to select and aggregate prior mental health diagnoses as
guided by the diagnosis hierarchy of the International Classification of
Diseases. By balancing the rarity and specificity of the EHR diagnosis records,
our strategy improves both prediction and model interpretation. We identify
important higher-level categories and subcategories of mental health conditions
and simultaneously determine the level of specificity needed for each of them
in predicting suicide risk."
7787,"However, two key challenges seriously
                                                   hinder the further research of GraphNAS.","arXiv:2206.09166v1 [cs.LG] 18 Jun 2022    NAS-Bench-Graph: Benchmarking Graph Neural
                                                             Architecture Search

                                                          Yijian Qin, Ziwei Zhang, Xin Wang∗, Zeyang Zhang, Wenwu Zhu∗
                                                                     Department of Computer Science and Technology
                                                                                        Tsinghua University
                                                                                      Beijing, China 100084

                                                 qinyj19@mails.tsinghua.edu.cn, {zwzhang,xin_wang}@tsinghua.edu.cn
                                                         zy-zhang20@mails.tsinghua.edu.cn, wwzhu@tsinghua.edu.cn

                                                                                    Abstract

                                                   Graph neural architecture search (GraphNAS) has recently aroused considerable
                                                   attention in both academia and industry.","First, since there is no consensus for the
                                                   experimental setting, the empirical results in different research papers are often not
                                                   comparable and even not reproducible, leading to unfair comparisons.",2022-06-18 10:17:15+00:00,NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yijian Qin'), arxiv.Result.Author('Ziwei Zhang'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Zeyang Zhang'), arxiv.Result.Author('Wenwu Zhu')]","Graph neural architecture search (GraphNAS) has recently aroused considerable
attention in both academia and industry. However, two key challenges seriously
hinder the further research of GraphNAS. First, since there is no consensus for
the experimental setting, the empirical results in different research papers
are often not comparable and even not reproducible, leading to unfair
comparisons. Secondly, GraphNAS often needs extensive computations, which makes
it highly inefficient and inaccessible to researchers without access to
large-scale computation. To solve these challenges, we propose NAS-Bench-Graph,
a tailored benchmark that supports unified, reproducible, and efficient
evaluations for GraphNAS. Specifically, we construct a unified, expressive yet
compact search space, covering 26,206 unique graph neural network (GNN)
architectures and propose a principled evaluation protocol. To avoid
unnecessary repetitive training, we have trained and evaluated all of these
architectures on nine representative graph datasets, recording detailed metrics
including train, validation, and test performance in each epoch, the latency,
the number of parameters, etc. Based on our proposed benchmark, the performance
of GNN architectures can be directly obtained by a look-up table without any
further computation, which enables fair, fully reproducible, and efficient
comparisons. To demonstrate its usage, we make in-depth analyses of our
proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS.
We also showcase how the benchmark can be easily compatible with GraphNAS open
libraries such as AutoGL and NNI. To the best of our knowledge, our work is the
first benchmark for graph neural architecture search."
7788,These observations may inspire further research of evolution algorithms for GraphNAS.,"Besides, we observe that the performance difference increases as the number of mutations in general
and changing operation choices usually leads to smaller differences than changing the macro space
choices.",Reinforcement Learning (RL) is also widely adopted in NAS.,2022-06-18 10:17:15+00:00,NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yijian Qin'), arxiv.Result.Author('Ziwei Zhang'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Zeyang Zhang'), arxiv.Result.Author('Wenwu Zhu')]","Graph neural architecture search (GraphNAS) has recently aroused considerable
attention in both academia and industry. However, two key challenges seriously
hinder the further research of GraphNAS. First, since there is no consensus for
the experimental setting, the empirical results in different research papers
are often not comparable and even not reproducible, leading to unfair
comparisons. Secondly, GraphNAS often needs extensive computations, which makes
it highly inefficient and inaccessible to researchers without access to
large-scale computation. To solve these challenges, we propose NAS-Bench-Graph,
a tailored benchmark that supports unified, reproducible, and efficient
evaluations for GraphNAS. Specifically, we construct a unified, expressive yet
compact search space, covering 26,206 unique graph neural network (GNN)
architectures and propose a principled evaluation protocol. To avoid
unnecessary repetitive training, we have trained and evaluated all of these
architectures on nine representative graph datasets, recording detailed metrics
including train, validation, and test performance in each epoch, the latency,
the number of parameters, etc. Based on our proposed benchmark, the performance
of GNN architectures can be directly obtained by a look-up table without any
further computation, which enables fair, fully reproducible, and efficient
comparisons. To demonstrate its usage, we make in-depth analyses of our
proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS.
We also showcase how the benchmark can be easily compatible with GraphNAS open
libraries such as AutoGL and NNI. To the best of our knowledge, our work is the
first benchmark for graph neural architecture search."
7789,The results indicate that further research on GraphNAS is still urgently needed.,"Surprisingly, Random Search is still a strong baseline when compared with other
     methods and even performs the best on two datasets, partially corroborating the ﬁndings in [28] for
     general NAS.","6 Conclusion

     In this paper, we propose NAS-Bench-Graph, the ﬁrst tailored NAS benchmark for graph neural
     networks.",2022-06-18 10:17:15+00:00,NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yijian Qin'), arxiv.Result.Author('Ziwei Zhang'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Zeyang Zhang'), arxiv.Result.Author('Wenwu Zhu')]","Graph neural architecture search (GraphNAS) has recently aroused considerable
attention in both academia and industry. However, two key challenges seriously
hinder the further research of GraphNAS. First, since there is no consensus for
the experimental setting, the empirical results in different research papers
are often not comparable and even not reproducible, leading to unfair
comparisons. Secondly, GraphNAS often needs extensive computations, which makes
it highly inefficient and inaccessible to researchers without access to
large-scale computation. To solve these challenges, we propose NAS-Bench-Graph,
a tailored benchmark that supports unified, reproducible, and efficient
evaluations for GraphNAS. Specifically, we construct a unified, expressive yet
compact search space, covering 26,206 unique graph neural network (GNN)
architectures and propose a principled evaluation protocol. To avoid
unnecessary repetitive training, we have trained and evaluated all of these
architectures on nine representative graph datasets, recording detailed metrics
including train, validation, and test performance in each epoch, the latency,
the number of parameters, etc. Based on our proposed benchmark, the performance
of GNN architectures can be directly obtained by a look-up table without any
further computation, which enables fair, fully reproducible, and efficient
comparisons. To demonstrate its usage, we make in-depth analyses of our
proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS.
We also showcase how the benchmark can be easily compatible with GraphNAS open
libraries such as AutoGL and NNI. To the best of our knowledge, our work is the
first benchmark for graph neural architecture search."
7809,"To further study the impact of our
                                        techniques we present experiments on classifying synthetic multivariate time
                                        series datasets with more than 100 channels, as well as a real-world case study
                                        on a dataset with 50 channels.","Furthermore, our methods enable even eﬃcient classiﬁers,
                                        such as ROCKET, to achieve better accuracy as compared to using no chan-
                                        nel selection or forward channel selection.","We ﬁnd that our channel selection methods
                                        lead to signiﬁcant data reduction with preserved or improved accuracy.",2022-06-18 19:57:46+00:00,Scalable Classifier-Agnostic Channel Selection for MTSC,cs.LG,['cs.LG'],"[arxiv.Result.Author('Bhaskar Dhariyal'), arxiv.Result.Author('Thach Le Nguyen'), arxiv.Result.Author('Georgiana Ifrim')]","Accuracy is a key focus of current work in time series classification.
However, speed and data reduction in many applications is equally important,
especially when the data scale and storage requirements increase rapidly.
Current MTSC algorithms need hundreds of compute hours to complete training and
prediction. This is due to the nature of multivariate time series data, which
grows with the number of time series, their length and the number of channels.
In many applications, not all the channels are useful for the classification
task; hence we require methods that can efficiently select useful channels and
thus save computational resources. We propose and evaluate two methods for
channel selection. Our techniques work by representing each class by a
prototype time series and performing channel selection based on the prototype
distance between classes. The main hypothesis is that useful channels enable
better separation between classes; hence, channels with the higher distance
between class prototypes are more useful. On the UEA Multivariate Time Series
Classification (MTSC) benchmark, we show that these techniques achieve
significant data reduction and classifier speedup for similar levels of
classification accuracy. Channel selection is applied as a pre-processing step
before training state-of-the-art MTSC algorithms and saves about 70\% of
computation time and data storage, with preserved accuracy. Furthermore, our
methods enable even efficient classifiers, such as ROCKET, to achieve better
accuracy than using no channel selection or forward channel selection. To
further study the impact of our techniques, we present experiments on
classifying synthetic multivariate time series datasets with more than 100
channels, as well as a real-world case study on a dataset with 50 channels. Our
channel selection methods lead to significant data reduction with preserved or
improved accuracy."
7816,"They
not seen much further research.","Zhou and        workers is also necessary to improve the quality of labels
He [95] made the ﬁrst attempt on this topic, however, we have      and should be treated as a strategy in active learning.","Some weakly supervised learn-      designed a strategy that picks the workers that are most bene-
ing techniques, such as graph matching in semi-supervised          ﬁcial to the performance improvement of the current learning
learning, self-taught learning [96], data programming [97],        model during each iteration of active learning.",2022-06-19 03:06:23+00:00,Knowledge Learning with Crowdsourcing: A Brief Review and Systematic Perspective,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC']",[arxiv.Result.Author('Jing Zhang')],"Big data have the characteristics of enormous volume, high velocity,
diversity, value-sparsity, and uncertainty, which lead the knowledge learning
from them full of challenges. With the emergence of crowdsourcing, versatile
information can be obtained on-demand so that the wisdom of crowds is easily
involved to facilitate the knowledge learning process. During the past thirteen
years, researchers in the AI community made great efforts to remove the
obstacles in the field of learning from crowds. This concentrated survey paper
comprehensively reviews the technical progress in crowdsourcing learning from a
systematic perspective that includes three dimensions of data, models, and
learning processes. In addition to reviewing existing important work, the paper
places a particular emphasis on providing some promising blueprints on each
dimension as well as discussing the lessons learned from our past research
work, which will light up the way for new researchers and encourage them to
pursue new contributions."
7818,"To further study how Fair-
a 2-dimensional isotropic Gaussian distribution with different mean         PUL performs, we conduct experiments on three publicly available
𝜇 and variance 𝜎2 : (i) Group 𝑎, Label 1: 𝜇 = (-1, -1), 𝜎2 = 0.8; (ii)      real-world data sets.",Each set of examples is sampled from           5.2.1 Data sets and experimental setup.,"In all the experiments, to obtain reliable es-
Group 𝑎, Label 0: 𝜇 = (1, 1), 𝜎2 = 0.8; (iii) Group 𝑏, Label 1: 𝜇 = (-0.5,  timates of classification performance and fairness, we repeatedly
-0.5), 𝜎2 = 0.5; (iv) Group 𝑏, Label 0: 𝜇 = (0.5, 0.5), 𝜎2 = 0.5.",2022-06-19 08:04:23+00:00,Fairness-aware Model-agnostic Positive and Unlabeled Learning,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Ziwei Wu'), arxiv.Result.Author('Jingrui He')]","With the increasing application of machine learning in high-stake
decision-making problems, potential algorithmic bias towards people from
certain social groups poses negative impacts on individuals and our society at
large. In the real-world scenario, many such problems involve positive and
unlabeled data such as medical diagnosis, criminal risk assessment and
recommender systems. For instance, in medical diagnosis, only the diagnosed
diseases will be recorded (positive) while others will not (unlabeled). Despite
the large amount of existing work on fairness-aware machine learning in the
(semi-)supervised and unsupervised settings, the fairness issue is largely
under-explored in the aforementioned Positive and Unlabeled Learning (PUL)
context, where it is usually more severe. In this paper, to alleviate this
tension, we propose a fairness-aware PUL method named FairPUL. In particular,
for binary classification over individuals from two populations, we aim to
achieve similar true positive rates and false positive rates in both
populations as our fairness metric. Based on the analysis of the optimal fair
classifier for PUL, we design a model-agnostic post-processing framework,
leveraging both the positive examples and unlabeled ones. Our framework is
proven to be statistically consistent in terms of both the classification error
and the fairness metric. Experiments on the synthetic and real-world data sets
demonstrate that our framework outperforms state-of-the-art in both PUL and
fair classification."
7819,"To further study how un-
100%                                                                        labeled examples affect our framework, we fix the number of posi-
90%    Lin.SVM FaNiarïPvUe L  0.604±0.018  0.194±0.020         0.219±0.032  tive examples and compare FairPUL’s performance with different
80%                           0.650±0.014  0.008±0.004         0.004±0.002  numbers of unlabeled examples.",Rates          Models              F1          AOD                 EOD      5.4.2 Effect of unlabeled samples (RQ3).,"Since the benchmark data sets
50%    Lin.LR        Naïve                                                  are not provided with additional unlabeled data, we deploy the
                    FairPUL   0.621±0.015  0.236±0.033         0.293±0.045  following data generation procedure: we randomly select 50% ex-
                              0.660±0.013  0.018±0.005         0.012±0.009  amples in the original training set.",2022-06-19 08:04:23+00:00,Fairness-aware Model-agnostic Positive and Unlabeled Learning,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Ziwei Wu'), arxiv.Result.Author('Jingrui He')]","With the increasing application of machine learning in high-stake
decision-making problems, potential algorithmic bias towards people from
certain social groups poses negative impacts on individuals and our society at
large. In the real-world scenario, many such problems involve positive and
unlabeled data such as medical diagnosis, criminal risk assessment and
recommender systems. For instance, in medical diagnosis, only the diagnosed
diseases will be recorded (positive) while others will not (unlabeled). Despite
the large amount of existing work on fairness-aware machine learning in the
(semi-)supervised and unsupervised settings, the fairness issue is largely
under-explored in the aforementioned Positive and Unlabeled Learning (PUL)
context, where it is usually more severe. In this paper, to alleviate this
tension, we propose a fairness-aware PUL method named FairPUL. In particular,
for binary classification over individuals from two populations, we aim to
achieve similar true positive rates and false positive rates in both
populations as our fairness metric. Based on the analysis of the optimal fair
classifier for PUL, we design a model-agnostic post-processing framework,
leveraging both the positive examples and unlabeled ones. Our framework is
proven to be statistically consistent in terms of both the classification error
and the fairness metric. Experiments on the synthetic and real-world data sets
demonstrate that our framework outperforms state-of-the-art in both PUL and
fair classification."
7828,"We further study the scalability of the partic-        Table 2 Test accuracy (%) of VolcanoML with and with-
ipant systems on the three aforementioned search           out the enrichment of “smote balancer” operator.",sidering embeddings.,spaces.,2022-06-19 14:53:29+00:00,Efficient End-to-End AutoML via Scalable Search Space Decomposition,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Yu Shen'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Bin Cui')]","End-to-end AutoML has attracted intensive interests from both academia and
industry which automatically searches for ML pipelines in a space induced by
feature engineering, algorithm/model selection, and hyper-parameter tuning.
Existing AutoML systems, however, suffer from scalability issues when applying
to application domains with large, high-dimensional search spaces. We present
VolcanoML, a scalable and extensible framework that facilitates systematic
exploration of large AutoML search spaces. VolcanoML introduces and implements
basic building blocks that decompose a large search space into smaller ones,
and allows users to utilize these building blocks to compose an execution plan
for the AutoML problem at hand. VolcanoML further supports a Volcano-style
execution model -- akin to the one supported by modern database systems -- to
execute the plan constructed. Our evaluation demonstrates that, not only does
VolcanoML raise the level of expressiveness for search space decomposition in
AutoML, it also leads to actual findings of decomposition strategies that are
significantly more efficient than the ones employed by state-of-the-art AutoML
systems such as auto-sklearn."
7829,"We can observe that, with
the increase of time budget and search space, Vol-
18                                                                   Yang Li, Yu Shen, Wentao Zhang, Ce Zhang, Bin Cui

This demonstrates the more powerful expressiveness of       execution plan (Plan 5) in VolcanoML sets up a very
neural networks (RankNet) than traditional ML algo-         competitive AutoML baseline for our further research
rithms (LightGBM ).",given diﬀerent time budgets.,"on VolcanoML, e.g., automatic plan generation.",2022-06-19 14:53:29+00:00,Efficient End-to-End AutoML via Scalable Search Space Decomposition,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Yu Shen'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Bin Cui')]","End-to-end AutoML has attracted intensive interests from both academia and
industry which automatically searches for ML pipelines in a space induced by
feature engineering, algorithm/model selection, and hyper-parameter tuning.
Existing AutoML systems, however, suffer from scalability issues when applying
to application domains with large, high-dimensional search spaces. We present
VolcanoML, a scalable and extensible framework that facilitates systematic
exploration of large AutoML search spaces. VolcanoML introduces and implements
basic building blocks that decompose a large search space into smaller ones,
and allows users to utilize these building blocks to compose an execution plan
for the AutoML problem at hand. VolcanoML further supports a Volcano-style
execution model -- akin to the one supported by modern database systems -- to
execute the plan constructed. Our evaluation demonstrates that, not only does
VolcanoML raise the level of expressiveness for search space decomposition in
AutoML, it also leads to actual findings of decomposition strategies that are
significantly more efficient than the ones employed by state-of-the-art AutoML
systems such as auto-sklearn."
7830,"We further study the scalability of the partic-        Table 2 Test accuracy (%) of VolcanoML with and with-
ipant systems on the three aforementioned search           out the enrichment of “smote balancer” operator.",sidering embeddings.,spaces.,2022-06-19 14:53:29+00:00,Efficient End-to-End AutoML via Scalable Search Space Decomposition,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Yu Shen'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Bin Cui')]","End-to-end AutoML has attracted intensive interests from both academia and
industry which automatically searches for ML pipelines in a space induced by
feature engineering, algorithm/model selection, and hyper-parameter tuning.
Existing AutoML systems, however, suffer from scalability issues when applying
to application domains with large, high-dimensional search spaces. We present
VolcanoML, a scalable and extensible framework that facilitates systematic
exploration of large AutoML search spaces. VolcanoML introduces and implements
basic building blocks that decompose a large search space into smaller ones,
and allows users to utilize these building blocks to compose an execution plan
for the AutoML problem at hand. VolcanoML further supports a Volcano-style
execution model -- akin to the one supported by modern database systems -- to
execute the plan constructed. Our evaluation demonstrates that, not only does
VolcanoML raise the level of expressiveness for search space decomposition in
AutoML, it also leads to actual findings of decomposition strategies that are
significantly more efficient than the ones employed by state-of-the-art AutoML
systems such as auto-sklearn. This paper is the extended version of the initial
VolcanoML paper appeared in VLDB 2021."
7831,"We can observe that, with
the increase of time budget and search space, Vol-
18                                                                   Yang Li, Yu Shen, Wentao Zhang, Ce Zhang, Bin Cui

This demonstrates the more powerful expressiveness of       execution plan (Plan 5) in VolcanoML sets up a very
neural networks (RankNet) than traditional ML algo-         competitive AutoML baseline for our further research
rithms (LightGBM ).",given diﬀerent time budgets.,"on VolcanoML, e.g., automatic plan generation.",2022-06-19 14:53:29+00:00,Efficient End-to-End AutoML via Scalable Search Space Decomposition,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Yu Shen'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Bin Cui')]","End-to-end AutoML has attracted intensive interests from both academia and
industry which automatically searches for ML pipelines in a space induced by
feature engineering, algorithm/model selection, and hyper-parameter tuning.
Existing AutoML systems, however, suffer from scalability issues when applying
to application domains with large, high-dimensional search spaces. We present
VolcanoML, a scalable and extensible framework that facilitates systematic
exploration of large AutoML search spaces. VolcanoML introduces and implements
basic building blocks that decompose a large search space into smaller ones,
and allows users to utilize these building blocks to compose an execution plan
for the AutoML problem at hand. VolcanoML further supports a Volcano-style
execution model -- akin to the one supported by modern database systems -- to
execute the plan constructed. Our evaluation demonstrates that, not only does
VolcanoML raise the level of expressiveness for search space decomposition in
AutoML, it also leads to actual findings of decomposition strategies that are
significantly more efficient than the ones employed by state-of-the-art AutoML
systems such as auto-sklearn. This paper is the extended version of the initial
VolcanoML paper appeared in VLDB 2021."
7834,"We hope
this work leads to further research which will improve results by utilising advances in RL or other methods.","As the results show, our RL-based method - LUNATC, clearly outperformed all baselines for the formulation.","We hope
that future work assesses the agent’s ability to use non-greedy actions, and a stop-game action, instead of relying on
oracle access to check for classiﬁcation change after each action.",2022-06-19 17:55:47+00:00,A Universal Adversarial Policy for Text Classifiers,cs.LG,"['cs.LG', 'cs.CL', 'cs.CR']","[arxiv.Result.Author('Gallil Maimon'), arxiv.Result.Author('Lior Rokach')]","Discovering the existence of universal adversarial perturbations had large
theoretical and practical impacts on the field of adversarial learning. In the
text domain, most universal studies focused on adversarial prefixes which are
added to all texts. However, unlike the vision domain, adding the same
perturbation to different inputs results in noticeably unnatural inputs.
Therefore, we introduce a new universal adversarial setup - a universal
adversarial policy, which has many advantages of other universal attacks but
also results in valid texts - thus making it relevant in practice. We achieve
this by learning a single search policy over a predefined set of semantics
preserving text alterations, on many texts. This formulation is universal in
that the policy is successful in finding adversarial examples on new texts
efficiently. Our approach uses text perturbations which were extensively shown
to produce natural attacks in the non-universal setup (specific synonym
replacements). We suggest a strong baseline approach for this formulation which
uses reinforcement learning. It's ability to generalise (from as few as 500
training texts) shows that universal adversarial patterns exist in the text
domain as well."
7843,"To understand the reasons behind the performance of
an explainabiality method, we can further study the properties of its explanations.",Mask properties and method performance.,"We look at four
properties of the mask:

 • Mask size: the number of edges selected by the method before any mask transformation.",2022-06-20 09:33:12+00:00,GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kenza Amara'), arxiv.Result.Author('Rex Ying'), arxiv.Result.Author('Zitao Zhang'), arxiv.Result.Author('Zhihao Han'), arxiv.Result.Author('Yinan Shan'), arxiv.Result.Author('Ulrik Brandes'), arxiv.Result.Author('Sebastian Schemm'), arxiv.Result.Author('Ce Zhang')]","As one of the most popular machine learning models today, graph neural
networks (GNNs) have attracted intense interest recently, and so does their
explainability. Users are increasingly interested in a better understanding of
GNN models and their outcomes. Unfortunately, today's evaluation frameworks for
GNN explainability often rely on synthetic datasets, leading to conclusions of
limited scope due to a lack of complexity in the problem instances. As GNN
models are deployed to more mission-critical applications, we are in dire need
for a common evaluation protocol of explainability methods of GNNs. In this
paper, we propose, to our best knowledge, the first systematic evaluation
framework for GNN explainability, considering explainability on three different
""user needs:"" explanation focus, mask nature, and mask transformation. We
propose a unique metric that combines the fidelity measures and classify
explanations based on their quality of being sufficient or necessary. We scope
ourselves to node classification tasks and compare the most representative
techniques in the field of input-level explainability for GNNs. For the widely
used synthetic benchmarks, surprisingly shallow techniques such as personalized
PageRank have the best performance for a minimum computation time. But when the
graph structure is more complex and nodes have meaningful features,
gradient-based methods, in particular Saliency, are the best according to our
evaluation criteria. However, none dominates the others on all evaluation
dimensions and there is always a trade-off. We further apply our evaluation
protocol in a case study on eBay graphs to reflect the production environment."
7844,"To understand the reasons behind the performance of
an explainabiality method, we can further study the properties of its explanations.",Mask properties and method performance.,"We look at four
properties of the mask:

 • Mask size: the number of edges selected by the method before any mask transformation.",2022-06-20 09:33:12+00:00,GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kenza Amara'), arxiv.Result.Author('Rex Ying'), arxiv.Result.Author('Zitao Zhang'), arxiv.Result.Author('Zhihao Han'), arxiv.Result.Author('Yinan Shan'), arxiv.Result.Author('Ulrik Brandes'), arxiv.Result.Author('Sebastian Schemm'), arxiv.Result.Author('Ce Zhang')]","As one of the most popular machine learning models today, graph neural
networks (GNNs) have attracted intense interest recently, and so does their
explainability. Users are increasingly interested in a better understanding of
GNN models and their outcomes. Unfortunately, today's evaluation frameworks for
GNN explainability often rely on synthetic datasets, leading to conclusions of
limited scope due to a lack of complexity in the problem instances. As GNN
models are deployed to more mission-critical applications, we are in dire need
for a common evaluation protocol of explainability methods of GNNs. In this
paper, we propose, to our best knowledge, the first systematic evaluation
framework for GNN explainability, considering explainability on three different
""user needs:"" explanation focus, mask nature, and mask transformation. We
propose a unique metric that combines the fidelity measures and classify
explanations based on their quality of being sufficient or necessary. We scope
ourselves to node classification tasks and compare the most representative
techniques in the field of input-level explainability for GNNs. For the widely
used synthetic benchmarks, surprisingly shallow techniques such as personalized
PageRank have the best performance for a minimum computation time. But when the
graph structure is more complex and nodes have meaningful features,
gradient-based methods, in particular Saliency, are the best according to our
evaluation criteria. However, none dominates the others on all evaluation
dimensions and there is always a trade-off. We further apply our evaluation
protocol in a case study on eBay graphs to reflect the production environment."
7845,"To understand the reasons behind the performance of
an explainabiality method, we can further study the properties of its explanations.",Mask properties and method performance.,"We look at four
properties of the mask:

 • Mask size: the number of edges selected by the method before any mask transformation.",2022-06-20 09:33:12+00:00,GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kenza Amara'), arxiv.Result.Author('Rex Ying'), arxiv.Result.Author('Zitao Zhang'), arxiv.Result.Author('Zhihao Han'), arxiv.Result.Author('Yinan Shan'), arxiv.Result.Author('Ulrik Brandes'), arxiv.Result.Author('Sebastian Schemm'), arxiv.Result.Author('Ce Zhang')]","As one of the most popular machine learning models today, graph neural
networks (GNNs) have attracted intense interest recently, and so does their
explainability. Users are increasingly interested in a better understanding of
GNN models and their outcomes. Unfortunately, today's evaluation frameworks for
GNN explainability often rely on synthetic datasets, leading to conclusions of
limited scope due to a lack of complexity in the problem instances. As GNN
models are deployed to more mission-critical applications, we are in dire need
for a common evaluation protocol of explainability methods of GNNs. In this
paper, we propose, to our best knowledge, the first systematic evaluation
framework for GNN explainability, considering explainability on three different
""user needs:"" explanation focus, mask nature, and mask transformation. We
propose a unique metric that combines the fidelity measures and classify
explanations based on their quality of being sufficient or necessary. We scope
ourselves to node classification tasks and compare the most representative
techniques in the field of input-level explainability for GNNs. For the widely
used synthetic benchmarks, surprisingly shallow techniques such as personalized
PageRank have the best performance for a minimum computation time. But when the
graph structure is more complex and nodes have meaningful features,
gradient-based methods, in particular Saliency, are the best according to our
evaluation criteria. However, none dominates the others on all evaluation
dimensions and there is always a trade-off. We further apply our evaluation
protocol in a case study on eBay graphs to reflect the production environment."
7846,"To understand the reasons behind the performance of
an explainabiality method, we can further study the properties of its explanations.",Mask properties and method performance.,"We look at four
properties of the mask:

                                                          20
• Mask size: the number of edges selected by the method before any mask transformation.",2022-06-20 09:33:12+00:00,GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kenza Amara'), arxiv.Result.Author('Rex Ying'), arxiv.Result.Author('Zitao Zhang'), arxiv.Result.Author('Zhihao Han'), arxiv.Result.Author('Yinan Shan'), arxiv.Result.Author('Ulrik Brandes'), arxiv.Result.Author('Sebastian Schemm'), arxiv.Result.Author('Ce Zhang')]","As one of the most popular machine learning models today, graph neural
networks (GNNs) have attracted intense interest recently, and so does their
explainability. Users are increasingly interested in a better understanding of
GNN models and their outcomes. Unfortunately, today's evaluation frameworks for
GNN explainability often rely on few inadequate synthetic datasets, leading to
conclusions of limited scope due to a lack of complexity in the problem
instances. As GNN models are deployed to more mission-critical applications, we
are in dire need for a common evaluation protocol of explainability methods of
GNNs. In this paper, we propose, to our best knowledge, the first systematic
evaluation framework for GNN explainability, considering explainability on
three different ""user needs"". We propose a unique metric that combines the
fidelity measures and classifies explanations based on their quality of being
sufficient or necessary. We scope ourselves to node classification tasks and
compare the most representative techniques in the field of input-level
explainability for GNNs. For the inadequate but widely used synthetic
benchmarks, surprisingly shallow techniques such as personalized PageRank have
the best performance for a minimum computation time. But when the graph
structure is more complex and nodes have meaningful features, gradient-based
methods are the best according to our evaluation criteria. However, none
dominates the others on all evaluation dimensions and there is always a
trade-off. We further apply our evaluation protocol in a case study for frauds
explanation on eBay transaction graphs to reflect the production environment."
7847,"Based on these ﬁndings, we would like to further study the
relationship between GNN models and epxlainability.","The
results are more consistent for Fidelity+.","D Evaluation framework of explainability methods for graph classiﬁcation

In this paper, we highlight the limitations of evaluation protocol of the explainability methods in the
context of node classiﬁcation.",2022-06-20 09:33:12+00:00,GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Kenza Amara'), arxiv.Result.Author('Rex Ying'), arxiv.Result.Author('Zitao Zhang'), arxiv.Result.Author('Zhihao Han'), arxiv.Result.Author('Yinan Shan'), arxiv.Result.Author('Ulrik Brandes'), arxiv.Result.Author('Sebastian Schemm'), arxiv.Result.Author('Ce Zhang')]","As one of the most popular machine learning models today, graph neural
networks (GNNs) have attracted intense interest recently, and so does their
explainability. Users are increasingly interested in a better understanding of
GNN models and their outcomes. Unfortunately, today's evaluation frameworks for
GNN explainability often rely on few inadequate synthetic datasets, leading to
conclusions of limited scope due to a lack of complexity in the problem
instances. As GNN models are deployed to more mission-critical applications, we
are in dire need for a common evaluation protocol of explainability methods of
GNNs. In this paper, we propose, to our best knowledge, the first systematic
evaluation framework for GNN explainability, considering explainability on
three different ""user needs"". We propose a unique metric that combines the
fidelity measures and classifies explanations based on their quality of being
sufficient or necessary. We scope ourselves to node classification tasks and
compare the most representative techniques in the field of input-level
explainability for GNNs. For the inadequate but widely used synthetic
benchmarks, surprisingly shallow techniques such as personalized PageRank have
the best performance for a minimum computation time. But when the graph
structure is more complex and nodes have meaningful features, gradient-based
methods are the best according to our evaluation criteria. However, none
dominates the others on all evaluation dimensions and there is always a
trade-off. We further apply our evaluation protocol in a case study for frauds
explanation on eBay transaction graphs to reflect the production environment."
7870,"Finally, in Section 6 we conclude our work with
further research directions.","In Section 5, we provide the experimental study
and discuss the obtained results.","2 Related Work

The study of imbalanced learning has been advocated over the years, as it poses
well-known challenges to standard predictive learning tasks [2].",2022-06-20 20:23:56+00:00,Model Optimization in Imbalanced Regression,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Aníbal Silva'), arxiv.Result.Author('Rita P. Ribeiro'), arxiv.Result.Author('Nuno Moniz')]","Imbalanced domain learning aims to produce accurate models in predicting
instances that, though underrepresented, are of utmost importance for the
domain. Research in this field has been mainly focused on classification tasks.
Comparatively, the number of studies carried out in the context of regression
tasks is negligible. One of the main reasons for this is the lack of loss
functions capable of focusing on minimizing the errors of extreme (rare)
values. Recently, an evaluation metric was introduced: Squared Error Relevance
Area (SERA). This metric posits a bigger emphasis on the errors committed at
extreme values while also accounting for the performance in the overall target
variable domain, thus preventing severe bias. However, its effectiveness as an
optimization metric is unknown. In this paper, our goal is to study the impacts
of using SERA as an optimization criterion in imbalanced regression tasks.
Using gradient boosting algorithms as proof of concept, we perform an
experimental study with 36 data sets of different domains and sizes. Results
show that models that used SERA as an objective function are practically better
than the models produced by their respective standard boosting algorithms at
the prediction of extreme values. This confirms that SERA can be embedded as a
loss function into optimization-based learning algorithms for imbalanced
regression scenarios."
7871,"Finally, in Section 6 we conclude our work with
further research directions.","In Section 5, we provide the experimental study
and discuss the obtained results.","2 Related Work

The study of imbalanced learning has been advocated over the years, as it poses
well-known challenges to standard predictive learning tasks [2].",2022-06-20 20:23:56+00:00,Model Optimization in Imbalanced Regression,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Aníbal Silva'), arxiv.Result.Author('Rita P. Ribeiro'), arxiv.Result.Author('Nuno Moniz')]","Imbalanced domain learning aims to produce accurate models in predicting
instances that, though underrepresented, are of utmost importance for the
domain. Research in this field has been mainly focused on classification tasks.
Comparatively, the number of studies carried out in the context of regression
tasks is negligible. One of the main reasons for this is the lack of loss
functions capable of focusing on minimizing the errors of extreme (rare)
values. Recently, an evaluation metric was introduced: Squared Error Relevance
Area (SERA). This metric posits a bigger emphasis on the errors committed at
extreme values while also accounting for the performance in the overall target
variable domain, thus preventing severe bias. However, its effectiveness as an
optimization metric is unknown. In this paper, our goal is to study the impacts
of using SERA as an optimization criterion in imbalanced regression tasks.
Using gradient boosting algorithms as proof of concept, we perform an
experimental study with 36 data sets of different domains and sizes. Results
show that models that used SERA as an objective function are practically better
than the models produced by their respective standard boosting algorithms at
the prediction of extreme values. This confirms that SERA can be embedded as a
loss function into optimization-based learning algorithms for imbalanced
regression scenarios."
7872,"In the future, we aim at investigating some unex-
from real images to train local models is essential to reach            plored properties of our method: for instance, unlike all other
good global performance: nevertheless, further research on the          existing methods based on parameter averaging is required, our
generation process might lead to improvements in this aspect.","Additionally, quantitative and qualitative analysis
   Standard                   0.712        0.772                        shows that our privacy-preserving generation approach is able
   Buffer-only sharing        0.696        0.679                        to synthesize samples that are signiﬁcantly different from real
   Synthetic-only training    0.585        0.517                        data, while correctly supporting the learning of discriminative
                                                                        features.","approach does not strictly require that all nodes share the same
                                                                        model architecture.",2022-06-20 23:49:38+00:00,Decentralized Distributed Learning with Privacy-Preserving Data Synthesis,cs.LG,"['cs.LG', 'cs.AI', '68T07', 'I.2.6']","[arxiv.Result.Author('Matteo Pennisi'), arxiv.Result.Author('Federica Proietto Salanitri'), arxiv.Result.Author('Giovanni Bellitto'), arxiv.Result.Author('Bruno Casella'), arxiv.Result.Author('Marco Aldinucci'), arxiv.Result.Author('Simone Palazzo'), arxiv.Result.Author('Concetto Spampinato')]","In the medical field, multi-center collaborations are often sought to yield
more generalizable findings by leveraging the heterogeneity of patient and
clinical data. However, recent privacy regulations hinder the possibility to
share data, and consequently, to come up with machine learning-based solutions
that support diagnosis and prognosis. Federated learning (FL) aims at
sidestepping this limitation by bringing AI-based solutions to data owners and
only sharing local AI models, or parts thereof, that need then to be
aggregated. However, most of the existing federated learning solutions are
still at their infancy and show several shortcomings, from the lack of a
reliable and effective aggregation scheme able to retain the knowledge learned
locally to weak privacy preservation as real data may be reconstructed from
model updates. Furthermore, the majority of these approaches, especially those
dealing with medical data, relies on a centralized distributed learning
strategy that poses robustness, scalability and trust issues. In this paper we
present a decentralized distributed method that, exploiting concepts from
experience replay and generative adversarial research, effectively integrates
features from local nodes, providing models able to generalize across multiple
datasets while maintaining privacy. The proposed approach is tested on two
tasks - tuberculosis and melanoma classification - using multiple datasets in
order to simulate realistic non-i.i.d. data scenarios. Results show that our
approach achieves performance comparable to both standard (non-federated)
learning and federated methods in their centralized (thus, more favourable)
formulation."
7878,"Why there is a drop when integrating action
the controller using action masking outperforms the safety            masking remains a subject for further research.",−7500).,"Furthermore,
layer version in all vehicle-related metrics (for stops the           the convergence of the agents in (c) was slightly faster
values are equal).",2022-06-21 05:53:23+00:00,Safe and Psychologically Pleasant Traffic Signal Control with Reinforcement Learning using Action Masking,cs.LG,['cs.LG'],"[arxiv.Result.Author('Arthur Müller'), arxiv.Result.Author('Matthia Sabatelli')]","Reinforcement learning (RL) for traffic signal control (TSC) has shown better
performance in simulation for controlling the traffic flow of intersections
than conventional approaches. However, due to several challenges, no RL-based
TSC has been deployed in the field yet. One major challenge for real-world
deployment is to ensure that all safety requirements are met at all times
during operation. We present an approach to ensure safety in a real-world
intersection by using an action space that is safe by design. The action space
encompasses traffic phases, which represent the combination of non-conflicting
signal colors of the intersection. Additionally, an action masking mechanism
makes sure that only appropriate phase transitions are carried out. Another
challenge for real-world deployment is to ensure a control behavior that avoids
stress for road users. We demonstrate how to achieve this by incorporating
domain knowledge through extending the action masking mechanism. We test and
verify our approach in a realistic simulation scenario. By ensuring safety and
psychologically pleasant control behavior, our approach drives development
towards real-world deployment of RL for TSC."
7879,"Comparison on Robustness to Structure Noise
    We conduct experiments to further study the model robustness to structure noise.",5.3.,"As mentioned before, we regard

the edge connecting nodes of diﬀerent labels as structure noise.",2022-06-21 06:56:58+00:00,Propagation with Adaptive Mask then Training for Node Classification on Attributed Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jinsong Chen'), arxiv.Result.Author('Boyu Li'), arxiv.Result.Author('Qiuting He'), arxiv.Result.Author('Kun He')]","Node classification on attributed networks is a semi-supervised task that is
crucial for network analysis. By decoupling two critical operations in Graph
Convolutional Networks (GCNs), namely feature transformation and neighborhood
aggregation, some recent works of decoupled GCNs could support the information
to propagate deeper and achieve advanced performance. However, they follow the
traditional structure-aware propagation strategy of GCNs, making it hard to
capture the attribute correlation of nodes and sensitive to the structure noise
described by edges whose two endpoints belong to different categories. To
address these issues, we propose a new method called the itshape Propagation
with Adaptive Mask then Training (PAMT). The key idea is to integrate the
attribute similarity mask into the structure-aware propagation process. In this
way, PAMT could preserve the attribute correlation of adjacent nodes during the
propagation and effectively reduce the influence of structure noise. Moreover,
we develop an iterative refinement mechanism to update the similarity mask
during the training process for improving the training performance. Extensive
experiments on four real-world datasets demonstrate the superior performance
and robustness of PAMT."
7880,"Comparison on Robustness to Structure Noise
    We conduct experiments to further study the model robustness to structure noise.",5.3.,"As mentioned before, we regard

the edge connecting nodes of diﬀerent labels as structure noise.",2022-06-21 06:56:58+00:00,Propagation with Adaptive Mask then Training for Node Classification on Attributed Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jinsong Chen'), arxiv.Result.Author('Boyu Li'), arxiv.Result.Author('Qiuting He'), arxiv.Result.Author('Kun He')]","Node classification on attributed networks is a semi-supervised task that is
crucial for network analysis. By decoupling two critical operations in Graph
Convolutional Networks (GCNs), namely feature transformation and neighborhood
aggregation, some recent works of decoupled GCNs could support the information
to propagate deeper and achieve advanced performance. However, they follow the
traditional structure-aware propagation strategy of GCNs, making it hard to
capture the attribute correlation of nodes and sensitive to the structure noise
described by edges whose two endpoints belong to different categories. To
address these issues, we propose a new method called the itshape Propagation
with Adaptive Mask then Training (PAMT). The key idea is to integrate the
attribute similarity mask into the structure-aware propagation process. In this
way, PAMT could preserve the attribute correlation of adjacent nodes during the
propagation and effectively reduce the influence of structure noise. Moreover,
we develop an iterative refinement mechanism to update the similarity mask
during the training process for improving the training performance. Extensive
experiments on four real-world datasets demonstrate the superior performance
and robustness of PAMT."
7890,"However,   this
                                                                                            j=1

conjecture needs to be studied in further research.","Hence, Assumption A.7 would be valid for

all “ﬂow steps” (see Example A.10) when starting with a mean-ﬁeld base distribution q(z) =  D    qj (zj ).",A.4.,2022-06-21 12:34:36+00:00,Marginal Tail-Adaptive Normalizing Flows,cs.LG,"['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Mike Laszkiewicz'), arxiv.Result.Author('Johannes Lederer'), arxiv.Result.Author('Asja Fischer')]","Learning the tail behavior of a distribution is a notoriously difficult
problem. By definition, the number of samples from the tail is small, and deep
generative models, such as normalizing flows, tend to concentrate on learning
the body of the distribution. In this paper, we focus on improving the ability
of normalizing flows to correctly capture the tail behavior and, thus, form
more accurate models. We prove that the marginal tailedness of an
autoregressive flow can be controlled via the tailedness of the marginals of
its base distribution. This theoretical insight leads us to a novel type of
flows based on flexible base distributions and data-driven linear layers. An
empirical analysis shows that the proposed method improves on the accuracy --
especially on the tails of the distribution -- and is able to generate
heavy-tailed data. We demonstrate its application on a weather and climate
example, in which capturing the tail behavior is essential."
7891,"However,   this
                                                                                            j=1

conjecture needs to be studied in further research.","Hence, Assumption A.7 would be valid for

all “ﬂow steps” (see Example A.10) when starting with a mean-ﬁeld base distribution q(z) =  D    qj (zj ).",A.4.,2022-06-21 12:34:36+00:00,Marginal Tail-Adaptive Normalizing Flows,cs.LG,"['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Mike Laszkiewicz'), arxiv.Result.Author('Johannes Lederer'), arxiv.Result.Author('Asja Fischer')]","Learning the tail behavior of a distribution is a notoriously difficult
problem. By definition, the number of samples from the tail is small, and deep
generative models, such as normalizing flows, tend to concentrate on learning
the body of the distribution. In this paper, we focus on improving the ability
of normalizing flows to correctly capture the tail behavior and, thus, form
more accurate models. We prove that the marginal tailedness of an
autoregressive flow can be controlled via the tailedness of the marginals of
its base distribution. This theoretical insight leads us to a novel type of
flows based on flexible base distributions and data-driven linear layers. An
empirical analysis shows that the proposed method improves on the accuracy --
especially on the tails of the distribution -- and is able to generate
heavy-tailed data. We demonstrate its application on a weather and climate
example, in which capturing the tail behavior is essential."
7898,"Lastly, further research is needed for settings in
of information in a holistic manner.","investigate possible approaches such as BioNumQA-BERT
Leveraging the unstructured data format of language, our         that improve on previous methods for encoding of numerical
method provides unparalleled ﬂexibility to encode all types      values [13].","For example, traditional    which the ﬁnal feature space is very high dimensional, which
data processing includes:                                        can affect predictive power as well as model interpretability.",2022-06-21 13:28:57+00:00,TabText: a Systematic Approach to Aggregate Knowledge Across Tabular Data Structures,cs.LG,['cs.LG'],"[arxiv.Result.Author('Dimitris Bertsimas'), arxiv.Result.Author('Kimberly Villalobos Carballo'), arxiv.Result.Author('Yu Ma'), arxiv.Result.Author('Liangyuan Na'), arxiv.Result.Author('Léonard Boussioux'), arxiv.Result.Author('Cynthia Zeng'), arxiv.Result.Author('Luis R. Soenksen'), arxiv.Result.Author('Ignacio Fuentes')]","Processing and analyzing tabular data in a productive and efficient way is
essential for building successful applications of machine learning in fields
such as healthcare. However, the lack of a unified framework for representing
and standardizing tabular information poses a significant challenge to
researchers and professionals alike. In this work, we present TabText, a
methodology that leverages the unstructured data format of language to encode
tabular data from different table structures and time periods efficiently and
accurately. We show using two healthcare datasets and four prediction tasks
that features extracted via TabText outperform those extracted with traditional
processing methods by 2-5%. Furthermore, we analyze the sensitivity of our
framework against different choices for sentence representations of missing
values, meta information and language descriptiveness, and provide insights
into winning strategies that improve performance."
7899,"By
leveraging the unstructured data format of language, our         Lastly, further research is needed for settings in which the
method provides unparalleled ﬂexibility to encode all types
of information holistically.",and structures in a standardized and harmonized format.,"For example, traditional data       ﬁnal feature space is very high dimensional, which can affect
processing includes:
                                                                 predictive power and model interpretability.",2022-06-21 13:28:57+00:00,TabText: a Systematic Approach to Aggregate Knowledge Across Tabular Data Structures,cs.LG,['cs.LG'],"[arxiv.Result.Author('Dimitris Bertsimas'), arxiv.Result.Author('Kimberly Villalobos Carballo'), arxiv.Result.Author('Yu Ma'), arxiv.Result.Author('Liangyuan Na'), arxiv.Result.Author('Léonard Boussioux'), arxiv.Result.Author('Cynthia Zeng'), arxiv.Result.Author('Luis R. Soenksen'), arxiv.Result.Author('Ignacio Fuentes')]","Processing and analyzing tabular data in a productive and efficient way is
essential for building successful applications of machine learning in fields
such as healthcare. However, the lack of a unified framework for representing
and standardizing tabular information poses a significant challenge to
researchers and professionals alike. In this work, we present TabText, a
methodology that leverages the unstructured data format of language to encode
tabular data from different table structures and time periods efficiently and
accurately. We show using two healthcare datasets and four prediction tasks
that features extracted via TabText outperform those extracted with traditional
processing methods by 2-5%. Furthermore, we analyze the sensitivity of our
framework against different choices for sentence representations of missing
values, meta information and language descriptiveness, and provide insights
into winning strategies that improve performance."
7915,"While there were previous attempts to explore
                                        the OOD detection for image classiﬁcation (Sinhamahapatra          In this work, we consider two common distinct classes of
                                        et al., 2022), their direct application to graph classiﬁcation     methods for OOD detection through uncertainty estimation:
                                        requires further research due to a complex network structure
                                        and additional attributes associated with the nodes.","et al., 2021).","• entropy-based methods, which use the entropy of pre-
                                                                                                                 dictive categorical distribution as the measure of un-
                                            1Skolkovo Institute of Science and Technology, Russia 2Criteo        certainty, treating the predictions with high entropy as
                                        AI, France 3Technology Innovation Institute, UAE.",2022-06-21 19:15:20+00:00,Towards OOD Detection in Graph Classification from Uncertainty Estimation Perspective,cs.LG,['cs.LG'],"[arxiv.Result.Author('Gleb Bazhenov'), arxiv.Result.Author('Sergei Ivanov'), arxiv.Result.Author('Maxim Panov'), arxiv.Result.Author('Alexey Zaytsev'), arxiv.Result.Author('Evgeny Burnaev')]","The problem of out-of-distribution detection for graph classification is far
from being solved. The existing models tend to be overconfident about OOD
examples or completely ignore the detection task. In this work, we consider
this problem from the uncertainty estimation perspective and perform the
comparison of several recently proposed methods. In our experiment, we find
that there is no universal approach for OOD detection, and it is important to
consider both graph representations and predictive categorical distribution."
7916,"First,         further research.","Additionally, the combined information
   All of the aforementioned approaches can be linked to           may also be used for biomarker identiﬁcation supporting
particular challenges in the machine learning task.","many of the datasets are sparse, often with some omics fea-
tures missing between samples or studies, or even omics               In this paper we provide several contributions to the
layers being unavailable for some patients.",2022-06-21 19:44:08+00:00,Multi-Omic Data Integration and Feature Selection for Survival-based Patient Stratification via Supervised Concrete Autoencoders,cs.LG,['cs.LG'],"[arxiv.Result.Author('Pedro Henrique da Costa Avelar'), arxiv.Result.Author('Roman Laddach'), arxiv.Result.Author('Sophia Karagiannis'), arxiv.Result.Author('Min Wu'), arxiv.Result.Author('Sophia Tsoka')]","Cancer is a complex disease with significant social and economic impact.
Advancements in high-throughput molecular assays and the reduced cost for
performing high-quality multi-omics measurements have fuelled insights through
machine learning . Previous studies have shown promise on using multiple omic
layers to predict survival and stratify cancer patients. In this paper, we
developed a Supervised Autoencoder (SAE) model for survival-based multi-omic
integration which improves upon previous work, and report a Concrete Supervised
Autoencoder model (CSAE), which uses feature selection to jointly reconstruct
the input features as well as predict survival. Our experiments show that our
models outperform or are on par with some of the most commonly used baselines,
while either providing a better survival separation (SAE) or being more
interpretable (CSAE). We also perform a feature selection stability analysis on
our models and notice that there is a power-law relationship with features
which are commonly associated with survival. The code for this project is
available at: https://github.com/phcavelar/coxae"
7917,"First,         further research.","Additionally, the combined information
   All of the aforementioned approaches can be linked to           may also be used for biomarker identiﬁcation supporting
particular challenges in the machine learning task.","many of the datasets are sparse, often with some omics fea-
tures missing between samples or studies, or even omics               In this paper we provide several contributions to the
layers being unavailable for some patients.",2022-06-21 19:44:08+00:00,Multi-Omic Data Integration and Feature Selection for Survival-based Patient Stratification via Supervised Concrete Autoencoders,cs.LG,['cs.LG'],"[arxiv.Result.Author('Pedro Henrique da Costa Avelar'), arxiv.Result.Author('Roman Laddach'), arxiv.Result.Author('Sophia Karagiannis'), arxiv.Result.Author('Min Wu'), arxiv.Result.Author('Sophia Tsoka')]","Cancer is a complex disease with significant social and economic impact.
Advancements in high-throughput molecular assays and the reduced cost for
performing high-quality multi-omics measurements have fuelled insights through
machine learning . Previous studies have shown promise on using multiple omic
layers to predict survival and stratify cancer patients. In this paper, we
developed a Supervised Autoencoder (SAE) model for survival-based multi-omic
integration which improves upon previous work, and report a Concrete Supervised
Autoencoder model (CSAE), which uses feature selection to jointly reconstruct
the input features as well as predict survival. Our experiments show that our
models outperform or are on par with some of the most commonly used baselines,
while either providing a better survival separation (SAE) or being more
interpretable (CSAE). We also perform a feature selection stability analysis on
our models and notice that there is a power-law relationship with features
which are commonly associated with survival. The code for this project is
available at: https://github.com/phcavelar/coxae"
7934,"We will make our dataset
public 1 for further research.","The dataset provides a solid and
useful test-bed for developing and experimenting with generative models.","Using this test-bed we train various likelihood models and evaluate their KL-divergence and reverse
KL-divergence.",2022-06-22 09:27:31+00:00,A Study on the Evaluation of Generative Models,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Eyal Betzalel'), arxiv.Result.Author('Coby Penso'), arxiv.Result.Author('Aviv Navon'), arxiv.Result.Author('Ethan Fetaya')]","Implicit generative models, which do not return likelihood values, such as
generative adversarial networks and diffusion models, have become prevalent in
recent years. While it is true that these models have shown remarkable results,
evaluating their performance is challenging. This issue is of vital importance
to push research forward and identify meaningful gains from random noise.
Currently, heuristic metrics such as the Inception score (IS) and Frechet
Inception Distance (FID) are the most common evaluation metrics, but what they
measure is not entirely clear. Additionally, there are questions regarding how
meaningful their score actually is. In this work, we study the evaluation
metrics of generative models by generating a high-quality synthetic dataset on
which we can estimate classical metrics for comparison. Our study shows that
while FID and IS do correlate to several f-divergences, their ranking of close
models can vary considerably making them problematic when used for fain-grained
comparison. We further used this experimental setting to study which evaluation
metric best correlates with our probabilistic metrics. Lastly, we look into the
base features used for metrics such as FID."
7946,"26
         This study has the following limitations, many of which should motivate further research.","In other words, land-use
policies transforming highly sparse suburban neighborhoods into moderately sparse suburban
neighborhoods (i.e., 5,000~6,000 persons per square mile) with higher supply of transit could
sufficiently reduce the Millennial suburbanites' auto usage.","First, as a data-driven, non-parametric approach, the GBDT models are not able to conduct
statistical inferences such as significant tests or confidence intervals.",2022-06-19 06:58:59+00:00,Generational Differences in Automobility: Comparing America's Millennials and Gen Xers Using Gradient Boosting Decision Trees,cs.LG,"['cs.LG', 'econ.GN', 'q-fin.EC', 'stat.AP']","[arxiv.Result.Author('Kailai Wang'), arxiv.Result.Author('Xize Wang')]","Whether the Millennials are less auto-centric than the previous generations
has been widely discussed in the literature. Most existing studies use
regression models and assume that all factors are linear-additive in
contributing to the young adults' driving behaviors. This study relaxes this
assumption by applying a non-parametric statistical learning method, namely the
gradient boosting decision trees (GBDT). Using U.S. nationwide travel surveys
for 2001 and 2017, this study examines the non-linear dose-response effects of
lifecycle, socio-demographic and residential factors on daily driving distances
of Millennial and Gen-X young adults. Holding all other factors constant,
Millennial young adults had shorter predicted daily driving distances than
their Gen-X counterparts. Besides, residential and economic factors explain
around 50% of young adults' daily driving distances, while the collective
contributions for life course events and demographics are about 33%. This study
also identifies the density ranges for formulating effective land use policies
aiming at reducing automobile travel demand."
7947,"Table III relates the performance in       We hope the venue of work we have started in this paper
each case for both the validation and test set along the various  opens up further research directions.","least 10 members, those with less than 30 members, and those
with at least 30 members.","We note that one of the
metrics: ACC, PRE, REC, and F1.",2022-06-17 18:40:19+00:00,Transformer Neural Networks Attending to Both Sequence and Structure for Protein Prediction Tasks,cs.LG,"['cs.LG', 'cs.AI', 'q-bio.QM']","[arxiv.Result.Author('Anowarul Kabir'), arxiv.Result.Author('Amarda Shehu')]","The increasing number of protein sequences decoded from genomes is opening up
new avenues of research on linking protein sequence to function with
transformer neural networks. Recent research has shown that the number of known
protein sequences supports learning useful, task-agnostic sequence
representations via transformers. In this paper, we posit that learning joint
sequence-structure representations yields better representations for
function-related prediction tasks. We propose a transformer neural network that
attends to both sequence and tertiary structure. We show that such joint
representations are more powerful than sequence-based representations only, and
they yield better performance on superfamily membership across various metrics."
7951,"In fact, several recent works published in 2022 have analyzed these methods both theoretically and
empirically, and have called for further study of these methods given their widespread adoption [19,
16, 9, 31, 43, 26].","F Choice of XAI methods, datasets, and models

While feature attribution-based explanation methods such as LIME, SHAP, and Gradient-based
methods have been proposed a few years back, they continue to be the most popular and widely used
post hoc explanation methods both in research [16, 43, 7, 33, 31] and in practice [23, 78, 34, 28].","Furthermore, recent research has also argued that there is little to no understanding
of the behavior and effectiveness of even basic post hoc explanation methods such as LIME, SHAP,
and gradient-based methods [43, 50, 64, 40, 10, 3], and developing such an understanding would be
a critical ﬁrst step towards the progress of the XAI ﬁeld.",2022-06-22 14:01:34+00:00,OpenXAI: Towards a Transparent Evaluation of Model Explanations,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Chirag Agarwal'), arxiv.Result.Author('Satyapriya Krishna'), arxiv.Result.Author('Eshika Saxena'), arxiv.Result.Author('Martin Pawelczyk'), arxiv.Result.Author('Nari Johnson'), arxiv.Result.Author('Isha Puri'), arxiv.Result.Author('Marinka Zitnik'), arxiv.Result.Author('Himabindu Lakkaraju')]","While several types of post hoc explanation methods (e.g., feature
attribution methods) have been proposed in recent literature, there is little
to no work on systematically benchmarking these methods in an efficient and
transparent manner. Here, we introduce OpenXAI, a comprehensive and extensible
open source framework for evaluating and benchmarking post hoc explanation
methods. OpenXAI comprises of the following key components: (i) a flexible
synthetic data generator and a collection of diverse real-world datasets,
pre-trained models, and state-of-the-art feature attribution methods, (ii)
open-source implementations of twenty-two quantitative metrics for evaluating
faithfulness, stability (robustness), and fairness of explanation methods, and
(iii) the first ever public XAI leaderboards to benchmark explanations. OpenXAI
is easily extensible, as users can readily evaluate custom explanation methods
and incorporate them into our leaderboards. Overall, OpenXAI provides an
automated end-to-end pipeline that not only simplifies and standardizes the
evaluation of post hoc explanation methods, but also promotes transparency and
reproducibility in benchmarking these methods. OpenXAI datasets and data
loaders, implementations of state-of-the-art explanation methods and evaluation
metrics, as well as leaderboards are publicly available at
https://open-xai.github.io/."
7975,"We go beyond them by
further studying semantic segmentation problems, not merely classiﬁcation problems.","We focus on visual recognition, which has been the focus of many FL works.","2 Related Work

Federated learning.",2022-06-23 06:02:33+00:00,On Pre-Training for Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Hong-You Chen'), arxiv.Result.Author('Cheng-Hao Tu'), arxiv.Result.Author('Ziwei Li'), arxiv.Result.Author('Han-Wei Shen'), arxiv.Result.Author('Wei-Lun Chao')]","In most of the literature on federated learning (FL), neural networks are
initialized with random weights. In this paper, we present an empirical study
on the effect of pre-training on FL. Specifically, we aim to investigate if
pre-training can alleviate the drastic accuracy drop when clients'
decentralized data are non-IID. We focus on FedAvg, the fundamental and most
widely used FL algorithm. We found that pre-training does largely close the gap
between FedAvg and centralized learning under non-IID data, but this does not
come from alleviating the well-known model drifting problem in FedAvg's local
training. Instead, how pre-training helps FedAvg is by making FedAvg's global
aggregation more stable. When pre-training using real data is not feasible for
FL, we propose a novel approach to pre-train with synthetic data. On various
image datasets (including one for segmentation), our approach with synthetic
pre-training leads to a notable gain, essentially a critical step toward
scaling up federated learning for real-world applications."
7976,"We go beyond them by
further studying semantic segmentation problems, not merely classiﬁcation problems.","We focus on visual recognition, which has been the focus of many FL works.","2
Preprint

2 RELATED WORK

Federated learning (FL).",2022-06-23 06:02:33+00:00,On the Importance and Applicability of Pre-Training for Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Hong-You Chen'), arxiv.Result.Author('Cheng-Hao Tu'), arxiv.Result.Author('Ziwei Li'), arxiv.Result.Author('Han-Wei Shen'), arxiv.Result.Author('Wei-Lun Chao')]","Pre-training is prevalent in nowadays deep learning to improve the learned
model's performance. However, in the literature on federated learning (FL),
neural networks are mostly initialized with random weights. These attract our
interest in conducting a systematic study to explore pre-training for FL.
Across multiple visual recognition benchmarks, we found that pre-training can
not only improve FL, but also close its accuracy gap to the counterpart
centralized learning, especially in the challenging cases of non-IID clients'
data. To make our findings applicable to situations where pre-trained models
are not directly available, we explore pre-training with synthetic data or even
with clients' data in a decentralized manner, and found that they can already
improve FL notably. Interesting, many of the techniques we explore are
complementary to each other to further boost the performance, and we view this
as a critical result toward scaling up deep FL for real-world applications. We
conclude our paper with an attempt to understand the effect of pre-training on
FL. We found that pre-training enables the learned global models under
different clients' data conditions to converge to the same loss basin, and
makes global aggregation in FL more stable. Nevertheless, pre-training seems to
not alleviate local model drifting, a fundamental problem in FL under non-IID
data."
8038,"Besides that, since the best
binarizers diﬀer in shape and clipping interval, further research is needed to ﬁnd a
binarizer that combines the two.","An interesting observation is that these ﬁxed-form binarizers outperformed
binarizers that gradually changed shape during training.","Whereas the clipping interval can be ﬁxed, the best feature normalization method
implicitly changed the interval using a division by the standard deviation (STD).",2022-06-24 14:45:33+00:00,How to train accurate BNNs for embedded systems?,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Floran de Putter'), arxiv.Result.Author('Henk Corporaal')]","A key enabler of deploying convolutional neural networks on
resource-constrained embedded systems is the binary neural network (BNN). BNNs
save on memory and simplify computation by binarizing both features and
weights. Unfortunately, binarization is inevitably accompanied by a severe
decrease in accuracy. To reduce the accuracy gap between binary and
full-precision networks, many repair methods have been proposed in the recent
past, which we have classified and put into a single overview in this chapter.
The repair methods are divided into two main branches, training techniques and
network topology changes, which can further be split into smaller categories.
The latter category introduces additional cost (energy consumption or
additional area) for an embedded system, while the former does not. From our
overview, we observe that progress has been made in reducing the accuracy gap,
but BNN papers are not aligned on what repair methods should be used to get
highly accurate BNNs. Therefore, this chapter contains an empirical review that
evaluates the benefits of many repair methods in isolation over the
ResNet-20\&CIFAR10 and ResNet-18\&CIFAR100 benchmarks. We found three repair
categories most beneficial: feature binarizer, feature normalization, and
double residual. Based on this review we discuss future directions and research
opportunities. We sketch the benefit and costs associated with BNNs on embedded
systems because it remains to be seen whether BNNs will be able to close the
accuracy gap while staying highly energy-efficient on resource-constrained
embedded systems."
8045,"Since including missing data, improves over standard
learning procedures, this observation opens avenues for further research that includes modeling or
sampling missing data.","In this setting as well, IMTPP outperforms other alternatives along with better
scalability and guaranteed convergence.","However, one unaddressed aspect of the missing data problem is partially
missing events i.e.",2022-06-23 18:23:20+00:00,Modeling Continuous Time Sequences with Intermittent Observations using Marked Temporal Point Processes,cs.LG,['cs.LG'],"[arxiv.Result.Author('Vinayak Gupta'), arxiv.Result.Author('Srikanta Bedathur'), arxiv.Result.Author('Sourangshu Bhattacharya'), arxiv.Result.Author('Abir De')]","A large fraction of data generated via human activities such as online
purchases, health records, spatial mobility etc. can be represented as a
sequence of events over a continuous-time. Learning deep learning models over
these continuous-time event sequences is a non-trivial task as it involves
modeling the ever-increasing event timestamps, inter-event time gaps, event
types, and the influences between different events within and across different
sequences. In recent years neural enhancements to marked temporal point
processes (MTPP) have emerged as a powerful framework to model the underlying
generative mechanism of asynchronous events localized in continuous time.
However, most existing models and inference methods in the MTPP framework
consider only the complete observation scenario i.e. the event sequence being
modeled is completely observed with no missing events -- an ideal setting that
is rarely applicable in real-world applications. A recent line of work which
considers missing events while training MTPP utilizes supervised learning
techniques that require additional knowledge of missing or observed label for
each event in a sequence, which further restricts its practicability as in
several scenarios the details of missing events is not known apriori. In this
work, we provide a novel unsupervised model and inference method for learning
MTPP in presence of event sequences with missing events. Specifically, we first
model the generative processes of observed events and missing events using two
MTPP, where the missing events are represented as latent random variables.
Then, we devise an unsupervised training method that jointly learns both the
MTPP by means of variational inference. Such a formulation can effectively
impute the missing data among the observed events and can identify the optimal
position of missing events in a sequence."
8059,"The collected
data was cleaned and prepared for further research steps.","The health
referral processing related data was collected from the repository of a leading MCO.",Outliers analysis was performed.,2022-06-25 04:42:52+00:00,Integrating Machine Learning with Discrete Event Simulation for Improving Health Referral Processing in a Care Management Setting,cs.LG,"['cs.LG', 'I.2, I.6, G.3,']",[arxiv.Result.Author('Mohammed Mahyoub')],"Post-discharge care management coordinates patients' referrals to improve
their health after being discharged from hospitals, especially elderly and
chronically ill patients. In a care management setting, health referrals are
processed by a specialized unit in the managed care organization (MCO), which
interacts with many other entities including inpatient hospitals, insurance
companies, and post-discharge care providers. In this paper, a
machine-learning-guided discrete event simulation framework to improve health
referrals processing is proposed. Random-forest-based prediction models are
developed to predict the LOS and referral type. Two simulation models are
constructed to represent the as-is configuration of the referral processing
system and the intelligent system after incorporating the prediction
functionality, respectively. By incorporating a prediction module for the
referral processing system to plan and prioritize referrals, the overall
performance was enhanced in terms of reducing the average referral creation
delay time. This research will emphasize the role of post-discharge care
management in improving health quality and reducing associated costs. Also, the
paper demonstrates how to use integrated systems engineering methods for
process improvement of complex healthcare systems."
8066,"It inspires us to further study the effect of
model architecture in backdoor learning and to design more robust architectures in the future.",different performance on different model architectures.,"(1) CIFAR10-5%: No defense                            (2) CIFAR10-5%: FT                                                        (3) CIFAR10-5%: FP

100                                                                                    100                                                              100

                          80                                                           80                                                               80

ASR (%)                   60                                                           60                                                               60

                          40                                                           40                                                               40

                          20                                                           20                                                               20

  0 BadNets Blended LC SIG LF SSBAInput-awareWaNet                                       0 BadNets Blended LC SIG LF SSBAInput-awareWaNet                 0 BadNets Blended LC SIG LF SSBAInput-awareWaNet
                                 Attacks                                                                                Attacks                                                          Attacks

                    (4) CIFAR10-5%: NAD                                                                     (5) CIFAR10-5%: NC                                               (6) CIFAR10-5%: ANP

100                                                                                    100                                                              100

                          80                                                           80                                                               80

ASR (%)                   60                                                           60                                                               60

                          40                                                           40                                                               40

                          20                                                           20                                                               20

  0 BadNets Blended LC SIG LF SSBAInput-awareWaNet                                       0 BadNets Blended LC SIG LF SSBAInput-awareWaNet                 0 BadNets Blended LC SIG LF SSBAInput-awareWaNet
                                 Attacks                                                                                Attacks                                                          Attacks

                     (7) CIFAR10-5%: AC                                                                   (8) CIFAR10-5%: Spectral                                           (9) CIFAR10-5%: ABL

100                                                                                    100                                                              100

                          80                                                           80                                                               80

ASR (%)                   60                                                           60                                                               60

                          40                                                           40                                                               40

                          20                                                           20                                                               20

                          0 BadNets Blended  LC SIG LF          SSBAInput-awareWaNet          0 BadNets Blended LC  SIG LF SSBAInput-awareWaNet         0 BadNets Blended LC       SIG LF SSBAInput-awareWaNet
                                                      Attacks                                                         Attacks                                                        Attacks
                                                                                                                                                                                              DenseNet-161
                                               PreAct-ResNet18                         VGG19                        EfficientNet-B3                     MobileNetV3-Large

Figure 5: The effects of different model architectures using different defense and attack methods.",2022-06-25 13:48:04+00:00,BackdoorBench: A Comprehensive Benchmark of Backdoor Learning,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Baoyuan Wu'), arxiv.Result.Author('Hongrui Chen'), arxiv.Result.Author('Mingda Zhang'), arxiv.Result.Author('Zihao Zhu'), arxiv.Result.Author('Shaokui Wei'), arxiv.Result.Author('Danni Yuan'), arxiv.Result.Author('Chao Shen')]","Backdoor learning is an emerging and vital topic for studying deep neural
networks' vulnerability (DNNs). Many pioneering backdoor attack and defense
methods are being proposed, successively or concurrently, in the status of a
rapid arms race. However, we find that the evaluations of new methods are often
unthorough to verify their claims and accurate performance, mainly due to the
rapid development, diverse settings, and the difficulties of implementation and
reproducibility. Without thorough evaluations and comparisons, it is not easy
to track the current progress and design the future development roadmap of the
literature. To alleviate this dilemma, we build a comprehensive benchmark of
backdoor learning called BackdoorBench. It consists of an extensible
modular-based codebase (currently including implementations of 8
state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a
standardized protocol of complete backdoor learning. We also provide
comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5
poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of
evaluations in total. We present abundant analysis from different perspectives
about these 8,000 evaluations, studying the effects of different factors in
backdoor learning. All codes and evaluations of BackdoorBench are publicly
available at \url{https://backdoorbench.github.io}."
8091,"Finally, we note that ALine-D
actually surpasses previous methods even when accuracy-on-the-line does not hold, suggesting that
the algorithm has additional beneﬁcial properties that require further study.","Other prediction methods
do not have any way of characterizing when they will be successful.","Dataset               ALine-D∗ ALine-S∗ ATC AC DOC Agreement

     CIFAR-10.1                 1.11            1.17        1.21 4.51 3.87                                                                               5.98

     CIFAR-10.2                 3.93            3.93        4.35 8.23 7.64                                                                               5.42

     ImageNetV2                 2.06            2.08        1.12 66.2 11.50                                                                              6.70

CIFAR-10C-Fog                   1.45            1.75        1.78 4.47 3.93                                                                               3.47

CIFAR-10C-Snow                  1.32            1.97        1.31 5.94 5.49                                                                               2.57

CIFAR10C-Saturate               0.41            0.77        0.69 2.03 1.51                                                                               4.14

     fMoW-WILDS                 1.30            1.44        1.53 2.89 2.60                                                                               8.99

     RxRx1-WILDS                0.27            0.52        2.97 2.46 0.65                                                                               8.67

Camelyon17-WILDS                5.47            8.31 11.93 13.30 13.57                                                                                   6.79

iWildCam-WILDS                  4.95            6.01 12.12 4.46 5.02                                                                                     7.53

Table 2: Mean Absolute Error (MAE) of the OOD accuracy predictions with % as units.",2022-06-27 07:50:47+00:00,Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Christina Baek'), arxiv.Result.Author('Yiding Jiang'), arxiv.Result.Author('Aditi Raghunathan'), arxiv.Result.Author('Zico Kolter')]","Recently, Miller et al. showed that a model's in-distribution (ID) accuracy
has a strong linear correlation with its out-of-distribution (OOD) accuracy on
several OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''.
While a useful tool for model selection (i.e., the model most likely to perform
the best OOD is the one with highest ID accuracy), this fact does not help
estimate the actual OOD performance of models without access to a labeled OOD
validation set. In this paper, we show a similar but surprising phenomenon also
holds for the agreement between pairs of neural network classifiers: whenever
accuracy-on-the-line holds, we observe that the OOD agreement between the
predictions of any two pairs of neural networks (with potentially different
architectures) also observes a strong linear correlation with their ID
agreement. Furthermore, we observe that the slope and bias of OOD vs ID
agreement closely matches that of OOD vs ID accuracy. This phenomenon, which we
call agreement-on-the-line, has important practical applications: without any
labeled data, we can predict the OOD accuracy of classifiers}, since OOD
agreement can be estimated with just unlabeled data. Our prediction algorithm
outperforms previous methods both in shifts where agreement-on-the-line holds
and, surprisingly, when accuracy is not on the line. This phenomenon also
provides new insights into deep neural networks: unlike accuracy-on-the-line,
agreement-on-the-line appears to only hold for neural network classifiers."
8095,"In further research,
we would like to investigate dedicated strategies to tune k, beyond an exhaustive search.","While in a practical application, it would be certainly beneficial to tune k carefully, in this
paper, mainly due to a lack of time, we work with this fixed value of 100.","For applying k nearest neighbors, a proper featurization should be applied.",2022-06-27 08:27:35+00:00,Enhancing Stochastic Petri Net-based Remaining Time Prediction using k-Nearest Neighbors,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jarne Vandenabeele'), arxiv.Result.Author('Gilles Vermaut'), arxiv.Result.Author('Jari Peeperkorn'), arxiv.Result.Author('Jochen De Weerdt')]","Reliable remaining time prediction of ongoing business processes is a highly
relevant topic. One example is order delivery, a key competitive factor in e.g.
retailing as it is a main driver of customer satisfaction. For realising timely
delivery, an accurate prediction of the remaining time of the delivery process
is crucial. Within the field of process mining, a wide variety of remaining
time prediction techniques have already been proposed. In this work, we extend
remaining time prediction based on stochastic Petri nets with generally
distributed transitions with k-nearest neighbors. The k-nearest neighbors
algorithm is performed on simple vectors storing the time passed to complete
previous activities. By only taking a subset of instances, a more
representative and stable stochastic Petri Net is obtained, leading to more
accurate time predictions. We discuss the technique and its basic
implementation in Python and use different real world data sets to evaluate the
predictive power of our extension. These experiments show clear advantages in
combining both techniques with regard to predictive power."
8098,"of such summarized metrics, further research is needed to gather                  [14] Siwon Kim et al.",CyberICPS.,2021.,2022-06-27 10:18:41+00:00,Local Evaluation of Time Series Anomaly Detection Algorithms,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexis Huet'), arxiv.Result.Author('Jose Manuel Navarro'), arxiv.Result.Author('Dario Rossi')]","In recent years, specific evaluation metrics for time series anomaly
detection algorithms have been developed to handle the limitations of the
classical precision and recall. However, such metrics are heuristically built
as an aggregate of multiple desirable aspects, introduce parameters and wipe
out the interpretability of the output. In this article, we first highlight the
limitations of the classical precision/recall, as well as the main issues of
the recent event-based metrics -- for instance, we show that an adversary
algorithm can reach high precision and recall on almost any dataset under weak
assumption. To cope with the above problems, we propose a theoretically
grounded, robust, parameter-free and interpretable extension to
precision/recall metrics, based on the concept of ``affiliation'' between the
ground truth and the prediction sets. Our metrics leverage measures of duration
between ground truth and predictions, and have thus an intuitive
interpretation. By further comparison against random sampling, we obtain a
normalized precision/recall, quantifying how much a given set of results is
better than a random baseline prediction. By construction, our approach keeps
the evaluation local regarding ground truth events, enabling fine-grained
visualization and interpretation of algorithmic results. We compare our
proposal against various public time series anomaly detection datasets,
algorithms and metrics. We further derive theoretical properties of the
affiliation metrics that give explicit expectations about their behavior and
ensure robustness against adversary strategies."
8117,"With this study on reservoir computing for continual learning, we hope to inspire further research into:

      i).","We compare favorably against baselines, showing promising results on several benchmark dynamical
systems.","Sequential generative continual learning, which is currently overshadowed by the easier-to-quantify classiﬁ-
          cation of static images.",2022-06-27 14:35:50+00:00,Continual Learning of Dynamical Systems with Competitive Federated Reservoir Computing,cs.LG,['cs.LG'],"[arxiv.Result.Author('Leonard Bereska'), arxiv.Result.Author('Efstratios Gavves')]","Machine learning recently proved efficient in learning differential equations
and dynamical systems from data. However, the data is commonly assumed to
originate from a single never-changing system. In contrast, when modeling
real-world dynamical processes, the data distribution often shifts due to
changes in the underlying system dynamics. Continual learning of these
processes aims to rapidly adapt to abrupt system changes without forgetting
previous dynamical regimes. This work proposes an approach to continual
learning based on reservoir computing, a state-of-the-art method for training
recurrent neural networks on complex spatiotemporal dynamical systems.
Reservoir computing fixes the recurrent network weights - hence these cannot be
forgotten - and only updates linear projection heads to the output. We propose
to train multiple competitive prediction heads concurrently. Inspired by
neuroscience's predictive coding, only the most predictive heads activate,
laterally inhibiting and thus protecting the inactive heads from forgetting
induced by interfering parameter updates. We show that this multi-head
reservoir minimizes interference and catastrophic forgetting on several
dynamical systems, including the Van-der-Pol oscillator, the chaotic Lorenz
attractor, and the high-dimensional Lorenz-96 weather model. Our results
suggest that reservoir computing is a promising candidate framework for the
continual learning of dynamical systems. We provide our code for data
generation, method, and comparisons at
\url{https://github.com/leonardbereska/multiheadreservoir}."
8129,"This additional review typically entails further research or contacting the customer directly to

                                                  conﬁrm that they are attempting to make the purchase.","Based on the provided information, the analyst must choose to

                                                  either: allow the transaction to proceed, generating revenue if the transaction is legitimate but

                                                  risking approving fraudulent one; reject the transaction to avoid fraud, but risking blocking a

                                                  legitimate transaction; or, escalate the transaction for further review by a more senior analyst.","While this escalation is much more

                                                  likely to arrive at the correct decision of the transaction, it is also far more costly in terms of the

                                                  time and resources involved to reach the decision.",2022-06-24 14:46:19+00:00,On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods,cs.LG,"['cs.LG', 'cs.HC']","[arxiv.Result.Author('Kasun Amarasinghe'), arxiv.Result.Author('Kit T. Rodolfa'), arxiv.Result.Author('Sérgio Jesus'), arxiv.Result.Author('Valerie Chen'), arxiv.Result.Author('Vladimir Balyan'), arxiv.Result.Author('Pedro Saleiro'), arxiv.Result.Author('Pedro Bizarro'), arxiv.Result.Author('Ameet Talwalkar'), arxiv.Result.Author('Rayid Ghani')]","Machine Learning (ML) models now inform a wide range of human decisions, but
using ``black box'' models carries risks such as relying on spurious
correlations or errant data. To address this, researchers have proposed methods
for supplementing models with explanations of their predictions. However,
robust evaluations of these methods' usefulness in real-world contexts have
remained elusive, with experiments tending to rely on simplified settings or
proxy tasks. We present an experimental study extending a prior explainable ML
evaluation experiment and bringing the setup closer to the deployment setting
by relaxing its simplifying assumptions. Our empirical study draws dramatically
different conclusions than the prior work, highlighting how seemingly trivial
experimental design choices can yield misleading results. Beyond the present
experiment, we believe this work holds lessons about the necessity of situating
the evaluation of any ML method and choosing appropriate tasks, data, users,
and metrics to match the intended deployment contexts."
8130,"This additional review typically entails further research or contacting the customer directly to

                                                  conﬁrm that they are attempting to make the purchase.","Based on the provided information, the analyst must choose to

                                                  either: allow the transaction to proceed, generating revenue if the transaction is legitimate but

                                                  risking approving fraudulent one; reject the transaction to avoid fraud, but risking blocking a

                                                  legitimate transaction; or, escalate the transaction for further review by a more senior analyst.","While this escalation is much more

                                                  likely to arrive at the correct decision of the transaction, it is also far more costly in terms of the

                                                  time and resources involved to reach the decision.",2022-06-24 14:46:19+00:00,On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods,cs.LG,"['cs.LG', 'cs.HC']","[arxiv.Result.Author('Kasun Amarasinghe'), arxiv.Result.Author('Kit T. Rodolfa'), arxiv.Result.Author('Sérgio Jesus'), arxiv.Result.Author('Valerie Chen'), arxiv.Result.Author('Vladimir Balayan'), arxiv.Result.Author('Pedro Saleiro'), arxiv.Result.Author('Pedro Bizarro'), arxiv.Result.Author('Ameet Talwalkar'), arxiv.Result.Author('Rayid Ghani')]","Machine Learning (ML) models now inform a wide range of human decisions, but
using ``black box'' models carries risks such as relying on spurious
correlations or errant data. To address this, researchers have proposed methods
for supplementing models with explanations of their predictions. However,
robust evaluations of these methods' usefulness in real-world contexts have
remained elusive, with experiments tending to rely on simplified settings or
proxy tasks. We present an experimental study extending a prior explainable ML
evaluation experiment and bringing the setup closer to the deployment setting
by relaxing its simplifying assumptions. Our empirical study draws dramatically
different conclusions than the prior work, highlighting how seemingly trivial
experimental design choices can yield misleading results. Beyond the present
experiment, we believe this work holds lessons about the necessity of situating
the evaluation of any ML method and choosing appropriate tasks, data, users,
and metrics to match the intended deployment contexts."
8131,"This additional review typically entails further research or contacting the customer directly to

                                                  conﬁrm that they are attempting to make the purchase.","Based on the provided information, the analyst must choose to

                                                  either: allow the transaction to proceed, generating revenue if the transaction is legitimate but

                                                  risking approving fraudulent one; reject the transaction to avoid fraud, but risking blocking a

                                                  legitimate transaction; or, escalate the transaction for further review by a more senior analyst.","While this escalation is much more

                                                  likely to arrive at the correct decision of the transaction, it is also far more costly in terms of the

                                                  time and resources involved to reach the decision.",2022-06-24 14:46:19+00:00,On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods,cs.LG,"['cs.LG', 'cs.HC']","[arxiv.Result.Author('Kasun Amarasinghe'), arxiv.Result.Author('Kit T. Rodolfa'), arxiv.Result.Author('Sérgio Jesus'), arxiv.Result.Author('Valerie Chen'), arxiv.Result.Author('Vladimir Balayan'), arxiv.Result.Author('Pedro Saleiro'), arxiv.Result.Author('Pedro Bizarro'), arxiv.Result.Author('Ameet Talwalkar'), arxiv.Result.Author('Rayid Ghani')]","Machine Learning (ML) models now inform a wide range of human decisions, but
using ``black box'' models carries risks such as relying on spurious
correlations or errant data. To address this, researchers have proposed methods
for supplementing models with explanations of their predictions. However,
robust evaluations of these methods' usefulness in real-world contexts have
remained elusive, with experiments tending to rely on simplified settings or
proxy tasks. We present an experimental study extending a prior explainable ML
evaluation experiment and bringing the setup closer to the deployment setting
by relaxing its simplifying assumptions. Our empirical study draws dramatically
different conclusions than the prior work, highlighting how seemingly trivial
experimental design choices can yield misleading results. Beyond the present
experiment, we believe this work holds lessons about the necessity of situating
the evaluation of any ML method and choosing appropriate tasks, data, users,
and metrics to match the intended deployment contexts."
8132,"Data Augmentation techniques in time series domain: A survey and taxonomy • 0:3

they set out a further study of the repercussions of these technologies, highlighting one of the major advantages
of generating synthesised data, the abstraction of privacy issues, and the ease of obtaining datasets.",Publication date: 2022.,"Despite the possibilities presented by new technologies in this area to improve the quality of datasets used for
time series, there are not many studies that compile all technologies.",2022-06-25 17:09:00+00:00,Data Augmentation techniques in time series domain: A survey and taxonomy,cs.LG,"['cs.LG', 'cs.AI', 'I.2.6']","[arxiv.Result.Author('Edgar Talavera'), arxiv.Result.Author('Guillermo Iglesias'), arxiv.Result.Author('Ángel González-Prieto'), arxiv.Result.Author('Alberto Mozo'), arxiv.Result.Author('Sandra Gómez-Canaval')]","With the latest advances in deep learning generative models, it has not taken
long to take advantage of their remarkable performance in the area of time
series. Deep neural networks used to work with time series depend heavily on
the breadth and consistency of the datasets used in training. These types of
characteristic are not usually abundant in the real world, where they are
usually limited and often with privacy constraints that must be guaranteed.
Therefore, an effective way is to increase the number of data using \gls{da}
techniques, either by adding noise or permutations and by generating new
synthetic data. It is systematically review the current state-of-the-art in the
area to provide an overview of all available algorithms and proposes a taxonomy
of the most relevant researches. The efficiency of the different variants will
be evaluated; as a vital part of the process, the different metrics to evaluate
the performance and the main problems concerning each model will be analysed.
The ultimate goal of this study is to provide a summary of the evolution and
performance of areas that produce better results to guide future researchers in
this field."
8150,It gets the worst                                          lowing questions that need further study.,"We have the fol-
was trained with only 1000 real data samples.",(1) From Fig.,2022-06-28 01:01:34+00:00,TTS-CGAN: A Transformer Time-Series Conditional GAN for Biosignal Data Augmentation,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xiaomin Li'), arxiv.Result.Author('Anne Hee Hiong Ngu'), arxiv.Result.Author('Vangelis Metsis')]","Signal measurement appearing in the form of time series is one of the most
common types of data used in medical machine learning applications. Such
datasets are often small in size, expensive to collect and annotate, and might
involve privacy issues, which hinders our ability to train large,
state-of-the-art deep learning models for biomedical applications. For
time-series data, the suite of data augmentation strategies we can use to
expand the size of the dataset is limited by the need to maintain the basic
properties of the signal. Generative Adversarial Networks (GANs) can be
utilized as another data augmentation tool. In this paper, we present TTS-CGAN,
a transformer-based conditional GAN model that can be trained on existing
multi-class datasets and generate class-specific synthetic time-series
sequences of arbitrary length. We elaborate on the model architecture and
design strategies. Synthetic sequences generated by our model are
indistinguishable from real ones, and can be used to complement or replace real
signals of the same type, thus achieving the goal of data augmentation. To
evaluate the quality of the generated data, we modify the wavelet coherence
metric to be able to compare the similarity between two sets of signals, and
also conduct a case study where a mix of synthetic and real data are used to
train a deep learning model for sequence classification. Together with other
visualization techniques and qualitative evaluation approaches, we demonstrate
that TTS-CGAN generated synthetic data are similar to real data, and that our
model performs better than the other state-of-the-art GAN models built for
time-series data generation."
8172,"We plot the time
for solving a single example PDE versus the number of nodes in      We conclude that further research is needed to enable the
the mesh using a logarithmic scale.",solver for predicting solutions on square meshes.,We compute each point on        learning of explicit or implicit physical constraints.,2022-06-28 15:39:06+00:00,Learning the Solution Operator of Boundary Value Problems using Graph Neural Networks,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Winfried Lötzsch'), arxiv.Result.Author('Simon Ohler'), arxiv.Result.Author('Johannes S. Otterbach')]","As an alternative to classical numerical solvers for partial differential
equations (PDEs) subject to boundary value constraints, there has been a surge
of interest in investigating neural networks that can solve such problems
efficiently. In this work, we design a general solution operator for two
different time-independent PDEs using graph neural networks (GNNs) and spectral
graph convolutions. We train the networks on simulated data from a finite
elements solver on a variety of shapes and inhomogeneities. In contrast to
previous works, we focus on the ability of the trained operator to generalize
to previously unseen scenarios. Specifically, we test generalization to meshes
with different shapes and superposition of solutions for a different number of
inhomogeneities. We find that training on a diverse dataset with lots of
variation in the finite element meshes is a key ingredient for achieving good
generalization results in all cases. With this, we believe that GNNs can be
used to learn solution operators that generalize over a range of properties and
produce solutions much faster than a generic solver. Our dataset, which we make
publicly available, can be used and extended to verify the robustness of these
models under varying conditions."
8182,"The further research question we may ask here is whether those momentary
unwell states indicate any temporary behavioral change or health variation?","One can see that most of the moments that fall in the unwell state
gather around an earlier part (2015-10-08 to 2015-10-16) of the study period.","13
Ground truth for such momentary ﬂuctuation is not available in the datasets
used in this article but we defer to future work to validate the utility of these
momentary states for predictive, real-time monitoring.",2022-06-28 18:33:46+00:00,Gaussian Latent Dirichlet Allocation for Discrete Human State Discovery,cs.LG,"['cs.LG', '62H22, 62H30, 60-04, 91C20']","[arxiv.Result.Author('Congyu Wu'), arxiv.Result.Author('Aaron Fisher'), arxiv.Result.Author('David Schnyer')]","In this article we propose and validate an unsupervised probabilistic model,
Gaussian Latent Dirichlet Allocation (GLDA), for the problem of discrete state
discovery from repeated, multivariate psychophysiological samples collected
from multiple, inherently distinct, individuals. Psychology and medical
research heavily involves measuring potentially related but individually
inconclusive variables from a cohort of participants to derive diagnosis,
necessitating clustering analysis. Traditional probabilistic clustering models
such as Gaussian Mixture Model (GMM) assume a global mixture of component
distributions, which may not be realistic for observations from different
patients. The GLDA model borrows the individual-specific mixture structure from
a popular topic model Latent Dirichlet Allocation (LDA) in Natural Language
Processing and merges it with the Gaussian component distributions of GMM to
suit continuous type data. We implemented GLDA using STAN (a probabilistic
modeling language) and applied it on two datasets, one containing Ecological
Momentary Assessments (EMA) and the other heart measures from electrocardiogram
and impedance cardiograph. We found that in both datasets the GLDA-learned
class weights achieved significantly higher correlations with clinically
assessed depression, anxiety, and stress scores than those produced by the
baseline GMM. Our findings demonstrate the advantage of GLDA over conventional
finite mixture models for human state discovery from repeated multivariate
data, likely due to better characterization of potential underlying
between-participant differences. Future work is required to validate the
utility of this model on a broader range of applications."
8184,"For
further research, we suggest the following improvements:

       1.","Nonetheless, we were able to demonstrate that the trading agent is certainly capable of performing proﬁciently.",Extending the target from trading a single asset to selecting from multiple assets.,2022-06-28 19:46:16+00:00,Applications of Reinforcement Learning in Finance -- Trading with a Double Deep Q-Network,cs.LG,"['cs.LG', 'q-fin.TR']","[arxiv.Result.Author('Frensi Zejnullahu'), arxiv.Result.Author('Maurice Moser'), arxiv.Result.Author('Joerg Osterrieder')]","This paper presents a Double Deep Q-Network algorithm for trading single
assets, namely the E-mini S&P 500 continuous futures contract. We use a proven
setup as the foundation for our environment with multiple extensions. The
features of our trading agent are constantly being expanded to include
additional assets such as commodities, resulting in four models. We also
respond to environmental conditions, including costs and crises. Our trading
agent is first trained for a specific time period and tested on new data and
compared with the long-and-hold strategy as a benchmark (market). We analyze
the differences between the various models and the in-sample/out-of-sample
performance with respect to the environment. The experimental results show that
the trading agent follows an appropriate behavior. It can adjust its policy to
different circumstances, such as more extensive use of the neutral position
when trading costs are present. Furthermore, the net asset value exceeded that
of the benchmark, and the agent outperformed the market in the test set. We
provide initial insights into the behavior of an agent in a financial domain
using a DDQN algorithm. The results of this study can be used for further
development."
8215,"One may con-
sider a further study of discretized multi-resolution schema including the properties of their archi-
tectures and optimization procedures.","We argue that this work could open perspectives to study novel multi-scale neural architectures,
beyond U-net, and V-F-W-cycle schema, suitable for multi-scale and/or scarce data.","This could help to design more interpretable and efﬁcient
architectures.",2022-06-29 14:42:03+00:00,Multi-scale Physical Representations for Approximating PDE Solutions with Graph Neural Operators,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Léon Migus'), arxiv.Result.Author('Yuan Yin'), arxiv.Result.Author('Jocelyn Ahmed Mazari'), arxiv.Result.Author('Patrick Gallinari')]","Representing physical signals at different scales is among the most
challenging problems in engineering. Several multi-scale modeling tools have
been developed to describe physical systems governed by \emph{Partial
Differential Equations} (PDEs). These tools are at the crossroad of principled
physical models and numerical schema. Recently, data-driven models have been
introduced to speed-up the approximation of PDE solutions compared to numerical
solvers. Among these recent data-driven methods, neural integral operators are
a class that learn a mapping between function spaces. These functions are
discretized on graphs (meshes) which are appropriate for modeling interactions
in physical phenomena. In this work, we study three multi-resolution schema
with integral kernel operators that can be approximated with \emph{Message
Passing Graph Neural Networks} (MPGNNs). To validate our study, we make
extensive MPGNNs experiments with well-chosen metrics considering steady and
unsteady PDEs."
8220,Several interesting directions remain for further study.,"We ﬁnd that our technique succeeds not only on datasets with
planted hard subpopulations, but also on natural and widely-used datasets; as such, we hope it will serve as
a useful tool for scalable dataset exploration and debiasing.","One of them is developing sophisticated methods
for caption candidate generation.",2022-06-29 16:35:24+00:00,Distilling Model Failures as Directions in Latent Space,cs.LG,['cs.LG'],"[arxiv.Result.Author('Saachi Jain'), arxiv.Result.Author('Hannah Lawrence'), arxiv.Result.Author('Ankur Moitra'), arxiv.Result.Author('Aleksander Madry')]","Existing methods for isolating hard subpopulations and spurious correlations
in datasets often require human intervention. This can make these methods
labor-intensive and dataset-specific. To address these shortcomings, we present
a scalable method for automatically distilling a model's failure modes.
Specifically, we harness linear classifiers to identify consistent error
patterns, and, in turn, induce a natural representation of these failure modes
as directions within the feature space. We demonstrate that this framework
allows us to discover and automatically caption challenging subpopulations
within the training dataset, and intervene to improve the model's performance
on these subpopulations. Code available at
https://github.com/MadryLab/failure-directions"
8221,Several interesting directions remain for further study.,"We ﬁnd that our technique succeeds not only on datasets with
planted hard subpopulations, but also on natural and widely-used datasets; as such, we hope it will serve as
a useful tool for scalable dataset exploration and debiasing.","One of them is developing sophisticated methods
for caption candidate generation.",2022-06-29 16:35:24+00:00,Distilling Model Failures as Directions in Latent Space,cs.LG,['cs.LG'],"[arxiv.Result.Author('Saachi Jain'), arxiv.Result.Author('Hannah Lawrence'), arxiv.Result.Author('Ankur Moitra'), arxiv.Result.Author('Aleksander Madry')]","Existing methods for isolating hard subpopulations and spurious correlations
in datasets often require human intervention. This can make these methods
labor-intensive and dataset-specific. To address these shortcomings, we present
a scalable method for automatically distilling a model's failure modes.
Specifically, we harness linear classifiers to identify consistent error
patterns, and, in turn, induce a natural representation of these failure modes
as directions within the feature space. We demonstrate that this framework
allows us to discover and automatically caption challenging subpopulations
within the training dataset. Moreover, by combining our framework with
off-the-shelf diffusion models, we can generate images that are especially
challenging for the analyzed model, and thus can be used to perform synthetic
data augmentation that helps remedy the model's failure modes. Code available
at https://github.com/MadryLab/failure-directions"
8259,"We present and analyze a comprehensive set of baseline
           methods for our proposed hybrid approach, and provide a suite of Python modules designed
           to facilitate further research from the ML community.",• Hybrid NWP-DL Baselines.,Post-NWP Optimization.,2022-06-30 12:41:32+00:00,Benchmark Dataset for Precipitation Forecasting by Post-Processing the Numerical Weather Prediction,cs.LG,"['cs.LG', 'physics.ao-ph']","[arxiv.Result.Author('Taehyeon Kim'), arxiv.Result.Author('Namgyu Ho'), arxiv.Result.Author('Donggyu Kim'), arxiv.Result.Author('Se-Young Yun')]","Precipitation forecasting is an important scientific challenge that has
wide-reaching impacts on society. Historically, this challenge has been tackled
using numerical weather prediction (NWP) models, grounded on physics-based
simulations. Recently, many works have proposed an alternative approach, using
end-to-end deep learning (DL) models to replace physics-based NWP. While these
DL methods show improved performance and computational efficiency, they exhibit
limitations in long-term forecasting and lack the explainability of NWP models.
In this work, we present a hybrid NWP-DL workflow to fill the gap between
standalone NWP and DL approaches. Under this workflow, the NWP output is fed
into a deep model, which post-processes the data to yield a refined
precipitation forecast. The deep model is trained with supervision, using
Automatic Weather Station (AWS) observations as ground-truth labels. This can
achieve the best of both worlds, and can even benefit from future improvements
in NWP technology. To facilitate study in this direction, we present a novel
dataset focused on the Korean Peninsula, termed KoMet (Korea Meteorological
Dataset), comprised of NWP predictions and AWS observations. For NWP, we use
the Global Data Assimilation and Prediction Systems-Korea Integrated Model
(GDAPS-KIM)."
8260,"We present and analyze a comprehensive set of baseline
           methods for our proposed hybrid approach, and provide a suite of Python modules designed
           to facilitate further research from the ML community.",• Hybrid NWP-DL Baselines.,Post-NWP Optimization.,2022-06-30 12:41:32+00:00,Benchmark Dataset for Precipitation Forecasting by Post-Processing the Numerical Weather Prediction,cs.LG,"['cs.LG', 'physics.ao-ph']","[arxiv.Result.Author('Taehyeon Kim'), arxiv.Result.Author('Namgyu Ho'), arxiv.Result.Author('Donggyu Kim'), arxiv.Result.Author('Se-Young Yun')]","Precipitation forecasting is an important scientific challenge that has
wide-reaching impacts on society. Historically, this challenge has been tackled
using numerical weather prediction (NWP) models, grounded on physics-based
simulations. Recently, many works have proposed an alternative approach, using
end-to-end deep learning (DL) models to replace physics-based NWP models. While
these DL methods show improved performance and computational efficiency, they
exhibit limitations in long-term forecasting and lack the explainability. In
this work, we present a hybrid NWP-DL workflow to fill the gap between
standalone NWP and DL approaches. Under this workflow, the outputs of NWP
models are fed into a deep neural network, which post-processes the data to
yield a refined precipitation forecast. The deep model is trained with
supervision, using Automatic Weather Station (AWS) observations as ground-truth
labels. This can achieve the best of both worlds, and can even benefit from
future improvements in NWP technology. To facilitate study in this direction,
we present a novel dataset focused on the Korean Peninsula, termed KoMet (Korea
Meteorological Dataset), comprised of NWP outputs and AWS observations. For the
NWP model, the Global Data Assimilation and Prediction Systems-Korea Integrated
Model (GDAPS-KIM) is utilized. We provide analysis on a comprehensive set of
baseline methods aimed at addressing the challenges of KoMet, including the
sparsity of AWS observations and class imbalance. To lower the barrier to entry
and encourage further study, we also provide an extensive open-source Python
package for data processing and model development. Our benchmark data and code
are available at https://github.com/osilab-kaist/KoMet-Benchmark-Dataset."
8272,"Using GAM Changer
as a research instrument, we plan to develop editing guidelines by            [2] Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich,
further research that engages with experts in diverse domains as
well as people who would be impacted by edited models.","https://www.lendingclub.com/
in a risk prediction model for prostate cancer.","Rich Caruana, and Geoffrey E Hinton.",2022-06-30 17:57:12+00:00,"Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values",cs.LG,"['cs.LG', 'cs.AI', 'cs.HC']","[arxiv.Result.Author('Zijie J. Wang'), arxiv.Result.Author('Alex Kale'), arxiv.Result.Author('Harsha Nori'), arxiv.Result.Author('Peter Stella'), arxiv.Result.Author('Mark E. Nunnally'), arxiv.Result.Author('Duen Horng Chau'), arxiv.Result.Author('Mihaela Vorvoreanu'), arxiv.Result.Author('Jennifer Wortman Vaughan'), arxiv.Result.Author('Rich Caruana')]","Machine learning (ML) interpretability techniques can reveal undesirable
patterns in data that models exploit to make predictions--potentially causing
harms once deployed. However, how to take action to address these patterns is
not always clear. In a collaboration between ML and human-computer interaction
researchers, physicians, and data scientists, we develop GAM Changer, the first
interactive system to help domain experts and data scientists easily and
responsibly edit Generalized Additive Models (GAMs) and fix problematic
patterns. With novel interaction techniques, our tool puts interpretability
into action--empowering users to analyze, validate, and align model behaviors
with their knowledge and values. Physicians have started to use our tool to
investigate and fix pneumonia and sepsis risk prediction models, and an
evaluation with 7 data scientists working in diverse domains highlights that
our tool is easy to use, meets their model editing needs, and fits into their
current workflows. Built with modern web technologies, our tool runs locally in
users' web browsers or computational notebooks, lowering the barrier to use.
GAM Changer is available at the following public demo link:
https://interpret.ml/gam-changer."
8287,"They successfully save memory and expedite            feedback, which may benefit further research.","‘catalog-based soft labeling’, ‘category-based negative sampling’,         • We apply e-CLIP to multilingual downstream classification and
‘multi-stream accumulation’, and ‘batch-size scheduler’ (see Sec-            clustering tasks in a real industrial scenario and obtain positive
tion 3.3 for details).","model convergence, enabling efficient training with satisfactory per-
formance on multiple e-commerce downstream tasks.",2022-07-01 05:16:47+00:00,e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Wonyoung Shin'), arxiv.Result.Author('Jonghun Park'), arxiv.Result.Author('Taekang Woo'), arxiv.Result.Author('Yongwoo Cho'), arxiv.Result.Author('Kwangjin Oh'), arxiv.Result.Author('Hwanjun Song')]","Understanding vision and language representations of product content is vital
for search and recommendation applications in e-commerce. As a backbone for
online shopping platforms and inspired by the recent success in representation
learning research, we propose a contrastive learning framework that aligns
language and visual models using unlabeled raw product text and images. We
present techniques we used to train large-scale representation learning models
and share solutions that address domain-specific challenges. We study the
performance using our pre-trained model as backbones for diverse downstream
tasks, including category classification, attribute extraction, product
matching, product clustering, and adult product recognition. Experimental
results show that our proposed method outperforms the baseline in each
downstream task regarding both single modality and multiple modalities."
8288,"In addition              • We apply e-CLIP to multilingual downstream classification and
to the framework, we present effective data preprocessing meth-              clustering tasks in a real industrial scenario and obtain positive
ods, including cleaning noisy data by removing invalid, duplicate,           feedback, which may benefit further research.","model convergence, enabling efficient training with satisfactory per-
formance on multiple e-commerce downstream tasks.",and inappropriate products.,2022-07-01 05:16:47+00:00,e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Wonyoung Shin'), arxiv.Result.Author('Jonghun Park'), arxiv.Result.Author('Taekang Woo'), arxiv.Result.Author('Yongwoo Cho'), arxiv.Result.Author('Kwangjin Oh'), arxiv.Result.Author('Hwanjun Song')]","Understanding vision and language representations of product content is vital
for search and recommendation applications in e-commerce. As a backbone for
online shopping platforms and inspired by the recent success in representation
learning research, we propose a contrastive learning framework that aligns
language and visual models using unlabeled raw product text and images. We
present techniques we used to train large-scale representation learning models
and share solutions that address domain-specific challenges. We study the
performance using our pre-trained model as backbones for diverse downstream
tasks, including category classification, attribute extraction, product
matching, product clustering, and adult product recognition. Experimental
results show that our proposed method outperforms the baseline in each
downstream task regarding both single modality and multiple modalities."
8319,"ods were able to (partially) detect the prerequisite relation-  Third, further research should be conducted to check which
ship between weeks, while relying on behavioral features        explanations (and explainability methods) lead to interven-
only (RQ3).","Choosing transparent shallow ar-
                                                                chitectures instead of black-boxes could also allow us to val-
Our analyses also conﬁrmed that all the evaluated meth-         idate our results against ground truth feature importances.",Our experimental design was inspired by [11]’s      tions that better improve learning outcomes.,2022-07-01 17:09:17+00:00,Evaluating the Explainers: Black-Box Explainable Machine Learning for Student Success Prediction in MOOCs,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Vinitra Swamy'), arxiv.Result.Author('Bahar Radmehr'), arxiv.Result.Author('Natasa Krco'), arxiv.Result.Author('Mirko Marras'), arxiv.Result.Author('Tanja Käser')]","Neural networks are ubiquitous in applied machine learning for education.
Their pervasive success in predictive performance comes alongside a severe
weakness, the lack of explainability of their decisions, especially relevant in
human-centric fields. We implement five state-of-the-art methodologies for
explaining black-box machine learning models (LIME, PermutationSHAP,
KernelSHAP, DiCE, CEM) and examine the strengths of each approach on the
downstream task of student performance prediction for five massive open online
courses. Our experiments demonstrate that the families of explainers do not
agree with each other on feature importance for the same Bidirectional LSTM
models with the same representative set of students. We use Principal Component
Analysis, Jensen-Shannon distance, and Spearman's rank-order correlation to
quantitatively cross-examine explanations across methods and courses.
Furthermore, we validate explainer performance across curriculum-based
prerequisite relationships. Our results come to the concerning conclusion that
the choice of explainer is an important decision and is in fact paramount to
the interpretation of the predictive results, even more so than the course the
model is trained on. Source code and models are released at
http://github.com/epfl-ml4ed/evaluating-explainers."
8360,"This information may be valuable when allocating
costly resources, required to further study the implications of the reported discoveries [12, 13].","In our genetic example, the ability to control the FDR allows us to form a
list of discoveries such that the majority of the selected mutations are expected to be conditionally
associated with drug resistance, on average.","The
power of the selection procedure (larger is better) is deﬁned as E |Sˆ ∩ H1|/|H1| , being the expected

proportion of non-nulls that are correctly selected among all the true non-nulls.",2022-07-03 12:29:25+00:00,Learning to Increase the Power of Conditional Randomization Tests,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Shalev Shaer'), arxiv.Result.Author('Yaniv Romano')]","The model-X conditional randomization test is a generic framework for
conditional independence testing, unlocking new possibilities to discover
features that are conditionally associated with a response of interest while
controlling type-I error rates. An appealing advantage of this test is that it
can work with any machine learning model to design powerful test statistics. In
turn, the common practice in the model-X literature is to form a test statistic
using machine learning models, trained to maximize predictive accuracy with the
hope to attain a test with good power. However, the ideal goal here is to drive
the model (during training) to maximize the power of the test, not merely the
predictive accuracy. In this paper, we bridge this gap by introducing, for the
first time, novel model-fitting schemes that are designed to explicitly improve
the power of model-X tests. This is done by introducing a new cost function
that aims at maximizing the test statistic used to measure violations of
conditional independence. Using synthetic and real data sets, we demonstrate
that the combination of our proposed loss function with various base predictive
models (lasso, elastic net, and deep neural networks) consistently increases
the number of correct discoveries obtained, while maintaining type-I error
rates under control."
8381,"2The M competitions [13, 14] are famous forecasting competitions that focus on
   We hope that our results can inspire further research beyond the       univariate time series forecasting.","The architecture shows comparable results to N-BEATS
        ods in long sequence forecasting tasks.",N-BEATS is proposed after the competition.,2022-07-04 04:03:00+00:00,Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Tianping Zhang'), arxiv.Result.Author('Yizhuo Zhang'), arxiv.Result.Author('Wei Cao'), arxiv.Result.Author('Jiang Bian'), arxiv.Result.Author('Xiaohan Yi'), arxiv.Result.Author('Shun Zheng'), arxiv.Result.Author('Jian Li')]","Multivariate time series forecasting has seen widely ranging applications in
various domains, including finance, traffic, energy, and healthcare. To capture
the sophisticated temporal patterns, plenty of research studies designed
complex neural network architectures based on many variants of RNNs, GNNs, and
Transformers. However, complex models are often computationally expensive and
thus face a severe challenge in training and inference efficiency when applied
to large-scale real-world datasets. In this paper, we introduce LightTS, a
light deep learning architecture merely based on simple MLP-based structures.
The key idea of LightTS is to apply an MLP-based structure on top of two
delicate down-sampling strategies, including interval sampling and continuous
sampling, inspired by a crucial fact that down-sampling time series often
preserves the majority of its information. We conduct extensive experiments on
eight widely used benchmark datasets. Compared with the existing
state-of-the-art methods, LightTS demonstrates better performance on five of
them and comparable performance on the rest. Moreover, LightTS is highly
efficient. It uses less than 5% FLOPS compared with previous SOTA methods on
the largest benchmark dataset. In addition, LightTS is robust and has a much
smaller variance in forecasting accuracy than previous SOTA methods in long
sequence forecasting tasks."
8441,"Though the paper suggests that this approach will improve the use of XAI in this ﬁeld,
there is no suggestion on how to implement XAI into this particular type of agent and that is left
up to further research.","The authors look to develop a Main Aggregator Server (MAS) with a Deep
Q-Learning (DQN) that aggregates the responses and makes the ﬁnal decision in allocations in the
network.","6.6 Other Applications

Two reviewed studies didn’t ﬁt the other areas of application, therefore are reviewed here.",2022-07-05 09:43:17+00:00,"Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thomas Hickling'), arxiv.Result.Author('Abdelhafid Zenati'), arxiv.Result.Author('Nabil Aouf'), arxiv.Result.Author('Phillippa Spencer')]","The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised."
8442,"The plan is for further research to
solve this problem by looking at ways to select the most crucial state features.","The authors felt that the
interface currently is limited by state and action scalability.","7 Limitations of the Current Methods

From these papers covered in the review, there are some limitations.",2022-07-05 09:43:17+00:00,"Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thomas Hickling'), arxiv.Result.Author('Abdelhafid Zenati'), arxiv.Result.Author('Nabil Aouf'), arxiv.Result.Author('Phillippa Spencer')]","The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised."
8443,"The recommendation is for further research into these areas by
applying some of the XAI methods that have been described in other applications.","In the other ﬁelds, it is hard to make any conclusions about what XAI methods are best as
there is such a limited sample.","In the ﬁeld of
health science, there are a great many methods that would seem to apply well.",2022-07-05 09:43:17+00:00,"Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thomas Hickling'), arxiv.Result.Author('Abdelhafid Zenati'), arxiv.Result.Author('Nabil Aouf'), arxiv.Result.Author('Phillippa Spencer')]","The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised."
8444,"Though the paper suggests that this approach will improve the use of XAI in this ﬁeld,
there is no suggestion on how to implement XAI into this particular type of agent and that is left
up to further research.","The authors look to develop a Main Aggregator Server (MAS) with a Deep
Q-Learning (DQN) that aggregates the responses and makes the ﬁnal decision in allocations in the
network.","6.6 Other Applications

Two reviewed studies didn’t ﬁt the other areas of application, therefore are reviewed here.",2022-07-05 09:43:17+00:00,"Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thomas Hickling'), arxiv.Result.Author('Abdelhafid Zenati'), arxiv.Result.Author('Nabil Aouf'), arxiv.Result.Author('Phillippa Spencer')]","The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised."
8445,"The plan is for further research to
solve this problem by looking at ways to select the most crucial state features.","The authors felt that the
interface currently is limited by state and action scalability.","7 Limitations of the Current Methods

From these papers covered in the review, there are some limitations.",2022-07-05 09:43:17+00:00,"Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thomas Hickling'), arxiv.Result.Author('Abdelhafid Zenati'), arxiv.Result.Author('Nabil Aouf'), arxiv.Result.Author('Phillippa Spencer')]","The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised."
8446,"The recommendation is for further research into these areas by
applying some of the XAI methods that have been described in other applications.","In the other ﬁelds, it is hard to make any conclusions about what XAI methods are best as
there is such a limited sample.","In the ﬁeld of
health science, there are a great many methods that would seem to apply well.",2022-07-05 09:43:17+00:00,"Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Thomas Hickling'), arxiv.Result.Author('Abdelhafid Zenati'), arxiv.Result.Author('Nabil Aouf'), arxiv.Result.Author('Phillippa Spencer')]","The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised."
8479,"While further research may yield models with even better thickness prediction accuracy, we
believe that this study opens a new direction of leveraging transfer learning in automatic
thickness extraction and serves as an inspiration for future research in the area of high-
throughput characterization.","We demonstrate the two-stage process of thicknessML on six experimental (MAPbI3)
films, achieving 6–19% MAPE on the predicted film thicknesses.",IV.,2022-06-14 16:26:15+00:00,Transfer Learning for Rapid Extraction of Thickness from Optical Spectra of Semiconductor Thin Films,cs.LG,"['cs.LG', 'cond-mat.mtrl-sci', 'eess.IV', 'physics.optics']","[arxiv.Result.Author('Siyu Isaac Parker Tian'), arxiv.Result.Author('Zekun Ren'), arxiv.Result.Author('Selvaraj Venkataraj'), arxiv.Result.Author('Yuanhang Cheng'), arxiv.Result.Author('Daniil Bash'), arxiv.Result.Author('Felipe Oviedo'), arxiv.Result.Author('J. Senthilnath'), arxiv.Result.Author('Vijila Chellappan'), arxiv.Result.Author('Yee-Fun Lim'), arxiv.Result.Author('Armin G. Aberle'), arxiv.Result.Author('Benjamin P MacLeod'), arxiv.Result.Author('Fraser G. L. Parlane'), arxiv.Result.Author('Curtis P. Berlinguette'), arxiv.Result.Author('Qianxiao Li'), arxiv.Result.Author('Tonio Buonassisi'), arxiv.Result.Author('Zhe Liu')]","High-throughput experimentation with autonomous workflows, increasingly used
to screen and optimize optoelectronic thin films, requires matching throughput
of downstream characterizations. Despite being essential, thickness
characterization lags in throughput. Although optical spectroscopic methods,
e.g., spectrophotometry, provide quick measurements, a critical bottleneck is
the ensuing manual fitting of optical oscillation models to the measured
reflection and transmission. This study presents a machine-learning (ML)
framework called thicknessML, which rapidly extracts film thickness from
spectroscopic reflection and transmission. thicknessML leverages transfer
learning to generalize to materials of different underlying optical oscillator
models (i.e., different material classes).We demonstrate that thicknessML can
extract film thickness from six perovskite samples in a two-stage process: (1)
pre-training on a generic simulated dataset of Tauc-Lorentz oscillator, and (2)
transfer learning to a simulated perovskite dataset of several literature
perovskite refractive indices. Results show a pre-training thickness mean
absolute percentage error (MAPE) of 5-7% and an experimental thickness MAPE of
6-19%."
8490,"This is while FL data do not comply with this assumption,
and, thus, further research is required to investigate the practicality of adversarial
training in FL [49].","To begin with, this approach was mainly designed for independent and
identically distributed data.","Furthermore, this approach can be very time-consuming.",2022-07-05 22:07:26+00:00,Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV', 'cs.DC']","[arxiv.Result.Author('Ehsan Hallaji'), arxiv.Result.Author('Roozbeh Razavi-Far'), arxiv.Result.Author('Mehrdad Saif')]","The advent of federated learning has facilitated large-scale data exchange
amongst machine learning models while maintaining privacy. Despite its brief
history, federated learning is rapidly evolving to make wider use more
practical. One of the most significant advancements in this domain is the
incorporation of transfer learning into federated learning, which overcomes
fundamental constraints of primary federated learning, particularly in terms of
security. This chapter performs a comprehensive survey on the intersection of
federated and transfer learning from a security point of view. The main goal of
this study is to uncover potential vulnerabilities and defense mechanisms that
might compromise the privacy and performance of systems that use federated and
transfer learning."
8514,"the existing body of work in this area and give motivation
                                       In order to keep pace, hardware designers have also been             for further research on power analysis for adversarial attacks
                                       developing new and innovative methods to accelerate and              against this class of hardware.","We believe that
                                          Advancements in machine learning (ML) are opening up              the results shown here represent an important contribution to
                                       new domains for artiﬁcial intelligence (AI) at record speed.","The rest of this paper proceeds
                                       improve the efﬁciency of ML algorithms [1].",2022-07-06 15:56:30+00:00,Enhancing Adversarial Attacks on Single-Layer NVM Crossbar-Based Neural Networks with Power Consumption Information,cs.LG,['cs.LG'],[arxiv.Result.Author('Cory Merkel')],"Adversarial attacks on state-of-the-art machine learning models pose a
significant threat to the safety and security of mission-critical autonomous
systems. This paper considers the additional vulnerability of machine learning
models when attackers can measure the power consumption of their underlying
hardware platform. In particular, we explore the utility of power consumption
information for adversarial attacks on non-volatile memory crossbar-based
single-layer neural networks. Our results from experiments with MNIST and
CIFAR-10 datasets show that power consumption can reveal important information
about the neural network's weight matrix, such as the 1-norm of its columns.
That information can be used to infer the sensitivity of the network's loss
with respect to different inputs. We also find that surrogate-based black box
attacks that utilize crossbar power information can lead to improved attack
efficiency."
8515,The   critical step towards further research in this area.,"While the results are theoretical and deal only
accuracy of the surrogate model on the test set vs. size of the    with ideal crossbar behavior, we believe that they represent a
training set used, which is obtained by querying the oracle.","Speciﬁcally,
center plot shows the results of using the surrogate to generate   our results show that power information alone can reveal the
FGSM adversarial inputs to the oracle with attack strength         sensitivity of a neural network’s loss with respect to different
of 0.1.",2022-07-06 15:56:30+00:00,Enhancing Adversarial Attacks on Single-Layer NVM Crossbar-Based Neural Networks with Power Consumption Information,cs.LG,['cs.LG'],[arxiv.Result.Author('Cory Merkel')],"Adversarial attacks on state-of-the-art machine learning models pose a
significant threat to the safety and security of mission-critical autonomous
systems. This paper considers the additional vulnerability of machine learning
models when attackers can measure the power consumption of their underlying
hardware platform. In particular, we explore the utility of power consumption
information for adversarial attacks on non-volatile memory crossbar-based
single-layer neural networks. Our results from experiments with MNIST and
CIFAR-10 datasets show that power consumption can reveal important information
about the neural network's weight matrix, such as the 1-norm of its columns.
That information can be used to infer the sensitivity of the network's loss
with respect to different inputs. We also find that surrogate-based black box
attacks that utilize crossbar power information can lead to improved attack
efficiency."
8516,"However, further study will be needed to          discovery in data mining, 2005, pp.","Another reason may be the relative complexity of           [7] D. Lowd and C. Meek, “Adversarial learning,” in Proceedings of
the CIFAR-10 dataset compared to MNIST and their different               the eleventh ACM SIGKDD international conference on Knowledge
underlying statistics.",641–647.,2022-07-06 15:56:30+00:00,Enhancing Adversarial Attacks on Single-Layer NVM Crossbar-Based Neural Networks with Power Consumption Information,cs.LG,['cs.LG'],[arxiv.Result.Author('Cory Merkel')],"Adversarial attacks on state-of-the-art machine learning models pose a
significant threat to the safety and security of mission-critical autonomous
systems. This paper considers the additional vulnerability of machine learning
models when attackers can measure the power consumption of their underlying
hardware platform. In particular, we explore the utility of power consumption
information for adversarial attacks on non-volatile memory crossbar-based
single-layer neural networks. Our results from experiments with MNIST and
CIFAR-10 datasets show that power consumption can reveal important information
about the neural network's weight matrix, such as the 1-norm of its columns.
That information can be used to infer the sensitivity of the network's loss
with respect to different inputs. We also find that surrogate-based black box
attacks that utilize crossbar power information can lead to improved attack
efficiency."
8522,"Once the initial γi are computed, these coefﬁcients         obtaining the clustering labels, these deserve further research.","(32) involves an undesired    concise strategies, such as rank constraint [41], [48], [52] or
hyper-parameter α, to get rid of its impact, we directly impose    one-pass manner [25], provide promising solutions of directly
α = 0.",will remain unchanged during the iteration.,2022-07-05 05:00:38+00:00,Local Sample-weighted Multiple Kernel Clustering with Consensus Discriminative Graph,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Liang Li'), arxiv.Result.Author('Siwei Wang'), arxiv.Result.Author('Xinwang Liu'), arxiv.Result.Author('En Zhu'), arxiv.Result.Author('Li Shen'), arxiv.Result.Author('Kenli Li'), arxiv.Result.Author('Keqin Li')]","Multiple kernel clustering (MKC) is committed to achieving optimal
information fusion from a set of base kernels. Constructing precise and local
kernel matrices is proved to be of vital significance in applications since the
unreliable distant-distance similarity estimation would degrade clustering
per-formance. Although existing localized MKC algorithms exhibit improved
performance compared to globally-designed competi-tors, most of them widely
adopt KNN mechanism to localize kernel matrix by accounting for {\tau} -nearest
neighbors. However, such a coarse manner follows an unreasonable strategy that
the ranking importance of different neighbors is equal, which is impractical in
applications. To alleviate such problems, this paper proposes a novel local
sample-weighted multiple kernel clustering (LSWMKC) model. We first construct a
consensus discriminative affinity graph in kernel space, revealing the latent
local structures. Further, an optimal neighborhood kernel for the learned
affinity graph is output with naturally sparse property and clear block
diagonal structure. Moreover, LSWMKC im-plicitly optimizes adaptive weights on
different neighbors with corresponding samples. Experimental results
demonstrate that our LSWMKC possesses better local manifold representation and
outperforms existing kernel or graph-based clustering algo-rithms. The source
code of LSWMKC can be publicly accessed from
https://github.com/liliangnudt/LSWMKC."
8523,"The green dash line denotes the tuning results, for       to this issue, which deserves our further research.","The red line denotes our reported         constraint or one-pass mechanism, provide promising solutions
results.","simplicity, α is ﬁxed at the index of the optimal results.",2022-07-05 05:00:38+00:00,Local Sample-weighted Multiple Kernel Clustering with Consensus Discriminative Graph,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Liang Li'), arxiv.Result.Author('Siwei Wang'), arxiv.Result.Author('Xinwang Liu'), arxiv.Result.Author('En Zhu'), arxiv.Result.Author('Li Shen'), arxiv.Result.Author('Kenli Li'), arxiv.Result.Author('Keqin Li')]","Multiple kernel clustering (MKC) is committed to achieving optimal
information fusion from a set of base kernels. Constructing precise and local
kernel matrices is proved to be of vital significance in applications since the
unreliable distant-distance similarity estimation would degrade clustering
per-formance. Although existing localized MKC algorithms exhibit improved
performance compared to globally-designed competi-tors, most of them widely
adopt KNN mechanism to localize kernel matrix by accounting for {\tau} -nearest
neighbors. However, such a coarse manner follows an unreasonable strategy that
the ranking importance of different neighbors is equal, which is impractical in
applications. To alleviate such problems, this paper proposes a novel local
sample-weighted multiple kernel clustering (LSWMKC) model. We first construct a
consensus discriminative affinity graph in kernel space, revealing the latent
local structures. Further, an optimal neighborhood kernel for the learned
affinity graph is output with naturally sparse property and clear block
diagonal structure. Moreover, LSWMKC im-plicitly optimizes adaptive weights on
different neighbors with corresponding samples. Experimental results
demonstrate that our LSWMKC possesses better local manifold representation and
outperforms existing kernel or graph-based clustering algo-rithms. The source
code of LSWMKC can be publicly accessed from
https://github.com/liliangnudt/LSWMKC."
8530,"We expect that sinusoidal embeddings of m/z will be a critical component of
further research that incorporates MS2 spectra modeling, and we look forward to exploring their use
in other contexts.","The property prediction results of are sufﬁcient quality to functionally inform and advance
drug-discovery efforts.","8
            Table 2: R2 for predicted chemical properties

                Property                Feed Forward    Tokenized m/z  Sinusoidal m/z
                                       Known Novel      Known Novel    Known Novel
                    all
              atomic log P             0.824     0.604  0.943  0.72    0.976    0.8
number of hydrogen bond acceptors      0.768     0.357  0.919  0.437   0.968   0.622
 number of hydrogen bond donors        0.835     0.762  0.969  0.887   0.987   0.941
           polar surface area          0.834     0.572  0.924  0.683   0.968   0.69
      number of rotatable bonds        0.851     0.716  0.965  0.862   0.986   0.907
      number of aromatic rings         0.801     0.669  0.939  0.792   0.976   0.838
      number of aliphatic rings        0.838     0.372  0.934  0.474   0.981   0.655
        number of heteroatoms          0.839     0.638  0.943  0.762   0.974   0.821
        fraction of sp3 carbons        0.823     0.719  0.968  0.902   0.988   0.946
quantitative estimate of druglikeness  0.846     0.618  0.951  0.723   0.977   0.821
                                       0.806     0.617  0.912  0.681   0.952   0.755

References

[1] Nist standard reference database 1a.",2022-07-06 21:27:13+00:00,Multi-scale Sinusoidal Embeddings Enable Learning on High Resolution Mass Spectrometry Data,cs.LG,"['cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Gennady Voronov'), arxiv.Result.Author('Rose Lightheart'), arxiv.Result.Author('Joe Davison'), arxiv.Result.Author('Christoph A. Krettler'), arxiv.Result.Author('David Healey'), arxiv.Result.Author('Thomas Butler')]","Small molecules in biological samples are studied to provide information
about disease states, environmental toxins, natural product drug discovery, and
many other applications. The primary window into the composition of small
molecule mixtures is tandem mass spectrometry (MS2), which produces data that
are of high sensitivity and part per million resolution. We adopt multi-scale
sinusoidal embeddings of the mass data in MS2 designed to meet the challenge of
learning from the full resolution of MS2 data. Using these embeddings, we
provide a new state of the art model for spectral library search, the standard
task for initial evaluation of MS2 data. We also introduce a new task, chemical
property prediction from MS2 data, that has natural applications in
high-throughput MS2 experiments and show that an average $R^2$ of 80\% for
novel compounds can be achieved across 10 chemical properties prioritized by
medicinal chemists. We use dimensionality reduction techniques and experiments
with different floating point resolutions to show the essential role
multi-scale sinusoidal embeddings play in learning from MS2 data."
8532,"Encouraging results promoted further research
with additions involving search cost inclusion as an optimization criterion [4], early forms of
online resource allocation and distributed search [5], unwanted parameter space pruning [6], or
replacement of single estimators with ensemble methods [7], among others.","Early applications [2, 3] found positive
outperformance on expert consensus across a range of benchmarked datasets through Gaussian
Process or Tree-structured Parzen estimators.","Though alternative approaches have surfaced [8], SMBO-based search has become one of the
most popular non-naïve search methods in the training of complex machine learning predictors,
with frequent appearances in competitions [9] and package releases [10].",2022-07-06 23:51:44+00:00,Model Agnostic Conformal Hyperparameter Optimization,cs.LG,['cs.LG'],[arxiv.Result.Author('Riccardo Doyle')],"Several novel frameworks for hyperparameter search have emerged in the last
decade, but most rely on strict, often normal, distributional assumptions,
limiting search model flexibility. This paper proposes a novel optimization
framework based on Conformal prediction, assuming only exchangeability, and
allowing for a larger choice of search model architectures and variance
estimators. Several such models are explored and benchmarked against random
hyperparameter search on both dense and convolutional neural networks with
consistent overperformance both in final loss achieved and time to achievement."
8548,"We believe that further research in hybrid approaches
D. COMPARISON WITH REINFORCEMENT LEARNING                      as our may give life to simpler and better metaheuristics
In this section, we compare the performance of oTS with        capable of producing near-optimal solutions in a shorter
the proposals relying on DRL.","hanced with machine learning guarantee to ﬁnd better solu-
                                                               tions.","The objective is to justify      amount of time thanks to the knowledge extracted with
our efforts by demonstrating once more the superiority         machine learning.",2022-07-07 11:53:10+00:00,Learning the Quality of Machine Permutations in Job Shop Scheduling,cs.LG,"['cs.LG', 'cs.NE', 'math.CO', '90B35', 'I.2.6; G.2.1']","[arxiv.Result.Author('Andrea Corsini'), arxiv.Result.Author('Simone Calderara'), arxiv.Result.Author(""Mauro Dell'Amico"")]","In recent years, the power demonstrated by Machine Learning (ML) has
increasingly attracted the interest of the optimization community that is
starting to leverage ML for enhancing and automating the design of optimal and
approximate algorithms. One combinatorial optimization problem that has been
tackled with ML is the Job Shop scheduling Problem (JSP). Most of the recent
works focusing on the JSP and ML are based on Deep Reinforcement Learning
(DRL), and only a few of them leverage supervised learning techniques. The
recurrent reasons for avoiding supervised learning seem to be the difficulty in
casting the right learning task, i.e., what is meaningful to predict, and how
to obtain labels. Therefore, we first propose a novel supervised learning task
that aims at predicting the quality of machine permutations. Then, we design an
original methodology to estimate this quality that allows to create an accurate
sequential deep learning model (binary accuracy above 95%). Finally, we
empirically demonstrate the value of predicting the quality of machine
permutations by enhancing the performance of a simple Tabu Search algorithm
inspired by the works in the literature."
8549,"We believe that further research in hybrid approaches   APPENDIX A SEQUENCE GENERATOR
as our may give life to simpler and better metaheuristics
capable of producing near-optimal solutions in a shorter       The sequence generator procedure takes in a sequence of
amount of time.","hanced with machine learning guarantee to ﬁnd better solu-
tions.","operations on some machine i ∈ M and generates s random
                                                               sequences.",2022-07-07 11:53:10+00:00,Learning the Quality of Machine Permutations in Job Shop Scheduling,cs.LG,"['cs.LG', 'cs.NE', 'math.CO', '90B35', 'I.2.6; G.2.1']","[arxiv.Result.Author('Andrea Corsini'), arxiv.Result.Author('Simone Calderara'), arxiv.Result.Author(""Mauro Dell'Amico"")]","In recent years, the power demonstrated by Machine Learning (ML) has
increasingly attracted the interest of the optimization community that is
starting to leverage ML for enhancing and automating the design of algorithms.
One combinatorial optimization problem recently tackled with ML is the Job Shop
scheduling Problem (JSP). Most of the works on the JSP using ML focus on Deep
Reinforcement Learning (DRL), and only a few of them leverage supervised
learning techniques. The recurrent reasons for avoiding supervised learning
seem to be the difficulty in casting the right learning task, i.e., what is
meaningful to predict, and how to obtain labels. Therefore, we first propose a
novel supervised learning task that aims at predicting the quality of machine
permutations. Then, we design an original methodology to estimate this quality,
and we use these estimations to create an accurate sequential deep learning
model (binary accuracy above 95%). Finally, we empirically demonstrate the
value of predicting the quality of machine permutations by enhancing the
performance of a simple Tabu Search algorithm inspired by the works in the
literature."
8570,"A lower
technical overhead and more data available for modeling could assist further research in these areas.","[16, 17] Data obtained from esports titles – including those gathered from high-level
tournament performance – may provide a path to improving the quality and reproducibility of research in this ﬁeld,
especially in contrast to the more variable data that is collected in laboratories and in less competitive settings.","[18–20]

The sparsity and methodological diversity of research on this topic remain as roadblocks in the study of how video
games can affect mental functioning.",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8571,"Some scholars recommended further research on esports as a potential path
forward.","[18–20]

The sparsity and methodological diversity of research on this topic remain as roadblocks in the study of how video
games can affect mental functioning.","[1] Despite the digital nature of esports – which are their greatest asset with respect to data gathering – there
seems to be a lack of high-quality pre-processed data published for scientiﬁc and practical use.",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8572,"The critical properties of the presented dataset are as follows:

        • To secure the availability of the raw replays for further research and extraction, the SC2ReSet: StarCraft II
           Esport Replaypack Set was created.","All replay packs required to construct the dataset were searched and downloaded manually from the public
domain.","[59]

                                                                      2
SC2EGSet: StarCraft II Esport Replay and Game-state Dataset  A PREPRINT

        • The replays were processed under the licenses provided by the game publisher: End User License Agreement
          (EULA), and ""Blizzard StarCraft II AI and Machine Learning License"" which is available in the subsection A.1
           supplementary material.",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8573,We recommend further research to use SC2ReSet [59] to compute game-engine simulated information.,"Further, it should be noted
that the experiments described here are more illustrative than investigative in nature, and could be greatly expanded upon
in future work.","We also do not provide simulation observation data that allows more detailed spatiotemporal information to be extracted
at a higher computational cost.",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8574,"Some scholars recommended further research on esports as a potential path
forward [1].","The sparsity and methodological diversity of research on this topic remain as roadblocks in the study of how video
games can affect mental functioning.","Despite the digital nature of esports – which are their greatest asset with respect to data gathering – there
seems to be a lack of high-quality pre-processed data published for scientiﬁc and practical use.",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8575,"A brief summary of the contributions stemming from this work is as follows: (1) The development of a set of four
tools to work with StarCraft II data; (2) The collected esports data from various public sources; (3) The publication
of a collection of raw replays after brief pre-processing [23]; (4) The processing of raw data with our toolset and
publication as a dataset [24]; (5) and the preparation of an ofﬁcial API to interact with our data using PyTorch and
PyTorch Lightning for ease of experimentation in further research [25].","The goal of our work is
to mitigate this issue by publishing datasets containing StarCraft II replays and pre-processed data from esports events,
classiﬁed as ""Premiere"" and ""Major"" by Liquipedia in the timeframe from 2016 until 2022 [22].","2 Related Work

While reviewing StarCraft II related sources, we were able to ﬁnd some publicly available datasets made in 2013
“SkillCraft1” [26] and 2017 “MSC” [27].",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8576,"The critical properties of the presented dataset are as follows:

        • To secure the availability of the raw replays for further research and extraction, the SC2ReSet: StarCraft II
           Esport Replaypack Set was created [23].","All replay packs required to construct the dataset were searched and downloaded manually from the public
domain.","• The replays were processed under the licenses provided by the game publisher: End User License Agreement
           (EULA), and ""Blizzard StarCraft II AI and Machine Learning License"" which is available in subsection A.1 of
           the supplementary material.",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8577,We recommend further research to use SC2ReSet [23] to compute game-engine simulated information.,"Further, it should be noted
that the experiments described here are more illustrative than investigative in nature, and could be greatly expanded upon
in future work.","We also do not provide simulation observation data that allows more detailed spatiotemporal information to be extracted
at a higher computational cost.",2022-07-07 16:52:53+00:00,SC2EGSet: StarCraft II Esport Replay and Game-state Dataset,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrzej Białecki'), arxiv.Result.Author('Natalia Jakubowska'), arxiv.Result.Author('Paweł Dobrowolski'), arxiv.Result.Author('Piotr Białecki'), arxiv.Result.Author('Leszek Krupiński'), arxiv.Result.Author('Andrzej Szczap'), arxiv.Result.Author('Robert Białecki'), arxiv.Result.Author('Jan Gajewski')]","As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated ""replays"" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament ""replaypacks""
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks."
8580,"The successes of the FedAvg algorithm as a means to improve performance and training times for systems have inspired
further research into how aggregation methods should be applied.","It is found the Lifelong
FRL increased the learning rate for smart navigation system when tested on robots in a cloud robotic system [14].","The design of the aggregation method is crucial in providing
performance beneﬁts to that of the base case where FRL is not applied.",2022-07-05 21:30:44+00:00,AVDDPG: Federated reinforcement learning applied to autonomous platoon control,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Christian Boin'), arxiv.Result.Author('Lei Lei'), arxiv.Result.Author('Simon X. Yang')]","Since 2016 federated learning (FL) has been an evolving topic of discussion
in the artificial intelligence (AI) research community. Applications of FL led
to the development and study of federated reinforcement learning (FRL). Few
works exist on the topic of FRL applied to autonomous vehicle (AV) platoons. In
addition, most FRL works choose a single aggregation method (usually weight or
gradient aggregation). We explore FRL's effectiveness as a means to improve AV
platooning by designing and implementing an FRL framework atop a custom AV
platoon environment. The application of FRL in AV platooning is studied under
two scenarios: (1) Inter-platoon FRL (Inter-FRL) where FRL is applied to AVs
across different platoons; (2) Intra-platoon FRL (Intra-FRL) where FRL is
applied to AVs within a single platoon. Both Inter-FRL and Intra-FRL are
applied to a custom AV platooning environment using both gradient and weight
aggregation to observe the performance effects FRL can have on AV platoons
relative to an AV platooning environment trained without FRL. It is concluded
that Intra-FRL using weight aggregation (Intra-FRLWA) provides the best
performance for controlling an AV platoon. In addition, we found that weight
aggregation in FRL for AV platooning provides increases in performance relative
to gradient aggregation. Finally, a performance analysis is conducted for
Intra-FRLWA versus a platooning environment without FRL for platoons of length
3, 4 and 5 vehicles. It is concluded that Intra-FRLWA largely out-performs the
platooning environment that is trained without FRL."
8582,further research to advance.,"classifiers (DT and RF) outperforms other models by achieving      With the accuracy that we achieved in the controlled environ-
highest accuracy (1.0) with a zero-standard deviation, recall      ment, we can confidently conclude that the proposed frame-
score and and precision score respectively (1.0 + 0.00) and        work has the robustness in hygiene monitoring that justifies
(1.0 + 0.00).","A foundation has been set through
                                                                   this research, we intend to implement advances further that
   Other classifiers (LR, NB, and SVM) provide reasonable          shall potentially lead to a real-world implementation.",2022-07-07 18:48:48+00:00,A Novel IoT-based Framework for Non-Invasive Human Hygiene Monitoring using Machine Learning Techniques,cs.LG,"['cs.LG', 'eess.SP']","[arxiv.Result.Author('Md Jobair Hossain Faruk'), arxiv.Result.Author('Shashank Trivedi'), arxiv.Result.Author('Mohammad Masum'), arxiv.Result.Author('Maria Valero'), arxiv.Result.Author('Hossain Shahriar'), arxiv.Result.Author('Sheikh Iqbal Ahamed')]","People's personal hygiene habits speak volumes about the condition of taking
care of their bodies and health in daily lifestyle. Maintaining good hygiene
practices not only reduces the chances of contracting a disease but could also
reduce the risk of spreading illness within the community. Given the current
pandemic, daily habits such as washing hands or taking regular showers have
taken primary importance among people, especially for the elderly population
living alone at home or in an assisted living facility. This paper presents a
novel and non-invasive framework for monitoring human hygiene using vibration
sensors where we adopt Machine Learning techniques. The approach is based on a
combination of a geophone sensor, a digitizer, and a cost-efficient computer
board in a practical enclosure. Monitoring daily hygiene routines may help
healthcare professionals be proactive rather than reactive in identifying and
controlling the spread of potential outbreaks within the community. The
experimental result indicates that applying a Support Vector Machine (SVM) for
binary classification exhibits a promising accuracy of ~95% in the
classification of different hygiene habits. Furthermore, both tree-based
classifier (Random Forrest and Decision Tree) outperforms other models by
achieving the highest accuracy (100%), which means that classifying hygiene
events using vibration and non-invasive sensors is possible for monitoring
hygiene activity."
8609,"At the same time, it can stimulate
scholars to further research on robust strategies of inventory theory, thus producing profound value
for the ﬁeld of robust optimization.","Such perfor-
mance results are exciting enough because the conservative attack on robust solutions persists even
though it no longer pretends to know the demand distribution.","On the other hand, the RLDQN algorithm brings more eﬃcient
learning to the pure DQN algorithm.",2022-07-08 10:13:10+00:00,Product Segmentation Newsvendor Problems: A Robust Learning Approach,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xiaoli Yan'), arxiv.Result.Author('Hui Yu'), arxiv.Result.Author('Jiawen Li'), arxiv.Result.Author('Frank Youhua Chen')]","We propose and analyze a product segmentation newsvendor problem, which
generalizes the phenomenon of segmentation sales of a class of perishable
items. The product segmentation newsvendor problem is a new variant of the
newsvendor problem, reflecting that sellers maximize profits by determining the
inventory of the whole item in the context of uncertain demand for sub-items.
We derive the closed-form robust ordering decision by assuming that the means
and covariance matrix of stochastic demand are available but not the
distributions. However, robust approaches that always trade-off in the
worst-case demand scenario face a concern in solution conservatism; thus, the
traditional robust schemes offer unsatisfactory. In this paper, we integrate
robust and deep reinforcement learning (DRL) techniques and propose a new
paradigm termed robust learning to increase the attractiveness of robust
policies. Notably, we take the robust decision as human domain knowledge and
implement it into the training process of DRL by designing a full-process
human-machine collaborative mechanism of teaching experience, normative
decision, and regularization return. Simulation results confirm that our
approach effectively improves robust performance and can generalize to various
problems that require robust but less conservative solutions. Simultaneously,
fewer training episodes, increased training stability, and interpretability of
behavior may have the opportunity to facilitate the deployment of DRL
algorithms in operational practice. Furthermore, the successful attempt of
RLDQN to solve the 1000-dimensional demand scenarios reveals that the algorithm
provides a path to solve complex operational problems through human-machine
collaboration and may have potential significance for solving other complex
operational management problems."
8610,"After that, Tagaras (1989),
Herer and Rashit (1999) carry out further research based on their model frame-
work.","The research on inventory transfer originated from Krishnan and Rao
(1965), who established a product transfer model among multiple retailers
with independent demand and centralized control.","In addition, the emergency transshipment strategy is ﬁrst proposed by
Lee (1987).",2022-07-08 10:13:10+00:00,Robust Newsvendor Problem in Global Market: Stable Operation Strategy for a Two-Market Stochastic System,cs.LG,['cs.LG'],[arxiv.Result.Author('Xiaoli Yan')],"The global markets provide enterprises with selling opportunities and
challenges in stabilizing operational strategies. From the perspective of
production management, it is important to improve the profitability of an
enterprise by exploiting the different timing of the selling season in
different markets to develop an operational strategy that is optimized and
configured on a global scale. This paper examines the above issue with an
insightful model of selling the product to two markets (a primary and a
secondary market) with multiple risks of changes in the market environment and
nonoverlapping selling seasons. We refer to this problem as the ""global robust
newsvendor"" problem. We provide closed-form solutions of the optimal operation
strategy for demand-independent and demand-related scenarios for the above two
market stochastic systems. The closed-form solutions fully reflect the
influence of the relationship between supply and demand on strategy selection.
We find that the demand correlation and the lack of demand information will not
substantially affect the operation strategy, and the enterprise's industrial
chain and supply chain remain stable. However, the reduction of inter-market
tariffs or logistics costs will cause changes, and the existence of the
secondary market will lead to more capacity planning in the primary market. In
addition, our model explicitly considers the impact of exchange rate
uncertainty on operating strategies."
8611,"In addition,
it will be fruitful to further study the global robust operation strategy from
the perspective of supply chain or competition.","A further natural research
question is whether the global operation strategy of an enterprise facing multi-
ple risks still maintains symmetry, independence and demand irrelevance if the
enterprise manager is risk-averse, or whether the operation strategy is more
aggressive or more reserved compared with the risk-neutral one.","References

[1] Andersson, J., J¨ornsten, K., Non˚as, S. L., Sandal, L., & Ubøe, J.",2022-07-08 10:13:10+00:00,Robust Newsvendor Problem in Global Market: Stable Operation Strategy for a Two-Market Stochastic System,cs.LG,['cs.LG'],[arxiv.Result.Author('Xiaoli Yan')],"The global markets provide enterprises with selling opportunities and
challenges in stabilizing operational strategies. From the perspective of
production management, it is important to improve the profitability of an
enterprise by exploiting the different timing of the selling season in
different markets to develop an operational strategy that is optimized and
configured on a global scale. This paper examines the above issue with an
insightful model of selling the product to two markets (a primary and a
secondary market) with multiple risks of changes in the market environment and
nonoverlapping selling seasons. We refer to this problem as the ""global robust
newsvendor"" problem. We provide closed-form solutions of the optimal operation
strategy for demand-independent and demand-related scenarios for the above two
market stochastic systems. The closed-form solutions fully reflect the
influence of the relationship between supply and demand on strategy selection.
We find that the demand correlation and the lack of demand information will not
substantially affect the operation strategy, and the enterprise's industrial
chain and supply chain remain stable. However, the reduction of inter-market
tariffs or logistics costs will cause changes, and the existence of the
secondary market will lead to more capacity planning in the primary market. In
addition, our model explicitly considers the impact of exchange rate
uncertainty on operating strategies."
8627,"To aid further research into understanding, modeling, and designing for
OOD robustness, we will publish a database of our pretrained models on CIFAR-10 (with diverse
effective robustness) with the full range of model architectures, data augmentations, and pruning types
we study.","9
RobustNets benchmark.",The models we study on ImageNet are already available publicly.,2022-07-08 18:05:58+00:00,Models Out of Line: A Fourier Lens on Distribution Shift Robustness,cs.LG,['cs.LG'],"[arxiv.Result.Author('Sara Fridovich-Keil'), arxiv.Result.Author('Brian R. Bartoldson'), arxiv.Result.Author('James Diffenderfer'), arxiv.Result.Author('Bhavya Kailkhura'), arxiv.Result.Author('Peer-Timo Bremer')]","Improving the accuracy of deep neural networks (DNNs) on out-of-distribution
(OOD) data is critical to an acceptance of deep learning (DL) in real world
applications. It has been observed that accuracies on in-distribution (ID)
versus OOD data follow a linear trend and models that outperform this baseline
are exceptionally rare (and referred to as ""effectively robust""). Recently,
some promising approaches have been developed to improve OOD robustness: model
pruning, data augmentation, and ensembling or zero-shot evaluating large
pretrained models. However, there still is no clear understanding of the
conditions on OOD data and model properties that are required to observe
effective robustness. We approach this issue by conducting a comprehensive
empirical study of diverse approaches that are known to impact OOD robustness
on a broad range of natural and synthetic distribution shifts of CIFAR-10 and
ImageNet. In particular, we view the ""effective robustness puzzle"" through a
Fourier lens and ask how spectral properties of both models and OOD data
influence the corresponding effective robustness. We find this Fourier lens
offers some insight into why certain robust models, particularly those from the
CLIP family, achieve OOD robustness. However, our analysis also makes clear
that no known metric is consistently the best explanation (or even a strong
explanation) of OOD robustness. Thus, to aid future research into the OOD
puzzle, we address the gap in publicly-available models with effective
robustness by introducing a set of pretrained models--RobustNets--with varying
levels of OOD robustness."
8629,"Despite the controllability advantages of our constrained
approach, further research is required for understanding the optimization dynamics of INRs, both in
the constrained and unconstrained settings.","As demonstrated by Schwarz and Teh [22], sparsity and meta-learning are complementary avenues
for improving INR-based compression.",For simplicity we concentrated on the unstructured sparsity case in this work.,2022-07-08 22:24:56+00:00,L$_0$onie: Compressing COINs with L$_0$-constraints,cs.LG,"['cs.LG', 'cs.CV', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Juan Ramirez'), arxiv.Result.Author('Jose Gallego-Posada')]","Advances in Implicit Neural Representations (INR) have motivated research on
domain-agnostic compression techniques. These methods train a neural network to
approximate an object, and then store the weights of the trained model. For
example, given an image, a network is trained to learn the mapping from pixel
locations to RGB values. In this paper, we propose L$_0$onie, a
sparsity-constrained extension of the COIN compression method. Sparsity allows
to leverage the faster learning of overparameterized networks, while retaining
the desirable compression rate of smaller models. Moreover, our constrained
formulation ensures that the final model respects a pre-determined compression
rate, dispensing of the need for expensive architecture search."
8632,"We hope this work will inspire the community to further study
                                                  and optimize multi-tenant FL.","Extensive
                                                  experiments demonstrate that MuFL outperforms other methods while consuming
                                                 40% less energy.","1 Introduction

                                       Federated learning (FL) [40] has attracted considerable attention as it enables privacy-preserving
                                       distributed model training among decentralized devices.",2022-07-09 06:22:39+00:00,Smart Multi-tenant Federated Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.DC']","[arxiv.Result.Author('Weiming Zhuang'), arxiv.Result.Author('Yonggang Wen'), arxiv.Result.Author('Shuai Zhang')]","Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous training activities could overload resource-constrained
devices. In this work, we propose a smart multi-tenant FL system, MuFL, to
effectively coordinate and execute simultaneous training activities. We first
formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and
introduce a vanilla multi-tenant FL system that trains activities sequentially
to form baselines. Then, we propose two approaches to optimize multi-tenant FL:
1) activity consolidation merges training activities into one activity with a
multi-task architecture; 2) after training it for rounds, activity splitting
divides it into groups by employing affinities among activities such that
activities within a group have better synergy. Extensive experiments
demonstrate that MuFL outperforms other methods while consuming 40% less
energy. We hope this work will inspire the community to further study and
optimize multi-tenant FL."
8640,"An approach for eﬃcient representation of the multi-head attention can be
regarded as a separate problem whose solution is a direction for further research.","Therefore,
the multi-head self-attention was given to show the fundamental possibility of generalizing the self-
attention-based RF.","7 Numerical experiments

In order to study the proposed approach for solving regression problems, we apply datasets which
are taken from open sources: the dataset Diabetes is available in the corresponding R Packages;
datasets Friedman 1, 2 3 can be found at site: https://www.stat.berkeley.edu/˜breiman/bagging.pdf;
Regression and Sparse datasets are available in package “Scikit-Learn”.",2022-07-09 16:15:53+00:00,Attention and Self-Attention in Random Forests,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","New models of random forests jointly using the attention and self-attention
mechanisms are proposed for solving the regression problem. The models can be
regarded as extensions of the attention-based random forest whose idea stems
from applying a combination of the Nadaraya-Watson kernel regression and the
Huber's contamination model to random forests. The self-attention aims to
capture dependencies of the tree predictions and to remove noise or anomalous
predictions in the random forest. The self-attention module is trained jointly
with the attention module for computing weights. It is shown that the training
process of attention weights is reduced to solving a single quadratic or linear
optimization problem. Three modifications of the general approach are proposed
and compared. A specific multi-head self-attention for the random forest is
also considered. Heads of the self-attention are obtained by changing its
tuning parameters including the kernel parameters and the contamination
parameter of models. Numerical experiments with various datasets illustrate the
proposed models and show that the supplement of the self-attention improves the
model performance for many datasets."
8641,"All the above
ideas can be regarded as direction for further research.",It is interesting to develop algorithms for implementing the multi-head self-attention.,"References

 [1] Z. Niu, G. Zhong, and H. Yu.",2022-07-09 16:15:53+00:00,Attention and Self-Attention in Random Forests,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","New models of random forests jointly using the attention and self-attention
mechanisms are proposed for solving the regression problem. The models can be
regarded as extensions of the attention-based random forest whose idea stems
from applying a combination of the Nadaraya-Watson kernel regression and the
Huber's contamination model to random forests. The self-attention aims to
capture dependencies of the tree predictions and to remove noise or anomalous
predictions in the random forest. The self-attention module is trained jointly
with the attention module for computing weights. It is shown that the training
process of attention weights is reduced to solving a single quadratic or linear
optimization problem. Three modifications of the general approach are proposed
and compared. A specific multi-head self-attention for the random forest is
also considered. Heads of the self-attention are obtained by changing its
tuning parameters including the kernel parameters and the contamination
parameter of models. Numerical experiments with various datasets illustrate the
proposed models and show that the supplement of the self-attention improves the
model performance for many datasets."
8660,"Still, this framework looks very promising for further research
in this domain.","This behavior is still dependent on the difﬁculty of the tasks and on the evolution of
the data distribution through time.","Our experiments with SCoLe show interesting insights on forgetting, knowledge
accumulation and knowledge retention.",2022-07-10 21:40:54+00:00,Scaling the Number of Tasks in Continual Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Timothée Lesort'), arxiv.Result.Author('Oleksiy Ostapenko'), arxiv.Result.Author('Diganta Misra'), arxiv.Result.Author('Md Rifat Arefin'), arxiv.Result.Author('Pau Rodríguez'), arxiv.Result.Author('Laurent Charlin'), arxiv.Result.Author('Irina Rish')]","Standard gradient descent algorithms applied to sequences of tasks are known
to produce catastrophic forgetting in deep neural networks. When trained on a
new task in a sequence, the model updates its parameters on the current task,
forgetting past knowledge.
  This article explores scenarios where we scale the number of tasks in a
finite environment. Those scenarios are composed of a long sequence of tasks
with reoccurring data.
  We show that in such setting, stochastic gradient descent can learn,
progress, and converge to a solution that according to existing literature
needs a continual learning algorithm. In other words, we show that the model
performs knowledge retention and accumulation without specific memorization
mechanisms.
  We propose a new experimentation framework, SCoLe (Scaling Continual
Learning), to study the knowledge retention and accumulation of algorithms in
potentially infinite sequences of tasks. To explore this setting, we performed
a large number of experiments on sequences of 1,000 tasks to better understand
this new family of settings.
  We also propose a slight modifications to the vanilla stochastic gradient
descent to facilitate continual learning in this setting.
  The SCoLe framework represents a good simulation of practical training
environments with reoccurring situations and allows the study of convergence
behavior in long sequences. Our experiments show that previous results on short
scenarios cannot always be extrapolated to longer scenarios."
8685,"Secondly, the generality of our
formulation allows translating any continual learning method into this model repairment setting, and opens
doors to further research.","Firstly, the framework
subsumes works on influence function [8] and data deletion [9] as specific examples, which are developed
independently, and our work reveals their close connections and limitations.","In particular, we extend Elastic Weight Consolidation [10] – a specific continual
learning algorithm – to cause identification and data removal, and demonstrate improvements over the prior
works in a variety of settings where training data are contaminated with annotation and/or input noise.",2022-07-11 12:07:39+00:00,Repairing Neural Networks by Leaving the Right Past Behind,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ryutaro Tanno'), arxiv.Result.Author('Melanie F. Pradier'), arxiv.Result.Author('Aditya Nori'), arxiv.Result.Author('Yingzhen Li')]","Prediction failures of machine learning models often arise from deficiencies
in training data, such as incorrect labels, outliers, and selection biases.
However, such data points that are responsible for a given failure mode are
generally not known a priori, let alone a mechanism for repairing the failure.
This work draws on the Bayesian view of continual learning, and develops a
generic framework for both, identifying training examples that have given rise
to the target failure, and fixing the model through erasing information about
them. This framework naturally allows leveraging recent advances in continual
learning to this new problem of model repairment, while subsuming the existing
works on influence functions and data deletion as specific instances.
Experimentally, the proposed approach outperforms the baselines for both
identification of detrimental training data and fixing model failures in a
generalisable manner."
8686,"Secondly, the generality of our
formulation allows translating any continual learning method into this model repairment setting, and opens
doors to further research.","Firstly, the framework
subsumes works on influence function [8] and data deletion [9] as specific examples, which are developed
independently, and our work reveals their close connections and limitations.","In particular, we extend Elastic Weight Consolidation [10] – a specific continual
learning algorithm – to cause identification and data removal, and demonstrate improvements over the prior
works in a variety of settings where training data are contaminated with annotation and/or input noise.",2022-07-11 12:07:39+00:00,Repairing Neural Networks by Leaving the Right Past Behind,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ryutaro Tanno'), arxiv.Result.Author('Melanie F. Pradier'), arxiv.Result.Author('Aditya Nori'), arxiv.Result.Author('Yingzhen Li')]","Prediction failures of machine learning models often arise from deficiencies
in training data, such as incorrect labels, outliers, and selection biases.
However, such data points that are responsible for a given failure mode are
generally not known a priori, let alone a mechanism for repairing the failure.
This work draws on the Bayesian view of continual learning, and develops a
generic framework for both, identifying training examples that have given rise
to the target failure, and fixing the model through erasing information about
them. This framework naturally allows leveraging recent advances in continual
learning to this new problem of model repairment, while subsuming the existing
works on influence functions and data deletion as specific instances.
Experimentally, the proposed approach outperforms the baselines for both
identification of detrimental training data and fixing model failures in a
generalisable manner."
8688,"It has enticed us with its    further research based on these codes, removes the ability
                                        ability to solve complex problems, such as image classiﬁ-     for others to use the approach for real-world applications
                                        cation, without the need for manual feature identiﬁcation.","This problem hinders
                                        niﬁcantly over the last decade.","and prevents the purpose of the ﬁeld, which is to replace
                                        However, this has been at the cost of now not knowing the     manually-designed architectures.",2022-07-11 12:40:26+00:00,Long-term Reproducibility for Neural Architecture Search,cs.LG,['cs.LG'],"[arxiv.Result.Author('David Towers'), arxiv.Result.Author('Matthew Forshaw'), arxiv.Result.Author('Amir Atapour-Abarghouei'), arxiv.Result.Author('Andrew Stephen McGough')]","It is a sad reflection of modern academia that code is often ignored after
publication -- there is no academic 'kudos' for bug fixes / maintenance. Code
is often unavailable or, if available, contains bugs, is incomplete, or relies
on out-of-date / unavailable libraries. This has a significant impact on
reproducibility and general scientific progress. Neural Architecture Search
(NAS) is no exception to this, with some prior work in reproducibility.
However, we argue that these do not consider long-term reproducibility issues.
We therefore propose a checklist for long-term NAS reproducibility. We evaluate
our checklist against common NAS approaches along with proposing how we can
retrospectively make these approaches more long-term reproducible."
8689,"It has enticed us with its    further research based on these codes, removes the ability
                                        ability to solve complex problems, such as image classiﬁ-     for others to use the approach for real-world applications
                                        cation, without the need for manual feature identiﬁcation.","This problem hinders
                                        niﬁcantly over the last decade.","and prevents the purpose of the ﬁeld, which is to replace
                                        However, this has been at the cost of now not knowing the     manually-designed architectures.",2022-07-11 12:40:26+00:00,Long-term Reproducibility for Neural Architecture Search,cs.LG,['cs.LG'],"[arxiv.Result.Author('David Towers'), arxiv.Result.Author('Matthew Forshaw'), arxiv.Result.Author('Amir Atapour-Abarghouei'), arxiv.Result.Author('Andrew Stephen McGough')]","It is a sad reflection of modern academia that code is often ignored after
publication -- there is no academic 'kudos' for bug fixes / maintenance. Code
is often unavailable or, if available, contains bugs, is incomplete, or relies
on out-of-date / unavailable libraries. This has a significant impact on
reproducibility and general scientific progress. Neural Architecture Search
(NAS) is no exception to this, with some prior work in reproducibility.
However, we argue that these do not consider long-term reproducibility issues.
We therefore propose a checklist for long-term NAS reproducibility. We evaluate
our checklist against common NAS approaches along with proposing how we can
retrospectively make these approaches more long-term reproducible."
8703,"To further study the accuracy and reliability of the formulated CR bounds, we measured the ratio
between the measured CR in an experiment and the expected CR bound IE and the upper CR bound
RLE (Section 3.4).","Figure 3 plots the CR bound curves (IE and RLE) as EC varies along with the
measures CR after network encoding, showing a good correlation between estimated and measured
values.","The closer the ratio is to 1, the higher the accuracy and reliability of the CR
bounds as SBNN design tools.",2022-07-11 15:54:41+00:00,Sparsifying Binary Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Riccardo Schiavone'), arxiv.Result.Author('Maria A. Zuluaga')]","Binary neural networks (BNNs) have demonstrated their ability to solve
complex tasks with comparable accuracy as full-precision deep neural networks
(DNNs), while also reducing computational power and storage requirements and
increasing the processing speed. These properties make them an attractive
alternative for the development and deployment of DNN-based applications in
Internet-of-Things (IoT) devices. Despite the recent improvements, they suffer
from a fixed and limited compression factor that may result insufficient for
certain devices with very limited resources. In this work, we propose sparse
binary neural networks (SBNNs), a novel model and training scheme which
introduces sparsity in BNNs and a new quantization function for binarizing the
network's weights. The proposed SBNN is able to achieve high compression
factors and it reduces the number of operations and parameters at inference
time. We also provide tools to assist the SBNN design, while respecting
hardware resource constraints. We study the generalization properties of our
method for different compression factors through a set of experiments on linear
and convolutional networks on three datasets. Our experiments confirm that
SBNNs can achieve high compression rates, without compromising generalization,
while further reducing the operations of BNNs, making SBNNs a viable option for
deploying DNNs in cheap, low-cost, limited-resources IoT devices and sensors."
8732,"2c represents               5) TGAN with VGMM Preprocessing: We further study
the results for this model.",Fig.,"All the component scores decline      the TGAN by replacing the preprocessing method Gaussian
with respect to TGAN.",2022-07-12 04:08:11+00:00,TabSynDex: A Universal Metric for Robust Evaluation of Synthetic Tabular Data,cs.LG,['cs.LG'],"[arxiv.Result.Author('Vikram S Chundawat'), arxiv.Result.Author('Ayush K Tarun'), arxiv.Result.Author('Murari Mandal'), arxiv.Result.Author('Mukund Lahoti'), arxiv.Result.Author('Pratik Narang')]","Synthetic tabular data generation becomes crucial when real data is limited,
expensive to collect, or simply cannot be used due to privacy concerns.
However, producing good quality synthetic data is challenging. Several
probabilistic, statistical, and generative adversarial networks (GANs) based
approaches have been presented for synthetic tabular data generation. Once
generated, evaluating the quality of the synthetic data is quite challenging.
Some of the traditional metrics have been used in the literature but there is
lack of a common, robust, and single metric. This makes it difficult to
properly compare the effectiveness of different synthetic tabular data
generation methods. In this paper we propose a new universal metric, TabSynDex,
for robust evaluation of synthetic data. TabSynDex assesses the similarity of
synthetic data with real data through different component scores which evaluate
the characteristics that are desirable for ""high quality"" synthetic data. Being
a single score metric, TabSynDex can also be used to observe and evaluate the
training of neural network based approaches. This would help in obtaining
insights that was not possible earlier. Further, we present several baseline
models for comparative analysis of the proposed evaluation metric with existing
generative models."
8733,"several future possibilities and the need for further research                       [25] Y. Yue, Y. Li, K. Yi, and Z. Wu, “Synthetic data approach for classi-
                                                                                           ﬁcation and regression,” in 2018 IEEE 29th International Conference
on better methods for tabular data synthesis.",634–635.,"on Application-speciﬁc Systems, Architectures and Processors (ASAP).",2022-07-12 04:08:11+00:00,TabSynDex: A Universal Metric for Robust Evaluation of Synthetic Tabular Data,cs.LG,['cs.LG'],"[arxiv.Result.Author('Vikram S Chundawat'), arxiv.Result.Author('Ayush K Tarun'), arxiv.Result.Author('Murari Mandal'), arxiv.Result.Author('Mukund Lahoti'), arxiv.Result.Author('Pratik Narang')]","Synthetic tabular data generation becomes crucial when real data is limited,
expensive to collect, or simply cannot be used due to privacy concerns.
However, producing good quality synthetic data is challenging. Several
probabilistic, statistical, and generative adversarial networks (GANs) based
approaches have been presented for synthetic tabular data generation. Once
generated, evaluating the quality of the synthetic data is quite challenging.
Some of the traditional metrics have been used in the literature but there is
lack of a common, robust, and single metric. This makes it difficult to
properly compare the effectiveness of different synthetic tabular data
generation methods. In this paper we propose a new universal metric, TabSynDex,
for robust evaluation of synthetic data. TabSynDex assesses the similarity of
synthetic data with real data through different component scores which evaluate
the characteristics that are desirable for ""high quality"" synthetic data. Being
a single score metric, TabSynDex can also be used to observe and evaluate the
training of neural network based approaches. This would help in obtaining
insights that was not possible earlier. Further, we present several baseline
models for comparative analysis of the proposed evaluation metric with existing
generative models."
8750,"Therefore, although we cannot verify the model in the face of such constraints,
having information about potential ﬁsheries in prohibitive areas can bring further information to the public policies
on sustainable ﬁsheries planning and open new doors for further research on similar lines where the required data is
available.","However, given the shared nature of the IUU ﬁshing activity, the
ﬁshing patterns should behave similarly.","An alternative to such problems would be to have an expert as part of the model (see Figure 16), which mitigates
the need for more data, as the expert would be able to validate the movement patterns further.",2022-07-12 13:17:37+00:00,A semi-supervised geometric-driven methodology for supervised fishing activity detection on multi-source AIS tracking messages,cs.LG,['cs.LG'],"[arxiv.Result.Author('Martha Dais Ferreira'), arxiv.Result.Author('Gabriel Spadon'), arxiv.Result.Author('Amilcar Soares'), arxiv.Result.Author('Stan Matwin')]","Automatic Identification System (AIS) messages are useful for tracking vessel
activity across oceans worldwide using radio links and satellite transceivers.
Such data plays a significant role in tracking vessel activity and mapping
mobility patterns such as those found in fishing. Accordingly, this paper
proposes a geometric-driven semi-supervised approach for fishing activity
detection from AIS data. Through the proposed methodology we show how to
explore the information included in the messages to extract features describing
the geometry of the vessel route. To this end, we leverage the unsupervised
nature of cluster analysis to label the trajectory geometry highlighting the
changes in the vessel's moving pattern which tends to indicate fishing
activity. The labels obtained by the proposed unsupervised approach are used to
detect fishing activities, which we approach as a time-series classification
task. In this context, we propose a solution using recurrent neural networks on
AIS data streams with roughly 87% of the overall $F$-score on the whole
trajectories of 50 different unseen fishing vessels. Such results are
accompanied by a broad benchmark study assessing the performance of different
Recurrent Neural Network (RNN) architectures. In conclusion, this work
contributes by proposing a thorough process that includes data preparation,
labeling, data modeling, and model validation. Therefore, we present a novel
solution for mobility pattern detection that relies upon unfolding the
trajectory in time and observing their inherent geometry."
8751,"Therefore, although we cannot verify the model in the face of such
                        constraints, having information about potential ﬁsheries in prohibitive areas can bring further
                        information to the public policies on sustainable ﬁsheries planning and open new doors for
                        further research on similar lines where the required data is available.","However, given the shared nature of the IUU ﬁshing activity, the ﬁshing patterns
                        should behave similarly.","An alternative to such problems would be to have an expert as part of the model (see
                        Figure 16), which mitigates the need for more data, as the expert would be able to validate the
                        movement patterns further.",2022-07-12 13:17:37+00:00,A semi-supervised methodology for fishing activity detection using the geometry behind the trajectory of multiple vessels,cs.LG,['cs.LG'],"[arxiv.Result.Author('Martha Dais Ferreira'), arxiv.Result.Author('Gabriel Spadon'), arxiv.Result.Author('Amilcar Soares'), arxiv.Result.Author('Stan Matwin')]","Automatic Identification System (AIS) messages are useful for tracking vessel
activity across oceans worldwide using radio links and satellite transceivers.
Such data plays a significant role in tracking vessel activity and mapping
mobility patterns such as those found in fishing. Accordingly, this paper
proposes a geometric-driven semi-supervised approach for fishing activity
detection from AIS data. Through the proposed methodology we show how to
explore the information included in the messages to extract features describing
the geometry of the vessel route. To this end, we leverage the unsupervised
nature of cluster analysis to label the trajectory geometry highlighting the
changes in the vessel's moving pattern which tends to indicate fishing
activity. The labels obtained by the proposed unsupervised approach are used to
detect fishing activities, which we approach as a time-series classification
task. In this context, we propose a solution using recurrent neural networks on
AIS data streams with roughly 87% of the overall $F$-score on the whole
trajectories of 50 different unseen fishing vessels. Such results are
accompanied by a broad benchmark study assessing the performance of different
Recurrent Neural Network (RNN) architectures. In conclusion, this work
contributes by proposing a thorough process that includes data preparation,
labeling, data modeling, and model validation. Therefore, we present a novel
solution for mobility pattern detection that relies upon unfolding the
trajectory in time and observing their inherent geometry."
8765,"UCI Airfoil                       Airfoil   5 1503                    At the same time, the proposed model is rather ﬂexible
                                                                   and this fact allows us to determine several directions for
   In all tables, we compare R2 and the MAE for three cases:       further research.","Numerical experiments have
                                                                   demonstrated that incorporating the attention model into the
UCI Yacht Hydrodynamics           Yacht     6 308                  GBM improves the original GBM.","First, we have investigated only the Huber’s ǫ-
(GBM) the GBM without the softmax and without attention            contamination model for incorporating the trainable parameter
model; (Non-param) a special case of the AGBoost model             into the attention.",2022-07-12 17:42:20+00:00,AGBoost: Attention-based Modification of Gradient Boosting Machine,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Andrei Konstantinov'), arxiv.Result.Author('Lev Utkin'), arxiv.Result.Author('Stanislav Kirpichenko')]","A new attention-based model for the gradient boosting machine (GBM) called
AGBoost (the attention-based gradient boosting) is proposed for solving
regression problems. The main idea behind the proposed AGBoost model is to
assign attention weights with trainable parameters to iterations of GBM under
condition that decision trees are base learners in GBM. Attention weights are
determined by applying properties of decision trees and by using the Huber's
contamination model which provides an interesting linear dependence between
trainable parameters of the attention and the attention weights. This
peculiarity allows us to train the attention weights by solving the standard
quadratic optimization problem with linear constraints. The attention weights
also depend on the discount factor as a tuning parameter, which determines how
much the impact of the weight is decreased with the number of iterations.
Numerical experiments performed for two types of base learners, original
decision trees and extremely randomized trees with various regression datasets
illustrate the proposed model."
8766,"This is also a
and AGBoost) are shown in Table II with the original decision      direction for further research.","It should be also noted that the proposed attention-
                                                                   based approach can be incorporated into other GBM models,
   Measures R2 and MAE for three cases (GBM, Non-param             for example, into XGBoost, pGBRT, SGB, etc.",trees as base learners.,2022-07-12 17:42:20+00:00,AGBoost: Attention-based Modification of Gradient Boosting Machine,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Andrei Konstantinov'), arxiv.Result.Author('Lev Utkin'), arxiv.Result.Author('Stanislav Kirpichenko')]","A new attention-based model for the gradient boosting machine (GBM) called
AGBoost (the attention-based gradient boosting) is proposed for solving
regression problems. The main idea behind the proposed AGBoost model is to
assign attention weights with trainable parameters to iterations of GBM under
condition that decision trees are base learners in GBM. Attention weights are
determined by applying properties of decision trees and by using the Huber's
contamination model which provides an interesting linear dependence between
trainable parameters of the attention and the attention weights. This
peculiarity allows us to train the attention weights by solving the standard
quadratic optimization problem with linear constraints. The attention weights
also depend on the discount factor as a tuning parameter, which determines how
much the impact of the weight is decreased with the number of iterations.
Numerical experiments performed for two types of base learners, original
decision trees and extremely randomized trees with various regression datasets
illustrate the proposed model."
8770,"We believe
these are interesting questions worthy of further research, but their investigation is beyond the scope of this work.","These approaches come with their own complications, however, and likely require answering similar questions.","Instead,
we present empirical work which demonstrates the convergence of RDRO in a variety of scenarios.",2022-07-12 18:11:23+00:00,Long Term Fairness for Minority Groups via Performative Distributionally Robust Optimization,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Liam Peet-Pare'), arxiv.Result.Author('Nidhi Hegde'), arxiv.Result.Author('Alona Fyshe')]","Fairness researchers in machine learning (ML) have coalesced around several
fairness criteria which provide formal definitions of what it means for an ML
model to be fair. However, these criteria have some serious limitations. We
identify four key shortcomings of these formal fairness criteria, and aim to
help to address them by extending performative prediction to include a
distributionally robust objective."
8789,"We
also ﬁnd that R1 can facilitate the formation of condensation during the training, which
may establish a connection between the ﬂatness and generalization for further study.","26
The unique implicit regularization term R1 in dropout, which is diﬀerent from SGD, is
found to be a key source to improve the generalization and the ﬂatness of solution.","This
work reveals the rich and unique properties of dropout, which is a basis for comprehensive
understanding dropout.",2022-07-13 04:09:14+00:00,Implicit regularization of dropout,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhongwang Zhang'), arxiv.Result.Author('Zhi-Qin John Xu')]","It is important to understand how the popular regularization method dropout
helps the neural network training find a good generalization solution. In this
work, we theoretically derive the implicit regularization of dropout and study
the relation between the Hessian matrix of the loss function and the covariance
matrix of the dropout noise, supported by a series of experiments. We then
numerically study two implications of the implicit regularization of dropout,
which intuitively rationalize why dropout helps generalization. First, we find
that the training with dropout finds the neural network with a flatter minimum
compared with standard gradient descent training in experiments, and the
implicit regularization is the key for finding flat solutions. Second, trained
with dropout, input weights of hidden neurons (the input weight of a hidden
neuron consists of the weight from its input layer to the hidden neuron and its
bias term) would tend to condense on isolated orientations. Condensation is a
feature in non-linear learning process, which makes the neural network low
complexity. Although our theory mainly focuses on the dropout used in the last
hidden layer, our experiments apply for general dropout in training neural
networks. This work points out the distinct characteristics of dropout compared
with stochastic gradient descent and serves as an important basis for fully
understanding dropout."
8791,"We
further study how proxies under different constraints ﬁt                                                                                                         Train 0.19 0.17                                                                                     0.10
with historical data on application III.",3.,"We show the Huber
loss [71] of the proxies in Table 8.",2022-07-13 06:44:17+00:00,Unsupervised Learning for Combinatorial Optimization with Principled Objective Design,cs.LG,"['cs.LG', 'cs.AR', 'math.OC']","[arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Nan Wu'), arxiv.Result.Author('Hang Yang'), arxiv.Result.Author('Cong Hao'), arxiv.Result.Author('Pan Li')]","Using machine learning to solve combinatorial optimization (CO) problems is
challenging, especially when the data is unlabeled. This work proposes an
unsupervised learning framework for CO problems. Our framework follows a
standard relaxation-plus-rounding approach and adopts neural networks to
parameterize the relaxed solutions so that simple back-propagation can train
the model end-to-end. Our key contribution is the observation that if the
relaxed objective satisfies entry-wise concavity, a low optimization loss
guarantees the quality of the final integral solutions. This observation
significantly broadens the applicability of the previous framework inspired by
Erdos' probabilistic method. In particular, this observation can guide the
design of objective models in applications where the objectives are not given
explicitly while requiring being modeled in prior. We evaluate our framework by
solving a synthetic graph optimization problem, and two real-world applications
including resource allocation in circuit design and approximate computing. Our
framework largely outperforms the baselines based on na\""{i}ve relaxation,
reinforcement learning, and Gumbel-softmax tricks."
8792,"We
further study how proxies under different constraints ﬁt                                                                                                         Train 0.19 0.17                                                                                     0.10
with historical data on application III.",3.,"We show the Huber
loss [71] of the proxies in Table 8.",2022-07-13 06:44:17+00:00,Unsupervised Learning for Combinatorial Optimization with Principled Objective Relaxation,cs.LG,"['cs.LG', 'cs.AR', 'math.OC']","[arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Nan Wu'), arxiv.Result.Author('Hang Yang'), arxiv.Result.Author('Cong Hao'), arxiv.Result.Author('Pan Li')]","Using machine learning to solve combinatorial optimization (CO) problems is
challenging, especially when the data is unlabeled. This work proposes an
unsupervised learning framework for CO problems. Our framework follows a
standard relaxation-plus-rounding approach and adopts neural networks to
parameterize the relaxed solutions so that simple back-propagation can train
the model end-to-end. Our key contribution is the observation that if the
relaxed objective satisfies entry-wise concavity, a low optimization loss
guarantees the quality of the final integral solutions. This observation
significantly broadens the applicability of the previous framework inspired by
Erdos' probabilistic method. In particular, this observation can guide the
design of objective models in applications where the objectives are not given
explicitly while requiring being modeled in prior. We evaluate our framework by
solving a synthetic graph optimization problem, and two real-world applications
including resource allocation in circuit design and approximate computing. Our
framework largely outperforms the baselines based on na\""{i}ve relaxation,
reinforcement learning, and Gumbel-softmax tricks."
8793,"We
further study how proxies under different constraints ﬁt                                                                                                                                   Train 0.19 0.17                           0.10
with historical data on application III.",4.,"We show the Huber
                                                                                                                                                                                           Test 0.19 0.18                            0.19

                                                                                                                                                          Table 9: Huber loss on application III.",2022-07-13 06:44:17+00:00,Unsupervised Learning for Combinatorial Optimization with Principled Objective Relaxation,cs.LG,"['cs.LG', 'cs.AR', 'math.OC']","[arxiv.Result.Author('Haoyu Wang'), arxiv.Result.Author('Nan Wu'), arxiv.Result.Author('Hang Yang'), arxiv.Result.Author('Cong Hao'), arxiv.Result.Author('Pan Li')]","Using machine learning to solve combinatorial optimization (CO) problems is
challenging, especially when the data is unlabeled. This work proposes an
unsupervised learning framework for CO problems. Our framework follows a
standard relaxation-plus-rounding approach and adopts neural networks to
parameterize the relaxed solutions so that simple back-propagation can train
the model end-to-end. Our key contribution is the observation that if the
relaxed objective satisfies entry-wise concavity, a low optimization loss
guarantees the quality of the final integral solutions. This observation
significantly broadens the applicability of the previous framework inspired by
Erdos' probabilistic method. In particular, this observation can guide the
design of objective models in applications where the objectives are not given
explicitly while requiring being modeled in prior. We evaluate our framework by
solving a synthetic graph optimization problem, and two real-world applications
including resource allocation in circuit design and approximate computing. Our
framework largely outperforms the baselines based on na\""{i}ve relaxation,
reinforcement learning, and Gumbel-softmax tricks."
8829,"Therefore, further researches on exploratory machine learning should be among the salient research problems in the iML
domain.","Similar ﬁndings reafﬁrm the importance of undergoing empirical model
evaluations in addition to subjective ones in order to decide the trustworthiness of a given model.","Besides, the design of user experience for iML should be studied further as it contributes for the effectiveness
of the human-in-the-loop modality.",2022-07-13 13:43:16+00:00,Interactive Machine Learning: A State of the Art Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Natnael A. Wondimu'), arxiv.Result.Author('Cédric Buche'), arxiv.Result.Author('Ubbo Visser')]","Machine learning has proved useful in many software disciplines, including
computer vision, speech and audio processing, natural language processing,
robotics and some other fields. However, its applicability has been
significantly hampered due its black-box nature and significant resource
consumption. Performance is achieved at the expense of enormous computational
resource and usually compromising the robustness and trustworthiness of the
model. Recent researches have been identifying a lack of interactivity as the
prime source of these machine learning problems. Consequently, interactive
machine learning (iML) has acquired increased attention of researchers on
account of its human-in-the-loop modality and relatively efficient resource
utilization. Thereby, a state-of-the-art review of interactive machine learning
plays a vital role in easing the effort toward building human-centred models.
In this paper, we provide a comprehensive analysis of the state-of-the-art of
iML. We analyze salient research works using merit-oriented and
application/task oriented mixed taxonomy. We use a bottom-up clustering
approach to generate a taxonomy of iML research works. Research works on
adversarial black-box attacks and corresponding iML based defense system,
exploratory machine learning, resource constrained learning, and iML
performance evaluation are analyzed under their corresponding theme in our
merit-oriented taxonomy. We have further classified these research works into
technical and sectoral categories. Finally, research opportunities that we
believe are inspiring for future work in iML are discussed thoroughly."
8830,"However, this area is still open for further research as there is no generalized and robust
performance evaluation technique for iML as there is for statistical machine learning algorithms.","[2014] and more speciﬁcally diagnosis of instances to leverage measures of local feature relevance are studied
by different researchers.","8 Conclusion and Future Works

We have seen how iML presents interesting challenges and prospects to conduct future research; not only in terms
of designing robust algorithms and interaction techniques, but also in terms of coherent evaluation methodologies.",2022-07-13 13:43:16+00:00,Interactive Machine Learning: A State of the Art Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Natnael A. Wondimu'), arxiv.Result.Author('Cédric Buche'), arxiv.Result.Author('Ubbo Visser')]","Machine learning has proved useful in many software disciplines, including
computer vision, speech and audio processing, natural language processing,
robotics and some other fields. However, its applicability has been
significantly hampered due its black-box nature and significant resource
consumption. Performance is achieved at the expense of enormous computational
resource and usually compromising the robustness and trustworthiness of the
model. Recent researches have been identifying a lack of interactivity as the
prime source of these machine learning problems. Consequently, interactive
machine learning (iML) has acquired increased attention of researchers on
account of its human-in-the-loop modality and relatively efficient resource
utilization. Thereby, a state-of-the-art review of interactive machine learning
plays a vital role in easing the effort toward building human-centred models.
In this paper, we provide a comprehensive analysis of the state-of-the-art of
iML. We analyze salient research works using merit-oriented and
application/task oriented mixed taxonomy. We use a bottom-up clustering
approach to generate a taxonomy of iML research works. Research works on
adversarial black-box attacks and corresponding iML based defense system,
exploratory machine learning, resource constrained learning, and iML
performance evaluation are analyzed under their corresponding theme in our
merit-oriented taxonomy. We have further classified these research works into
technical and sectoral categories. Finally, research opportunities that we
believe are inspiring for future work in iML are discussed thoroughly."
8831,"Therefore,
we recommend further researches in building exploratory machine learning frameworks.",Not much work has been done in integrating iML features with contemporary machine learning algorithms.,"8.4 Low Resource Learning

Resource constrained learning is among the critical problems a machine learning industry face these days.",2022-07-13 13:43:16+00:00,Interactive Machine Learning: A State of the Art Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Natnael A. Wondimu'), arxiv.Result.Author('Cédric Buche'), arxiv.Result.Author('Ubbo Visser')]","Machine learning has proved useful in many software disciplines, including
computer vision, speech and audio processing, natural language processing,
robotics and some other fields. However, its applicability has been
significantly hampered due its black-box nature and significant resource
consumption. Performance is achieved at the expense of enormous computational
resource and usually compromising the robustness and trustworthiness of the
model. Recent researches have been identifying a lack of interactivity as the
prime source of these machine learning problems. Consequently, interactive
machine learning (iML) has acquired increased attention of researchers on
account of its human-in-the-loop modality and relatively efficient resource
utilization. Thereby, a state-of-the-art review of interactive machine learning
plays a vital role in easing the effort toward building human-centred models.
In this paper, we provide a comprehensive analysis of the state-of-the-art of
iML. We analyze salient research works using merit-oriented and
application/task oriented mixed taxonomy. We use a bottom-up clustering
approach to generate a taxonomy of iML research works. Research works on
adversarial black-box attacks and corresponding iML based defense system,
exploratory machine learning, resource constrained learning, and iML
performance evaluation are analyzed under their corresponding theme in our
merit-oriented taxonomy. We have further classified these research works into
technical and sectoral categories. Finally, research opportunities that we
believe are inspiring for future work in iML are discussed thoroughly."
8832,"Therefore, we suggest further researching to enhance performance evaluation techniques for iML.","The
subjective, co-operation and co-adaptation nature of iML makes it hard to set benchmarks for performance evaluation.","9 Acknowledgements

This work would not have been possible without the ﬁnancial support of the French Embassy in Ethiopia, Brittany
region administration and the Ethiopia Ministry of Education (MoE).",2022-07-13 13:43:16+00:00,Interactive Machine Learning: A State of the Art Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Natnael A. Wondimu'), arxiv.Result.Author('Cédric Buche'), arxiv.Result.Author('Ubbo Visser')]","Machine learning has proved useful in many software disciplines, including
computer vision, speech and audio processing, natural language processing,
robotics and some other fields. However, its applicability has been
significantly hampered due its black-box nature and significant resource
consumption. Performance is achieved at the expense of enormous computational
resource and usually compromising the robustness and trustworthiness of the
model. Recent researches have been identifying a lack of interactivity as the
prime source of these machine learning problems. Consequently, interactive
machine learning (iML) has acquired increased attention of researchers on
account of its human-in-the-loop modality and relatively efficient resource
utilization. Thereby, a state-of-the-art review of interactive machine learning
plays a vital role in easing the effort toward building human-centred models.
In this paper, we provide a comprehensive analysis of the state-of-the-art of
iML. We analyze salient research works using merit-oriented and
application/task oriented mixed taxonomy. We use a bottom-up clustering
approach to generate a taxonomy of iML research works. Research works on
adversarial black-box attacks and corresponding iML based defense system,
exploratory machine learning, resource constrained learning, and iML
performance evaluation are analyzed under their corresponding theme in our
merit-oriented taxonomy. We have further classified these research works into
technical and sectoral categories. Finally, research opportunities that we
believe are inspiring for future work in iML are discussed thoroughly."
8837,"This allows for a quick and easy
implementation of diﬀerent models, which might be an interesting approach for further research.",The concrete ML model can be exchanged easily due to an interface.,"In
the present work, DNNs as a type of ML model are used for network ﬂow characteristic prediction.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8838,Another issue requiring further research concerns the application of a replay buﬀer.,"As a consequence, the equal-depth frequency partitioning method is merely used to
demonstrate the feasibility.","This buﬀer
could include hold-out samples that are continuously updated and used for under-represented classes.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8839,"Excellent results would, however, reinforce the need for further research into even more realistic
investigation protocols.","The application of the realistic
evaluation protocol including simple SLTs results in limited CL performance for the examined models.","More realistic evaluation protocols include, for example, the recognition
and reaction to the addition of new knowledge/classes in diﬀerent changing data distributions (see
section 2.3.1).",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8840,"Moreover, the model oﬀers various functionalities that can be used as a basis for
further research.","The evaluation of DCGMM results in similar or better performances compared to known
standard methods.","In particular, this chapter addresses research question RQ 3 and describes how a novel deep learning
model can look like that is not directly subject to the CF eﬀect.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8841,"The chapter is concluded in
section 9.4, where open issues for further research are outlined.","These procedures include a strategy which addresses
the limitation of applying pseudo-rehearsal approaches in CL scenarios.","9.1 Gaussian Mixture Replay

The Gaussian Mixture Replay (GMR) procedure is based on the DCGMMs presented in chapter 8.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8842,"It is nevertheless acceptable
for the conduction of further research.",This value still does not equal the common 99 % on MNIST.,"Considering that the diﬀerent components of the GR approaches
use convolutional layers, the CL performance of the GMR is quite acceptable.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8843,"Implementing the detection of forgetting
and proper reactions is considered a subject for further research, even though it merely shifts the
problem.","DCGMMs provides the required
functionalities (i.e., density estimation) to detect forgetting.","If the forgetting rate increases, more knowledge needs to be retrained and the more complex
the training becomes for new tasks.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8844,"Finally, newly emerging issues are addressed, which imply a need for further research.","Furthermore, a collection of contributions is
presented.","11.1 Summary and Conclusion

The ability to continuously accumulate knowledge has been a decisive factor for the evolution of the
human species.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8845,"Thus, GMR
oﬀers basic functionalities that can help perform further research on the fulﬁllment of remaining
application-oriented CL requirements.","Similar
CL capabilities can be achieved when comparing GMR with other replay processes.","In sum, the main contributions of this work include the following aspects:

 • A detailed description of the investigation protocol for CL scenarios, which includes application-
    oriented requirement extracted from an exemplary real-world scenario, is provided.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8846,"Another fundamental need for further research concerns the novel deep learning
model DCGMM and its application in various scenarios.","In this context, the assessment of possible application scenarios for diﬀerent models
seems interesting.","Furthermore, the transfer of DCGMMs to
other contexts such as reinforcement learning or the processing of sequences can constitute the focus
of future research.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8847,"In the following, some open issues and promising ideas for further research are presented.","Furthermore, the transfer of DCGMMs to
other contexts such as reinforcement learning or the processing of sequences can constitute the focus
of future research.","They are
dedicated to the three major ﬁndings of this work:

 • Investigation protocol and study:

     – Adding further real-world requirements related to a changing data distribution.",2022-07-12 10:13:33+00:00,Continual Learning with Deep Learning Methods in an Application-Oriented Context,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Benedikt Pfülb')],"Abstract knowledge is deeply grounded in many computer-based applications. An
important research area of Artificial Intelligence (AI) deals with the
automatic derivation of knowledge from data. Machine learning offers the
according algorithms. One area of research focuses on the development of
biologically inspired learning algorithms. The respective machine learning
methods are based on neurological concepts so that they can systematically
derive knowledge from data and store it. One type of machine learning
algorithms that can be categorized as ""deep learning"" model is referred to as
Deep Neural Networks (DNNs). DNNs consist of multiple artificial neurons
arranged in layers that are trained by using the backpropagation algorithm.
These deep learning methods exhibit amazing capabilities for inferring and
storing complex knowledge from high-dimensional data. However, DNNs are
affected by a problem that prevents new knowledge from being added to an
existing base. The ability to continuously accumulate knowledge is an important
factor that contributed to evolution and is therefore a prerequisite for the
development of strong AIs. The so-called ""catastrophic forgetting"" (CF) effect
causes DNNs to immediately loose already derived knowledge after a few training
iterations on a new data distribution. Only an energetically expensive
retraining with the joint data distribution of past and new data enables the
abstraction of the entire new set of knowledge. In order to counteract the
effect, various techniques have been and are still being developed with the
goal to mitigate or even solve the CF problem. These published CF avoidance
studies usually imply the effectiveness of their approaches for various
continual learning tasks. This dissertation is set in the context of continual
machine learning with deep learning methods. The first part deals with the
development of an ..."
8848,"Effectiveness of domain-decomposition on practical industry problems motivated us to further study it’s
effects on NS-PINNs.","A single-PINN proved very difﬁcult to capture the dynamics of the APH, hence the authors
used domain decomposition to split the domain into Gas-section, and two air-sections and then employed PINNs on
each domain.","In this case study, we try to answer a two-step question: 1) On a smaller simpliﬁed domain, can
Symbolic-PINN identify the correct functional operators to approximate and generalize better, as opposed to a large
domain?",2022-07-11 16:04:20+00:00,On NeuroSymbolic Solutions for PDEs,cs.LG,"['cs.LG', 'cs.AI', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Ritam Majumdar'), arxiv.Result.Author('Vishal Jadhav'), arxiv.Result.Author('Anirudh Deodhar'), arxiv.Result.Author('Shirish Karande'), arxiv.Result.Author('Lovekesh Vig')]","Physics Informed Neural Networks (PINNs) have gained immense popularity as an
alternate method for numerically solving PDEs. Despite their empirical success
we are still building an understanding of the convergence properties of
training on such constraints with gradient descent. It is known that, in the
absence of an explicit inductive bias, Neural Networks can struggle to learn or
approximate even simple and well known functions in a sample efficient manner.
Thus the numerical approximation induced from few collocation points may not
generalize over the entire domain. Meanwhile, a symbolic form can exhibit good
generalization, with interpretability as a useful byproduct. However, symbolic
approximations can struggle to simultaneously be concise and accurate.
Therefore in this work we explore a NeuroSymbolic approach to approximate the
solution for PDEs. We observe that our approach work for several simple cases.
We illustrate the efficacy of our approach on Navier Stokes: Kovasznay flow
where there are multiple physical quantities of interest governed with
non-linear coupled PDE system. Domain splitting is now becoming a popular trick
to help PINNs approximate complex functions. We observe that a NeuroSymbolic
approach can help such complex functions as well. We demonstrate
Domain-splitting assisted NeuroSymbolic approach on a temporally varying
two-dimensional Burger's equation. Finally we consider the scenario where PINNs
have to be solved for parameterized PDEs, for changing Initial-Boundary
Conditions and changes in the coefficient of the PDEs. Hypernetworks have shown
to hold promise to overcome these challenges. We show that one can design
Hyper-NeuroSymbolic Networks which can combine the benefits of speed and
increased accuracy. We observe that that the NeuroSymbolic approximations are
consistently 1-2 order of magnitude better than just the neural or symbolic
approximations."
8853,"An interesting
6 CONCLUSION                                                                                                                                avenue for further research would be to develop methods to detect
                                                                                                                                            and characterize these bias patterns without prior knowledge.","We have proposed a data bias taxonomy, and studied several
                                                                                                                                            biases by injecting them synthetically into real data.","The underlying causes for algorithmic unfairness in prediction
tasks remain elusive.",2022-07-13 15:18:30+00:00,Understanding Unfairness in Fraud Detection through Model and Data Bias Interactions,cs.LG,"['cs.LG', 'cs.CY', 'q-fin.ST']","[arxiv.Result.Author('José Pombal'), arxiv.Result.Author('André F. Cruz'), arxiv.Result.Author('João Bravo'), arxiv.Result.Author('Pedro Saleiro'), arxiv.Result.Author('Mário A. T. Figueiredo'), arxiv.Result.Author('Pedro Bizarro')]","In recent years, machine learning algorithms have become ubiquitous in a
multitude of high-stakes decision-making applications. The unparalleled ability
of machine learning algorithms to learn patterns from data also enables them to
incorporate biases embedded within. A biased model can then make decisions that
disproportionately harm certain groups in society -- limiting their access to
financial services, for example. The awareness of this problem has given rise
to the field of Fair ML, which focuses on studying, measuring, and mitigating
unfairness in algorithmic prediction, with respect to a set of protected groups
(e.g., race or gender). However, the underlying causes for algorithmic
unfairness still remain elusive, with researchers divided between blaming
either the ML algorithms or the data they are trained on. In this work, we
maintain that algorithmic unfairness stems from interactions between models and
biases in the data, rather than from isolated contributions of either of them.
To this end, we propose a taxonomy to characterize data bias and we study a set
of hypotheses regarding the fairness-accuracy trade-offs that fairness-blind ML
algorithms exhibit under different data bias settings. On our real-world
account-opening fraud use case, we find that each setting entails specific
trade-offs, affecting fairness in expected value and variance -- the latter
often going unnoticed. Moreover, we show how algorithms compare differently in
terms of accuracy and fairness, depending on the biases affecting the data.
Finally, we note that under specific data bias conditions, simple
pre-processing interventions can successfully balance group-wise error rates,
while the same techniques fail in more complex settings."
8875,"We give evidence that standard DNNs trained to
interpolation exhibit tempered over tting, not benign over tting, motivating the further study of tempered over tting
in the pursuit of understanding modern machine learning methods.","In Section 4, we empirically study over tting for DNNs.","We additionally study the time dynamics of
over tting, and the e ect of early-stopping.",2022-07-14 00:23:01+00:00,"Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting",cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Neil Mallinar'), arxiv.Result.Author('James B. Simon'), arxiv.Result.Author('Amirhesam Abedsoltan'), arxiv.Result.Author('Parthe Pandit'), arxiv.Result.Author('Mikhail Belkin'), arxiv.Result.Author('Preetum Nakkiran')]","The practical success of overparameterized neural networks has motivated the
recent scientific study of interpolating methods, which perfectly fit their
training data. Certain interpolating methods, including neural networks, can
fit noisy training data without catastrophically bad test performance, in
defiance of standard intuitions from statistical learning theory. Aiming to
explain this, a body of recent work has studied $\textit{benign overfitting}$,
a phenomenon where some interpolating methods approach Bayes optimality, even
in the presence of noise. In this work we argue that while benign overfitting
has been instructive and fruitful to study, many real interpolating methods
like neural networks $\textit{do not fit benignly}$: modest noise in the
training set causes nonzero (but non-infinite) excess risk at test time,
implying these models are neither benign nor catastrophic but rather fall in an
intermediate regime. We call this intermediate regime $\textit{tempered
overfitting}$, and we initiate its systematic study. We first explore this
phenomenon in the context of kernel (ridge) regression (KR) by obtaining
conditions on the ridge parameter and kernel eigenspectrum under which KR
exhibits each of the three behaviors. We find that kernels with powerlaw
spectra, including Laplace kernels and ReLU neural tangent kernels, exhibit
tempered overfitting. We then empirically study deep neural networks through
the lens of our taxonomy, and find that those trained to interpolation are
tempered, while those stopped early are benign. We hope our work leads to a
more refined understanding of overfitting in modern learning."
8876,"We give
evidence that standard DNNs trained to interpolation exhibit tempered over tting, not benign over tting, motivating
the further study of tempered over tting in the pursuit of understanding modern machine learning methods.","In Section 4, we empirically study over tting for DNNs.","We
additionally study the time dynamics of over tting, and the e ect of early-stopping.",2022-07-14 00:23:01+00:00,"Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting",cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Neil Mallinar'), arxiv.Result.Author('James B. Simon'), arxiv.Result.Author('Amirhesam Abedsoltan'), arxiv.Result.Author('Parthe Pandit'), arxiv.Result.Author('Mikhail Belkin'), arxiv.Result.Author('Preetum Nakkiran')]","The practical success of overparameterized neural networks has motivated the
recent scientific study of interpolating methods, which perfectly fit their
training data. Certain interpolating methods, including neural networks, can
fit noisy training data without catastrophically bad test performance, in
defiance of standard intuitions from statistical learning theory. Aiming to
explain this, a body of recent work has studied benign overfitting, a
phenomenon where some interpolating methods approach Bayes optimality, even in
the presence of noise. In this work we argue that while benign overfitting has
been instructive and fruitful to study, many real interpolating methods like
neural networks do not fit benignly: modest noise in the training set causes
nonzero (but non-infinite) excess risk at test time, implying these models are
neither benign nor catastrophic but rather fall in an intermediate regime. We
call this intermediate regime tempered overfitting, and we initiate its
systematic study. We first explore this phenomenon in the context of kernel
(ridge) regression (KR) by obtaining conditions on the ridge parameter and
kernel eigenspectrum under which KR exhibits each of the three behaviors. We
find that kernels with powerlaw spectra, including Laplace kernels and ReLU
neural tangent kernels, exhibit tempered overfitting. We then empirically study
deep neural networks through the lens of our taxonomy, and find that those
trained to interpolation are tempered, while those stopped early are benign. We
hope our work leads to a more refined understanding of overfitting in modern
learning."
8880,"We illustrate how DropNet can potentially re-        Source Code: To encourage further research
duce network size by up to 90% without any signiﬁcant            on iterative pruning techniques, the source code
loss of accuracy.","eratively drops nodes/ﬁlters and, hence, reduces network
complexity.","Also, there does not need to be a particu-     used for our experiments is publicly available at
lar initialization required when dropping up to 70% of the       https://github.com/tanchongmin/DropNet.",2022-07-14 03:42:11+00:00,DropNet: Reducing Neural Network Complexity via Iterative Pruning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('John Tan Chong Min'), arxiv.Result.Author('Mehul Motani')]","Modern deep neural networks require a significant amount of computing time
and power to train and deploy, which limits their usage on edge devices.
Inspired by the iterative weight pruning in the Lottery Ticket Hypothesis, we
propose DropNet, an iterative pruning method which prunes nodes/filters to
reduce network complexity. DropNet iteratively removes nodes/filters with the
lowest average post-activation value across all training samples. Empirically,
we show that DropNet is robust across diverse scenarios, including MLPs and
CNNs using the MNIST, CIFAR-10 and Tiny ImageNet datasets. We show that up to
90% of the nodes/filters can be removed without any significant loss of
accuracy. The final pruned network performs well even with reinitialization of
the weights and biases. DropNet also has similar accuracy to an oracle which
greedily removes nodes/filters one at a time to minimise training loss,
highlighting its effectiveness."
8892,"However, recent research developed additional few-shot learning models, which
require further research in the context of HITL systems and hybrid intelligence.","Third, our work draws upon the popular implementation of
PNN.",This work includes several directions for future work.,2022-07-14 11:45:43+00:00,Instance Selection Mechanisms for Human-in-the-Loop Systems in Few-Shot Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Johannes Jakubik'), arxiv.Result.Author('Benedikt Blumenstiel'), arxiv.Result.Author('Michael Vössing'), arxiv.Result.Author('Patrick Hemmer')]","Business analytics and machine learning have become essential success factors
for various industries - with the downside of cost-intensive gathering and
labeling of data. Few-shot learning addresses this challenge and reduces data
gathering and labeling costs by learning novel classes with very few labeled
data. In this paper, we design a human-in-the-loop (HITL) system for few-shot
learning and analyze an extensive range of mechanisms that can be used to
acquire human expert knowledge for instances that have an uncertain prediction
outcome. We show that the acquisition of human expert knowledge significantly
accelerates the few-shot model performance given a negligible labeling effort.
We validate our findings in various experiments on a benchmark dataset in
computer vision and real-world datasets. We further demonstrate the
cost-effectiveness of HITL systems for few-shot learning. Overall, our work
aims at supporting researchers and practitioners in effectively adapting
machine learning models to novel classes at reduced costs."
8919,"Through the proposed
     solutions we show that the identiﬁed issues are valid and we lay the grounds
     for further research

 – Creation of an adapted attention model that improves performance in both
     upscaling and downscaling scenarios

 – Proposition of a modiﬁed REINFORCE loss for sparse attention activation
     functions (α − entmax) and a modiﬁed VRP training scheme that reduces
     training time and increases performance

 – Proposition of an inference-stage data augmentation method that boosts
     both regular performance and generalization ability for most ML-based VRP
     construction methods.","Our contribution becomes as follows:

 – Identiﬁcation, classiﬁcation, and attempted resolution of the model and
     problem inconsistencies that hinder generalization.","2 Related Work

The attention model by kool et al.",2022-07-14 21:36:51+00:00,"Attention, Filling in The Gaps for Generalization in Routing Problems",cs.LG,['cs.LG'],"[arxiv.Result.Author('Ahmad Bdeir'), arxiv.Result.Author('Jonas K. Falkner'), arxiv.Result.Author('Lars Schmidt-Thieme')]","Machine Learning (ML) methods have become a useful tool for tackling vehicle
routing problems, either in combination with popular heuristics or as
standalone models. However, current methods suffer from poor generalization
when tackling problems of different sizes or different distributions. As a
result, ML in vehicle routing has witnessed an expansion phase with new
methodologies being created for particular problem instances that become
infeasible at larger problem sizes.
  This paper aims at encouraging the consolidation of the field through
understanding and improving current existing models, namely the attention model
by Kool et al. We identify two discrepancy categories for VRP generalization.
The first is based on the differences that are inherent to the problems
themselves, and the second relates to architectural weaknesses that limit the
model's ability to generalize. Our contribution becomes threefold: We first
target model discrepancies by adapting the Kool et al. method and its loss
function for Sparse Dynamic Attention based on the alpha-entmax activation. We
then target inherent differences through the use of a mixed instance training
method that has been shown to outperform single instance training in certain
scenarios. Finally, we introduce a framework for inference level data
augmentation that improves performance by leveraging the model's lack of
invariance to rotation and dilation changes."
8940,"• When the relational masking ratio is greater than 0.3, the

   1) Relational Masking Ratio: To further study the effect             model performance will decrease signiﬁcantly.",still used as HoGRN’s scoring function for analysis.,"The possible
of the key design: stochastic relational masking in high-order          reason is that when the relational masking ratio is too
reasoning component.",2022-07-14 10:16:56+00:00,Explainable Sparse Knowledge Graph Completion via High-order Graph Reasoning Network,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Weijian Chen'), arxiv.Result.Author('Yixin Cao'), arxiv.Result.Author('Fuli Feng'), arxiv.Result.Author('Xiangnan He'), arxiv.Result.Author('Yongdong Zhang')]","Knowledge Graphs (KGs) are becoming increasingly essential infrastructures in
many applications while suffering from incompleteness issues. The KG completion
task (KGC) automatically predicts missing facts based on an incomplete KG.
However, existing methods perform unsatisfactorily in real-world scenarios. On
the one hand, their performance will dramatically degrade along with the
increasing sparsity of KGs. On the other hand, the inference procedure for
prediction is an untrustworthy black box.
  This paper proposes a novel explainable model for sparse KGC, compositing
high-order reasoning into a graph convolutional network, namely HoGRN. It can
not only improve the generalization ability to mitigate the information
insufficiency issue but also provide interpretability while maintaining the
model's effectiveness and efficiency. There are two main components that are
seamlessly integrated for joint optimization. First, the high-order reasoning
component learns high-quality relation representations by capturing endogenous
correlation among relations. This can reflect logical rules to justify a
broader of missing facts. Second, the entity updating component leverages a
weight-free Graph Convolutional Network (GCN) to efficiently model KG
structures with interpretability. Unlike conventional methods, we conduct
entity aggregation and design composition-based attention in the relational
space without additional parameters. The lightweight design makes HoGRN better
suitable for sparse settings. For evaluation, we have conducted extensive
experiments-the results of HoGRN on several sparse KGs present impressive
improvements (9% MRR gain on average). Further ablation and case studies
demonstrate the effectiveness of the main components. Our codes will be
released upon acceptance."
8974,"In section 7, conclusions and problems for further study are given.","In section 6, three types of adversarial games are compared when the data set is ﬁnite.","3
2 Preliminaries

2.1 Adversarial training and robustness of DNN

Let C : X → Rm be a classiﬁcation DNN with m labels in Y = [m] = {1, .",2022-07-17 11:06:48+00:00,Achieve Optimal Adversarial Accuracy for Adversarial Deep Learning using Stackelberg Game,cs.LG,"['cs.LG', 'cs.AI', 'cs.GT']","[arxiv.Result.Author('Xiao-Shan Gao'), arxiv.Result.Author('Shuang Liu'), arxiv.Result.Author('Lijia Yu')]","Adversarial deep learning is to train robust DNNs against adversarial
attacks, which is one of the major research focuses of deep learning. Game
theory has been used to answer some of the basic questions about adversarial
deep learning such as the existence of a classifier with optimal robustness and
the existence of optimal adversarial samples for a given class of classifiers.
In most previous work, adversarial deep learning was formulated as a
simultaneous game and the strategy spaces are assumed to be certain probability
distributions in order for the Nash equilibrium to exist. But, this assumption
is not applicable to the practical situation. In this paper, we give answers to
these basic questions for the practical case where the classifiers are DNNs
with a given structure, by formulating the adversarial deep learning as
sequential games. The existence of Stackelberg equilibria for these games are
proved. Furthermore, it is shown that the equilibrium DNN has the largest
adversarial accuracy among all DNNs with the same structure, when
Carlini-Wagner's margin loss is used. Trade-off between robustness and accuracy
in adversarial deep learning is also studied from game theoretical aspect."
9009,"By further studying the properties of general aggregator,
                                                    i=1                   we can gain more insights into its role in machine learning
                                                                          algorithms.",Lavg(S) = n si.,"We identify a list of properties that desirable
        Note that there is a variant of the average aggrega-              aggregators tend to possess:

        tor,  which  omits  the  factor  1  and    then     becomes  the  1) Invariant to permutations: L(S) should not change
                                         n                                     as the elements of S undergone a permutation op-
                                                                               eration, i.e., L(S) = L( S ).",2022-07-18 17:21:01+00:00,Rank-based Decomposable Losses in Machine Learning: A Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shu Hu'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Siwei Lyu')]","Recent works have revealed an essential paradigm in designing loss functions
that differentiate individual losses vs. aggregate losses. The individual loss
measures the quality of the model on a sample, while the aggregate loss
combines individual losses/scores over each training sample. Both have a common
procedure that aggregates a set of individual values to a single numerical
value. The ranking order reflects the most fundamental relation among
individual values in designing losses. In addition, decomposability, in which a
loss can be decomposed into an ensemble of individual terms, becomes a
significant property of organizing losses/scores. This survey provides a
systematic and comprehensive review of rank-based decomposable losses in
machine learning. Specifically, we provide a new taxonomy of loss functions
that follows the perspectives of aggregate loss and individual loss. We
identify the aggregator to form such losses, which are examples of set
functions. We organize the rank-based decomposable losses into eight
categories. Following these categories, we review the literature on rank-based
aggregate losses and rank-based individual losses. We describe general formulas
for these losses and connect them with existing research topics. We also
suggest future research directions spanning unexplored, remaining, and emerging
issues in rank-based decomposable losses."
9010,"We note that a promising avenue for further research would be to ﬁnd
other ways to break rotation invariance which might be less computationally costly than embeddings.","The fact that very different types of embeddings seem to improve
performance suggests that the sheer presence of an embedding which breaks the invariance is a key
part of these improvements.","6 Discussion and conclusion

Limitation Our study leaves open questions for future work: which other inductive biases of
tree-based models explain their performances on tabular data?",2022-07-18 08:36:08+00:00,Why do tree-based models still outperform deep learning on tabular data?,cs.LG,"['cs.LG', 'cs.AI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Léo Grinsztajn'), arxiv.Result.Author('Edouard Oyallon'), arxiv.Result.Author('Gaël Varoquaux')]","While deep learning has enabled tremendous progress on text and image
datasets, its superiority on tabular data is not clear. We contribute extensive
benchmarks of standard and novel deep learning methods as well as tree-based
models such as XGBoost and Random Forests, across a large number of datasets
and hyperparameter combinations. We define a standard set of 45 datasets from
varied domains with clear characteristics of tabular data and a benchmarking
methodology accounting for both fitting models and finding good
hyperparameters. Results show that tree-based models remain state-of-the-art on
medium-sized data ($\sim$10K samples) even without accounting for their
superior speed. To understand this gap, we conduct an empirical investigation
into the differing inductive biases of tree-based models and Neural Networks
(NNs). This leads to a series of challenges which should guide researchers
aiming to build tabular-specific NNs: 1. be robust to uninformative features,
2. preserve the orientation of the data, and 3. be able to easily learn
irregular functions. To stimulate research on tabular architectures, we
contribute a standard benchmark and raw data for baselines: every point of a 20
000 compute hours hyperparameter search for each learner."
9011,"However, the implementation of diﬀerent data augmentation methods shows a
promising path to improve the performance of simple classiﬁers (and/or recent ensemble architectures)
and requires further research.","The study of data augmentation mechanisms in ensembles of simple classiﬁers have achieved state-of-
the-art performance not only 10 years ago [16, 17], but also when compared to modern deep learning
architectures [18, 19, 46].",A research application that was not frequently found in the literature was small dataset augmentation.,2022-07-18 11:38:32+00:00,Research Trends and Applications of Data Augmentation Algorithms,cs.LG,['cs.LG'],"[arxiv.Result.Author('Joao Fonseca'), arxiv.Result.Author('Fernando Bacao')]","In the Machine Learning research community, there is a consensus regarding
the relationship between model complexity and the required amount of data and
computation power. In real world applications, these computational requirements
are not always available, motivating research on regularization methods. In
addition, current and past research have shown that simpler classification
algorithms can reach state-of-the-art performance on computer vision tasks
given a robust method to artificially augment the training dataset. Because of
this, data augmentation techniques became a popular research topic in recent
years. However, existing data augmentation methods are generally less
transferable than other regularization methods. In this paper we identify the
main areas of application of data augmentation algorithms, the types of
algorithms used, significant research trends, their progression over time and
research gaps in data augmentation literature. To do this, the related
literature was collected through the Scopus database. Its analysis was done
following network science, text mining and exploratory analysis approaches. We
expect readers to understand the potential of data augmentation, as well as
identify future research directions and open questions within data augmentation
research."
9012,"Other less popular topics, such as small data augmentation, appear to have a relevant practical impor-
tance and require further research.","Although,
recent research shows that the same can be done for simpler classiﬁer conﬁgurations in order to achieve
a classiﬁcation performance comparable to that of state-of-the-art deep learning, which require further
conﬁrmation, as well as the development of less computational intensive data augmentation methods.","In addition, other limitations of data augmentation algorithms should
be addressed.",2022-07-18 11:38:32+00:00,Research Trends and Applications of Data Augmentation Algorithms,cs.LG,['cs.LG'],"[arxiv.Result.Author('Joao Fonseca'), arxiv.Result.Author('Fernando Bacao')]","In the Machine Learning research community, there is a consensus regarding
the relationship between model complexity and the required amount of data and
computation power. In real world applications, these computational requirements
are not always available, motivating research on regularization methods. In
addition, current and past research have shown that simpler classification
algorithms can reach state-of-the-art performance on computer vision tasks
given a robust method to artificially augment the training dataset. Because of
this, data augmentation techniques became a popular research topic in recent
years. However, existing data augmentation methods are generally less
transferable than other regularization methods. In this paper we identify the
main areas of application of data augmentation algorithms, the types of
algorithms used, significant research trends, their progression over time and
research gaps in data augmentation literature. To do this, the related
literature was collected through the Scopus database. Its analysis was done
following network science, text mining and exploratory analysis approaches. We
expect readers to understand the potential of data augmentation, as well as
identify future research directions and open questions within data augmentation
research."
9013,"However, the implementation of diﬀerent data augmentation methods shows a
promising path to improve the performance of simple classiﬁers (and/or recent ensemble architectures)
and requires further research.","The study of data augmentation mechanisms in ensembles of simple classiﬁers have achieved state-of-
the-art performance not only 10 years ago [16, 17], but also when compared to modern deep learning
architectures [18, 19, 46].",A research application that was not frequently found in the literature was small dataset augmentation.,2022-07-18 11:38:32+00:00,Research Trends and Applications of Data Augmentation Algorithms,cs.LG,['cs.LG'],"[arxiv.Result.Author('Joao Fonseca'), arxiv.Result.Author('Fernando Bacao')]","In the Machine Learning research community, there is a consensus regarding
the relationship between model complexity and the required amount of data and
computation power. In real world applications, these computational requirements
are not always available, motivating research on regularization methods. In
addition, current and past research have shown that simpler classification
algorithms can reach state-of-the-art performance on computer vision tasks
given a robust method to artificially augment the training dataset. Because of
this, data augmentation techniques became a popular research topic in recent
years. However, existing data augmentation methods are generally less
transferable than other regularization methods. In this paper we identify the
main areas of application of data augmentation algorithms, the types of
algorithms used, significant research trends, their progression over time and
research gaps in data augmentation literature. To do this, the related
literature was collected through the Scopus database. Its analysis was done
following network science, text mining and exploratory analysis approaches. We
expect readers to understand the potential of data augmentation, as well as
identify future research directions and open questions within data augmentation
research."
9014,"Other less popular topics, such as small data augmentation, appear to have a relevant practical impor-
tance and require further research.","Although,
recent research shows that the same can be done for simpler classiﬁer conﬁgurations in order to achieve
a classiﬁcation performance comparable to that of state-of-the-art deep learning, which require further
conﬁrmation, as well as the development of less computational intensive data augmentation methods.","In addition, other limitations of data augmentation algorithms should
be addressed.",2022-07-18 11:38:32+00:00,Research Trends and Applications of Data Augmentation Algorithms,cs.LG,['cs.LG'],"[arxiv.Result.Author('Joao Fonseca'), arxiv.Result.Author('Fernando Bacao')]","In the Machine Learning research community, there is a consensus regarding
the relationship between model complexity and the required amount of data and
computation power. In real world applications, these computational requirements
are not always available, motivating research on regularization methods. In
addition, current and past research have shown that simpler classification
algorithms can reach state-of-the-art performance on computer vision tasks
given a robust method to artificially augment the training dataset. Because of
this, data augmentation techniques became a popular research topic in recent
years. However, existing data augmentation methods are generally less
transferable than other regularization methods. In this paper we identify the
main areas of application of data augmentation algorithms, the types of
algorithms used, significant research trends, their progression over time and
research gaps in data augmentation literature. To do this, the related
literature was collected through the Scopus database. Its analysis was done
following network science, text mining and exploratory analysis approaches. We
expect readers to understand the potential of data augmentation, as well as
identify future research directions and open questions within data augmentation
research."
9015,"Mochales
and Moens (2011) have used argumentation mining to structure better legal
arguments, capturing main issues and evidence of a given corpus, also stating
that the method needs further research to automatically acquire the neces-
sary background knowledge and more speciﬁcally common sense and world
knowledge.","In this work, the author also compares whether or not legal experts and people
with a non-legal background agree in their judgments, discovering that domain
experts and non-domain experts might evaluate topics diﬀerently.","Le et al (2015) have used index extraction using structural information of
sentences for Japanese legal documents, assigning each token with a weight,
which is a statistical score to indicate its importance.",2022-07-18 16:24:34+00:00,Using attention methods to predict judicial outcomes,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CY', '68T50', 'I.2.7; J.5']","[arxiv.Result.Author('Vithor Gomes Ferreira Bertalan'), arxiv.Result.Author('Evandro Eduardo Seron Ruiz')]","Legal Judgment Prediction is one of the most acclaimed fields for the
combined area of NLP, AI, and Law. By legal prediction we mean an intelligent
systems capable to predict specific judicial characteristics, such as judicial
outcome, a judicial class, predict an specific case. In this research, we have
used AI classifiers to predict judicial outcomes in the Brazilian legal system.
For this purpose, we developed a text crawler to extract data from the official
Brazilian electronic legal systems. These texts formed a dataset of
second-degree murder and active corruption cases. We applied different
classifiers, such as Support Vector Machines and Neural Networks, to predict
judicial outcomes by analyzing textual features from the dataset. Our research
showed that Regression Trees, Gated Recurring Units and Hierarchical Attention
Networks presented higher metrics for different subsets. As a final goal, we
explored the weights of one of the algorithms, the Hierarchical Attention
Networks, to find a sample of the most important words used to absolve or
convict defendants."
9016,"Finding that CART works better in
Portuguese, Italian, and English than other methods is a remarkable discovery
that deserves further research.","Languages
have many diﬀerent characteristics, particularly those with diﬀerent families,
such as romantic or germanic languages.","As another possible expansion in our scope, we plan to expand our legal
text dataset, increasing the number of homicide and corruption cases and ap-
proaching other judicial topics.",2022-07-18 16:24:34+00:00,Using attention methods to predict judicial outcomes,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CY', '68T50', 'I.2.7; J.5']","[arxiv.Result.Author('Vithor Gomes Ferreira Bertalan'), arxiv.Result.Author('Evandro Eduardo Seron Ruiz')]","Legal Judgment Prediction is one of the most acclaimed fields for the
combined area of NLP, AI, and Law. By legal prediction we mean an intelligent
systems capable to predict specific judicial characteristics, such as judicial
outcome, a judicial class, predict an specific case. In this research, we have
used AI classifiers to predict judicial outcomes in the Brazilian legal system.
For this purpose, we developed a text crawler to extract data from the official
Brazilian electronic legal systems. These texts formed a dataset of
second-degree murder and active corruption cases. We applied different
classifiers, such as Support Vector Machines and Neural Networks, to predict
judicial outcomes by analyzing textual features from the dataset. Our research
showed that Regression Trees, Gated Recurring Units and Hierarchical Attention
Networks presented higher metrics for different subsets. As a final goal, we
explored the weights of one of the algorithms, the Hierarchical Attention
Networks, to find a sample of the most important words used to absolve or
convict defendants."
9043,This is another direction for further research.,"Therefore, it is interesting to modify TNW-CATE to take
into account the violation.","Another interesting direction
for research is to incorporate robust procedures and imprecise statistical models to deal with small
datasets into TNW-CATE.",2022-07-19 09:21:01+00:00,Heterogeneous Treatment Effect with Trained Kernels of the Nadaraya-Watson Regression,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Andrei V. Konstantinov'), arxiv.Result.Author('Stanislav R. Kirpichenko'), arxiv.Result.Author('Lev V. Utkin')]","A new method for estimating the conditional average treatment effect is
proposed in the paper. It is called TNW-CATE (the Trainable Nadaraya-Watson
regression for CATE) and based on the assumption that the number of controls is
rather large whereas the number of treatments is small. TNW-CATE uses the
Nadaraya-Watson regression for predicting outcomes of patients from the control
and treatment groups. The main idea behind TNW-CATE is to train kernels of the
Nadaraya-Watson regression by using a weight sharing neural network of a
specific form. The network is trained on controls, and it replaces standard
kernels with a set of neural subnetworks with shared parameters such that every
subnetwork implements the trainable kernel, but the whole network implements
the Nadaraya-Watson estimator. The network memorizes how the feature vectors
are located in the feature space. The proposed approach is similar to the
transfer learning when domains of source and target data are similar, but tasks
are different. Various numerical simulation experiments illustrate TNW-CATE and
compare it with the well-known T-learner, S-learner and X-learner for several
types of the control and treatment outcome functions. The code of proposed
algorithms implementing TNW-CATE is available in
https://github.com/Stasychbr/TNW-CATE."
9046,"(4) With these results, we revisit previous studies and recommendations,
reinterpreting their conclusions, resolving some contradictions, and suggesting critical areas for
further research.","(3) In general, in-distribution
classiﬁcation error (accuracy) is the best predictor of OOD accuracy, but other secondary metrics can
provide additional insights.","2 Experimental setup

We follow the modern workﬂow of applications of computer vision to (long-tail) downstream
tasks from existing pre-trained backbones.",2022-07-19 12:52:33+00:00,Assaying Out-Of-Distribution Generalization in Transfer Learning,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Florian Wenzel'), arxiv.Result.Author('Andrea Dittadi'), arxiv.Result.Author('Peter Vincent Gehler'), arxiv.Result.Author('Carl-Johann Simon-Gabriel'), arxiv.Result.Author('Max Horn'), arxiv.Result.Author('Dominik Zietlow'), arxiv.Result.Author('David Kernert'), arxiv.Result.Author('Chris Russell'), arxiv.Result.Author('Thomas Brox'), arxiv.Result.Author('Bernt Schiele'), arxiv.Result.Author('Bernhard Schölkopf'), arxiv.Result.Author('Francesco Locatello')]","Since out-of-distribution generalization is a generally ill-posed problem,
various proxy targets (e.g., calibration, adversarial robustness, algorithmic
corruptions, invariance across shifts) were studied across different research
programs resulting in different recommendations. While sharing the same
aspirational goal, these approaches have never been tested under the same
experimental conditions on real data. In this paper, we take a unified view of
previous work, highlighting message discrepancies that we address empirically,
and providing recommendations on how to measure the robustness of a model and
how to improve it. To this end, we collect 172 publicly available dataset pairs
for training and out-of-distribution evaluation of accuracy, calibration error,
adversarial attacks, environment invariance, and synthetic corruptions. We
fine-tune over 31k networks, from nine different architectures in the many- and
few-shot setting. Our findings confirm that in- and out-of-distribution
accuracies tend to increase jointly, but show that their relation is largely
dataset-dependent, and in general more nuanced and more complex than posited by
previous, smaller scale studies."
9047,"In light of these results, we suggest three critical areas for further research.","As such, they should be a key focus of any practitioner who
worries about distribution shifts at test time.","(1) Creating synthetic
interventional distributions is an appealing alternative to hand-crafted augmentations and corruptions
to both evaluate and improve robustness.",2022-07-19 12:52:33+00:00,Assaying Out-Of-Distribution Generalization in Transfer Learning,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Florian Wenzel'), arxiv.Result.Author('Andrea Dittadi'), arxiv.Result.Author('Peter Vincent Gehler'), arxiv.Result.Author('Carl-Johann Simon-Gabriel'), arxiv.Result.Author('Max Horn'), arxiv.Result.Author('Dominik Zietlow'), arxiv.Result.Author('David Kernert'), arxiv.Result.Author('Chris Russell'), arxiv.Result.Author('Thomas Brox'), arxiv.Result.Author('Bernt Schiele'), arxiv.Result.Author('Bernhard Schölkopf'), arxiv.Result.Author('Francesco Locatello')]","Since out-of-distribution generalization is a generally ill-posed problem,
various proxy targets (e.g., calibration, adversarial robustness, algorithmic
corruptions, invariance across shifts) were studied across different research
programs resulting in different recommendations. While sharing the same
aspirational goal, these approaches have never been tested under the same
experimental conditions on real data. In this paper, we take a unified view of
previous work, highlighting message discrepancies that we address empirically,
and providing recommendations on how to measure the robustness of a model and
how to improve it. To this end, we collect 172 publicly available dataset pairs
for training and out-of-distribution evaluation of accuracy, calibration error,
adversarial attacks, environment invariance, and synthetic corruptions. We
fine-tune over 31k networks, from nine different architectures in the many- and
few-shot setting. Our findings confirm that in- and out-of-distribution
accuracies tend to increase jointly, but show that their relation is largely
dataset-dependent, and in general more nuanced and more complex than posited by
previous, smaller scale studies."
9048,"(4) With these results, we revisit previous studies and recommendations,
reinterpreting their conclusions, resolving some contradictions, and suggesting critical areas for
further research.","(3) In general, in-distribution
classiﬁcation error (accuracy) is the best predictor of OOD accuracy, but other secondary metrics can
provide additional insights.","2 Experimental setup

We follow the modern workﬂow of applications of computer vision to (long-tail) downstream tasks
from existing pre-trained backbones.",2022-07-19 12:52:33+00:00,Assaying Out-Of-Distribution Generalization in Transfer Learning,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Florian Wenzel'), arxiv.Result.Author('Andrea Dittadi'), arxiv.Result.Author('Peter Vincent Gehler'), arxiv.Result.Author('Carl-Johann Simon-Gabriel'), arxiv.Result.Author('Max Horn'), arxiv.Result.Author('Dominik Zietlow'), arxiv.Result.Author('David Kernert'), arxiv.Result.Author('Chris Russell'), arxiv.Result.Author('Thomas Brox'), arxiv.Result.Author('Bernt Schiele'), arxiv.Result.Author('Bernhard Schölkopf'), arxiv.Result.Author('Francesco Locatello')]","Since out-of-distribution generalization is a generally ill-posed problem,
various proxy targets (e.g., calibration, adversarial robustness, algorithmic
corruptions, invariance across shifts) were studied across different research
programs resulting in different recommendations. While sharing the same
aspirational goal, these approaches have never been tested under the same
experimental conditions on real data. In this paper, we take a unified view of
previous work, highlighting message discrepancies that we address empirically,
and providing recommendations on how to measure the robustness of a model and
how to improve it. To this end, we collect 172 publicly available dataset pairs
for training and out-of-distribution evaluation of accuracy, calibration error,
adversarial attacks, environment invariance, and synthetic corruptions. We
fine-tune over 31k networks, from nine different architectures in the many- and
few-shot setting. Our findings confirm that in- and out-of-distribution
accuracies tend to increase jointly, but show that their relation is largely
dataset-dependent, and in general more nuanced and more complex than posited by
previous, smaller scale studies."
9049,"In light of these results, we suggest three critical areas for further research.","As
such, they should be a key focus of any practitioner who worries about distribution shifts at test time.","(1) Creating synthetic
interventional distributions is an appealing alternative to hand-crafted augmentations and corruptions
to both evaluate and improve robustness.",2022-07-19 12:52:33+00:00,Assaying Out-Of-Distribution Generalization in Transfer Learning,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Florian Wenzel'), arxiv.Result.Author('Andrea Dittadi'), arxiv.Result.Author('Peter Vincent Gehler'), arxiv.Result.Author('Carl-Johann Simon-Gabriel'), arxiv.Result.Author('Max Horn'), arxiv.Result.Author('Dominik Zietlow'), arxiv.Result.Author('David Kernert'), arxiv.Result.Author('Chris Russell'), arxiv.Result.Author('Thomas Brox'), arxiv.Result.Author('Bernt Schiele'), arxiv.Result.Author('Bernhard Schölkopf'), arxiv.Result.Author('Francesco Locatello')]","Since out-of-distribution generalization is a generally ill-posed problem,
various proxy targets (e.g., calibration, adversarial robustness, algorithmic
corruptions, invariance across shifts) were studied across different research
programs resulting in different recommendations. While sharing the same
aspirational goal, these approaches have never been tested under the same
experimental conditions on real data. In this paper, we take a unified view of
previous work, highlighting message discrepancies that we address empirically,
and providing recommendations on how to measure the robustness of a model and
how to improve it. To this end, we collect 172 publicly available dataset pairs
for training and out-of-distribution evaluation of accuracy, calibration error,
adversarial attacks, environment invariance, and synthetic corruptions. We
fine-tune over 31k networks, from nine different architectures in the many- and
few-shot setting. Our findings confirm that in- and out-of-distribution
accuracies tend to increase jointly, but show that their relation is largely
dataset-dependent, and in general more nuanced and more complex than posited by
previous, smaller scale studies."
9083,"identify some open problems for further study and discuss
                                                                     possible directions for future research on SL-FL combination
  Split Learning (SL) is another collaborative learning ap-          in Section VI.","However, FL requires individual user devices to have         the state-of-the-art technologies for combining split learning
sufficient resources for training a full ML model, which is          with federated learning is presented in Section IV and privacy
not realistic to resource-constrained IoT devices especially for     protection for split learning is discussed in Section V. We
training complex ML models such as deep neural networks.",We draw conclusions in Section VII.,2022-07-20 01:31:22+00:00,Combined Federated and Split Learning in Edge Computing for Ubiquitous Intelligence in Internet of Things: State of the Art and Future Directions,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Qiang Duan'), arxiv.Result.Author('Shijing Hu'), arxiv.Result.Author('Ruijun Deng'), arxiv.Result.Author('Zhihui Lu')]","Federated learning (FL) and split learning (SL) are two emerging
collaborative learning methods that may greatly facilitate ubiquitous
intelligence in Internet of Things (IoT). Federated learning enables machine
learning (ML) models locally trained using private data to be aggregated into a
global model. Split learning allows different portions of an ML model to be
collaboratively trained on different workers in a learning framework. Federated
learning and split learning, each has unique advantages and respective
limitations, may complement each other toward ubiquitous intelligence in IoT.
Therefore, combination of federated learning and split learning recently became
an active research area attracting extensive interest. In this article, we
review the latest developments in federated learning and split learning and
present a survey on the state-of-the-art technologies for combining these two
learning methods in an edge computing-based IoT environment. We also identify
some open problems and discuss possible directions for future research in this
area with a hope to further arouse the research community's interest in this
emerging field."
9084,"In this section, we attempt to
task then the gradient values received from the server should       identify some open problems for further study and discuss
be noticeably different for the fake batches and regular batches.","The main idea        is still on an early stage with many open issues that still need
of SplitGuard is that if the client model is learning its intended  to be thoroughly investigated.",possible directions for future research in this emerging field.,2022-07-20 01:31:22+00:00,Combined Federated and Split Learning in Edge Computing for Ubiquitous Intelligence in Internet of Things: State of the Art and Future Directions,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Qiang Duan'), arxiv.Result.Author('Shijing Hu'), arxiv.Result.Author('Ruijun Deng'), arxiv.Result.Author('Zhihui Lu')]","Federated learning (FL) and split learning (SL) are two emerging
collaborative learning methods that may greatly facilitate ubiquitous
intelligence in Internet of Things (IoT). Federated learning enables machine
learning (ML) models locally trained using private data to be aggregated into a
global model. Split learning allows different portions of an ML model to be
collaboratively trained on different workers in a learning framework. Federated
learning and split learning, each has unique advantages and respective
limitations, may complement each other toward ubiquitous intelligence in IoT.
Therefore, combination of federated learning and split learning recently became
an active research area attracting extensive interest. In this article, we
review the latest developments in federated learning and split learning and
present a survey on the state-of-the-art technologies for combining these two
learning methods in an edge computing-based IoT environment. We also identify
some open problems and discuss possible directions for future research in this
area with a hope to further arouse the research community's interest in this
emerging field."
9085,"Some open issues for further study include
a standard benchmark) for comparative performance study for        design and deployment of vertical SL-FL frameworks that
both existing and emerging SL-FL frameworks in various de-         match the underlying edge platform, analysis and selection of
ployment settings, which plays a key role in choosing the most     the mechanisms for aggregating vertically split client models,
appropriate learning framework for a given environment where       evaluation of performance, costs, and scalability of vertical
the learning tasks are expected to be deployed.","It is critical to come up with general methods (e.g.,  fully investigated.","Performance        SL-FL in an IoT environment, and privacy and security issues
evaluation for hybrid SL-FL upon an edge computing platform        related to vertical SL-FL frameworks.",2022-07-20 01:31:22+00:00,Combined Federated and Split Learning in Edge Computing for Ubiquitous Intelligence in Internet of Things: State of the Art and Future Directions,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Qiang Duan'), arxiv.Result.Author('Shijing Hu'), arxiv.Result.Author('Ruijun Deng'), arxiv.Result.Author('Zhihui Lu')]","Federated learning (FL) and split learning (SL) are two emerging
collaborative learning methods that may greatly facilitate ubiquitous
intelligence in Internet of Things (IoT). Federated learning enables machine
learning (ML) models locally trained using private data to be aggregated into a
global model. Split learning allows different portions of an ML model to be
collaboratively trained on different workers in a learning framework. Federated
learning and split learning, each has unique advantages and respective
limitations, may complement each other toward ubiquitous intelligence in IoT.
Therefore, combination of federated learning and split learning recently became
an active research area attracting extensive interest. In this article, we
review the latest developments in federated learning and split learning and
present a survey on the state-of-the-art technologies for combining these two
learning methods in an edge computing-based IoT environment. We also identify
some open problems and discuss possible directions for future research in this
area with a hope to further arouse the research community's interest in this
emerging field."
9091,Our code will be released for further study.,"The authors in [74]             • We conduct intensive experiments to validate our re-
propose MATCHA to decompose the set of possible com-                    sults.",munications into pairs of clients.,2022-07-20 05:22:26+00:00,Multigraph Topology Design for Cross-Silo Federated Learning,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Binh X. Nguyen'), arxiv.Result.Author('Tuong Do'), arxiv.Result.Author('Hien Nguyen'), arxiv.Result.Author('Vuong Pham'), arxiv.Result.Author('Toan Tran'), arxiv.Result.Author('Erman Tjiputra'), arxiv.Result.Author('Quang Tran'), arxiv.Result.Author('Anh Nguyen')]","Cross-silo federated learning utilizes a few hundred reliable data silos with
high-speed access links to jointly train a model. While this approach becomes a
popular setting in federated learning, designing a robust topology to reduce
the training time is still an open problem. In this paper, we present a new
multigraph topology for cross-silo federated learning. We first construct the
multigraph using the overlay graph. We then parse this multigraph into
different simple graphs with isolated nodes. The existence of isolated nodes
allows us to perform model aggregation without waiting for other nodes, hence
reducing the training time. We further propose a new distributed learning
algorithm to use with our multigraph topology. The intensive experiments on
public datasets show that our proposed method significantly reduces the
training time compared with recent state-of-the-art topologies while ensuring
convergence and maintaining the model's accuracy."
9092,Our code will be released for further study.,"The authors in [74]             • We conduct intensive experiments to validate our re-
propose MATCHA to decompose the set of possible com-                    sults.",munications into pairs of clients.,2022-07-20 05:22:26+00:00,Multigraph Topology Design for Cross-Silo Federated Learning,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Binh X. Nguyen'), arxiv.Result.Author('Tuong Do'), arxiv.Result.Author('Hien Nguyen'), arxiv.Result.Author('Vuong Pham'), arxiv.Result.Author('Toan Tran'), arxiv.Result.Author('Erman Tjiputra'), arxiv.Result.Author('Quang Tran'), arxiv.Result.Author('Anh Nguyen')]","Cross-silo federated learning utilizes a few hundred reliable data silos with
high-speed access links to jointly train a model. While this approach becomes a
popular setting in federated learning, designing a robust topology to reduce
the training time is still an open problem. In this paper, we present a new
multigraph topology for cross-silo federated learning. We first construct the
multigraph using the overlay graph. We then parse this multigraph into
different simple graphs with isolated nodes. The existence of isolated nodes
allows us to perform model aggregation without waiting for other nodes, hence
reducing the training time. We further propose a new distributed learning
algorithm to use with our multigraph topology. The intensive experiments on
public datasets show that our proposed method significantly reduces the
training time compared with recent state-of-the-art topologies while ensuring
convergence and maintaining the model's accuracy."
9099,"While GANs produce state-
                                        facilitate further research of non-uniform diffusion models.","Our theoretical and experimental ﬁndings are ac-       cobian log-determinant that is computationally tractable),
                                        companied by an open source library MSDiff which can            thus limiting their expressivity.","of-the art quality samples, they don’t allow for likelihood

                                        1.",2022-07-20 09:59:28+00:00,Non-Uniform Diffusion Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Georgios Batzolis'), arxiv.Result.Author('Jan Stanczuk'), arxiv.Result.Author('Carola-Bibiane Schönlieb'), arxiv.Result.Author('Christian Etmann')]","Diffusion models have emerged as one of the most promising frameworks for
deep generative modeling. In this work, we explore the potential of non-uniform
diffusion models. We show that non-uniform diffusion leads to multi-scale
diffusion models which have similar structure to this of multi-scale
normalizing flows. We experimentally find that in the same or less training
time, the multi-scale diffusion model achieves better FID score than the
standard uniform diffusion model. More importantly, it generates samples $4.4$
times faster in $128\times 128$ resolution. The speed-up is expected to be
higher in higher resolutions where more scales are used. Moreover, we show that
non-uniform diffusion leads to a novel estimator for the conditional score
function which achieves on par performance with the state-of-the-art
conditional denoising estimator. Our theoretical and experimental findings are
accompanied by an open source library MSDiff which can facilitate further
research of non-uniform diffusion models."
9100,"We provide an open-source library MSDiff, to facil-
                                                                      itate further research on conditional and non-uniform
   In addition to achieving state-of-the art performance in           diffusion models.",6.,"1
both image generation and likelihood estimation, score-
based diffusion models don’t suffer from training instabili-    2.",2022-07-20 09:59:28+00:00,Non-Uniform Diffusion Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Georgios Batzolis'), arxiv.Result.Author('Jan Stanczuk'), arxiv.Result.Author('Carola-Bibiane Schönlieb'), arxiv.Result.Author('Christian Etmann')]","Diffusion models have emerged as one of the most promising frameworks for
deep generative modeling. In this work, we explore the potential of non-uniform
diffusion models. We show that non-uniform diffusion leads to multi-scale
diffusion models which have similar structure to this of multi-scale
normalizing flows. We experimentally find that in the same or less training
time, the multi-scale diffusion model achieves better FID score than the
standard uniform diffusion model. More importantly, it generates samples $4.4$
times faster in $128\times 128$ resolution. The speed-up is expected to be
higher in higher resolutions where more scales are used. Moreover, we show that
non-uniform diffusion leads to a novel estimator for the conditional score
function which achieves on par performance with the state-of-the-art
conditional denoising estimator. Our theoretical and experimental findings are
accompanied by an open source library MSDiff which can facilitate further
research of non-uniform diffusion models."
9107,"We believe that
                                                  our ﬁndings can provide helpful insights for further research on the multi-source
                                                  learning of EHRs.","To empirically demonstrate the efﬁcacy of our work, we conducted extensive
                                                  experiments using various datasets, model structures, and tasks.","Artifact Availability The source code, data, supplementary materials, and/or other artifacts have
                                        been made available at github.1

                                        1 Introduction

                                        Patient medical records are accumulated regularly in the form of Electronic Health Records (EHR),
                                        enabling quality treatment based on patients’ medical history.",2022-07-20 12:46:26+00:00,UniHPF : Universal Healthcare Predictive Framework with Zero Domain Knowledge,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kyunghoon Hur'), arxiv.Result.Author('Jungwoo Oh'), arxiv.Result.Author('Junu Kim'), arxiv.Result.Author('Min Jae Lee'), arxiv.Result.Author('Eunbyeol Choi'), arxiv.Result.Author('Jiyoun Kim'), arxiv.Result.Author('Seong-Eun Moon'), arxiv.Result.Author('Young-Hak Kim'), arxiv.Result.Author('Edward Choi')]","Despite the abundance of Electronic Healthcare Records (EHR), its
heterogeneity restricts the utilization of medical data in building predictive
models. To address this challenge, we propose Universal Healthcare Predictive
Framework (UniHPF), which requires no medical domain knowledge and minimal
pre-processing for multiple prediction tasks. Experimental results demonstrate
that UniHPF is capable of building large-scale EHR models that can process any
form of medical data from distinct EHR systems. Our framework significantly
outperforms baseline models in multi-source learning tasks, including transfer
and pooled learning, while also showing comparable results when trained on a
single medical dataset. To empirically demonstrate the efficacy of our work, we
conducted extensive experiments using various datasets, model structures, and
tasks. We believe that our findings can provide helpful insights for further
research on the multi-source learning of EHRs."
9108,"We believe that our ﬁndings can provide
    helpful insights for further research on the multi-source learning of EHR.","• To empirically demonstrate the efﬁcacy of our work, we conducted extensive experiments with
    various datasets, model structures, and prediction tasks.","2 Related work

Domain-knowledge-based predictive models.",2022-07-20 12:46:26+00:00,UniHPF : Universal Healthcare Predictive Framework with Zero Domain Knowledge,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kyunghoon Hur'), arxiv.Result.Author('Jungwoo Oh'), arxiv.Result.Author('Junu Kim'), arxiv.Result.Author('Min Jae Lee'), arxiv.Result.Author('Eunbyeol Choi'), arxiv.Result.Author('Jiyoun Kim'), arxiv.Result.Author('Seong-Eun Moon'), arxiv.Result.Author('Young-Hak Kim'), arxiv.Result.Author('Edward Choi')]","Despite the abundance of Electronic Healthcare Records (EHR), its
heterogeneity restricts the utilization of medical data in building predictive
models. To address this challenge, we propose Universal Healthcare Predictive
Framework (UniHPF), which requires no medical domain knowledge and minimal
pre-processing for multiple prediction tasks. Experimental results demonstrate
that UniHPF is capable of building large-scale EHR models that can process any
form of medical data from distinct EHR systems. Our framework significantly
outperforms baseline models in multi-source learning tasks, including transfer
and pooled learning, while also showing comparable results when trained on a
single medical dataset. To empirically demonstrate the efficacy of our work, we
conducted extensive experiments using various datasets, model structures, and
tasks. We believe that our findings can provide helpful insights for further
research on the multi-source learning of EHRs."
9109,"We believe that
                                                  our ﬁndings can provide helpful insights for further research on the multi-source
                                                  learning of EHRs.","To empirically demonstrate the efﬁcacy of our work, we conducted extensive
                                                  experiments using various datasets, model structures, and tasks.","Artifact Availability The source code, data, supplementary materials, and/or other artifacts have
                                        been made available at github.1

                                        1 Introduction

                                        Patient medical records are accumulated regularly in the form of Electronic Health Records (EHR),
                                        enabling quality treatment based on patients’ medical history.",2022-07-20 12:46:26+00:00,UniHPF : Universal Healthcare Predictive Framework with Zero Domain Knowledge,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kyunghoon Hur'), arxiv.Result.Author('Jungwoo Oh'), arxiv.Result.Author('Junu Kim'), arxiv.Result.Author('Min Jae Lee'), arxiv.Result.Author('Eunbyeol Cho'), arxiv.Result.Author('Jiyoun Kim'), arxiv.Result.Author('Seong-Eun Moon'), arxiv.Result.Author('Young-Hak Kim'), arxiv.Result.Author('Edward Choi')]","Despite the abundance of Electronic Healthcare Records (EHR), its
heterogeneity restricts the utilization of medical data in building predictive
models. To address this challenge, we propose Universal Healthcare Predictive
Framework (UniHPF), which requires no medical domain knowledge and minimal
pre-processing for multiple prediction tasks. Experimental results demonstrate
that UniHPF is capable of building large-scale EHR models that can process any
form of medical data from distinct EHR systems. Our framework significantly
outperforms baseline models in multi-source learning tasks, including transfer
and pooled learning, while also showing comparable results when trained on a
single medical dataset. To empirically demonstrate the efficacy of our work, we
conducted extensive experiments using various datasets, model structures, and
tasks. We believe that our findings can provide helpful insights for further
research on the multi-source learning of EHRs."
9110,"We believe that our ﬁndings can provide
    helpful insights for further research on the multi-source learning of EHR.","• To empirically demonstrate the efﬁcacy of our work, we conducted extensive experiments with
    various datasets, model structures, and prediction tasks.","2 Related work

Domain-knowledge-based predictive models.",2022-07-20 12:46:26+00:00,UniHPF : Universal Healthcare Predictive Framework with Zero Domain Knowledge,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kyunghoon Hur'), arxiv.Result.Author('Jungwoo Oh'), arxiv.Result.Author('Junu Kim'), arxiv.Result.Author('Min Jae Lee'), arxiv.Result.Author('Eunbyeol Cho'), arxiv.Result.Author('Jiyoun Kim'), arxiv.Result.Author('Seong-Eun Moon'), arxiv.Result.Author('Young-Hak Kim'), arxiv.Result.Author('Edward Choi')]","Despite the abundance of Electronic Healthcare Records (EHR), its
heterogeneity restricts the utilization of medical data in building predictive
models. To address this challenge, we propose Universal Healthcare Predictive
Framework (UniHPF), which requires no medical domain knowledge and minimal
pre-processing for multiple prediction tasks. Experimental results demonstrate
that UniHPF is capable of building large-scale EHR models that can process any
form of medical data from distinct EHR systems. Our framework significantly
outperforms baseline models in multi-source learning tasks, including transfer
and pooled learning, while also showing comparable results when trained on a
single medical dataset. To empirically demonstrate the efficacy of our work, we
conducted extensive experiments using various datasets, model structures, and
tasks. We believe that our findings can provide helpful insights for further
research on the multi-source learning of EHRs."
9119,"Additionally, due         in III-A may serve as starting points for further research in
to differences in total node size in our stochastically generated      this direction.","For example, the
sparsity is presented in the form of average node degree               torch_cluster library and other top operations proﬁled
which is inversely proportional to sparsity.","synthetic dataset, we present our results as GPU time normal-
ized by the total node size of the input.",2022-07-20 15:01:12+00:00,Operation-Level Performance Benchmarking of Graph Neural Networks for Scientific Applications,cs.LG,"['cs.LG', 'cs.AI', 'cs.AR', 'cs.PF']","[arxiv.Result.Author('Ryien Hosseini'), arxiv.Result.Author('Filippo Simini'), arxiv.Result.Author('Venkatram Vishwanath')]","As Graph Neural Networks (GNNs) increase in popularity for scientific machine
learning, their training and inference efficiency is becoming increasingly
critical. Additionally, the deep learning field as a whole is trending towards
wider and deeper networks, and ever increasing data sizes, to the point where
hard hardware bottlenecks are often encountered. Emerging specialty hardware
platforms provide an exciting solution to this problem. In this paper, we
systematically profile and select low-level operations pertinent to GNNs for
scientific computing implemented in the Pytorch Geometric software framework.
These are then rigorously benchmarked on NVIDIA A100 GPUs for several various
combinations of input values, including tensor sparsity. We then analyze these
results for each operation. At a high level, we conclude that on NVIDIA
systems: (1) confounding bottlenecks such as memory inefficiency often dominate
runtime costs moreso than data sparsity alone, (2) native Pytorch operations
are often as or more competitive than their Pytorch Geometric equivalents,
especially at low to moderate levels of input data sparsity, and (3) many
operations central to state-of-the-art GNN architectures have little to no
optimization for sparsity. We hope that these results serve as a baseline for
those developing these operations on specialized hardware and that our
subsequent analysis helps to facilitate future software and hardware based
optimizations of these operations and thus scalable GNN performance as a whole."
9162,"To make
use of these methods in biology, we now need computational methods to probe the temporal structure in these
still large time-series datasets, and learn representations amenable to comparison and further study.","In the ﬁeld of video-based
behavior analysis, there has been a lot of progress in tools for tracking the pose of people and animals.","The MABe 2022 dataset is a new animal behavior dataset, intended to a) serve as a benchmark dataset for
comparison of unsupervised or self-supervised behavior analysis tools, and establish community standards
for evaluation of unsupervised techniques, b) highlight critical challenges in computational behavior analysis,
particularly pertaining to unsupervised representation learning, and c) foster interaction between behavioral
biologists and the greater machine learning community.",2022-07-21 15:51:30+00:00,The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'cs.MA']","[arxiv.Result.Author('Jennifer J. Sun'), arxiv.Result.Author('Andrew Ulmer'), arxiv.Result.Author('Dipam Chakraborty'), arxiv.Result.Author('Brian Geuther'), arxiv.Result.Author('Edward Hayes'), arxiv.Result.Author('Heng Jia'), arxiv.Result.Author('Vivek Kumar'), arxiv.Result.Author('Zachary Partridge'), arxiv.Result.Author('Alice Robie'), arxiv.Result.Author('Catherine E. Schretter'), arxiv.Result.Author('Chao Sun'), arxiv.Result.Author('Keith Sheppard'), arxiv.Result.Author('Param Uttarwar'), arxiv.Result.Author('Pietro Perona'), arxiv.Result.Author('Yisong Yue'), arxiv.Result.Author('Kristin Branson'), arxiv.Result.Author('Ann Kennedy')]","Real-world behavior is often shaped by complex interactions between multiple
agents. To scalably study multi-agent behavior, advances in unsupervised and
self-supervised learning have enabled a variety of different behavioral
representations to be learned from trajectory data. To date, there does not
exist a unified set of benchmarks that can enable comparing methods
quantitatively and systematically across a broad set of behavior analysis
settings. We aim to address this by introducing a large-scale, multi-agent
trajectory dataset from real-world behavioral neuroscience experiments that
covers a range of behavior analysis tasks. Our dataset consists of trajectory
data from common model organisms, with 9.6 million frames of mouse data and 4.4
million frames of fly data, in a variety of experimental settings, such as
different strains, lengths of interaction, and optogenetic stimulation. A
subset of the frames also consist of expert-annotated behavior labels.
Improvements on our dataset corresponds to behavioral representations that work
across multiple organisms and is able to capture differences for common
behavior analysis tasks."
9179,"Even with advanced synthetic bug generation techniques the performance on
           real bugs will not be adequate, which calls for further research into realistic bug detection.","• An empirical demonstration of the hardness of the real benchmark as compared to a syn-
           thetic dataset.","By providing a large and diverse dataset of synthetic and real bugs from a multitude of projects
without any extra information outside of code, we hope to push the direction of research towards
line-level long-context bug localization for better performance on APR tasks.",2022-07-21 20:17:53+00:00,BigIssue: A Realistic Bug Localization Benchmark,cs.LG,"['cs.LG', 'cs.SE']","[arxiv.Result.Author('Paul Kassianik'), arxiv.Result.Author('Erik Nijkamp'), arxiv.Result.Author('Bo Pang'), arxiv.Result.Author('Yingbo Zhou'), arxiv.Result.Author('Caiming Xiong')]","As machine learning tools progress, the inevitable question arises: How can
machine learning help us write better code? With significant progress being
achieved in natural language processing with models like GPT-3 and Bert, the
applications of natural language processing techniques to code are starting to
be explored. Most of the research has been focused on automatic program repair
(APR), and while the results on synthetic or highly filtered datasets are
promising, such models are hard to apply in real-world scenarios because of
inadequate bug localization. We propose BigIssue: a benchmark for realistic bug
localization. The goal of the benchmark is two-fold. We provide (1) a general
benchmark with a diversity of real and synthetic Java bugs and (2) a motivation
to improve bug localization capabilities of models through attention to the
full repository context. With the introduction of BigIssue, we hope to advance
the state of the art in bug localization, in turn improving APR performance and
increasing its applicability to the modern development cycle."
9180,"The lack of
scalability for this method motivates further research into the problem of bug localization.","While elementary and simple to implement, it relies
heavily on the quality and quantity of test cases, especially for large programs [18].","3 BigIssue Synthetic Dataset

3.1 Motivation
Error localization in the context of automatic program repair has been a widely studied research topic
[34] [8] [27] [28] [30].",2022-07-21 20:17:53+00:00,BigIssue: A Realistic Bug Localization Benchmark,cs.LG,"['cs.LG', 'cs.SE']","[arxiv.Result.Author('Paul Kassianik'), arxiv.Result.Author('Erik Nijkamp'), arxiv.Result.Author('Bo Pang'), arxiv.Result.Author('Yingbo Zhou'), arxiv.Result.Author('Caiming Xiong')]","As machine learning tools progress, the inevitable question arises: How can
machine learning help us write better code? With significant progress being
achieved in natural language processing with models like GPT-3 and Bert, the
applications of natural language processing techniques to code are starting to
be explored. Most of the research has been focused on automatic program repair
(APR), and while the results on synthetic or highly filtered datasets are
promising, such models are hard to apply in real-world scenarios because of
inadequate bug localization. We propose BigIssue: a benchmark for realistic bug
localization. The goal of the benchmark is two-fold. We provide (1) a general
benchmark with a diversity of real and synthetic Java bugs and (2) a motivation
to improve bug localization capabilities of models through attention to the
full repository context. With the introduction of BigIssue, we hope to advance
the state of the art in bug localization, in turn improving APR performance and
increasing its applicability to the modern development cycle."
9195,"In addition to the ﬁndings of the analysis the
paper has created a high quality open-source dataset that allows further research
as well as a code pipeline to create new datasets as required.","This data can be
used by sociologists, economist and policy makers to ensure that attempts to
reduce money laundering and high property prices are based on detailed data
that reﬂect the real situation.","It has been said that
ﬁghting money laundering requires openness and transparency [5], the OCOD
dataset was a step in that direction, we hope that the Enhanced OCOD dataset
is the next.",2022-07-22 08:08:21+00:00,What's in the laundromat? Mapping and characterising offshore owned domestic property in London,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jonathan Bourne'), arxiv.Result.Author('Andrea Ingianni'), arxiv.Result.Author('Rex McKenzie')]","The UK, particularly London, is a global hub for money laundering, a
significant portion of which uses domestic property. However, understanding the
distribution and characteristics of offshore domestic property in the UK is
challenging due to data availability. This paper attempts to remedy that
situation by enhancing a publicly available dataset of UK property owned by
offshore companies. We create a data processing pipeline which draws on several
datasets and machine learning techniques to create a parsed set of addresses
classified into six use classes. The enhanced dataset contains 138,000
properties 44,000 more than the original dataset. The majority are domestic
(95k), with a disproportionate amount of those in London (42k). The average
offshore domestic property in London is worth 1.33 million GBP collectively
this amounts to approximately 56 Billion GBP. We perform an in-depth analysis
of the offshore domestic property in London, comparing the price, distribution
and entropy/concentration with Airbnb property, low-use/empty property and
conventional domestic property. We estimate that the total amount of offshore,
low-use and airbnb property in London is between 144,000 and 164,000 and that
they are collectively worth between 145-174 billion GBP. Furthermore, offshore
domestic property is more expensive and has higher entropy/concentration than
all other property types. In addition, we identify two different types of
offshore property, nested and individual, which have different price and
distribution characteristics. Finally, we release the enhanced offshore
property dataset, the complete low-use London dataset and the pipeline for
creating the enhanced dataset to reduce the barriers to studying this topic."
9217,"Having gained recognition,
PINN spawned further research, e.g, [BS19, SAR20, ZBYZ20, MFK21].","Related work was presented
in [KZK21] and [SS18] using other loss functions, respectively, the Galerkin method
for hp-Variational Physics-Informed Neural Networks (hp-VPINNs) and the Petrov-
Galerkin method for the Deep Galerkin Method (DGM).","Another significant recent approach is Convolutional Neural Networks (CNN) used
as PDE solvers, mostly employing encoder-decoder architectures.",2022-07-25 12:43:50+00:00,Learning Relaxation for Multigrid,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']",[arxiv.Result.Author('Dmitry Kuznichov')],"During the last decade, Neural Networks (NNs) have proved to be extremely
effective tools in many fields of engineering, including autonomous vehicles,
medical diagnosis and search engines, and even in art creation. Indeed, NNs
often decisively outperform traditional algorithms. One area that is only
recently attracting significant interest is using NNs for designing numerical
solvers, particularly for discretized partial differential equations. Several
recent papers have considered employing NNs for developing multigrid methods,
which are a leading computational tool for solving discretized partial
differential equations and other sparse-matrix problems. We extend these new
ideas, focusing on so-called relaxation operators (also called smoothers),
which are an important component of the multigrid algorithm that has not yet
received much attention in this context. We explore an approach for using NNs
to learn relaxation parameters for an ensemble of diffusion operators with
random coefficients, for Jacobi type smoothers and for 4Color GaussSeidel
smoothers. The latter yield exceptionally efficient and easy to parallelize
Successive Over Relaxation (SOR) smoothers. Moreover, this work demonstrates
that learning relaxation parameters on relatively small grids using a two-grid
method and Gelfand's formula as a loss function can be implemented easily.
These methods efficiently produce nearly-optimal parameters, thereby
significantly improving the convergence rate of multigrid algorithms on large
grids."
9220,We further study the transferability for the non-linear model and the linear model.,"We observe that Theorem 2 still well
describes the asymptotic performance when n → ∞.","We follow the
setting in Theorem 3 by rotating µ, ν → µ , ν .",2022-07-22 19:36:12+00:00,Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Rongzhe Wei'), arxiv.Result.Author('Haoteng Yin'), arxiv.Result.Author('Junteng Jia'), arxiv.Result.Author('Austin R. Benson'), arxiv.Result.Author('Pan Li')]","Graph neural networks (GNNs) have shown superiority in many prediction tasks
over graphs due to their impressive capability of capturing nonlinear relations
in graph-structured data. However, for node classification tasks, often, only
marginal improvement of GNNs over their linear counterparts has been observed.
Previous works provide very few understandings of this phenomenon. In this
work, we resort to Bayesian learning to deeply investigate the functions of
non-linearity in GNNs for node classification tasks. Given a graph generated
from the statistical model CSBM, we observe that the max-a-posterior estimation
of a node label given its own and neighbors' attributes consists of two types
of non-linearity, a possibly non-linear transformation of node attributes and a
ReLU-activated feature aggregation from neighbors. The latter surprisingly
matches the type of non-linearity used in many GNN models. By further imposing
Gaussian assumption on node attributes, we prove that the superiority of those
ReLU activations is only significant when the node attributes are far more
informative than the graph structure, which nicely matches many previous
empirical observations. A similar argument can be achieved when there is a
distribution shift of node attributes between the training and testing
datasets. Finally, we verify our theory on both synthetic and real-world
networks."
9221,We further study the transferability for the non-linear model and the linear model.,"We observe that Theorem 2 still well
describes the asymptotic performance when n → ∞.","We follow the
setting in Theorem 3 by rotating µ, ν → µ , ν .",2022-07-22 19:36:12+00:00,Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Rongzhe Wei'), arxiv.Result.Author('Haoteng Yin'), arxiv.Result.Author('Junteng Jia'), arxiv.Result.Author('Austin R. Benson'), arxiv.Result.Author('Pan Li')]","Graph neural networks (GNNs) have shown superiority in many prediction tasks
over graphs due to their impressive capability of capturing nonlinear relations
in graph-structured data. However, for node classification tasks, often, only
marginal improvement of GNNs over their linear counterparts has been observed.
Previous works provide very few understandings of this phenomenon. In this
work, we resort to Bayesian learning to deeply investigate the functions of
non-linearity in GNNs for node classification tasks. Given a graph generated
from the statistical model CSBM, we observe that the max-a-posterior estimation
of a node label given its own and neighbors' attributes consists of two types
of non-linearity, a possibly non-linear transformation of node attributes and a
ReLU-activated feature aggregation from neighbors. The latter surprisingly
matches the type of non-linearity used in many GNN models. By further imposing
Gaussian assumption on node attributes, we prove that the superiority of those
ReLU activations is only significant when the node attributes are far more
informative than the graph structure, which nicely matches many previous
empirical observations. A similar argument can be achieved when there is a
distribution shift of node attributes between the training and testing
datasets. Finally, we verify our theory on both synthetic and real-world
networks."
9222,We further study the transferability for the non-linear model and the linear model.,"We observe that Theorem 2 still well
describes the asymptotic performance when n → ∞.","We follow the
setting in Theorem 3 by rotating µ, ν → µ , ν .",2022-07-22 19:36:12+00:00,Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Rongzhe Wei'), arxiv.Result.Author('Haoteng Yin'), arxiv.Result.Author('Junteng Jia'), arxiv.Result.Author('Austin R. Benson'), arxiv.Result.Author('Pan Li')]","Graph neural networks (GNNs) have shown superiority in many prediction tasks
over graphs due to their impressive capability of capturing nonlinear relations
in graph-structured data. However, for node classification tasks, often, only
marginal improvement of GNNs over their linear counterparts has been observed.
Previous works provide very few understandings of this phenomenon. In this
work, we resort to Bayesian learning to deeply investigate the functions of
non-linearity in GNNs for node classification tasks. Given a graph generated
from the statistical model CSBM, we observe that the max-a-posterior estimation
of a node label given its own and neighbors' attributes consists of two types
of non-linearity, a possibly non-linear transformation of node attributes and a
ReLU-activated feature aggregation from neighbors. The latter surprisingly
matches the type of non-linearity used in many GNN models. By further imposing
Gaussian assumption on node attributes, we prove that the superiority of those
ReLU activations is only significant when the node attributes are far more
informative than the graph structure, which nicely matches many previous
empirical observations. A similar argument can be achieved when there is a
distribution shift of node attributes between the training and testing
datasets. Finally, we verify our theory on both synthetic and real-world
networks."
9223,"Then based on the methods
of this research, further study may build a more complete salary predictive model with higher accuracy
and explore the relation between other related features of the players (like the popularity of players)
and their salary or market value.","Therefore, to make the research
more comprehensive, the popularity data of players need to be obtained.","Moreover, the research only successfully gets the salary data of players in the top 5 leagues and
does not get players’ data in some other leagues or some secondary leagues, like leagues of Portugal
and Netherlands or secondary leagues of the top 5 leagues.",2022-07-22 22:34:52+00:00,Machine Learning Modeling to Evaluate the Value of Football Players,cs.LG,['cs.LG'],"[arxiv.Result.Author('Chenyao Li'), arxiv.Result.Author('Stylianos Kampakis'), arxiv.Result.Author('Philip Treleaven')]","In most sports, especially football, most coaches and analysts search for key
performance indicators using notational analysis. This method utilizes a
statistical summary of events based on video footage and numerical records of
goal scores. Unfortunately, this approach is now obsolete owing to the
continuous evolutionary increase in technology that simplifies the analysis of
more complex process variables through machine learning (ML). Machine learning,
a form of artificial intelligence (AI), uses algorithms to detect meaningful
patterns and define a structure based on positional data. This research
investigates a new method to evaluate the value of current football players,
based on establishing the machine learning models to investigate the relations
among the various features of players, the salary of players, and the market
value of players. The data of the football players used for this project is
from several football websites. The data on the salary of football players will
be the proxy for evaluating the value of players, and other features will be
used to establish and train the ML model for predicting the suitable salary for
the players. The motivation is to explore what are the relations between
different features of football players and their salaries - how each feature
affects their salaries, or which are the most important features to affect the
salary? Although many standards can reflect the value of football players, the
salary of the players is one of the most intuitive and crucial indexes, so this
study will use the salary of players as the proxy to evaluate their value.
Moreover, many features of players can affect the valuation of the football
players, but the value of players is mainly decided by three types of factors:
basic characteristics, performance on the court, and achievements at the club."
9230,"More broadly, our
of model parameters which is modelled using random-walk             paper motivates further research between the topics of dis-
update state transition equations (Brockwell & Davis, 2009).","In the StateSpace model, the state is the vector           be updated in response to streaming data.","tribution shift, time series prediction and hyper-parameter
ARIMA is an autoregressive model (not applicable to the             learning.",2022-07-23 10:32:37+00:00,Time Series Prediction under Distribution Shift using Differentiable Forgetting,cs.LG,"['cs.LG', 'q-fin.ST']","[arxiv.Result.Author('Stefanos Bennett'), arxiv.Result.Author('Jase Clarkson')]","Time series prediction is often complicated by distribution shift which
demands adaptive models to accommodate time-varying distributions. We frame
time series prediction under distribution shift as a weighted empirical risk
minimisation problem. The weighting of previous observations in the empirical
risk is determined by a forgetting mechanism which controls the trade-off
between the relevancy and effective sample size that is used for the estimation
of the predictive model. In contrast to previous work, we propose a
gradient-based learning method for the parameters of the forgetting mechanism.
This speeds up optimisation and therefore allows more expressive forgetting
mechanisms."
9253,"We can speculate that increasing the complexity of
these methodologies as well as increasing the number of source tasks have improve the accuracy on
the target task via a transfer chain, but further research would be needed to conﬁrm this.","We have so far only used relatively simplistic oﬀ-the-shelf
base learners and hill-climbing methodologies.",We have only looked at multi-label classiﬁcation for the source and target tasks.,2022-07-24 13:37:25+00:00,From Multi-label Learning to Cross-Domain Transfer: A Model-Agnostic Approach,cs.LG,['cs.LG'],[arxiv.Result.Author('Jesse Read')],"In multi-label learning, a particular case of multi-task learning where a
single data point is associated with multiple target labels, it was widely
assumed in the literature that, to obtain best accuracy, the dependence among
the labels should be explicitly modeled. This premise led to a proliferation of
methods offering techniques to learn and predict labels together, for example
where the prediction for one label influences predictions for other labels.
Even though it is now acknowledged that in many contexts a model of dependence
is not required for optimal performance, such models continue to outperform
independent models in some of those very contexts, suggesting alternative
explanations for their performance beyond label dependence, which the
literature is only recently beginning to unravel. Leveraging and extending
recent discoveries, we turn the original premise of multi-label learning on its
head, and approach the problem of joint-modeling specifically under the absence
of any measurable dependence among task labels; for example, when task labels
come from separate problem domains. We shift insights from this study towards
building an approach for transfer learning that challenges the long-held
assumption that transferability of tasks comes from measurements of similarity
between the source and target domains or models. This allows us to design and
test a method for transfer learning, which is model driven rather than purely
data driven, and furthermore it is black box and model-agnostic (any base model
class can be considered). We show that essentially we can create
task-dependence based on source-model capacity. The results we obtain have
important implications and provide clear directions for future work, both in
the areas of multi-label and transfer learning."
9292,"To further study the performance
of CAML on heterogeneous task distributions, in this experiment, we considered DomainNet, a
multi-domain benchmark.",CAML is consistently better in multi-domain task adaptation.,"While both the training and testing tasks were drawn from the same
collection of domains (ClipArt, InfoGraph, Painting, QuickDraw), we ensured that the set of classes
Ctr and Cte were disjoint.",2022-07-25 17:01:29+00:00,Contrastive Knowledge-Augmented Meta-Learning for Few-Shot Classification,cs.LG,['cs.LG'],"[arxiv.Result.Author('Rakshith Subramanyam'), arxiv.Result.Author('Mark Heimann'), arxiv.Result.Author('Jayram Thathachar'), arxiv.Result.Author('Rushil Anirudh'), arxiv.Result.Author('Jayaraman J. Thiagarajan')]","Model agnostic meta-learning algorithms aim to infer priors from several
observed tasks that can then be used to adapt to a new task with few examples.
Given the inherent diversity of tasks arising in existing benchmarks, recent
methods use separate, learnable structure, such as hierarchies or graphs, for
enabling task-specific adaptation of the prior. While these approaches have
produced significantly better meta learners, our goal is to improve their
performance when the heterogeneous task distribution contains challenging
distribution shifts and semantic disparities. To this end, we introduce CAML
(Contrastive Knowledge-Augmented Meta Learning), a novel approach for
knowledge-enhanced few-shot learning that evolves a knowledge graph to
effectively encode historical experience, and employs a contrastive
distillation strategy to leverage the encoded knowledge for task-aware
modulation of the base learner. Using standard benchmarks, we evaluate the
performance of CAML in different few-shot learning scenarios. In addition to
the standard few-shot task adaptation, we also consider the more challenging
multi-domain task adaptation and few-shot dataset generalization settings in
our empirical studies. Our results shows that CAML consistently outperforms
best known approaches and achieves improved generalization."
9312,"Here, we further study how methods of different
levels beneﬁt various downstream tasks.","ProNet-Amino Acid      0.857
                                                                       ProNet-Backbone        0.858
                                                                       ProNet-All-Atom        0.871

6.5 Observations and Ablation Studies

Observations on methods of different levels.","As shown in Table 1, our ProNet-backbone outperforms the
methods of the other two levels on function prediction tasks, including fold and reaction classiﬁcation
tasks.",2022-07-26 01:55:25+00:00,Learning Protein Representations via Complete 3D Graph Networks,cs.LG,"['cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Limei Wang'), arxiv.Result.Author('Haoran Liu'), arxiv.Result.Author('Yi Liu'), arxiv.Result.Author('Jerry Kurtin'), arxiv.Result.Author('Shuiwang Ji')]","We consider representation learning for proteins with 3D structures. We build
3D graphs based on protein structures and develop graph networks to learn their
representations. Depending on the levels of details that we wish to capture,
protein representations can be computed at different levels, \emph{e.g.}, the
amino acid, backbone, or all-atom levels. Importantly, there exist hierarchical
relations among different levels. In this work, we propose to develop a novel
hierarchical graph network, known as ProNet, to capture the relations. Our
ProNet is very flexible and can be used to compute protein representations at
different levels of granularity. We show that, given a base 3D graph network
that is complete, our ProNet representations are also complete at all levels.
To close the loop, we develop a complete and efficient 3D graph network to be
used as a base model, making our ProNet complete. We conduct experiments on
multiple downstream tasks. Results show that ProNet outperforms recent methods
on most datasets. In addition, results indicate that different downstream tasks
may require representations at different levels. Our code is available as part
of the DIG library (\url{https://github.com/divelab/DIG})."
9318,"However, the above studies mainly focus on low and moderate sea states, and
further research is still needed for the prediction of roll motion in high sea states with stronger nonlinearity and
large roll angles.","The coupled CNN and LSTM (Zhang
et al., 2019) was adopted to tackle the multidimension data from USV sensor so as to predict the one step of roll
motion of USV under low sea state.","For data-driven approaches, datasets play a key role in model training.",2022-07-26 06:26:00+00:00,A Data Driven Method for Multi-step Prediction of Ship Roll Motion in High Sea States,cs.LG,"['cs.LG', 'physics.flu-dyn']","[arxiv.Result.Author('Dan Zhang'), arxiv.Result.Author('Xi Zhou'), arxiv.Result.Author('Zi-Hao Wang'), arxiv.Result.Author('Yan Peng'), arxiv.Result.Author('Shao-Rong Xie')]","Accurate prediction of roll motion in high sea state is significant for the
operability, safety and survivability of marine vehicles. This paper presents a
novel data-driven methodology for achieving the multi-step prediction of ship
roll motion in high sea states. A hybrid neural network, named ConvLSTMPNet, is
proposed to execute long short-term memory (LSTM) and one-dimensional
convolutional neural networks (CNN) in parallel to extract time-dependent and
spatio-temporal information from multidimensional inputs. Taken KCS as the
study object, the numerical solution of computational fluid dynamics method is
utilized to generate the ship motion data in sea state 7 with different wave
directions. An in-depth comparative study on the selection of feature space is
conducted, considering the effects of time history of motion states and wave
height. The comparison results demonstrate the superiority of selecting both
motion states and wave heights as the feature space for multi-step prediction.
In addition, the results demonstrate that ConvLSTMNet achieves more accurate
than LSTM and CNN methods in multi-step prediction of roll motion, validating
the efficiency of the proposed method."
9319,"However, only one step of prediction was tested in
these studies, and further research on multi-step prediction is still required.","As can be seen, wave height was chosen as the input feature in both studies, which is
expected to provide direct information about wave impacts.","Until recently, few studies have focused on multi-step prediction of ship roll motion in high sea states.",2022-07-26 06:26:00+00:00,A Data Driven Method for Multi-step Prediction of Ship Roll Motion in High Sea States,cs.LG,"['cs.LG', 'physics.flu-dyn']","[arxiv.Result.Author('Dan Zhang'), arxiv.Result.Author('Xi Zhou'), arxiv.Result.Author('Zi-Hao Wang'), arxiv.Result.Author('Yan Peng'), arxiv.Result.Author('Shao-Rong Xie')]","Accurate prediction of roll motion in high sea state is significant for the
operability, safety and survivability of marine vehicles. This paper presents a
novel data-driven methodology for achieving the multi-step prediction of ship
roll motion in high sea states. A hybrid neural network, named ConvLSTMPNet, is
proposed to execute long short-term memory (LSTM) and one-dimensional
convolutional neural networks (CNN) in parallel to extract time-dependent and
spatio-temporal information from multidimensional inputs. Taken KCS as the
study object, the numerical solution of computational fluid dynamics method is
utilized to generate the ship motion data in sea state 7 with different wave
directions. An in-depth comparative study on the selection of feature space is
conducted, considering the effects of time history of motion states and wave
height. The comparison results demonstrate the superiority of selecting both
motion states and wave heights as the feature space for multi-step prediction.
In addition, the results demonstrate that ConvLSTMNet achieves more accurate
than LSTM and CNN methods in multi-step prediction of roll motion, validating
the efficiency of the proposed method."
9320,"However, the above studies mainly focus on low and moderate sea states, and
further research is still needed for the prediction of roll motion in high sea states with large roll angles.","The coupled CNN and LSTM (Zhang
et al., 2019) was adopted to tackle the multidimension data from USV sensor so as to predict the one step of roll
motion of USV under low sea state.","For data-driven approaches, datasets play a key role in model training.",2022-07-26 06:26:00+00:00,A Data Driven Method for Multi-step Prediction of Ship Roll Motion in High Sea States,cs.LG,"['cs.LG', 'physics.flu-dyn']","[arxiv.Result.Author('Dan Zhang'), arxiv.Result.Author('Xi Zhou'), arxiv.Result.Author('Zi-Hao Wang'), arxiv.Result.Author('Yan Peng'), arxiv.Result.Author('Shao-Rong Xie')]","Ship roll motion in high sea state has large amplitude and nonlinear
dynamics, and its prediction is significant for the operability, safety and
survivability. This paper presents a novel data-driven methodology to provide
multi-step prediction of the ship roll motion in high sea states. A hybrid
neural network is proposed that combines long short-term memory (LSTM) and
convolutional neural network (CNN) in parallel. The motivation is to extract
the nonlinear dynamics characteristics and the hydrodynamic memory information
through the advantage of CNN and LSTM, respectively. For the feature selection,
the time histories of motion states and wave heights are selected to involve
sufficient information. Taken a scaled KCS as the study object, the ship
motions in sea state 7 irregular long crested waves are simulated and used for
the validation. The results show that at least one period of roll motion can be
accurately predicted by using the proposed method. Compared with the single
LSTM and CNN method, the proposed method has better performance in the
prediction of the amplitude of roll angles. Besides, the comparison results
also demonstrate that selecting motion states and wave heights as feature space
improves the prediction accuracy, verifying the effectiveness of the proposed
method."
9321,"However, only one step of prediction was tested in
these studies, and further research on multi-step prediction is still required.","As can be seen, wave height was chosen as the input feature in both studies, which is
expected to provide direct information about wave impacts.","Until recently, few studies have focused on multi-step prediction of ship roll motion in high sea states.",2022-07-26 06:26:00+00:00,A Data Driven Method for Multi-step Prediction of Ship Roll Motion in High Sea States,cs.LG,"['cs.LG', 'physics.flu-dyn']","[arxiv.Result.Author('Dan Zhang'), arxiv.Result.Author('Xi Zhou'), arxiv.Result.Author('Zi-Hao Wang'), arxiv.Result.Author('Yan Peng'), arxiv.Result.Author('Shao-Rong Xie')]","Ship roll motion in high sea state has large amplitude and nonlinear
dynamics, and its prediction is significant for the operability, safety and
survivability. This paper presents a novel data-driven methodology to provide
multi-step prediction of the ship roll motion in high sea states. A hybrid
neural network is proposed that combines long short-term memory (LSTM) and
convolutional neural network (CNN) in parallel. The motivation is to extract
the nonlinear dynamics characteristics and the hydrodynamic memory information
through the advantage of CNN and LSTM, respectively. For the feature selection,
the time histories of motion states and wave heights are selected to involve
sufficient information. Taken a scaled KCS as the study object, the ship
motions in sea state 7 irregular long crested waves are simulated and used for
the validation. The results show that at least one period of roll motion can be
accurately predicted by using the proposed method. Compared with the single
LSTM and CNN method, the proposed method has better performance in the
prediction of the amplitude of roll angles. Besides, the comparison results
also demonstrate that selecting motion states and wave heights as feature space
improves the prediction accuracy, verifying the effectiveness of the proposed
method."
9344,for further research.,"In
help filter false positives and this is definitely an interesting avenue                          Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.","Other important directions which we plan to
pursue is the identification of the features of tasks where silent                          [15] Ekaterina Redekop, Karthik V. Sarma, Adam Kinnaird, Anthony Sisk, Steven S.
failure is less common, as well as understanding which aspects of                                 Raman, Leonard S. Marks, William Speier, and Corey W. Arnold.",2022-07-26 16:25:38+00:00,Is Attention Interpretation? A Quantitative Assessment On Sets,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jonathan Haab'), arxiv.Result.Author('Nicolas Deutschmann'), arxiv.Result.Author('Maria Rodríguez Martínez')]","The debate around the interpretability of attention mechanisms is centered on
whether attention scores can be used as a proxy for the relative amounts of
signal carried by sub-components of data. We propose to study the
interpretability of attention in the context of set machine learning, where
each data point is composed of an unordered collection of instances with a
global label. For classical multiple-instance-learning problems and simple
extensions, there is a well-defined ""importance"" ground truth that can be
leveraged to cast interpretation as a binary classification problem, which we
can quantitatively evaluate. By building synthetic datasets over several data
modalities, we perform a systematic assessment of attention-based
interpretations. We find that attention distributions are indeed often
reflective of the relative importance of individual instances, but that silent
failures happen where a model will have high classification performance but
attention patterns that do not align with expectations. Based on these
observations, we propose to use ensembling to minimize the risk of misleading
attention-based explanations."
9367,"We plan
to further study its properties along with potential improvements in the future.","Our re-
sults indicate that the proposed regularization term partially mitigates this phe-
nomenon, constituting a ﬁrst approach towards this research direction.","Also, investigating the exact conditions under which a model replicates the last
observed values of the time series is on our agenda for future work.",2022-07-27 10:39:00+00:00,Time Series Forecasting Models Copy the Past: How to Mitigate,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Chrysoula Kosma'), arxiv.Result.Author('Giannis Nikolentzos'), arxiv.Result.Author('Nancy Xu'), arxiv.Result.Author('Michalis Vazirgiannis')]","Time series forecasting is at the core of important application domains
posing significant challenges to machine learning algorithms. Recently neural
network architectures have been widely applied to the problem of time series
forecasting. Most of these models are trained by minimizing a loss function
that measures predictions' deviation from the real values. Typical loss
functions include mean squared error (MSE) and mean absolute error (MAE). In
the presence of noise and uncertainty, neural network models tend to replicate
the last observed value of the time series, thus limiting their applicability
to real-world data. In this paper, we provide a formal definition of the above
problem and we also give some examples of forecasts where the problem is
observed. We also propose a regularization term penalizing the replication of
previously seen values. We evaluate the proposed regularization term both on
synthetic and real-world datasets. Our results indicate that the regularization
term mitigates to some extent the aforementioned problem and gives rise to more
robust models."
9434,"The framework has the following
properties:

    • it can produce a topological view of the original dataset through pictures

    • the visualization can provide clues for any sample of interest to be inspected

    • it is highly scalable and can process large datasets with thousands of classes

    • it can provide intuitive insights and suggest places that are worth a further study for
       users without any prior knowledge on the model or the data

§1.1 Background: Topological Data Analysis and the Mapper Algorithm

    Our method is rooted in the growing ﬁeld of computational topology and topological data
analysis and the framework is closely related to the mapper algorithm [40] for topological
data analysis (TDA).","7
                            Detailed Discussion And Description of GTDA

  §1 – Our GTDA method for Reeb nets & prediction functions – 8
  §2 – Demonstration in graph based prediction – 20
  §3 – Understanding image predictions – 22
  §4 – Comparing models on ImageNet-1k predictions – 32
  §5 – Understanding Malignant Gene Mutation Predictions – 39
  §6 – Inspecting chest X-ray images – 46
  §7 – Parameter selection of GTDA – 48
  §8 – Performance and scaling – 50
  §9 – Comparing to tSNE and UMAP – 51
 §10 – Code availability – 53

§1 Our GTDA method for Reeb nets & prediction functions

    In this paper, we developed a framework to inspect the predictions of complex models by
visualizing the interactions between predictions and data.","Mapper builds a discrete approximation of a Reeb graph or Reeb
space (see Section §1.6, Figure 7).",2022-07-28 19:28:05+00:00,Topological structure of complex predictions,cs.LG,"['cs.LG', 'cs.HC', 'cs.SI', 'math.AT']","[arxiv.Result.Author('Meng Liu'), arxiv.Result.Author('Tamal K. Dey'), arxiv.Result.Author('David F. Gleich')]","Complex prediction models such as deep learning are the output from fitting
machine learning, neural networks, or AI models to a set of training data.
These are now standard tools in science. A key challenge with the current
generation of models is that they are highly parameterized, which makes
describing and interpreting the prediction strategies difficult. We use
topological data analysis to transform these complex prediction models into
pictures representing a topological view. The result is a map of the
predictions that enables inspection. The methods scale up to large datasets
across different domains and enable us to detect labeling errors in training
data, understand generalization in image classification, and inspect
predictions of likely pathogenic mutations in the BRCA1 gene."
9435,"The framework has the following
properties:

    • it can produce a topological view of the original dataset through pictures

    • the visualization can provide clues for any sample of interest to be inspected

    • it is highly scalable and can process large datasets with thousands of classes

    • it can provide intuitive insights and suggest places that are worth a further study for
       users without any prior knowledge on the model or the data

§1.1 Background: Topological Data Analysis and the Mapper Algorithm

    Our method is rooted in the growing ﬁeld of computational topology and topological data
analysis and the framework is closely related to the mapper algorithm [40] for topological
data analysis (TDA).","7
                            Detailed Discussion And Description of GTDA

  §1 – Our GTDA method for Reeb nets & prediction functions – 8
  §2 – Demonstration in graph based prediction – 20
  §3 – Understanding image predictions – 22
  §4 – Comparing models on ImageNet-1k predictions – 32
  §5 – Understanding Malignant Gene Mutation Predictions – 39
  §6 – Inspecting chest X-ray images – 46
  §7 – Parameter selection of GTDA – 48
  §8 – Performance and scaling – 50
  §9 – Comparing to tSNE and UMAP – 51
 §10 – Code availability – 53

§1 Our GTDA method for Reeb nets & prediction functions

    In this paper, we developed a framework to inspect the predictions of complex models by
visualizing the interactions between predictions and data.","Mapper builds a discrete approximation of a Reeb graph or Reeb
space (see Section §1.6, Figure 7).",2022-07-28 19:28:05+00:00,Topological structure of complex predictions,cs.LG,"['cs.LG', 'cs.HC', 'cs.SI', 'math.AT']","[arxiv.Result.Author('Meng Liu'), arxiv.Result.Author('Tamal K. Dey'), arxiv.Result.Author('David F. Gleich')]","Complex prediction models such as deep learning are the output from fitting
machine learning, neural networks, or AI models to a set of training data.
These are now standard tools in science. A key challenge with the current
generation of models is that they are highly parameterized, which makes
describing and interpreting the prediction strategies difficult. We use
topological data analysis to transform these complex prediction models into
pictures representing a topological view. The result is a map of the
predictions that enables inspection. The methods scale up to large datasets
across different domains and enable us to detect labeling errors in training
data, understand generalization in image classification, and inspect
predictions of likely pathogenic mutations in the BRCA1 gene."
9436,"The framework has the following
properties:

    • it can produce a topological view of the original dataset through pictures

    • the visualization can provide clues for any sample of interest to be inspected

    • it is highly scalable and can process large datasets with thousands of classes

    • it can provide intuitive insights and suggest places that are worth a further study for
       users without any prior knowledge on the model or the data

§1.1 Background: Topological Data Analysis and the Mapper Algorithm

    Our method is rooted in the growing ﬁeld of computational topology and topological data
analysis and the framework is closely related to the mapper algorithm [41] for topological
data analysis (TDA).","7
                            Detailed Discussion And Description of GTDA

  §1 – Our GTDA method for Reeb nets & prediction functions – 8
  §2 – Demonstration in graph based prediction – 20
  §3 – Understanding image predictions – 22
  §4 – Comparing models on ImageNet-1k predictions – 32
  §5 – Understanding Malignant Gene Mutation Predictions – 39
  §6 – Inspecting chest X-ray images – 50
  §7 – Parameter selection of GTDA – 52
  §8 – Performance and scaling – 54
  §9 – Comparing to tSNE and UMAP – 55
 §10 – Code availability – 57

§1 Our GTDA method for Reeb nets & prediction functions

    In this paper, we developed a framework to inspect the predictions of complex models by
visualizing the interactions between predictions and data.","Mapper builds a discrete approximation of a Reeb graph or Reeb
space (see Section §1.6, Figure 7).",2022-07-28 19:28:05+00:00,Topological structure of complex predictions,cs.LG,"['cs.LG', 'cs.HC', 'cs.SI', 'math.AT']","[arxiv.Result.Author('Meng Liu'), arxiv.Result.Author('Tamal K. Dey'), arxiv.Result.Author('David F. Gleich')]","Complex prediction models such as deep learning are the output from fitting
machine learning, neural networks, or AI models to a set of training data.
These are now standard tools in science. A key challenge with the current
generation of models is that they are highly parameterized, which makes
describing and interpreting the prediction strategies difficult. We use
topological data analysis to transform these complex prediction models into
pictures representing a topological view. The result is a map of the
predictions that enables inspection. The methods scale up to large datasets
across different domains and enable us to detect labeling errors in training
data, understand generalization in image classification, and inspect
predictions of likely pathogenic mutations in the BRCA1 gene."
9444,"This is exciting because it opens the door to
the further research on self-play for code generation and other problems.",its own performance on held-out test problems.,"As discussed in
Section 5, self-play can be combined with various other search and RL strategies for code
generation which also require a pool of problems.",2022-07-29 06:43:28+00:00,Language Models Can Teach Themselves to Program Better,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Patrick Haluptzok'), arxiv.Result.Author('Matthew Bowers'), arxiv.Result.Author('Adam Tauman Kalai')]","Recent Language Models (LMs) achieve breakthrough performance in code
generation when trained on human-authored problems, even solving some
competitive-programming problems. Self-play has proven useful in games such as
Go, and thus it is natural to ask whether LMs can generate their own
instructive programming problems to improve their performance. We show that it
is possible for an LM to synthesize programming problems and solutions, which
are filtered for correctness by a Python interpreter. The LM's performance is
then seen to improve when it is fine-tuned on its own synthetic problems and
verified solutions; thus the model 'improves itself' using the Python
interpreter. Problems are specified formally as programming puzzles [Schuster
et al., 2021], a code-based problem format where solutions can easily be
verified for correctness by execution. In experiments on publicly-available
LMs, test accuracy more than doubles. This work demonstrates the potential for
code LMs, with an interpreter, to generate instructive problems and improve
their own performance."
9447,"We also discuss open research issues and outline possible directions
                                                   forward, with the hope of spurring further research on this blooming research topic.","To this end, we draw a conceptual map of the state-of-the-art, grouping relevant approaches
                                                   based on their intended purpose and on how they structure the interaction, highlighting similarities
                                                   and differences between them.","Keywords Human-in-the-Loop · Explainable AI · Interactive Machine Learning · Debugging · Model Editing

                                        1 Introduction

                                        The ﬁelds of eXplainable Artiﬁcial Intelligence (XAI) and Interactive Machine Learning (IML) have traditionally been
                                        explored separately.",2022-07-29 07:46:11+00:00,Leveraging Explanations in Interactive Machine Learning: An Overview,cs.LG,['cs.LG'],"[arxiv.Result.Author('Stefano Teso'), arxiv.Result.Author('Öznur Alkan'), arxiv.Result.Author('Wolfang Stammer'), arxiv.Result.Author('Elizabeth Daly')]","Explanations have gained an increasing level of interest in the AI and
Machine Learning (ML) communities in order to improve model transparency and
allow users to form a mental model of a trained ML model. However, explanations
can go beyond this one way communication as a mechanism to elicit user control,
because once users understand, they can then provide feedback. The goal of this
paper is to present an overview of research where explanations are combined
with interactive capabilities as a mean to learn new models from scratch and to
edit and debug existing ones. To this end, we draw a conceptual map of the
state-of-the-art, grouping relevant approaches based on their intended purpose
and on how they structure the interaction, highlighting similarities and
differences between them. We also discuss open research issues and outline
possible directions forward, with the hope of spurring further research on this
blooming research topic."
9448,"In this section,
we outline a selection of urgent open issues and, wherever possible, suggest possible directions forward, with the hope
of spurring further research on this important topic.","6 Open Problems

Despite recent progress on integrating explanations and interaction, many unresolved problems remain.","6.1 Handling Human Factors

Machine explanations are only effective insomuch as they are understood by the person at the receiving end.",2022-07-29 07:46:11+00:00,Leveraging Explanations in Interactive Machine Learning: An Overview,cs.LG,['cs.LG'],"[arxiv.Result.Author('Stefano Teso'), arxiv.Result.Author('Öznur Alkan'), arxiv.Result.Author('Wolfang Stammer'), arxiv.Result.Author('Elizabeth Daly')]","Explanations have gained an increasing level of interest in the AI and
Machine Learning (ML) communities in order to improve model transparency and
allow users to form a mental model of a trained ML model. However, explanations
can go beyond this one way communication as a mechanism to elicit user control,
because once users understand, they can then provide feedback. The goal of this
paper is to present an overview of research where explanations are combined
with interactive capabilities as a mean to learn new models from scratch and to
edit and debug existing ones. To this end, we draw a conceptual map of the
state-of-the-art, grouping relevant approaches based on their intended purpose
and on how they structure the interaction, highlighting similarities and
differences between them. We also discuss open research issues and outline
possible directions forward, with the hope of spurring further research on this
blooming research topic."
9449,"In addition, we identiﬁed a number of
open problems impacting the human and machine sides of explanatory interaction and highlighted noteworthy paths of
future, with the goal of spurring further research into this novel and promising approach, helping to bridge the gap
towards human-centric machine learning and AI.","We categorized existing approaches along four dimensions, namely algorithmic goal, type of machine
explanations involved, human feedback received, and incorporation strategy, facilitating the identiﬁcation of links
between different approaches as well as respective strengths and limitations.","References

   [1] A. Adadi and M. Berrada.",2022-07-29 07:46:11+00:00,Leveraging Explanations in Interactive Machine Learning: An Overview,cs.LG,['cs.LG'],"[arxiv.Result.Author('Stefano Teso'), arxiv.Result.Author('Öznur Alkan'), arxiv.Result.Author('Wolfang Stammer'), arxiv.Result.Author('Elizabeth Daly')]","Explanations have gained an increasing level of interest in the AI and
Machine Learning (ML) communities in order to improve model transparency and
allow users to form a mental model of a trained ML model. However, explanations
can go beyond this one way communication as a mechanism to elicit user control,
because once users understand, they can then provide feedback. The goal of this
paper is to present an overview of research where explanations are combined
with interactive capabilities as a mean to learn new models from scratch and to
edit and debug existing ones. To this end, we draw a conceptual map of the
state-of-the-art, grouping relevant approaches based on their intended purpose
and on how they structure the interaction, highlighting similarities and
differences between them. We also discuss open research issues and outline
possible directions forward, with the hope of spurring further research on this
blooming research topic."
9450,"We also discuss open research issues and outline possible directions
                                                  forward, with the hope of spurring further research on this blooming research topic.","To this end, we draw a conceptual map of the state-of-the-art, grouping relevant approaches
                                                  based on their intended purpose and on how they structure the interaction, highlighting similarities
                                                  and differences between them.","Keywords Human-in-the-Loop · Explainable AI · Interactive Machine Learning · Debugging · Model Editing

                                       1 Introduction

                                       The ﬁelds of eXplainable Artiﬁcial Intelligence (XAI) and Interactive Machine Learning (IML) have traditionally been
                                       explored separately.",2022-07-29 07:46:11+00:00,Leveraging Explanations in Interactive Machine Learning: An Overview,cs.LG,['cs.LG'],"[arxiv.Result.Author('Stefano Teso'), arxiv.Result.Author('Öznur Alkan'), arxiv.Result.Author('Wolfang Stammer'), arxiv.Result.Author('Elizabeth Daly')]","Explanations have gained an increasing level of interest in the AI and
Machine Learning (ML) communities in order to improve model transparency and
allow users to form a mental model of a trained ML model. However, explanations
can go beyond this one way communication as a mechanism to elicit user control,
because once users understand, they can then provide feedback. The goal of this
paper is to present an overview of research where explanations are combined
with interactive capabilities as a mean to learn new models from scratch and to
edit and debug existing ones. To this end, we draw a conceptual map of the
state-of-the-art, grouping relevant approaches based on their intended purpose
and on how they structure the interaction, highlighting similarities and
differences between them. We also discuss open research issues and outline
possible directions forward, with the hope of spurring further research on this
blooming research topic."
9451,"In this section,
we outline a selection of urgent open issues and, wherever possible, suggest possible directions forward, with the hope
of spurring further research on this important topic.","6 Open Problems

Despite recent progress on integrating explanations and interaction, many unresolved problems remain.","6.1 Handling Human Factors

Machine explanations are only effective insomuch as they are understood by the person at the receiving end.",2022-07-29 07:46:11+00:00,Leveraging Explanations in Interactive Machine Learning: An Overview,cs.LG,['cs.LG'],"[arxiv.Result.Author('Stefano Teso'), arxiv.Result.Author('Öznur Alkan'), arxiv.Result.Author('Wolfang Stammer'), arxiv.Result.Author('Elizabeth Daly')]","Explanations have gained an increasing level of interest in the AI and
Machine Learning (ML) communities in order to improve model transparency and
allow users to form a mental model of a trained ML model. However, explanations
can go beyond this one way communication as a mechanism to elicit user control,
because once users understand, they can then provide feedback. The goal of this
paper is to present an overview of research where explanations are combined
with interactive capabilities as a mean to learn new models from scratch and to
edit and debug existing ones. To this end, we draw a conceptual map of the
state-of-the-art, grouping relevant approaches based on their intended purpose
and on how they structure the interaction, highlighting similarities and
differences between them. We also discuss open research issues and outline
possible directions forward, with the hope of spurring further research on this
blooming research topic."
9452,"In addition, we identiﬁed a number of
open problems impacting the human and machine sides of explanatory interaction and highlighted noteworthy paths of
future, with the goal of spurring further research into this novel and promising approach, helping to bridge the gap
towards human-centric machine learning and AI.","We categorized existing approaches along four dimensions, namely algorithmic goal, type of machine
explanations involved, human feedback received, and incorporation strategy, facilitating the identiﬁcation of links
between different approaches as well as respective strengths and limitations.","Acknowledgements

The research of ST was partially supported by TAILOR, a project funded by EU Horizon 2020 research and innovation
programme under GA No 952215.",2022-07-29 07:46:11+00:00,Leveraging Explanations in Interactive Machine Learning: An Overview,cs.LG,['cs.LG'],"[arxiv.Result.Author('Stefano Teso'), arxiv.Result.Author('Öznur Alkan'), arxiv.Result.Author('Wolfang Stammer'), arxiv.Result.Author('Elizabeth Daly')]","Explanations have gained an increasing level of interest in the AI and
Machine Learning (ML) communities in order to improve model transparency and
allow users to form a mental model of a trained ML model. However, explanations
can go beyond this one way communication as a mechanism to elicit user control,
because once users understand, they can then provide feedback. The goal of this
paper is to present an overview of research where explanations are combined
with interactive capabilities as a mean to learn new models from scratch and to
edit and debug existing ones. To this end, we draw a conceptual map of the
state-of-the-art, grouping relevant approaches based on their intended purpose
and on how they structure the interaction, highlighting similarities and
differences between them. We also discuss open research issues and outline
possible directions forward, with the hope of spurring further research on this
blooming research topic."
9474,"Since CˆL is relevant to both model depth, i.e., number of layers L, and the structure of the
graph data set, we further study the impacts of the two factors on the selection of compression ratios.","Therefore, we would expect the compression ratio to be negatively correlated
to the CˆL.",Model depth.,2022-07-29 14:07:35+00:00,BiFeat: Supercharge GNN Training via Graph Feature Quantization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yuxin Ma'), arxiv.Result.Author('Ping Gong'), arxiv.Result.Author('Jun Yi'), arxiv.Result.Author('Zhewei Yao'), arxiv.Result.Author('Minjie Wang'), arxiv.Result.Author('Cheng Li'), arxiv.Result.Author('Yuxiong He'), arxiv.Result.Author('Feng Yan')]","Graph Neural Networks (GNNs) is a promising approach for applications with
nonEuclidean data. However, training GNNs on large scale graphs with hundreds
of millions nodes is both resource and time consuming. Different from DNNs,
GNNs usually have larger memory footprints, and thus the GPU memory capacity
and PCIe bandwidth are the main resource bottlenecks in GNN training. To
address this problem, we present BiFeat: a graph feature quantization
methodology to accelerate GNN training by significantly reducing the memory
footprint and PCIe bandwidth requirement so that GNNs can take full advantage
of GPU computing capabilities. Our key insight is that unlike DNN, GNN is less
prone to the information loss of input features caused by quantization. We
identify the main accuracy impact factors in graph feature quantization and
theoretically prove that BiFeat training converges to a network where the loss
is within $\epsilon$ of the optimal loss of uncompressed network. We perform
extensive evaluation of BiFeat using several popular GNN models and datasets,
including GraphSAGE on MAG240M, the largest public graph dataset. The results
demonstrate that BiFeat achieves a compression ratio of more than 30 and
improves GNN training speed by 200%-320% with marginal accuracy loss. In
particular, BiFeat achieves a record by training GraphSAGE on MAG240M within
one hour using only four GPUs."
9497,"To further study the downstream impact of our robust trajectory
model in the AD stack, we plug it into a planner.",Impacts to downstream planners.,"We select a MPC-based planner and evaluate the
collision rates under the attack.",2022-07-29 22:35:05+00:00,Robust Trajectory Prediction against Adversarial Attacks,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Yulong Cao'), arxiv.Result.Author('Danfei Xu'), arxiv.Result.Author('Xinshuo Weng'), arxiv.Result.Author('Zhuoqing Mao'), arxiv.Result.Author('Anima Anandkumar'), arxiv.Result.Author('Chaowei Xiao'), arxiv.Result.Author('Marco Pavone')]","Trajectory prediction using deep neural networks (DNNs) is an essential
component of autonomous driving (AD) systems. However, these methods are
vulnerable to adversarial attacks, leading to serious consequences such as
collisions. In this work, we identify two key ingredients to defend trajectory
prediction models against adversarial attacks including (1) designing effective
adversarial training methods and (2) adding domain-specific data augmentation
to mitigate the performance degradation on clean data. We demonstrate that our
method is able to improve the performance by 46% on adversarial data and at the
cost of only 3% performance degradation on clean data, compared to the model
trained with clean data. Additionally, compared to existing robust methods, our
method can improve performance by 21% on adversarial examples and 9% on clean
data. Our robust model is evaluated with a planner to study its downstream
impacts. We demonstrate that our model can significantly reduce the severe
accident rates (e.g., collisions and off-road driving)."
9501,"Getting rid of any potentially problem-dependent coefﬁcients from this strategy is
a curious direction for further research.","The optimal
weights are potentially problem-dependent, although the strategy appears to be barely sensitive to
these weights.","References

Barto, A. G., Sutton, R. S., and Anderson, C. W. (1983).",2022-07-30 07:44:05+00:00,Reinforcement learning with experience replay and adaptation of action dispersion,cs.LG,"['cs.LG', 'I.2.6']","[arxiv.Result.Author('Paweł Wawrzyński'), arxiv.Result.Author('Wojciech Masarczyk'), arxiv.Result.Author('Mateusz Ostaszewski')]","Effective reinforcement learning requires a proper balance of exploration and
exploitation defined by the dispersion of action distribution. However, this
balance depends on the task, the current stage of the learning process, and the
current environment state. Existing methods that designate the action
distribution dispersion require problem-dependent hyperparameters. In this
paper, we propose to automatically designate the action distribution dispersion
using the following principle: This distribution should have sufficient
dispersion to enable the evaluation of future policies. To that end, the
dispersion should be tuned to assure a sufficiently high probability
(densities) of the actions in the replay buffer and the modes of the
distributions that generated them, yet this dispersion should not be higher.
This way, a policy can be effectively evaluated based on the actions in the
buffer, but exploratory randomness in actions decreases when this policy
converges. The above principle is verified here on challenging benchmarks Ant,
HalfCheetah, Hopper, and Walker2D, with good results. Our method makes the
action standard deviations converge to values similar to those resulting from
trial-and-error optimization."
9513,"However, we note that any formal proof of convergence is expected to be highly non-trivial
as the operator is not monotone, and this requires further study.","Exploring the convergence of these operators, we consider results from simulations and the
speciﬁc case where there are only two states in Appendix A; these give aﬃrmative evidence
for this conjecture, showing numerical evidence that all but a special case converge to the
ﬁxed point and this ﬁxed point is the optimal solution of the convex program Program 4.2.","We expect similar results
for the operators deduced from the other norms and divergences.",2022-07-31 01:26:12+00:00,Convex duality for stochastic shortest path problems in known and unknown environments,cs.LG,"['cs.LG', 'math.OC']",[arxiv.Result.Author('Kelli Francis-Staite')],"This paper gives an introduction to Stochastic Shortest Path (SSP) problems
in known and unknown environments from the perspective of convex optimisation.
It first recalls results in the known parameter case, and develops
understanding through different proofs. It then focuses on the unknown
parameter case, where it studies extended value iteration (EVI) operators. This
includes the existing operators used in Rosenberg et al. [26] and Tarbouriech
et al. [31] based on the l-1 norm and supremum norm, as well as defining EVI
operators corresponding to other norms and divergences, such as the
KL-divergence. This paper shows in general how the EVI operators relate to
convex programs, and the form of their dual, where strong duality is exhibited.
  This paper then focuses on whether the bounds from finite horizon research of
Neu and Pike-Burke [21] can be applied to these extended value iteration
operators in the SSP setting. It shows that similar bounds to [21] for these
operators exist, however they lead to operators that are not in general
monotone and have more complex convergence properties. In a special case we
observe oscillating behaviour. This paper generates open questions on how
research may progress, with several examples that require further examination."
9514,"However, we note that any formal proof of convergence is expected to be highly non-trivial
as the operator is not monotone, and this requires further study.","Exploring the convergence of these operators, we consider results from simulations and the
speciﬁc case where there are only two states in Appendix A; these give aﬃrmative evidence
for this conjecture, showing numerical evidence that all but a special case converge to the
ﬁxed point and this ﬁxed point is the optimal solution of the convex program Program 4.2.","We expect similar results
for the operators deduced from the other norms and divergences.",2022-07-31 01:26:12+00:00,Convex duality for stochastic shortest path problems in known and unknown environments,cs.LG,"['cs.LG', 'math.OC']",[arxiv.Result.Author('Kelli Francis-Staite')],"This paper studies Stochastic Shortest Path (SSP) problems in known and
unknown environments from the perspective of convex optimisation. It first
recalls results in the known parameter case, and develops understanding through
different proofs. It then focuses on the unknown parameter case, where it
studies extended value iteration (EVI) operators. This includes the existing
operators used in Rosenberg et al. [26] and Tarbouriech et al. [31] based on
the l-1 norm and supremum norm, as well as defining EVI operators corresponding
to other norms and divergences, such as the KL-divergence. This paper shows in
general how the EVI operators relate to convex programs, and the form of their
dual, where strong duality is exhibited.
  This paper then focuses on whether the bounds from finite horizon research of
Neu and Pike-Burke [21] can be applied to these extended value iteration
operators in the SSP setting. It shows that similar bounds to [21] for these
operators exist, however they lead to operators that are not in general
monotone and have more complex convergence properties. In a special case we
observe oscillating behaviour. This paper generates open questions on how
research may progress, with several examples that require further examination."
9526,"How to quantitively determine the
optimal TBM operating parameter adjustment with respect to the different rock conditions are an important
topic worth further research.","The
decision-making model developed in a single project can be applied in different rock condition and present a
valuable reference for the changing trend of the TBM operating parameter.",5.,2022-07-31 09:23:30+00:00,Intelligent decision-making method of TBM operating parameters based on multiple constraints and objective optimization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Bin Liu'), arxiv.Result.Author('Jiwen Wang'), arxiv.Result.Author('Ruirui Wang'), arxiv.Result.Author('Yaxu Wang'), arxiv.Result.Author('Guangzu Zhao')]","The decision-making of TBM operating parameters has an important guiding
significance for TBM safe and efficient construction, and it has been one of
the research hotpots in the field of TBM tunneling. For this purpose, this
paper introduces rock-breaking rules into machine learning method, and a
rock-machine mapping dual-driven by physical-rule and data-mining is
established with high accuracy. This dual-driven mappings are subsequently used
as objective function and constraints to build a decision-making method for TBM
operating parameters. By searching the revolution per minute and penetration
corresponding to the extremum of the objective function subject to the
constraints, the optimal operating parameters can be obtained. This method is
verified in the field of the Second Water Source Channel of Hangzhou, China,
resulting in the average penetration rate increased by 11.3%, and the total
cost decreased by 10.0%, which proves the practicability and effectiveness of
the developed decision-making model."
9534,There are however further research questions we would like to                        [18] Michael Gutmann and Aapo Hyvärinen.,5767–5777.,2010.,2022-07-31 18:54:52+00:00,Scrutinizing Shipment Records To Thwart Illegal Timber Trade,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Debanjan Datta'), arxiv.Result.Author('Sathappan Muthiah'), arxiv.Result.Author('John Simeone'), arxiv.Result.Author('Amelia Meadows'), arxiv.Result.Author('Naren Ramakrishnan')]","Timber and forest products made from wood, like furniture, are valuable
commodities, and like the global trade of many highly-valued natural resources,
face challenges of corruption, fraud, and illegal harvesting. These grey and
black market activities in the wood and forest products sector are not limited
to the countries where the wood was harvested, but extend throughout the global
supply chain and have been tied to illicit financial flows, like trade-based
money laundering, document fraud, species mislabeling, and other illegal
activities. The task of finding such fraudulent activities using trade data, in
the absence of ground truth, can be modelled as an unsupervised anomaly
detection problem. However existing approaches suffer from certain shortcomings
in their applicability towards large scale trade data. Trade data is
heterogeneous, with both categorical and numerical attributes in a tabular
format. The overall challenge lies in the complexity, volume and velocity of
data, with large number of entities and lack of ground truth labels. To
mitigate these, we propose a novel unsupervised anomaly detection --
Contrastive Learning based Heterogeneous Anomaly Detection (CHAD) that is
generally applicable for large-scale heterogeneous tabular data. We demonstrate
our model CHAD performs favorably against multiple comparable baselines for
public benchmark datasets, and outperforms them in the case of trade data. More
importantly we demonstrate our approach reduces assumptions and efforts
required hyperparameter tuning, which is a key challenging aspect in an
unsupervised training paradigm. Specifically, our overarching objective
pertains to detecting suspicious timber shipments and patterns using Bill of
Lading trade record data. Detecting anomalous transactions in shipment records
can enable further investigation by government agencies and supply chain
constituents."
9575,"Finally, we encourage further research into quantifying the explain-
ability of these visual attribution methods, developing benchmarks against
which new visual attribution methods can be measured to accelerate model
explainability research, and the provision of open access tumor boundary seg-
mented dataset so as to test new saliency algorithms in ground truth expert
segmented datasets.","It
provides a path toward building stakeholder trust given that healthcare re-
quires critical evaluation of assistive technologies before adoption and general
usage.","Funding

    This research received no external funding.",2022-08-01 16:05:14+00:00,What do Deep Neural Networks Learn in Medical Images?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yusuf Brima'), arxiv.Result.Author('Marcellin Atemkeng')]","Deep learning is increasingly gaining rapid adoption in healthcare to help
improve patient outcomes. This is more so in medical image analysis which
requires extensive training to gain the requisite expertise to become a trusted
practitioner. However, while deep learning techniques have continued to provide
state-of-the-art predictive performance, one of the primary challenges that
stands to hinder this progress in healthcare is the opaque nature of the
inference mechanism of these models. So, attribution has a vital role in
building confidence in stakeholders for the predictions made by deep learning
models to inform clinical decisions. This work seeks to answer the question:
what do deep neural network models learn in medical images? In that light, we
present a novel attribution framework using adaptive path-based gradient
integration techniques. Results show a promising direction of building trust in
domain experts to improve healthcare outcomes by allowing them to understand
the input-prediction correlative structures, discover new bio-markers, and
reveal potential model biases."
9588,"Although such violations can be partially resolved
through a post-hoc editing process before data are shared, this phenomenon suggests that further research
is necessary for the development of synthesis methods, such as embedding constraints of violations as a
penalty into the learning process21.","This phenomenon implies that GAN models are unable to perfectly recognize
and learn from the record-level knowledge in EHR.","Limitations
Despite the merits of this study, there are several limitations that provide opportunities for future
improvement.",2022-08-02 03:44:45+00:00,A Multifaceted Benchmarking of Synthetic Electronic Health Record Generation Models,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Chao Yan'), arxiv.Result.Author('Yao Yan'), arxiv.Result.Author('Zhiyu Wan'), arxiv.Result.Author('Ziqi Zhang'), arxiv.Result.Author('Larsson Omberg'), arxiv.Result.Author('Justin Guinney'), arxiv.Result.Author('Sean D. Mooney'), arxiv.Result.Author('Bradley A. Malin')]","Synthetic health data have the potential to mitigate privacy concerns when
sharing data to support biomedical research and the development of innovative
healthcare applications. Modern approaches for data generation based on machine
learning, generative adversarial networks (GAN) methods in particular, continue
to evolve and demonstrate remarkable potential. Yet there is a lack of a
systematic assessment framework to benchmark methods as they emerge and
determine which methods are most appropriate for which use cases. In this work,
we introduce a generalizable benchmarking framework to appraise key
characteristics of synthetic health data with respect to utility and privacy
metrics. We apply the framework to evaluate synthetic data generation methods
for electronic health records (EHRs) data from two large academic medical
centers with respect to several use cases. The results illustrate that there is
a utility-privacy tradeoff for sharing synthetic EHR data. The results further
indicate that no method is unequivocally the best on all criteria in each use
case, which makes it evident why synthetic data generation methods need to be
assessed in context."
9625,"Meaning that further research is needed in both data sets – especially from a
problem centric perspective with quantitative methods like the ones we suggest.","To further problematize, we want to point out that
meta-learning methods are not better than transfer learning as observed by [20] – as observed in our
synthetic experiments.","9
We also have theoretical results from a statistical decision perspective in the supplementary section L
that inspired this work and suggest that when the distance between tasks is zero – then the predictions
of transfer learning, meta-learning and even a ﬁxed model with no adaptation are all equivalent (with
the l2 loss).",2022-08-02 15:49:11+00:00,The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence,cs.LG,['cs.LG'],"[arxiv.Result.Author('Brando Miranda'), arxiv.Result.Author('Patrick Yu'), arxiv.Result.Author('Yu-Xiong Wang'), arxiv.Result.Author('Sanmi Koyejo')]","Recently, it has been observed that a transfer learning solution might be all
we need to solve many few-shot learning benchmarks -- thus raising important
questions about when and how meta-learning algorithms should be deployed. In
this paper, we seek to clarify these questions by 1. proposing a novel metric
-- the diversity coefficient -- to measure the diversity of tasks in a few-shot
learning benchmark and 2. by comparing Model-Agnostic Meta-Learning (MAML) and
transfer learning under fair conditions (same architecture, same optimizer, and
all models trained to convergence). Using the diversity coefficient, we show
that the popular MiniImageNet and CIFAR-FS few-shot learning benchmarks have
low diversity. This novel insight contextualizes claims that transfer learning
solutions are better than meta-learned solutions in the regime of low diversity
under a fair comparison. Specifically, we empirically find that a low diversity
coefficient correlates with a high similarity between transfer learning and
MAML learned solutions in terms of accuracy at meta-test time and
classification layer similarity (using feature based distance metrics like
SVCCA, PWCCA, CKA, and OPD). To further support our claim, we find this
meta-test accuracy holds even as the model size changes. Therefore, we conclude
that in the low diversity regime, MAML and transfer learning have equivalent
meta-test performance when both are compared fairly. We also hope our work
inspires more thoughtful constructions and quantitative evaluations of
meta-learning benchmarks in the future."
9627,"tion 5, we end up with the conclusions of our work, and
suggest lines of further research.",In the ﬁnal Sec-  types of new tasks.,"Let us consider a model with parameters θ and a parametric

2.",2022-08-02 16:19:54+00:00,Stochastic Deep Networks with Linear Competing Units for Model-Agnostic Meta-Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Konstantinos Kalais'), arxiv.Result.Author('Sotirios Chatzis')]","This work addresses meta-learning (ML) by considering deep networks with
stochastic local winner-takes-all (LWTA) activations. This type of network
units results in sparse representations from each model layer, as the units are
organized into blocks where only one unit generates a non-zero output. The main
operating principle of the introduced units rely on stochastic principles, as
the network performs posterior sampling over competing units to select the
winner. Therefore, the proposed networks are explicitly designed to extract
input data representations of sparse stochastic nature, as opposed to the
currently standard deterministic representation paradigm. Our approach produces
state-of-the-art predictive accuracy on few-shot image classification and
regression experiments, as well as reduced predictive error on an active
learning setting; these improvements come with an immensely reduced
computational cost."
9629,"We show examples on the effectiveness of the methods and hope to
encourage further research in this field.","Conclusions: This study investigates approaches to perform patient stratification analysis at scale using large EHR datasets and
multiple clustering methods for clinical research.","Keywords: Patient stratification; Electronic Health Records (EHR); Clinical evaluation; Machine learning; real-world data analysis
Introduction

Research in the life sciences has come to rely heavily on large scale digital data acquisition and analysis.",2022-08-02 17:31:03+00:00,Enabling scalable clinical interpretation of ML-based phenotypes using real world data,cs.LG,"['cs.LG', 'cs.IR', '92C50, 92C60, 68T09, 62H30', 'J.3; I.5; I.2']","[arxiv.Result.Author('Owen Parsons'), arxiv.Result.Author('Nathan E Barlow'), arxiv.Result.Author('Janie Baxter'), arxiv.Result.Author('Karen Paraschin'), arxiv.Result.Author('Andrea Derix'), arxiv.Result.Author('Peter Hein'), arxiv.Result.Author('Robert Dürichen')]","The availability of large and deep electronic healthcare records (EHR)
datasets has the potential to enable a better understanding of real-world
patient journeys, and to identify novel subgroups of patients. ML-based
aggregation of EHR data is mostly tool-driven, i.e., building on available or
newly developed methods. However, these methods, their input requirements, and,
importantly, resulting output are frequently difficult to interpret, especially
without in-depth data science or statistical training. This endangers the final
step of analysis where an actionable and clinically meaningful interpretation
is needed.This study investigates approaches to perform patient stratification
analysis at scale using large EHR datasets and multiple clustering methods for
clinical research. We have developed several tools to facilitate the clinical
evaluation and interpretation of unsupervised patient stratification results,
namely pattern screening, meta clustering, surrogate modeling, and curation.
These tools can be used at different stages within the analysis. As compared to
a standard analysis approach, we demonstrate the ability to condense results
and optimize analysis time. In the case of meta clustering, we demonstrate that
the number of patient clusters can be reduced from 72 to 3 in one example. In
another stratification result, by using surrogate models, we could quickly
identify that heart failure patients were stratified if blood sodium
measurements were available. As this is a routine measurement performed for all
patients with heart failure, this indicated a data bias. By using further
cohort and feature curation, these patients and other irrelevant features could
be removed to increase the clinical meaningfulness. These examples show the
effectiveness of the proposed methods and we hope to encourage further research
in this field."
9651,There are several promising directions for further study.,"The underlying
mechanism is characterized by a stage-coupled analysis for the state-action distribution matching.","The ﬁrst is to investigate the function approxi-
mation.",2022-08-03 08:03:33+00:00,Understanding Adversarial Imitation Learning in Small Sample Regime: A Stage-coupled Analysis,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tian Xu'), arxiv.Result.Author('Ziniu Li'), arxiv.Result.Author('Yang Yu'), arxiv.Result.Author('Zhi-Quan Luo')]","Imitation learning learns a policy from expert trajectories. While the expert
data is believed to be crucial for imitation quality, it was found that a kind
of imitation learning approach, adversarial imitation learning (AIL), can have
exceptional performance. With as little as only one expert trajectory, AIL can
match the expert performance even in a long horizon, on tasks such as
locomotion control. There are two mysterious points in this phenomenon. First,
why can AIL perform well with only a few expert trajectories? Second, why does
AIL maintain good performance despite the length of the planning horizon? In
this paper, we theoretically explore these two questions. For a
total-variation-distance-based AIL (called TV-AIL), our analysis shows a
horizon-free imitation gap $\mathcal O(\{\min\{1, \sqrt{|\mathcal S|/N} \})$ on
a class of instances abstracted from locomotion control tasks. Here $|\mathcal
S|$ is the state space size for a tabular Markov decision process, and $N$ is
the number of expert trajectories. We emphasize two important features of our
bound. First, this bound is meaningful in both small and large sample regimes.
Second, this bound suggests that the imitation gap of TV-AIL is at most 1
regardless of the planning horizon. Therefore, this bound can explain the
empirical observation. Technically, we leverage the structure of multi-stage
policy optimization in TV-AIL and present a new stage-coupled analysis via
dynamic programming"
9659,"This is as expected,    Also, further research into why CIFAR-10 does not show any
                                                               increase could be explored.",total number of evaluations performed.,"As with the Data Step method, the Data Increment method
                                                               shows that reducing the number of evaluations performed
                                                               decreases the runtime.",2022-08-03 12:22:18+00:00,Maintaining Performance with Less Data,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Dominic Sanderson'), arxiv.Result.Author('Tatiana Kalgonova')]","We propose a novel method for training a neural network for image
classification to reduce input data dynamically, in order to reduce the costs
of training a neural network model. As Deep Learning tasks become more popular,
their computational complexity increases, leading to more intricate algorithms
and models which have longer runtimes and require more input data. The result
is a greater cost on time, hardware, and environmental resources. By using data
reduction techniques, we reduce the amount of work performed, and therefore the
environmental impact of AI techniques, and with dynamic data reduction we show
that accuracy may be maintained while reducing runtime by up to 50%, and
reducing carbon emission proportionally."
9674,"In  practice,  we  default  to
suggests that the mechanisms by which both of these methods
mitigate over-smoothing are somewhat complementary - at least                                                                                𝑏
in our architecture - and warrants further study.",This observation                              feature  vectors  of  the  block  (denoted  𝐾    ).,"𝑆 = 5,𝑇 = 5, 𝐾 = 256 with number of attention heads 𝑁ℎ = 4.",2022-08-03 13:37:27+00:00,GROWN+UP: A Graph Representation Of a Webpage Network Utilizing Pre-training,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'cs.SI']","[arxiv.Result.Author('Benedict Yeoh'), arxiv.Result.Author('Huijuan Wang')]","Large pre-trained neural networks are ubiquitous and critical to the success
of many downstream tasks in natural language processing and computer vision.
However, within the field of web information retrieval, there is a stark
contrast in the lack of similarly flexible and powerful pre-trained models that
can properly parse webpages. Consequently, we believe that common machine
learning tasks like content extraction and information mining from webpages
have low-hanging gains that yet remain untapped.
  We aim to close the gap by introducing an agnostic deep graph neural network
feature extractor that can ingest webpage structures, pre-train self-supervised
on massive unlabeled data, and fine-tune to arbitrary tasks on webpages
effectually.
  Finally, we show that our pre-trained model achieves state-of-the-art results
using multiple datasets on two very different benchmarks: webpage boilerplate
removal and genre classification, thus lending support to its potential
application in diverse downstream tasks."
9675,"of our model and benchmarking implementations for reference and
                                                                       to encourage further research in this area.","We release the source code
in prior works [49, 61].","To classify at the webpage or graph level, our model with 𝑆 =
5,𝑇 = 5 produces a graph feature representation from the CLS              Moving forward, there are several clear avenues of exploration to
node at the output of the last Transformer block.",2022-08-03 13:37:27+00:00,GROWN+UP: A Graph Representation Of a Webpage Network Utilizing Pre-training,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'cs.SI']","[arxiv.Result.Author('Benedict Yeoh'), arxiv.Result.Author('Huijuan Wang')]","Large pre-trained neural networks are ubiquitous and critical to the success
of many downstream tasks in natural language processing and computer vision.
However, within the field of web information retrieval, there is a stark
contrast in the lack of similarly flexible and powerful pre-trained models that
can properly parse webpages. Consequently, we believe that common machine
learning tasks like content extraction and information mining from webpages
have low-hanging gains that yet remain untapped.
  We aim to close the gap by introducing an agnostic deep graph neural network
feature extractor that can ingest webpage structures, pre-train self-supervised
on massive unlabeled data, and fine-tune to arbitrary tasks on webpages
effectually.
  Finally, we show that our pre-trained model achieves state-of-the-art results
using multiple datasets on two very different benchmarks: webpage boilerplate
removal and genre classification, thus lending support to its potential
application in diverse downstream tasks."
9676,We choose to include the class and id attributes of a tag                            in our architecture - and warrants further study.,"The selected features are detailed in Table 1.                                      suggests that the mechanisms by which both of these methods

   The text strings, class and id attributes are cross-lingual [13]                            mitigate over-smoothing are somewhat complementary - at least
sentence vectors generated using Universal Sentence Encoder (USE)
[10, 24].","since it may contain useful semantic information [58], while the tag
type and font properties were also shown to be useful for predicting                           Crucially, we also append Transformer [71] blocks in the vein of
relevant web content [79].",2022-08-03 13:37:27+00:00,GROWN+UP: A Graph Representation Of a Webpage Network Utilizing Pre-training,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'cs.SI']","[arxiv.Result.Author('Benedict Yeoh'), arxiv.Result.Author('Huijuan Wang')]","Large pre-trained neural networks are ubiquitous and critical to the success
of many downstream tasks in natural language processing and computer vision.
However, within the field of web information retrieval, there is a stark
contrast in the lack of similarly flexible and powerful pre-trained models that
can properly parse webpages. Consequently, we believe that common machine
learning tasks like content extraction and information mining from webpages
have low-hanging gains that yet remain untapped.
  We aim to close the gap by introducing an agnostic deep graph neural network
feature extractor that can ingest webpage structures, pre-train self-supervised
on massive unlabeled data, and fine-tune to arbitrary tasks on webpages
effectually.
  Finally, we show that our pre-trained model achieves state-of-the-art results
using multiple datasets on two very different benchmarks: webpage boilerplate
removal and genre classification, thus lending support to its potential
application in diverse downstream tasks."
9677,"of our model and benchmarking implementations for reference and
                                                                        to encourage further research in this area.","We release the source code
in prior works [49, 61].","To classify at the webpage or graph level, our model with 𝑆 =
5,𝑇 = 5 produces a graph feature representation from the CLS               Moving forward, there are several clear avenues of exploration to
node at the output of the last Transformer block.",2022-08-03 13:37:27+00:00,GROWN+UP: A Graph Representation Of a Webpage Network Utilizing Pre-training,cs.LG,"['cs.LG', 'cs.AI', 'cs.IR', 'cs.SI']","[arxiv.Result.Author('Benedict Yeoh'), arxiv.Result.Author('Huijuan Wang')]","Large pre-trained neural networks are ubiquitous and critical to the success
of many downstream tasks in natural language processing and computer vision.
However, within the field of web information retrieval, there is a stark
contrast in the lack of similarly flexible and powerful pre-trained models that
can properly parse webpages. Consequently, we believe that common machine
learning tasks like content extraction and information mining from webpages
have low-hanging gains that yet remain untapped.
  We aim to close the gap by introducing an agnostic deep graph neural network
feature extractor that can ingest webpage structures, pre-train self-supervised
on massive unlabeled data, and fine-tune to arbitrary tasks on webpages
effectually.
  Finally, we show that our pre-trained model achieves state-of-the-art results
using multiple datasets on two very different benchmarks: webpage boilerplate
removal and genre classification, thus lending support to its potential
application in diverse downstream tasks."
9822,"However, further research is needed to conﬁrm
that these indeed capture ﬂow (e. g., through physiological measurements).","First, our model ﬁnds two states – “sticky” and “non-sticky” browsing –
that share similarities with the concept of ﬂow.","Second, our partner
company informed us that the vast majority of users are not “logged in” (i. e., more than 90 %),
and, hence, customer variables cannot be obtained for them.",2022-08-08 07:00:58+00:00,Detecting User Exits from Online Behavior: A Duration-Dependent Latent State Model,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Tobias Hatt'), arxiv.Result.Author('Stefan Feuerriegel')]","In order to steer e-commerce users towards making a purchase, marketers rely
upon predictions of when users exit without purchasing. Previously, such
predictions were based upon hidden Markov models (HMMs) due to their ability of
modeling latent shopping phases with different user intents. In this work, we
develop a duration-dependent hidden Markov model. In contrast to traditional
HMMs, it explicitly models the duration of latent states and thereby allows
states to become ""sticky"". The proposed model is superior to prior HMMs in
detecting user exits: out of 100 user exits without purchase, it correctly
identifies an additional 18. This helps marketers in better managing the online
behavior of e-commerce customers. The reason for the superior performance of
our model is the duration dependence, which allows our model to recover latent
states that are characterized by a distorted sense of time. We finally provide
a theoretical explanation for this, which builds upon the concept of ""flow""."
9849,"Therefore, how to select the number of              [4] H. Yoon and J. Li, “A novel positive transfer learning approach for
transferred subjects adaptively needs further study.","1-18, Aug. 2017.
development trends to each target subject in the dataset may
not be the same.","In addition,        telemonitoring of Parkinson's disease,” IEEE Trans.",2022-08-07 02:04:27+00:00,Patient-Specific Game-Based Transfer Method for Parkinson's Disease Severity Prediction,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Zaifa Xue'), arxiv.Result.Author('Huibin Lu'), arxiv.Result.Author('Tao Zhang'), arxiv.Result.Author('Max A. Little')]","Dysphonia is one of the early symptoms of Parkinson's disease (PD). Most
existing methods use feature selection methods to find the optimal subset of
voice features for all PD patients to improve the prediction performance. Few
have considered the heterogeneity between patients, which implies the need to
provide specific prediction models for different patients. However, building
this prediction model for each patient faces the challenge of small sample
size, which makes it lack generalization ability. Instance transfer is an
effective way to make up for this deficiency. Therefore, this paper proposes a
patient-specific game-based transfer (PSGT) method for PD severity prediction.
First, a selection mechanism is used to select PD patients with similar disease
trends to the target patient from the source domain, which greatly reduces the
scope of instance transfer and reduces the risk of negative transfer. Then, the
contribution of the transferred subjects and their instances to the disease
estimation of the target subject is fairly evaluated by the Shapley value,
which improves the interpretability of the method. Next, the proportion of
valid instances is determined according to the contribution of transferred
subjects, and the instances with higher contribution are transferred based on
this proportion to further reduce the difference between the transferred
instance subset and the target subject. Finally, the selected subset of
instances is added to the training set of the target subject, and the extended
data is fed into the random forest to improve the performance of the PD
severity prediction method. Parkinson's telemonitoring dataset is used to
evaluate the feasibility and effectiveness. Experiment results show that the
proposed PSGT method has better performance in both prediction error and
stability over compared methods."
9850,transferred subjects adaptively needs further study.,"6, pp.","In addition,        104-111, Jun.",2022-08-07 02:04:27+00:00,Patient-Specific Game-Based Transfer Method for Parkinson's Disease Severity Prediction,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Zaifa Xue'), arxiv.Result.Author('Huibin Lu'), arxiv.Result.Author('Tao Zhang'), arxiv.Result.Author('Max A. Little')]","Dysphonia is one of the early symptoms of Parkinson's disease (PD). Most
existing methods use feature selection methods to find the optimal subset of
voice features for all PD patients. Few have considered the heterogeneity
between patients, which implies the need to provide specific prediction models
for different patients. However, building the specific model faces the
challenge of small sample size, which makes it lack generalization ability.
Instance transfer is an effective way to solve this problem. Therefore, this
paper proposes a patient-specific game-based transfer (PSGT) method for PD
severity prediction. First, a selection mechanism is used to select PD patients
with similar disease trends to the target patient from the source domain, which
greatly reduces the risk of negative transfer. Then, the contribution of the
transferred subjects and their instances to the disease estimation of the
target subject is fairly evaluated by the Shapley value, which improves the
interpretability of the method. Next, the proportion of valid instances in the
transferred subjects is determined, and the instances with higher contribution
are transferred to further reduce the difference between the transferred
instance subset and the target subject. Finally, the selected subset of
instances is added to the training set of the target subject, and the extended
data is fed into the random forest to improve the performance of the method.
Parkinson's telemonitoring dataset is used to evaluate the feasibility and
effectiveness. Experiment results show that the PSGT has better performance in
both prediction error and stability over compared methods."
9855,"This unintuitive result must be approached with
   caution and perhaps further research.","• On balance, neither the opportunistic or systematic curation of meta-knowledge
   proves more valuable than the other.","Certainly, both automl-meta and default-meta have diﬀer-
   ing pros and cons.",2022-08-08 19:22:24+00:00,On Taking Advantage of Opportunistic Meta-knowledge to Reduce Configuration Spaces for Automated Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('David Jacob Kedziora'), arxiv.Result.Author('Tien-Dung Nguyen'), arxiv.Result.Author('Katarzyna Musial'), arxiv.Result.Author('Bogdan Gabrys')]","The automated machine learning (AutoML) process can require searching through
complex configuration spaces of not only machine learning (ML) components and
their hyperparameters but also ways of composing them together, i.e. forming ML
pipelines. Optimisation efficiency and the model accuracy attainable for a
fixed time budget suffer if this pipeline configuration space is excessively
large. A key research question is whether it is both possible and practical to
preemptively avoid costly evaluations of poorly performing ML pipelines by
leveraging their historical performance for various ML tasks, i.e.
meta-knowledge. The previous experience comes in the form of
classifier/regressor accuracy rankings derived from either (1) a substantial
but non-exhaustive number of pipeline evaluations made during historical AutoML
runs, i.e. 'opportunistic' meta-knowledge, or (2) comprehensive cross-validated
evaluations of classifiers/regressors with default hyperparameters, i.e.
'systematic' meta-knowledge. Numerous experiments with the AutoWeka4MCPS
package suggest that (1) opportunistic/systematic meta-knowledge can improve ML
outcomes, typically in line with how relevant that meta-knowledge is, and (2)
configuration-space culling is optimal when it is neither too conservative nor
too radical. However, the utility and impact of meta-knowledge depend
critically on numerous facets of its generation and exploitation, warranting
extensive analysis; these are often overlooked/underappreciated within AutoML
and meta-learning literature. In particular, we observe strong sensitivity to
the `challenge' of a dataset, i.e. whether specificity in choosing a predictor
leads to significantly better performance. Ultimately, identifying `difficult'
datasets, thus defined, is crucial to both generating informative
meta-knowledge bases and understanding optimal search-space reduction
strategies."
9859,"results show the overall superiority in performance for the        This motivates further study of new structurally consistent
CNN-based classiﬁer.",Our     vectors increases the performance of structure identiﬁcation.,Figs.,2022-08-08 20:32:28+00:00,Recovering the Graph Underlying Networked Dynamical Systems under Partial Observability: A Deep Learning Approach,cs.LG,"['cs.LG', 'cs.MA', 'stat.ME', '62D20, 93B30', 'I.2.m; G.3']","[arxiv.Result.Author('Sérgio Machado'), arxiv.Result.Author('Anirudh Sridhar'), arxiv.Result.Author('Paulo Gil'), arxiv.Result.Author('Jorge Henriques'), arxiv.Result.Author('José M. F. Moura'), arxiv.Result.Author('Augusto Santos')]","We study the problem of graph structure identification, i.e., of recovering
the graph of dependencies among time series. We model these time series data as
components of the state of linear stochastic networked dynamical systems. We
assume partial observability, where the state evolution of only a subset of
nodes comprising the network is observed. We devise a new feature vector
computed from the observed time series and prove that these features are
linearly separable, i.e., there exists a hyperplane that separates the cluster
of features associated with connected pairs of nodes from those associated with
disconnected pairs. This renders the features amenable to train a variety of
classifiers to perform causal inference. In particular, we use these features
to train Convolutional Neural Networks (CNNs). The resulting causal inference
mechanism outperforms state-of-the-art counterparts w.r.t. sample-complexity.
The trained CNNs generalize well over structurally distinct networks (dense or
sparse) and noise-level profiles. Remarkably, they also generalize well to
real-world networks while trained over a synthetic network (realization of a
random graph). Finally, the proposed method consistently reconstructs the graph
in a pairwise manner, that is, by deciding if an edge or arrow is present or
absent in each pair of nodes, from the corresponding time series of each pair.
This fits the framework of large-scale systems, where observation or processing
of all nodes in the network is prohibitive."
9860,"This motivates further study of new        Annals of Statistics, 40(4): 1935–1967.","The
structure identiﬁcation.","structurally consistent matrix-valued estimators as building
blocks for feature vectors or tensor-valued estimators.",2022-08-08 20:32:28+00:00,Recovering the Graph Underlying Networked Dynamical Systems under Partial Observability: A Deep Learning Approach,cs.LG,"['cs.LG', 'cs.MA', 'stat.ME', '62D20, 93B30', 'I.2.m; G.3']","[arxiv.Result.Author('Sérgio Machado'), arxiv.Result.Author('Anirudh Sridhar'), arxiv.Result.Author('Paulo Gil'), arxiv.Result.Author('Jorge Henriques'), arxiv.Result.Author('José M. F. Moura'), arxiv.Result.Author('Augusto Santos')]","We study the problem of graph structure identification, i.e., of recovering
the graph of dependencies among time series. We model these time series data as
components of the state of linear stochastic networked dynamical systems. We
assume partial observability, where the state evolution of only a subset of
nodes comprising the network is observed. We devise a new feature vector
computed from the observed time series and prove that these features are
linearly separable, i.e., there exists a hyperplane that separates the cluster
of features associated with connected pairs of nodes from those associated with
disconnected pairs. This renders the features amenable to train a variety of
classifiers to perform causal inference. In particular, we use these features
to train Convolutional Neural Networks (CNNs). The resulting causal inference
mechanism outperforms state-of-the-art counterparts w.r.t. sample-complexity.
The trained CNNs generalize well over structurally distinct networks (dense or
sparse) and noise-level profiles. Remarkably, they also generalize well to
real-world networks while trained over a synthetic network (realization of a
random graph). Finally, the proposed method consistently reconstructs the graph
in a pairwise manner, that is, by deciding if an edge or arrow is present or
absent in each pair of nodes, from the corresponding time series of each pair.
This fits the framework of large-scale systems, where observation or processing
of all nodes in the network is prohibitive."
9897,"Further, our empirical work demonstrates that neural networks with
the Hat activation function train signiﬁcantly more quickly than ReLU networks, which motivates
further study into the use of the Hat activation.","The advantage of our
approach is that our analysis holds for networks of arbitrary width and shows how the spectral bias
increases with increasing width.","3 Spectral Bias of Neural Networks

The spectral bias of neural networks has been well-established in prior works.",2022-08-09 17:40:57+00:00,On the Activation Function Dependence of the Spectral Bias of Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Qingguo Hong'), arxiv.Result.Author('Qinyang Tan'), arxiv.Result.Author('Jonathan W. Siegel'), arxiv.Result.Author('Jinchao Xu')]","Neural networks are universal function approximators which are known to
generalize well despite being dramatically overparameterized. We study this
phenomenon from the point of view of the spectral bias of neural networks. Our
contributions are two-fold. First, we provide a theoretical explanation for the
spectral bias of ReLU neural networks by leveraging connections with the theory
of finite element methods. Second, based upon this theory we predict that
switching the activation function to a piecewise linear B-spline, namely the
Hat function, will remove this spectral bias, which we verify empirically in a
variety of settings. Our empirical studies also show that neural networks with
the Hat activation function are trained significantly faster using stochastic
gradient descent and ADAM. Combined with previous work showing that the Hat
activation function also improves generalization accuracy on image
classification tasks, this indicates that using the Hat activation provides
significant advantages over the ReLU on certain problems."
9898,"Further, our empirical work demonstrates that neural networks with
the Hat activation function train signiﬁcantly more quickly than ReLU networks, which motivates
further study into the use of the Hat activation.","The advantage of our
approach is that our analysis holds for networks of arbitrary width and shows how the spectral bias
increases with increasing width.","3 Spectral Bias of Neural Networks

The spectral bias of neural networks has been well-established in prior works.",2022-08-09 17:40:57+00:00,On the Activation Function Dependence of the Spectral Bias of Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Qingguo Hong'), arxiv.Result.Author('Qinyang Tan'), arxiv.Result.Author('Jonathan W. Siegel'), arxiv.Result.Author('Jinchao Xu')]","Neural networks are universal function approximators which are known to
generalize well despite being dramatically overparameterized. We study this
phenomenon from the point of view of the spectral bias of neural networks. Our
contributions are two-fold. First, we provide a theoretical explanation for the
spectral bias of ReLU neural networks by leveraging connections with the theory
of finite element methods. Second, based upon this theory we predict that
switching the activation function to a piecewise linear B-spline, namely the
Hat function, will remove this spectral bias, which we verify empirically in a
variety of settings. Our empirical studies also show that neural networks with
the Hat activation function are trained significantly faster using stochastic
gradient descent and ADAM. Combined with previous work showing that the Hat
activation function also improves generalization accuracy on image
classification tasks, this indicates that using the Hat activation provides
significant advantages over the ReLU on certain problems."
9899,"Further, our empirical work demonstrates that neural networks with
the Hat activation function train signiﬁcantly more quickly than ReLU networks, which motivates
further study into the use of the Hat activation.","The advantage of our
approach is that our analysis holds for networks of arbitrary width and shows how the spectral bias
increases with increasing width.","3 Spectral Bias of Neural Networks

The spectral bias of neural networks has been well-established in prior works.",2022-08-09 17:40:57+00:00,On the Activation Function Dependence of the Spectral Bias of Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Qingguo Hong'), arxiv.Result.Author('Jonathan W. Siegel'), arxiv.Result.Author('Qinyang Tan'), arxiv.Result.Author('Jinchao Xu')]","Neural networks are universal function approximators which are known to
generalize well despite being dramatically overparameterized. We study this
phenomenon from the point of view of the spectral bias of neural networks. Our
contributions are two-fold. First, we provide a theoretical explanation for the
spectral bias of ReLU neural networks by leveraging connections with the theory
of finite element methods. Second, based upon this theory we predict that
switching the activation function to a piecewise linear B-spline, namely the
Hat function, will remove this spectral bias, which we verify empirically in a
variety of settings. Our empirical studies also show that neural networks with
the Hat activation function are trained significantly faster using stochastic
gradient descent and ADAM. Combined with previous work showing that the Hat
activation function also improves generalization accuracy on image
classification tasks, this indicates that using the Hat activation provides
significant advantages over the ReLU on certain problems."
9917,"Results from the former
                                        dataset showcase our system’s ability to detect degradation, while results
                                        from the latter point to directions for further research within the area.","We test the ModularHI
                                        on two open datasets, CMAPSS and N-CMAPSS.","The
                                        results shows that our novel approach is able to detect system degradation
                                        without historical data.",2022-08-10 08:12:43+00:00,A data-driven modular architecture with denoising autoencoders for health indicator construction in a manufacturing process,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Emil Blixt Hansen'), arxiv.Result.Author('Helge Langseth'), arxiv.Result.Author('Nadeem Iftikhar'), arxiv.Result.Author('Simon Bøgh')]","Within the field of prognostics and health management (PHM), health
indicators (HI) can be used to aid the production and, e.g. schedule
maintenance and avoid failures. However, HI is often engineered to a specific
process and typically requires large amounts of historical data for set-up.
This is especially a challenge for SMEs, which often lack sufficient resources
and knowledge to benefit from PHM. In this paper, we propose ModularHI, a
modular approach in the construction of HI for a system without historical
data. With ModularHI, the operator chooses which sensor inputs are available,
and then ModularHI will compute a baseline model based on data collected during
a burn-in state. This baseline model will then be used to detect if the system
starts to degrade over time. We test the ModularHI on two open datasets, CMAPSS
and N-CMAPSS. Results from the former dataset showcase our system's ability to
detect degradation, while results from the latter point to directions for
further research within the area. The results shows that our novel approach is
able to detect system degradation without historical data."
9936,"[8] S. Mohammadinejad, J. V. Deshmukh, A. G. Puranic, M. Vazquez-
   The fact that different formulas can yield the same MCR                  Chanlatte, and A. Donze´, “Interpretable Classiﬁcation of Time-Series
highlights that further research is required to discover more               Data Using Efﬁcient Enumerative Techniques,” in Proceedings of the
appropriate metrics.","Otherwise, a FERNN model may be more suitable.","In our qualitative examinations, we looked             23rd International Conference on Hybrid Systems: Computation and
at tightness of bounds, simplicity of the formula in terms                  Control, ser.",2022-08-10 16:52:23+00:00,Differentiable Inference of Temporal Logic Formulas,cs.LG,"['cs.LG', 'cs.AI', 'cs.LO', 'cs.SE']","[arxiv.Result.Author('Nicole Fronda'), arxiv.Result.Author('Houssam Abbas')]","We demonstrate the first Recurrent Neural Network architecture for learning
Signal Temporal Logic formulas, and present the first systematic comparison of
formula inference methods. Legacy systems embed much expert knowledge which is
not explicitly formalized. There is great interest in learning formal
specifications that characterize the ideal behavior of such systems -- that is,
formulas in temporal logic that are satisfied by the system's output signals.
Such specifications can be used to better understand the system's behavior and
improve design of its next iteration. Previous inference methods either assumed
certain formula templates, or did a heuristic enumeration of all possible
templates. This work proposes a neural network architecture that infers the
formula structure via gradient descent, eliminating the need for imposing any
specific templates. It combines learning of formula structure and parameters in
one optimization. Through systematic comparison, we demonstrate that this
method achieves similar or better mis-classification rates (MCR) than
enumerative and lattice methods. We also observe that different formulas can
achieve similar MCR, empirically demonstrating the under-determinism of the
problem of temporal logic inference."
9944,"Overall our results demonstrate that simply gathering a large amount of data from
                                                the web is not the most eﬀective way to build a pre-training dataset for robust generalization,
                                                necessitating further study into dataset design.","In addition, our theoretical model provides a candidate explanation
                                                for the success of the CLIP-based data ﬁltering technique recently employed in the LAION
                                                dataset.","1 Introduction

                                        Large models pre-trained on web-scale datasets are becoming a cornerstone of machine learning.",2022-08-10 18:24:23+00:00,Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Thao Nguyen'), arxiv.Result.Author('Gabriel Ilharco'), arxiv.Result.Author('Mitchell Wortsman'), arxiv.Result.Author('Sewoong Oh'), arxiv.Result.Author('Ludwig Schmidt')]","Web-crawled datasets have enabled remarkable generalization capabilities in
recent image-text models such as CLIP (Contrastive Language-Image pre-training)
or Flamingo, but little is known about the dataset creation processes. In this
work, we introduce a testbed of six publicly available data sources - YFCC,
LAION, Conceptual Captions, WIT, RedCaps, Shutterstock - to investigate how
pre-training distributions induce robustness in CLIP. We find that the
performance of the pre-training data varies substantially across distribution
shifts, with no single data source dominating. Moreover, we systematically
study the interactions between these data sources and find that combining
multiple sources does not necessarily yield better models, but rather dilutes
the robustness of the best individual data source. We complement our empirical
findings with theoretical insights from a simple setting, where combining the
training data also results in diluted robustness. In addition, our theoretical
model provides a candidate explanation for the success of the CLIP-based data
filtering technique recently employed in the LAION dataset. Overall our results
demonstrate that simply gathering a large amount of data from the web is not
the most effective way to build a pre-training dataset for robust
generalization, necessitating further study into dataset design."
9945,"Overall our results demonstrate that simply gathering a large
                                                amount of data from the web is not the most eﬀective way to build a pre-training dataset
                                                for robust generalization, necessitating further study into dataset design.","In addition, our theoretical model provides
                                                a candidate explanation for the success of the CLIP-based data ﬁltering technique recently
                                                employed in the LAION dataset.","Code is available at
                                                https://github.com/mlfoundations/clip_quality_not_quantity.",2022-08-10 18:24:23+00:00,Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Thao Nguyen'), arxiv.Result.Author('Gabriel Ilharco'), arxiv.Result.Author('Mitchell Wortsman'), arxiv.Result.Author('Sewoong Oh'), arxiv.Result.Author('Ludwig Schmidt')]","Web-crawled datasets have enabled remarkable generalization capabilities in
recent image-text models such as CLIP (Contrastive Language-Image pre-training)
or Flamingo, but little is known about the dataset creation processes. In this
work, we introduce a testbed of six publicly available data sources - YFCC,
LAION, Conceptual Captions, WIT, RedCaps, Shutterstock - to investigate how
pre-training distributions induce robustness in CLIP. We find that the
performance of the pre-training data varies substantially across distribution
shifts, with no single data source dominating. Moreover, we systematically
study the interactions between these data sources and find that combining
multiple sources does not necessarily yield better models, but rather dilutes
the robustness of the best individual data source. We complement our empirical
findings with theoretical insights from a simple setting, where combining the
training data also results in diluted robustness. In addition, our theoretical
model provides a candidate explanation for the success of the CLIP-based data
filtering technique recently employed in the LAION dataset. Overall our results
demonstrate that simply gathering a large amount of data from the web is not
the most effective way to build a pre-training dataset for robust
generalization, necessitating further study into dataset design. Code is
available at https://github.com/mlfoundations/clip_quality_not_quantity."
9946,"Overall our results demonstrate that simply gathering a large
                                                amount of data from the web is not the most eﬀective way to build a pre-training dataset
                                                for robust generalization, necessitating further study into dataset design.","In addition, our theoretical model provides
                                                a candidate explanation for the success of the CLIP-based data ﬁltering technique recently
                                                employed in the LAION dataset.","Code is available at
                                                https://github.com/mlfoundations/clip_quality_not_quantity.",2022-08-10 18:24:23+00:00,Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Thao Nguyen'), arxiv.Result.Author('Gabriel Ilharco'), arxiv.Result.Author('Mitchell Wortsman'), arxiv.Result.Author('Sewoong Oh'), arxiv.Result.Author('Ludwig Schmidt')]","Web-crawled datasets have enabled remarkable generalization capabilities in
recent image-text models such as CLIP (Contrastive Language-Image pre-training)
or Flamingo, but little is known about the dataset creation processes. In this
work, we introduce a testbed of six publicly available data sources - YFCC,
LAION, Conceptual Captions, WIT, RedCaps, Shutterstock - to investigate how
pre-training distributions induce robustness in CLIP. We find that the
performance of the pre-training data varies substantially across distribution
shifts, with no single data source dominating. Moreover, we systematically
study the interactions between these data sources and find that combining
multiple sources does not necessarily yield better models, but rather dilutes
the robustness of the best individual data source. We complement our empirical
findings with theoretical insights from a simple setting, where combining the
training data also results in diluted robustness. In addition, our theoretical
model provides a candidate explanation for the success of the CLIP-based data
filtering technique recently employed in the LAION dataset. Overall our results
demonstrate that simply gathering a large amount of data from the web is not
the most effective way to build a pre-training dataset for robust
generalization, necessitating further study into dataset design. Code is
available at https://github.com/mlfoundations/clip_quality_not_quantity."
9962,"We
present the results of our initial experimental implementations of this idea and discuss related research in this
domain which may offer direction for further research.","the input data for each of the tasks might contain missing values, the scale and resolution of the values is not
consistent across tasks and the data contains non-independent and identically distributed (non-IID) instances.","Keywords: Multi-task metric learning, Fine-grained change detection, Regression on uncertain data.",2022-08-11 12:57:11+00:00,Regressing Relative Fine-Grained Change for Sub-Groups in Unreliable Heterogeneous Data Through Deep Multi-Task Metric Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author(""Niall O' Mahony""), arxiv.Result.Author('Sean Campbell'), arxiv.Result.Author('Lenka Krpalkova'), arxiv.Result.Author('Joseph Walsh'), arxiv.Result.Author('Daniel Riordan')]","Fine-Grained Change Detection and Regression Analysis are essential in many
applications of ArtificialIntelligence. In practice, this task is often
challenging owing to the lack of reliable ground truth information
andcomplexity arising from interactions between the many underlying factors
affecting a system. Therefore,developing a framework which can represent the
relatedness and reliability of multiple sources of informationbecomes critical.
In this paper, we investigate how techniques in multi-task metric learning can
be applied for theregression of fine-grained change in real data.The key idea
is that if we incorporate the incremental change in a metric of interest
between specific instancesof an individual object as one of the tasks in a
multi-task metric learning framework, then interpreting thatdimension will
allow the user to be alerted to fine-grained change invariant to what the
overall metric isgeneralised to be. The techniques investigated are
specifically tailored for handling heterogeneous data sources,i.e. the input
data for each of the tasks might contain missing values, the scale and
resolution of the values is notconsistent across tasks and the data contains
non-independent and identically distributed (non-IID) instances. Wepresent the
results of our initial experimental implementations of this idea and discuss
related research in thisdomain which may offer direction for further research."
9971,"Given that
detailed, “under-the-hood” analyses of these models are possible, the existence and nature of this
compromise seems like a promising area for further study.","We know, for example, that the
regression coeﬃcients in a tensor network model are necessarily coupled together by the elements
of the component tensors, which may force the model to use suboptimal coeﬃcients for the lower
interaction degrees in order to avoid harmful contributions from the higher degrees.","Taken together, the results discussed here support the following two conclusions:

   1.",2022-08-11 20:17:27+00:00,Interaction Decompositions for Tensor Network Regression,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ian Convy'), arxiv.Result.Author('K. Birgitta Whaley')]","It is well known that tensor network regression models operate on an
exponentially large feature space, but questions remain as to how effectively
they are able to utilize this space. Using the polynomial featurization from
Novikov et al., we propose the interaction decomposition as a tool that can
assess the relative importance of different regressors as a function of their
polynomial degree. We apply this decomposition to tensor ring and tree tensor
network models trained on the MNIST and Fashion MNIST datasets, and find that
up to 75% of interaction degrees are contributing meaningfully to these models.
We also introduce a new type of tensor network model that is explicitly trained
on only a small subset of interaction degrees, and find that these models are
able to match or even outperform the full models using only a fraction of the
exponential feature space. This suggests that standard tensor network models
utilize their polynomial regressors in an inefficient manner, with the lower
degree terms being vastly under-utilized."
10028,"Our
                                        experiments also show that ASAM could improve the generalization          max LS(w + ) − LS(w)                          (1)
                                        performance on un-normalized data, but further research is needed to             2≤ρ
                                        conﬁrm this.","More
                                        experiments show that sharpness aware-based optimization techniques       speciﬁcally, [3] deﬁnes the sharpness of loss function as Equation 1.
                                        could help to provide models with strong generalization ability.","And the sharpness-aware minimization can be deﬁned as the
                                           Index Terms—Machine Learning, Optimization, Loss sharpness, SAM,       following min-max optimization:
                                        ASAM.",2022-08-14 20:50:17+00:00,Model Generalization: A Sharpness Aware Optimization Perspective,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jozef Marus Coldenhoff'), arxiv.Result.Author('Chengkun Li'), arxiv.Result.Author('Yurui Zhu')]","Sharpness-Aware Minimization (SAM) and adaptive sharpness-aware minimization
(ASAM) aim to improve the model generalization. And in this project, we
proposed three experiments to valid their generalization from the sharpness
aware perspective. And our experiments show that sharpness aware-based
optimization techniques could help to provide models with strong generalization
ability. Our experiments also show that ASAM could improve the generalization
performance on un-normalized data, but further research is needed to confirm
this."
10029,"We aim to encourage further research
in knowledge transfer for achieving increasingly knowledgeable continual learning systems.","8 DISCUSSION AND CONCLUSION

We proposed a theory for knowledge transfer in supervised continual learning.","Our proposed framework
relies on the assumption of relatedness among tasks in a speciﬁc environment.",2022-08-14 22:28:26+00:00,A Theory for Knowledge Transfer in Continual Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Diana Benavides-Prado'), arxiv.Result.Author('Patricia Riddle')]","Continual learning of a stream of tasks is an active area in deep neural
networks. The main challenge investigated has been the phenomenon of
catastrophic forgetting or interference of newly acquired knowledge with
knowledge from previous tasks. Recent work has investigated forward knowledge
transfer to new tasks. Backward transfer for improving knowledge gained during
previous tasks has received much less attention. There is in general limited
understanding of how knowledge transfer could aid tasks learned continually. We
present a theory for knowledge transfer in continual supervised learning, which
considers both forward and backward transfer. We aim at understanding their
impact for increasingly knowledgeable learners. We derive error bounds for each
of these transfer mechanisms. These bounds are agnostic to specific
implementations (e.g. deep neural networks). We demonstrate that, for a
continual learner that observes related tasks, both forward and backward
transfer can contribute to an increasing performance as more tasks are
observed."
10031,"These
results are lower than on the training set (MAE of 6.05 and R2 of 0.89), which is
expected in machine learning – but this gap could likely be closed with further research.","Considering that sentences range from 0 to over
ten years, an error of under one year for a first exploratory study is encouraging.","Having said that, it is impossible to expect error value closing to 0, because we operate
on the set of decisions that are already inconsistent.",2022-08-15 02:52:18+00:00,Explainable Artificial Intelligence for Assault Sentence Prediction in New Zealand,cs.LG,"['cs.LG', 'cs.CL', 'cs.CY', 'cs.NE']","[arxiv.Result.Author('Harry Rodger'), arxiv.Result.Author('Andrew Lensen'), arxiv.Result.Author('Marcin Betkier')]","The judiciary has historically been conservative in its use of Artificial
Intelligence, but recent advances in machine learning have prompted scholars to
reconsider such use in tasks like sentence prediction. This paper investigates
by experimentation the potential use of explainable artificial intelligence for
predicting imprisonment sentences in assault cases in New Zealand's courts. We
propose a proof-of-concept explainable model and verify in practice that it is
fit for purpose, with predicted sentences accurate to within one year. We
further analyse the model to understand the most influential phrases in
sentence length prediction. We conclude the paper with an evaluative discussion
of the future benefits and risks of different ways of using such an AI model in
New Zealand's courts."
10033,"The most interesting or useful property of autoencoders is the lower-dimensional
representation of higher-dimensional data, which needs further study on more speciﬁc
problems.","More solutions should be explored under this framework, until the one that
the training method reaches is found or veriﬁed.","Our explanation of denoising autoencoders and variational autoencoders is
only a general framework.",2022-08-15 03:51:40+00:00,On a Mechanism Framework of Autoencoders,cs.LG,"['cs.LG', 'cs.CV', '68T07', 'I.2.6']",[arxiv.Result.Author('Changcun Huang')],"This paper proposes a theoretical framework on the mechanism of autoencoders.
To the encoder part, under the main use of dimensionality reduction, we
investigate its two fundamental properties: bijective maps and data
disentangling. The general construction methods of an encoder that satisfies
either or both of the above two properties are given. To the decoder part, as a
consequence of the encoder constructions, we present a new basic principle of
the solution, without using affine transforms. The generalization mechanism of
autoencoders is modeled. The results of ReLU autoencoders are generalized to
some non-ReLU cases, particularly for the sigmoid-unit autoencoder. Based on
the theoretical framework above, we explain some experimental results of
variational autoencoders, denoising autoencoders, and linear-unit autoencoders,
with emphasis on the interpretation of the lower-dimensional representation of
data via encoders; and the mechanism of image restoration through autoencoders
is natural to be understood by those explanations. Compared to PCA and decision
trees, the advantages of (generalized) autoencoders on dimensionality reduction
and classification are demonstrated, respectively. Convolutional neural
networks and randomly weighted neural networks are also interpreted by this
framework."
10034,"The most interesting or useful property of autoencoders is the lower-dimensional
representation of higher-dimensional data, which needs further study on more speciﬁc
problems.","More solutions should be explored under this framework, until the one that
the training method reaches is found or veriﬁed.","Our explanation of denoising autoencoders and variational autoencoders is
only a general framework.",2022-08-15 03:51:40+00:00,On a Mechanism Framework of Autoencoders,cs.LG,"['cs.LG', 'cs.CV', '68T07', 'I.2.6']",[arxiv.Result.Author('Changcun Huang')],"This paper proposes a theoretical framework on the mechanism of autoencoders.
To the encoder part, under the main use of dimensionality reduction, we
investigate its two fundamental properties: bijective maps and data
disentangling. The general construction methods of an encoder that satisfies
either or both of the above two properties are given. To the decoder part, as a
consequence of the encoder constructions, we present a new basic principle of
the solution, without using affine transforms. The generalization mechanism of
autoencoders is modeled. The results of ReLU autoencoders are generalized to
some non-ReLU cases, particularly for the sigmoid-unit autoencoder. Based on
the theoretical framework above, we explain some experimental results of
variational autoencoders, denoising autoencoders, and linear-unit autoencoders,
with emphasis on the interpretation of the lower-dimensional representation of
data via encoders; and the mechanism of image restoration through autoencoders
is natural to be understood by those explanations. Compared to PCA and decision
trees, the advantages of (generalized) autoencoders on dimensionality reduction
and classification are demonstrated, respectively. Convolutional neural
networks and randomly weighted neural networks are also interpreted by this
framework."
10035,"The most interesting or useful property of autoencoders is the lower-dimensional
representation of higher-dimensional data, which needs further study on more speciﬁc
problems.","More solutions should be explored under this framework, until the one that
the training method reaches is found or veriﬁed.","Our explanation of denoising autoencoders and variational autoencoders is
only a general framework.",2022-08-15 03:51:40+00:00,On a Mechanism Framework of Autoencoders,cs.LG,"['cs.LG', 'cs.CV', '68T07', 'I.2.6']",[arxiv.Result.Author('Changcun Huang')],"This paper proposes a theoretical framework on the mechanism of autoencoders.
To the encoder part, under the main use of dimensionality reduction, we
investigate its two fundamental properties: bijective maps and data
disentangling. The general construction methods of an encoder that satisfies
either or both of the above two properties are given. The generalization
mechanism of autoencoders is modeled. Based on the theoretical framework above,
we explain some experimental results of variational autoencoders, denoising
autoencoders, and linear-unit autoencoders, with emphasis on the interpretation
of the lower-dimensional representation of data via encoders; and the mechanism
of image restoration through autoencoders is natural to be understood by those
explanations. Compared to PCA and decision trees, the advantages of
(generalized) autoencoders on dimensionality reduction and classification are
demonstrated, respectively. Convolutional neural networks and randomly weighted
neural networks are also interpreted by this framework."
10036,"The most interesting or useful property of autoencoders is the lower-dimensional
representation of higher-dimensional data, which needs further study on more speciﬁc
problems.","More solutions should be explored under this framework, until the one that
the training method reaches is found or veriﬁed.","Our explanation of denoising autoencoders and variational autoencoders is
only a general framework.",2022-08-15 03:51:40+00:00,On a Mechanism Framework of Autoencoders,cs.LG,"['cs.LG', 'cs.CV', '68T07', 'I.2.6']",[arxiv.Result.Author('Changcun Huang')],"This paper proposes a theoretical framework on the mechanism of autoencoders.
To the encoder part, under the main use of dimensionality reduction, we
investigate its two fundamental properties: bijective maps and data
disentangling. The general construction methods of an encoder that satisfies
either or both of the above two properties are given. The generalization
mechanism of autoencoders is modeled. Based on the theoretical framework above,
we explain some experimental results of variational autoencoders, denoising
autoencoders, and linear-unit autoencoders, with emphasis on the interpretation
of the lower-dimensional representation of data via encoders; and the mechanism
of image restoration through autoencoders is natural to be understood by those
explanations. Compared to PCA and decision trees, the advantages of
(generalized) autoencoders on dimensionality reduction and classification are
demonstrated, respectively. Convolutional neural networks and randomly weighted
neural networks are also interpreted by this framework."
10053,"Tiny ImageNet is a            structured errors without sacrificing generalizability remains to be
smaller version of the ImageNet challenge [6] in which the num-        answered by further research.","Whether the AGN model can be adapted to capture more
more complex Tiny ImageNet dataset [18].","Despite this, we show that the trade-
ber of classes is reduced from 1000 to 200.",2022-08-15 15:17:34+00:00,Combining Gradients and Probabilities for Heterogeneous Approximation of Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Elias Trommer'), arxiv.Result.Author('Bernd Waschneck'), arxiv.Result.Author('Akash Kumar')]","This work explores the search for heterogeneous approximate multiplier
configurations for neural networks that produce high accuracy and low energy
consumption. We discuss the validity of additive Gaussian noise added to
accurate neural network computations as a surrogate model for behavioral
simulation of approximate multipliers. The continuous and differentiable
properties of the solution space spanned by the additive Gaussian noise model
are used as a heuristic that generates meaningful estimates of layer robustness
without the need for combinatorial optimization techniques. Instead, the amount
of noise injected into the accurate computations is learned during network
training using backpropagation. A probabilistic model of the multiplier error
is presented to bridge the gap between the domains; the model estimates the
standard deviation of the approximate multiplier error, connecting solutions in
the additive Gaussian noise space to actual hardware instances. Our experiments
show that the combination of heterogeneous approximation and neural network
retraining reduces the energy consumption for multiplications by 70% to 79% for
different ResNet variants on the CIFAR-10 dataset with a Top-1 accuracy loss
below one percentage point. For the more complex Tiny ImageNet task, our VGG16
model achieves a 53 % reduction in energy consumption with a drop in Top-5
accuracy of 0.5 percentage points. We further demonstrate that our error model
can predict the parameters of an approximate multiplier in the context of the
commonly used additive Gaussian noise (AGN) model with high accuracy. Our
software implementation is available under
https://github.com/etrommer/agn-approx."
10140,"The results of this analysis suggest
                                                                           that machine learning has the potential to predict OFC outcomes and reveal relevant clinical
                                                                           factors for further study.","Model interpretation via
                                                                           SHapley Additive exPlanations (SHAP) indicate that speciﬁc IgE, along with wheal and ﬂare
                                                                           values from SPTs, are highly predictive of OFC outcomes.",1.,2022-08-17 12:56:37+00:00,Prediction of Oral Food Challenges via Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Justin Zhang'), arxiv.Result.Author('Deborah Lee'), arxiv.Result.Author('Kylie Jungles'), arxiv.Result.Author('Diane Shaltis'), arxiv.Result.Author('Kayvan Najarian'), arxiv.Result.Author('Rajan Ravikumar'), arxiv.Result.Author('Georgiana Sanders'), arxiv.Result.Author('Jonathan Gryak')]","Oral Food Challenges (OFCs) are essential to accurately diagnosing food
allergy in patients. However, patients are hesitant to undergo OFCs, and for
those that do, there is limited access to allergists in rural/community
healthcare settings. The prediction of OFC outcomes through machine learning
methods can facilitate the de-labeling of food allergens at home, improve
patient and physician comfort during OFCs, and economize medical resources by
minimizing the number of OFCs performed. Clinical data was gathered from 1,112
patients who collectively underwent a total of 1,284 OFCs, and consisted of
clinical factors including serum specific IgE, total IgE, skin prick tests
(SPTs), symptoms, sex, and age. Using these clinical features, machine learning
models were constructed to predict outcomes for peanut, egg, and milk
challenge. The best performing model for each allergen was created using the
Learning Using Concave and Convex Kernels (LUCCK) method, which achieved an
Area under the Curve (AUC) for peanut, egg, and milk OFC prediction of 0.76,
0.68, and 0.70, respectively. Model interpretation via SHapley Additive
exPlanations (SHAP) indicate that specific IgE, along with wheal and flare
values from SPTs, are highly predictive of OFC outcomes. The results of this
analysis suggest that machine learning has the potential to predict OFC
outcomes and reveal relevant clinical factors for further study."
10141,"The
results of this analysis suggest that machine learning has the potential to predict OFC outcomes and reveal relevant
clinical factors for further study.","Model interpretation via SHapley Additive exPlanations (SHAP)
indicate that speciﬁc IgE, along with wheal and ﬂare values from SPTs, are highly predictive of OFC outcomes.",2.,2022-08-17 12:56:37+00:00,Prediction of Oral Food Challenges via Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Justin Zhang'), arxiv.Result.Author('Deborah Lee'), arxiv.Result.Author('Kylie Jungles'), arxiv.Result.Author('Diane Shaltis'), arxiv.Result.Author('Kayvan Najarian'), arxiv.Result.Author('Rajan Ravikumar'), arxiv.Result.Author('Georgiana Sanders'), arxiv.Result.Author('Jonathan Gryak')]","Oral Food Challenges (OFCs) are essential to accurately diagnosing food
allergy in patients. However, patients are hesitant to undergo OFCs, and for
those that do, there is limited access to allergists in rural/community
healthcare settings. The prediction of OFC outcomes through machine learning
methods can facilitate the de-labeling of food allergens at home, improve
patient and physician comfort during OFCs, and economize medical resources by
minimizing the number of OFCs performed. Clinical data was gathered from 1,112
patients who collectively underwent a total of 1,284 OFCs, and consisted of
clinical factors including serum specific IgE, total IgE, skin prick tests
(SPTs), symptoms, sex, and age. Using these clinical features, machine learning
models were constructed to predict outcomes for peanut, egg, and milk
challenge. The best performing model for each allergen was created using the
Learning Using Concave and Convex Kernels (LUCCK) method, which achieved an
Area under the Curve (AUC) for peanut, egg, and milk OFC prediction of 0.76,
0.68, and 0.70, respectively. Model interpretation via SHapley Additive
exPlanations (SHAP) indicate that specific IgE, along with wheal and flare
values from SPTs, are highly predictive of OFC outcomes. The results of this
analysis suggest that machine learning has the potential to predict OFC
outcomes and reveal relevant clinical factors for further study."
10142,"The results of this analysis suggest that
machine learning has the potential to predict OFC outcomes and reveal relevant clinical factors for further study.","Model interpretation via SHapley Additive exPlanations (SHAP) indicate that speciﬁc IgE, along with
wheal and ﬂare values from SPTs, are highly predictive of OFC outcomes.","In
future work, other inﬂuential features from data modalities such as genomic data and other immunologic measures will
be extracted.",2022-08-17 12:56:37+00:00,Prediction of Oral Food Challenges via Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Justin Zhang'), arxiv.Result.Author('Deborah Lee'), arxiv.Result.Author('Kylie Jungles'), arxiv.Result.Author('Diane Shaltis'), arxiv.Result.Author('Kayvan Najarian'), arxiv.Result.Author('Rajan Ravikumar'), arxiv.Result.Author('Georgiana Sanders'), arxiv.Result.Author('Jonathan Gryak')]","Oral Food Challenges (OFCs) are essential to accurately diagnosing food
allergy in patients. However, patients are hesitant to undergo OFCs, and for
those that do, there is limited access to allergists in rural/community
healthcare settings. The prediction of OFC outcomes through machine learning
methods can facilitate the de-labeling of food allergens at home, improve
patient and physician comfort during OFCs, and economize medical resources by
minimizing the number of OFCs performed. Clinical data was gathered from 1,112
patients who collectively underwent a total of 1,284 OFCs, and consisted of
clinical factors including serum specific IgE, total IgE, skin prick tests
(SPTs), symptoms, sex, and age. Using these clinical features, machine learning
models were constructed to predict outcomes for peanut, egg, and milk
challenge. The best performing model for each allergen was created using the
Learning Using Concave and Convex Kernels (LUCCK) method, which achieved an
Area under the Curve (AUC) for peanut, egg, and milk OFC prediction of 0.76,
0.68, and 0.70, respectively. Model interpretation via SHapley Additive
exPlanations (SHAP) indicate that specific IgE, along with wheal and flare
values from SPTs, are highly predictive of OFC outcomes. The results of this
analysis suggest that machine learning has the potential to predict OFC
outcomes and reveal relevant clinical factors for further study."
10143,"The
                                                                           results of this analysis suggest that ensemble learning has the potential to predict OFC outcomes
                                                                           and reveal relevant clinical factors for further study.","Model interpretation via SHapley Additive exPlanations (SHAP) indicates that speciﬁc
                                                                           IgE, along with wheal and ﬂare values from SPTs, are highly predictive of OFC outcomes.",1.,2022-08-17 12:56:37+00:00,Prediction of Oral Food Challenge Outcomes via Ensemble Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Justin Zhang'), arxiv.Result.Author('Deborah Lee'), arxiv.Result.Author('Kylie Jungles'), arxiv.Result.Author('Diane Shaltis'), arxiv.Result.Author('Kayvan Najarian'), arxiv.Result.Author('Rajan Ravikumar'), arxiv.Result.Author('Georgiana Sanders'), arxiv.Result.Author('Jonathan Gryak')]","Oral Food Challenges (OFCs) are essential to accurately diagnosing food
allergy due to the limitations of existing clinical testing. However, some
patients are hesitant to undergo OFCs, while those willing suffer from limited
access to allergists in rural/community healthcare settings. Despite its
success in predicting patient outcomes in other clinical settings, few
applications of machine learning to food allergy have been developed. Thus, in
this study, we seek to leverage machine learning methodologies for OFC outcome
prediction. Retrospective data was gathered from 1,112 patients who
collectively underwent a total of 1,284 OFCs, and consisted of clinical factors
including serum-specific Immunoglobulin E (IgE), total IgE, skin prick tests
(SPTs), comorbidities, sex, and age. Using these features, multiple machine
learning models were constructed to predict OFC outcomes for three common
allergens: peanut, egg, and milk. The best performing model for each allergen
was an ensemble of random forest (egg) or Learning Using Concave and Convex
Kernels (LUCCK) (peanut, milk) models, which achieved an Area under the Curve
(AUC) of 0.91, 0.96, and 0.94, in predicting OFC outcomes for peanut, egg, and
milk, respectively. Moreover, all such models had sensitivity and specificity
values 89%. Model interpretation via SHapley Additive exPlanations (SHAP)
indicates that specific IgE, along with wheal and flare values from SPTs, are
highly predictive of OFC outcomes. The results of this analysis suggest that
ensemble learning has the potential to predict OFC outcomes and reveal relevant
clinical factors for further study."
10144,"The results of this
analysis suggest that ensemble learning has the potential to predict OFC outcomes with high performance and reveal
relevant clinical factors for further study.","The interpretation of these models, limitations on their applicability, and an analysis of model decision making via
SHAP are discussed in Section 5, including the signiﬁcance of SPT ﬂare size to OFC prediction.",2.,2022-08-17 12:56:37+00:00,Prediction of Oral Food Challenge Outcomes via Ensemble Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Justin Zhang'), arxiv.Result.Author('Deborah Lee'), arxiv.Result.Author('Kylie Jungles'), arxiv.Result.Author('Diane Shaltis'), arxiv.Result.Author('Kayvan Najarian'), arxiv.Result.Author('Rajan Ravikumar'), arxiv.Result.Author('Georgiana Sanders'), arxiv.Result.Author('Jonathan Gryak')]","Oral Food Challenges (OFCs) are essential to accurately diagnosing food
allergy due to the limitations of existing clinical testing. However, some
patients are hesitant to undergo OFCs, while those willing suffer from limited
access to allergists in rural/community healthcare settings. Despite its
success in predicting patient outcomes in other clinical settings, few
applications of machine learning to food allergy have been developed. Thus, in
this study, we seek to leverage machine learning methodologies for OFC outcome
prediction. Retrospective data was gathered from 1,112 patients who
collectively underwent a total of 1,284 OFCs, and consisted of clinical factors
including serum-specific Immunoglobulin E (IgE), total IgE, skin prick tests
(SPTs), comorbidities, sex, and age. Using these features, multiple machine
learning models were constructed to predict OFC outcomes for three common
allergens: peanut, egg, and milk. The best performing model for each allergen
was an ensemble of random forest (egg) or Learning Using Concave and Convex
Kernels (LUCCK) (peanut, milk) models, which achieved an Area under the Curve
(AUC) of 0.91, 0.96, and 0.94, in predicting OFC outcomes for peanut, egg, and
milk, respectively. Moreover, all such models had sensitivity and specificity
values 89%. Model interpretation via SHapley Additive exPlanations (SHAP)
indicates that specific IgE, along with wheal and flare values from SPTs, are
highly predictive of OFC outcomes. The results of this analysis suggest that
ensemble learning has the potential to predict OFC outcomes and reveal relevant
clinical factors for further study."
10145,"The results of this analysis suggest that ensemble learning has
the potential to predict OFC outcomes with high performance and reveal relevant clinical factors for further study.","The signiﬁcance of ﬂare is novel, as ﬂare is typically
given low importance by clinicians in OFC prediction.","In
future work, other inﬂuential features from data modalities such as genomic data and other immunologic measures will
be extracted.",2022-08-17 12:56:37+00:00,Prediction of Oral Food Challenge Outcomes via Ensemble Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Justin Zhang'), arxiv.Result.Author('Deborah Lee'), arxiv.Result.Author('Kylie Jungles'), arxiv.Result.Author('Diane Shaltis'), arxiv.Result.Author('Kayvan Najarian'), arxiv.Result.Author('Rajan Ravikumar'), arxiv.Result.Author('Georgiana Sanders'), arxiv.Result.Author('Jonathan Gryak')]","Oral Food Challenges (OFCs) are essential to accurately diagnosing food
allergy due to the limitations of existing clinical testing. However, some
patients are hesitant to undergo OFCs, while those willing suffer from limited
access to allergists in rural/community healthcare settings. Despite its
success in predicting patient outcomes in other clinical settings, few
applications of machine learning to food allergy have been developed. Thus, in
this study, we seek to leverage machine learning methodologies for OFC outcome
prediction. Retrospective data was gathered from 1,112 patients who
collectively underwent a total of 1,284 OFCs, and consisted of clinical factors
including serum-specific Immunoglobulin E (IgE), total IgE, skin prick tests
(SPTs), comorbidities, sex, and age. Using these features, multiple machine
learning models were constructed to predict OFC outcomes for three common
allergens: peanut, egg, and milk. The best performing model for each allergen
was an ensemble of random forest (egg) or Learning Using Concave and Convex
Kernels (LUCCK) (peanut, milk) models, which achieved an Area under the Curve
(AUC) of 0.91, 0.96, and 0.94, in predicting OFC outcomes for peanut, egg, and
milk, respectively. Moreover, all such models had sensitivity and specificity
values 89%. Model interpretation via SHapley Additive exPlanations (SHAP)
indicates that specific IgE, along with wheal and flare values from SPTs, are
highly predictive of OFC outcomes. The results of this analysis suggest that
ensemble learning has the potential to predict OFC outcomes and reveal relevant
clinical factors for further study."
10158,"[20], which is a lock-free asynchronous parallel implementa-
• We further study the generalization performance of SYNTHESIS            tion of SGD under the shared memory architecture with a sublinear
   under both distributed memory and shared memory architec-              convergence rate for strongly convex problems.","One of the earliest asynchronous SGD-type algorithms is HOG-
                                                                          WILD!","Roughly the same
   tures.",2022-08-17 17:42:33+00:00,SYNTHESIS: A Semi-Asynchronous Path-Integrated Stochastic Gradient Method for Distributed Learning in Computing Clusters,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhuqing Liu'), arxiv.Result.Author('Xin Zhang'), arxiv.Result.Author('Jia Liu')]","To increase the training speed of distributed learning, recent years have
witnessed a significant amount of interest in developing both synchronous and
asynchronous distributed stochastic variance-reduced optimization methods.
However, all existing synchronous and asynchronous distributed training
algorithms suffer from various limitations in either convergence speed or
implementation complexity. This motivates us to propose an algorithm called
\algname (\ul{s}emi-as\ul{yn}chronous pa\ul{th}-int\ul{e}grated \ul{s}tochastic
grad\ul{i}ent \ul{s}earch), which leverages the special structure of the
variance-reduction framework to overcome the limitations of both synchronous
and asynchronous distributed learning algorithms, while retaining their salient
features. We consider two implementations of \algname under distributed and
shared memory architectures. We show that our \algname algorithms have
\(O(\sqrt{N}\epsilon^{-2}(\Delta+1)+N)\) and
\(O(\sqrt{N}\epsilon^{-2}(\Delta+1) d+N)\) computational complexities for
achieving an \(\epsilon\)-stationary point in non-convex learning under
distributed and shared memory architectures, respectively, where \(N\) denotes
the total number of training samples and \(\Delta\) represents the maximum
delay of the workers. Moreover, we investigate the generalization performance
of \algname by establishing algorithmic stability bounds for quadratic strongly
convex and non-convex optimization. We further conduct extensive numerical
experiments to verify our theoretical findings"
10159,"[20], which is a lock-free asynchronous parallel implementa-
• We further study the generalization performance of SYNTHESIS            tion of SGD under the shared memory architecture with a sublinear
   under both distributed memory and shared memory architec-              convergence rate for strongly convex problems.","One of the earliest asynchronous SGD-type algorithms is HOG-
                                                                          WILD!","Roughly the same
   tures.",2022-08-17 17:42:33+00:00,SYNTHESIS: A Semi-Asynchronous Path-Integrated Stochastic Gradient Method for Distributed Learning in Computing Clusters,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhuqing Liu'), arxiv.Result.Author('Xin Zhang'), arxiv.Result.Author('Jia Liu')]","To increase the training speed of distributed learning, recent years have
witnessed a significant amount of interest in developing both synchronous and
asynchronous distributed stochastic variance-reduced optimization methods.
However, all existing synchronous and asynchronous distributed training
algorithms suffer from various limitations in either convergence speed or
implementation complexity. This motivates us to propose an algorithm called
STNTHESIS (semi-asynchronous path-integrated stochastic gradient search), which
leverages the special structure of the variance-reduction framework to overcome
the limitations of both synchronous and asynchronous distributed learning
algorithms while retaining their salient features. We consider two
implementations of STNTHESIS under distributed and shared memory architectures.
We show that our STNTHESIS algorithms have
$O(\sqrt{N}\epsilon^{-2}(\Delta+1)+N)$ and $O(\sqrt{N}\epsilon^{-2}(\Delta+1)
d+N)$ computational complexities for achieving an $\epsilon$-stationary point
in non-convex learning under distributed and shared memory architectures,
respectively, where N denotes the total number of training samples and $\Delta$
represents the maximum delay of the workers. Moreover, we investigate the
generalization performance of \algname by establishing algorithmic stability
bounds for quadratic strongly convex and non-convex optimization. We further
conduct extensive numerical experiments to verify our theoretical findings"
10170,"Experiment with human experts

We further study the performance of the proposed method in a real-world setting with
actual human experts.","We also provide additional experiment results with
diﬀerent parameter settings in Appendix C.

4.2.2.","To that end, we conduct a simple user experiment involving mem-
ory abilities.",2022-08-18 09:49:21+00:00,Bayesian Optimization Augmented with Actively Elicited Expert Knowledge,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Daolang Huang'), arxiv.Result.Author('Louis Filstroff'), arxiv.Result.Author('Petrus Mikkola'), arxiv.Result.Author('Runkai Zheng'), arxiv.Result.Author('Samuel Kaski')]","Bayesian optimization (BO) is a well-established method to optimize black-box
functions whose direct evaluations are costly. In this paper, we tackle the
problem of incorporating expert knowledge into BO, with the goal of further
accelerating the optimization, which has received very little attention so far.
We design a multi-task learning architecture for this task, with the goal of
jointly eliciting the expert knowledge and minimizing the objective function.
In particular, this allows for the expert knowledge to be transferred into the
BO task. We introduce a specific architecture based on Siamese neural networks
to handle the knowledge elicitation from pairwise queries. Experiments on
various benchmark functions with both simulated and actual human experts show
that the proposed method significantly speeds up BO even when the expert
knowledge is biased compared to the objective function."
10177,"However, the approach does not quantify prediction errors and further research is
                                                needed to understand and improve model transferability.","Partial convolutions can be added as layers to other
                                                types of neural networks, making it relatively easy to integrate with existing deep learning
                                                models.","The implementation of spatiotemporal
                                                partial convolutions and the U-Net-like model is available as open-source software.",2022-08-18 11:32:04+00:00,Efficient data-driven gap filling of satellite image time series using deep neural networks with partial convolutions,cs.LG,"['cs.LG', 'cs.CV']",[arxiv.Result.Author('Marius Appel')],"The abundance of gaps in satellite image time series often complicates the
application of deep learning models such as convolutional neural networks for
spatiotemporal modeling. Based on previous work in computer vision on image
inpainting, this paper shows how three-dimensional spatiotemporal partial
convolutions can be used as layers in neural networks to fill gaps in satellite
image time series. To evaluate the approach, we apply a U-Net-like model on
incomplete image time series of quasi-global carbon monoxide observations from
the Sentinel-5P satellite. Prediction errors were comparable to two considered
statistical approaches while computation times for predictions were up to three
orders of magnitude faster, making the approach applicable to process large
amounts of satellite data. Partial convolutions can be added as layers to other
types of neural networks, making it relatively easy to integrate with existing
deep learning models. However, the approach does not quantify prediction errors
and further research is needed to understand and improve model transferability.
The implementation of spatiotemporal partial convolutions and the U-Net-like
model is available as open-source software."
10186,"20
We consider further research in two directions.","However, we have observed that some types of faults are really difﬁcult to detect with no
labels, so we have proposed to use few-shot ﬁne-tuning – the technique that helps to detect difﬁcult
faults using very few labeled data.","The ﬁrst is the exploration of other SSL techniques,
including methods that incorporate chemical and physical domain knowledge.",2022-08-17 10:24:37+00:00,SensorSCAN: Self-Supervised Learning and Deep Clustering for Fault Diagnosis in Chemical Processes,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Maksim Golyadkin'), arxiv.Result.Author('Vitaliy Pozdnyakov'), arxiv.Result.Author('Leonid Zhukov'), arxiv.Result.Author('Ilya Makarov')]","Modern industrial facilities generate large volumes of raw sensor data during
production process. This data is used to monitor and control the processes and
can be analyzed to detect and predict process abnormalities. Typically, the
data has to be annotated by experts to be further used in predictive modeling.
Most of today's research is focusing on either unsupervised anomaly detection
algorithms or supervised methods, that require manually annotated data. The
studies are often done using process simulator generated data for a narrow
class of events and proposed algorithms are rarely verified on publicly
available datasets. In this paper, we propose a novel method SensorSCAN for
unsupervised fault detection and diagnosis designed for industrial chemical
sensor data. We demonstrate our model performance on two publicly available
datasets based on the Tennessee Eastman Process with various fault types.
Results show that our method significantly outperforms existing approaches
(+0.2-0.3 TPR for a fixed FPR) and detects most of the process faults without
the use of expert annotation. In addition, we performed experiments to show
that our method is suitable for real-world applications where the number of
fault types is not known in advance."
10280,"This fact, along with all the aforementioned
properties, provides a broad perspective for further study and improvements of the proposed algorithm.","This way, we first provide and study a general Neural SDE framework in CPD setting, which makes possible to
apply a wide range of modern deep learning techniques to CPD problems.",7.,2022-08-22 13:53:13+00:00,Latent Neural Stochastic Differential Equations for Change Point Detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Artem Ryzhikov'), arxiv.Result.Author('Mikhail Hushchyn'), arxiv.Result.Author('Denis Derkach')]","The purpose of change point detection algorithms is to locate an abrupt
change in the time evolution of a process. In this paper, we introduce an
application of latent neural stochastic differential equations for change point
detection problem. We demonstrate the detection capabilities and performance of
our model on a range of synthetic and real-world datasets and benchmarks. Most
of the studied scenarios show that the proposed algorithm outperforms the
state-of-the-art algorithms. We also discuss the strengths and limitations of
this approach and indicate directions for further improvements."
10281,"To our knowledge, this is the first studied application of
a Neural SDE to change point detection problem, which opens new prospects for further research and DL applications
to change point detection task.","The model shows high robustness and a performance which is higher than other
state-of-the-art algorithms in most of the scenarios and metrics.",8.,2022-08-22 13:53:13+00:00,Latent Neural Stochastic Differential Equations for Change Point Detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Artem Ryzhikov'), arxiv.Result.Author('Mikhail Hushchyn'), arxiv.Result.Author('Denis Derkach')]","The purpose of change point detection algorithms is to locate an abrupt
change in the time evolution of a process. In this paper, we introduce an
application of latent neural stochastic differential equations for change point
detection problem. We demonstrate the detection capabilities and performance of
our model on a range of synthetic and real-world datasets and benchmarks. Most
of the studied scenarios show that the proposed algorithm outperforms the
state-of-the-art algorithms. We also discuss the strengths and limitations of
this approach and indicate directions for further improvements."
10318,"Therefore, how to improve the training speed is also worth further study.","However, proﬁt opportunities are
                                      ﬂeeting in the stock market.","The main contribution of this paper is as follows:
                                          Firstly, this study proposes a new algorithm, called risk-return reinforcement learning(R3L), on the framework of actor-critic.",2022-08-23 03:20:06+00:00,An intelligent algorithmic trading based on a risk-return reinforcement learning algorithm,cs.LG,"['cs.LG', 'q-fin.RM']",[arxiv.Result.Author('Boyi Jin')],"This scientific paper propose a novel portfolio optimization model using an
improved deep reinforcement learning algorithm. The objective function of the
optimization model is the weighted sum of the expectation and value at
risk(VaR) of portfolio cumulative return. The proposed algorithm is based on
actor-critic architecture, in which the main task of critical network is to
learn the distribution of portfolio cumulative return using quantile
regression, and actor network outputs the optimal portfolio weight by
maximizing the objective function mentioned above. Meanwhile, we exploit a
linear transformation function to realize asset short selling. Finally, A
multi-process method is used, called Ape-x, to accelerate the speed of deep
reinforcement learning training. To validate our proposed approach, we conduct
backtesting for two representative portfolios and observe that the proposed
model in this work is superior to the benchmark strategies."
10319,"fore, how to improve the training speed is also worth
                                                                                                     further study.","There-
                                        tions to maximize portfolio return.","Currently, researchers exploited diﬀerent reinforce-
                                        ment learning algorithms to study ﬁnancial trading               The main contribution of this paper is as follows:
                                        problems, including the actor-only method(also called
                                        the value-based method), the critic-only method(also             Firstly, this study proposes a new algorithm for the
                                                                                                     actor-critic framework, called risk-return reinforcement
                                            ∗Corresponding author: 20110983@sdufe.edu.cn             learning(R3L).",2022-08-23 03:20:06+00:00,An intelligent algorithmic trading based on a risk-return reinforcement learning algorithm,cs.LG,"['cs.LG', 'q-fin.RM']",[arxiv.Result.Author('Boyi Jin')],"This scientific paper propose a novel portfolio optimization model using an
improved deep reinforcement learning algorithm. The objective function of the
optimization model is the weighted sum of the expectation and value at
risk(VaR) of portfolio cumulative return. The proposed algorithm is based on
actor-critic architecture, in which the main task of critical network is to
learn the distribution of portfolio cumulative return using quantile
regression, and actor network outputs the optimal portfolio weight by
maximizing the objective function mentioned above. Meanwhile, we exploit a
linear transformation function to realize asset short selling. Finally, A
multi-process method is used, called Ape-x, to accelerate the speed of deep
reinforcement learning training. To validate our proposed approach, we conduct
backtesting for two representative portfolios and observe that the proposed
model in this work is superior to the benchmark strategies."
10327,"In
this context, we will further study the here presented eﬃcacy metric for those
algorithms that aim to obtain a model close to a retrained one, since the results
of our experiments look quite promising.","Given the insights from this work, we want to do a larger survey on evaluating
Machine Unlearning algorithms in the future, where we categorize the algorithms
depending on how and in which direction the model parameters are updated.","Finally, ﬁnding the relation between
the eﬃcacy and certiﬁed removal is also an important direction for future work,
since this will allow relating the metric to privacy guarantees in the sense of
diﬀerential privacy.",2022-08-23 09:37:31+00:00,Evaluating Machine Unlearning via Epistemic Uncertainty,cs.LG,"['cs.LG', 'I.2.6']","[arxiv.Result.Author('Alexander Becker'), arxiv.Result.Author('Thomas Liebig')]","There has been a growing interest in Machine Unlearning recently, primarily
due to legal requirements such as the General Data Protection Regulation (GDPR)
and the California Consumer Privacy Act. Thus, multiple approaches were
presented to remove the influence of specific target data points from a trained
model. However, when evaluating the success of unlearning, current approaches
either use adversarial attacks or compare their results to the optimal
solution, which usually incorporates retraining from scratch. We argue that
both ways are insufficient in practice. In this work, we present an evaluation
metric for Machine Unlearning algorithms based on epistemic uncertainty. This
is the first definition of a general evaluation metric for Machine Unlearning
to our best knowledge."
10328,"In
this context, we will further study the here presented eﬃcacy metric for those
algorithms that aim to obtain a model close to a retrained one, since the results
of our experiments look quite promising.","Given the insights from this work, we want to do a larger survey on evaluating
Machine Unlearning algorithms in the future, where we categorize the algorithms
depending on how and in which direction the model parameters are updated.","Finally, ﬁnding the relation between
the eﬃcacy and certiﬁed removal is also an important direction for future work,
since this will allow relating the metric to privacy guarantees in the sense of
diﬀerential privacy.",2022-08-23 09:37:31+00:00,Evaluating Machine Unlearning via Epistemic Uncertainty,cs.LG,"['cs.LG', 'I.2.6']","[arxiv.Result.Author('Alexander Becker'), arxiv.Result.Author('Thomas Liebig')]","There has been a growing interest in Machine Unlearning recently, primarily
due to legal requirements such as the General Data Protection Regulation (GDPR)
and the California Consumer Privacy Act. Thus, multiple approaches were
presented to remove the influence of specific target data points from a trained
model. However, when evaluating the success of unlearning, current approaches
either use adversarial attacks or compare their results to the optimal
solution, which usually incorporates retraining from scratch. We argue that
both ways are insufficient in practice. In this work, we present an evaluation
metric for Machine Unlearning algorithms based on epistemic uncertainty. This
is the first definition of a general evaluation metric for Machine Unlearning
to our best knowledge."
10361,"Our experiments show that such loss functions
                                                               can enhance the performance of deep learning neural
                                                               networks, thereby encouraging further research on
                                                               including the Weber Fechner Law in machine learning.","We presented a general methodology for incorporating the
                                                               Weber Fechner Law into the loss function of any learning
                                                               algorithm.","Acknowledgements

                                                               Thanks to Jay Jawahar, Steven Johnson and Prasad
                                                               Tadepalli for comments and suggestions.",2022-08-23 23:57:40+00:00,Psychophysical Machine Learning,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('B. N. Kausik')],"The Weber Fechner Law of psychophysics observes that human perception is
logarithmic in the stimulus. We present an algorithm for incorporating the
Weber Fechner law into loss functions for machine learning, and use the
algorithm to enhance the performance of deep learning networks."
10362,"Our experiments show that such loss functions
                                                               can enhance the performance of deep learning neural
                                                               networks, thereby encouraging further research on
                                                               including the Weber Fechner Law in machine learning.","We presented a general methodology for incorporating the
                                                               Weber Fechner Law into the loss function of any learning
                                                               algorithm.","Acknowledgements

                                                               Thanks to Jay Jawahar, Steven Johnson and Prasad
                                                               Tadepalli for comments and suggestions.",2022-08-23 23:57:40+00:00,Psychophysical Machine Learning,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('B. N. Kausik')],"The Weber Fechner Law of psychophysics observes that human perception is
logarithmic in the stimulus. We present an algorithm for incorporating the
Weber Fechner law into loss functions for machine learning, and use the
algorithm to enhance the performance of deep learning networks."
10363,"Our experiments show that such loss functions
can enhance the performance of deep learning neural
networks, thereby encouraging further research on
including the Weber Fechner Law in machine learning.","We presented a general methodology for incorporating the
Weber Fechner Law into the loss function of any learning
algorithm.","Acknowledgements

Thanks to Jay Jawahar, Steven Johnson and Prasad
Tadepalli for comments and suggestions.",2022-08-23 23:57:40+00:00,Psychophysical Machine Learning,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('B. N. Kausik')],"The Weber Fechner Law of psychophysics observes that human perception is
logarithmic in the stimulus. We present an algorithm for incorporating the
Weber Fechner law into loss functions for machine learning, and use the
algorithm to enhance the performance of deep learning networks."
10364,"Our experiments show that such loss functions
can enhance the performance of deep learning neural
networks, thereby encouraging further research on
including the Weber Fechner Law in machine learning.","We presented a general methodology for incorporating the
Weber Fechner Law into the loss function of any learning
algorithm.","Acknowledgements

Thanks to Jay Jawahar, Steven Johnson and Prasad
Tadepalli for comments and suggestions.",2022-08-23 23:57:40+00:00,Psychophysical Machine Learning,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('B. N. Kausik')],"The Weber Fechner Law of psychophysics observes that human perception is
logarithmic in the stimulus. We present an algorithm for incorporating the
Weber Fechner law into loss functions for machine learning, and use the
algorithm to enhance the performance of deep learning networks."
10400,"(3) We imple-
possible impact caused by different clients, we further study    ment a proof-of-concept in the framework, spanning a range
the performance of PROMPTFL with different clients from          of popular image classiﬁcation tasks.","data on each device private, aiming to learn global prompts
                                                                 updated only by communicating gradients rather than the
Comparison with different clients Next, to eliminate the         data itself, and thus not less private than FL.","We ﬁnd PROMPTFL
16 to 32 to 64, with the iid mode that each client owns ran-     to be competitive with strong FL baselines.",2022-08-24 15:50:58+00:00,PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models -- Federated Learning in Age of Foundation Model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tao Guo'), arxiv.Result.Author('Song Guo'), arxiv.Result.Author('Junxiao Wang'), arxiv.Result.Author('Wenchao Xu')]","Quick global aggregation of effective distributed parameters is crucial to
federated learning (FL), which requires adequate bandwidth for parameters
communication and sufficient user data for local training. Otherwise, FL may
cost excessive training time for convergence and produce inaccurate models. In
this paper, we propose a brand-new FL framework, PromptFL, that replaces the
federated model training with the federated prompt training, i.e., let
federated participants train prompts instead of a shared model, to
simultaneously achieve the efficient global aggregation and local training on
insufficient data by exploiting the power of foundation models (FM) in a
distributed way. PromptFL ships an off-the-shelf FM, i.e., CLIP, to distributed
clients who would cooperatively train shared soft prompts based on very few
local data. Since PromptFL only needs to update the prompts instead of the
whole model, both the local training and the global aggregation can be
significantly accelerated. And FM trained over large scale data can provide
strong adaptation capability to distributed users tasks with the trained soft
prompts. We empirically analyze the PromptFL via extensive experiments, and
show its superiority in terms of system feasibility, user privacy, and
performance."
10403,"Furthermore, the paper concludes with an overview of areas where such a system would
beneﬁt from further research, most notably: improvements in the method used to update the
set of knowledge once a new task is learnt and designing a system that is able to work on an
unordered set of programs.","Given a machine M , that takes a string and returns
a real number, the goal of a time limited optimisation problem is to ﬁnd a string x, within a
ﬁxed time limit T , such as M (x) is as large as possible.","9
2.2 Multi-task learning

In this thesis, we refer to multiple Machine Learning paradigms related to multi-task learning.",2022-08-24 16:53:54+00:00,Constraint-driven multi-task learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bogdan Cretu'), arxiv.Result.Author('Andrew Cropper')]","Inductive logic programming is a form of machine learning based on
mathematical logic that generates logic programs from given examples and
background knowledge.
  In this project, we extend the Popper ILP system to make use of multi-task
learning. We implement the state-of-the-art approach and several new strategies
to improve search performance. Furthermore, we introduce constraint
preservation, a technique that improves overall performance for all approaches.
  Constraint preservation allows the system to transfer knowledge between
updates on the background knowledge set. Consequently, we reduce the amount of
repeated work performed by the system. Additionally, constraint preservation
allows us to transition from the current state-of-the-art iterative deepening
search approach to a more efficient breadth first search approach.
  Finally, we experiment with curriculum learning techniques and show their
potential benefit to the field."
10404,"We conclude that
the answer for Q3 is yes, but we outline the need for further research in this area, with the
purpose of developing more accurate heuristics.","We consider this a possible upside, as it improves the system’s reliability.","46
                                                          ID PRESERVE

                            80                            RESET-BFS PRESERVE

Percentage of tasks solved                                PRIO-EX PRESERVE

                                                          PRIO-CONS PRESERVE

                            60

                            40

                            20

                            0        20  40  60   80 100
                                  0

                                         Minutes

Figure 5.11: Graph showing the eﬀects of priority ordering for string data (showing standard
                                                         error)

5.3.4 Robot movement

This data set contains single examples for each task because any positive examples generated
would be part of the same equivalence class (e.g.",2022-08-24 16:53:54+00:00,Constraint-driven multi-task learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bogdan Cretu'), arxiv.Result.Author('Andrew Cropper')]","Inductive logic programming is a form of machine learning based on
mathematical logic that generates logic programs from given examples and
background knowledge.
  In this project, we extend the Popper ILP system to make use of multi-task
learning. We implement the state-of-the-art approach and several new strategies
to improve search performance. Furthermore, we introduce constraint
preservation, a technique that improves overall performance for all approaches.
  Constraint preservation allows the system to transfer knowledge between
updates on the background knowledge set. Consequently, we reduce the amount of
repeated work performed by the system. Additionally, constraint preservation
allows us to transition from the current state-of-the-art iterative deepening
search approach to a more efficient breadth first search approach.
  Finally, we experiment with curriculum learning techniques and show their
potential benefit to the field."
10405,"We suggest further research in developing
more accurate heuristics to predict which tasks are closer to being solved.","In this project, we have shown the potential of task ordering in improving
the learning capacity of multi-task ILP systems.","One of the solutions
we propose is the use of other Machine Learning techniques in order to identify more manage-
able tasks and give them priority.",2022-08-24 16:53:54+00:00,Constraint-driven multi-task learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bogdan Cretu'), arxiv.Result.Author('Andrew Cropper')]","Inductive logic programming is a form of machine learning based on
mathematical logic that generates logic programs from given examples and
background knowledge.
  In this project, we extend the Popper ILP system to make use of multi-task
learning. We implement the state-of-the-art approach and several new strategies
to improve search performance. Furthermore, we introduce constraint
preservation, a technique that improves overall performance for all approaches.
  Constraint preservation allows the system to transfer knowledge between
updates on the background knowledge set. Consequently, we reduce the amount of
repeated work performed by the system. Additionally, constraint preservation
allows us to transition from the current state-of-the-art iterative deepening
search approach to a more efficient breadth first search approach.
  Finally, we experiment with curriculum learning techniques and show their
potential benefit to the field."
10410,"The results show that only
lib.umich.edu/data/concern/data_sets/6d56zw997 for re-          CSADA was able to cut the cost (by half) and prevent some
producibility purposes and to encourage further research        critical errors.",on different pairs in Table 6.,"In turn, AP and SOSR exhibited no statisti-
along this line.",2022-08-24 19:00:30+00:00,Rethinking Cost-sensitive Classification in Deep Learning via Adversarial Data Augmentation,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Qiyuan Chen'), arxiv.Result.Author('Raed Al Kontar'), arxiv.Result.Author('Maher Nouiehed'), arxiv.Result.Author('Jessie Yang'), arxiv.Result.Author('Corey Lester')]","Cost-sensitive classification is critical in applications where
misclassification errors widely vary in cost. However, over-parameterization
poses fundamental challenges to the cost-sensitive modeling of deep neural
networks (DNNs). The ability of a DNN to fully interpolate a training dataset
can render a DNN, evaluated purely on the training set, ineffective in
distinguishing a cost-sensitive solution from its overall accuracy maximization
counterpart. This necessitates rethinking cost-sensitive classification in
DNNs. To address this challenge, this paper proposes a cost-sensitive
adversarial data augmentation (CSADA) framework to make over-parameterized
models cost-sensitive. The overarching idea is to generate targeted adversarial
examples that push the decision boundary in cost-aware directions. These
targeted adversarial samples are generated by maximizing the probability of
critical misclassifications and used to train a model with more conservative
decisions on costly pairs. Experiments on well-known datasets and a pharmacy
medication image (PMI) dataset made publicly available show that our method can
effectively minimize the overall cost and reduce critical errors, while
achieving comparable performance in terms of overall accuracy."
10429,There are several avenues for further research.,"Our numerical examples
      show that CAS4DL outperforms MC sampling, both attaining a smaller error
      from a ﬁxed sampling budget across a range of functions and dimensions and
      possessing better stability properties.","First, it is intuitively desir-
      able (and also supported by our experiments) that the numerical dimension
      n of the learned subspace be as close to full as possible (i.e., n ≈ N ).",2022-08-25 16:21:17+00:00,CAS4DL: Christoffel Adaptive Sampling for function approximation via Deep Learning,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Ben Adcock'), arxiv.Result.Author('Juan M. Cardenas'), arxiv.Result.Author('Nick Dexter')]","The problem of approximating smooth, multivariate functions from sample
points arises in many applications in scientific computing, e.g., in
computational Uncertainty Quantification (UQ) for science and engineering. In
these applications, the target function may represent a desired quantity of
interest of a parameterized Partial Differential Equation (PDE). Due to the
large cost of solving such problems, where each sample is computed by solving a
PDE, sample efficiency is a key concerning these applications. Recently, there
has been increasing focus on the use of Deep Neural Networks (DNN) and Deep
Learning (DL) for learning such functions from data. In this work, we propose
an adaptive sampling strategy, CAS4DL (Christoffel Adaptive Sampling for Deep
Learning) to increase the sample efficiency of DL for multivariate function
approximation. Our novel approach is based on interpreting the second to last
layer of a DNN as a dictionary of functions defined by the nodes on that layer.
With this viewpoint, we then define an adaptive sampling strategy motivated by
adaptive sampling schemes recently proposed for linear approximation schemes,
wherein samples are drawn randomly with respect to the Christoffel function of
the subspace spanned by this dictionary. We present numerical experiments
comparing CAS4DL with standard Monte Carlo (MC) sampling. Our results
demonstrate that CAS4DL often yields substantial savings in the number of
samples required to achieve a given accuracy, particularly in the case of
smooth activation functions, and it shows a better stability in comparison to
MC. These results therefore are a promising step towards fully adapting DL
towards scientific computing applications."
10441,"We further study the MBC property in neural set encoding functions, establishing
                                                   a method for converting arbitrary non-MBC models to satisfy MBC.","Subsequently,
                                                   Mini-Batch Consistency (MBC), the ability to sequentially process any permutation
                                                   of any random set partition scheme while maintaining consistency guarantees on
                                                   the output, has been established but with limited options for network architectures.","In doing so,
                                                  we provide a framework for a universally-MBC (UMBC) class of set functions.",2022-08-26 02:13:38+00:00,Universal Mini-Batch Consistency for Set Encoding Functions,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Jeffrey Willette'), arxiv.Result.Author('Andreis Bruno'), arxiv.Result.Author('Juho Lee'), arxiv.Result.Author('Sung Ju Hwang')]","Previous works have established solid foundations for neural set functions,
as well as effective architectures which preserve the necessary properties for
operating on sets, such as being invariant to permutations of the set elements.
Subsequently, Mini-Batch Consistency (MBC), the ability to sequentially process
any permutation of any random set partition scheme while maintaining
consistency guarantees on the output, has been established but with limited
options for network architectures. We further study the MBC property in neural
set encoding functions, establishing a method for converting arbitrary non-MBC
models to satisfy MBC. In doing so, we provide a framework for a
universally-MBC (UMBC) class of set functions. Additionally, we explore an
interesting dropout strategy made possible by our framework, and investigate
its effects on probabilistic calibration under test-time distributional shifts.
We validate UMBC with proofs backed by unit tests, also providing
qualitative/quantitative experiments on toy data, clean and corrupted point
cloud classification, and amortized clustering on ImageNet. The results
demonstrate the utility of UMBC, and we further discover that our dropout
strategy improves uncertainty calibration."
10476,"We expect our work will
ing that the optimized vertex sequences using spectral       stimulate further research in these directions.","10 and S10–S14), suggest-     quence reﬂects group labels.","ordering convey some information about a non-random
structure.",2022-08-27 05:55:26+00:00,Consistency between ordering and clustering methods for graphs,cs.LG,"['cs.LG', 'cs.SI', 'physics.soc-ph']","[arxiv.Result.Author('Tatsuro Kawamoto'), arxiv.Result.Author('Masaki Ochi'), arxiv.Result.Author('Teruyoshi Kobayashi')]","A relational dataset is often analyzed by optimally assigning a label to each
element through clustering or ordering. While similar characterizations of a
dataset would be achieved by both clustering and ordering methods, the former
has been studied much more actively than the latter, particularly for the data
represented as graphs. This study fills this gap by investigating
methodological relationships between several clustering and ordering methods,
focusing on spectral techniques. Furthermore, we evaluate the resulting
performance of the clustering and ordering methods. To this end, we propose a
measure called the label continuity error, which generically quantifies the
degree of consistency between a sequence and partition for a set of elements.
Based on synthetic and real-world datasets, we evaluate the extents to which an
ordering method identifies a module structure and a clustering method
identifies a banded structure."
10494,"It will be important to further study how high-level
clinical concepts perform across di erent se ings, and to quantify the extent to which classi ers learned
on top of these concepts might be transferable across hospitals.","In the future, we plan to integrate our models with the healthcare system, and to continue
to monitor the performance of the models over time.","As COVID-19 continues to evolve over
time, we will also investigate whether new concepts become relevant for prediction.",2022-08-28 02:59:35+00:00,Learning Clinical Concepts for Predicting Risk of Progression to Severe COVID-19,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Helen Zhou'), arxiv.Result.Author('Cheng Cheng'), arxiv.Result.Author('Kelly J. Shields'), arxiv.Result.Author('Gursimran Kochhar'), arxiv.Result.Author('Tariq Cheema'), arxiv.Result.Author('Zachary C. Lipton'), arxiv.Result.Author('Jeremy C. Weiss')]","With COVID-19 now pervasive, identification of high-risk individuals is
crucial. Using data from a major healthcare provider in Southwestern
Pennsylvania, we develop survival models predicting severe COVID-19
progression. In this endeavor, we face a tradeoff between more accurate models
relying on many features and less accurate models relying on a few features
aligned with clinician intuition. Complicating matters, many EHR features tend
to be under-coded, degrading the accuracy of smaller models. In this study, we
develop two sets of high-performance risk scores: (i) an unconstrained model
built from all available features; and (ii) a pipeline that learns a small set
of clinical concepts before training a risk predictor. Learned concepts boost
performance over the corresponding features (C-index 0.858 vs. 0.844) and
demonstrate improvements over (i) when evaluated out-of-sample (subsequent time
periods). Our models outperform previous works (C-index 0.844-0.872 vs.
0.598-0.810)."
10525,"Nevertheless,       3.4 Experimental Set-up
since this method was fairly trivial, further research study-
ing better encoding strategies for the frequency domain           All features were scaled separately using a standard scaler,
could be desirable.","stations were assigned to the input edge features as eij =
Instead, the concatenation of frequency values to the FFT         [(lonj − loni), (latj − lati)], for two stations, i and j.
outputs, served somewhat the same purpose as the posi-
tional encoding did for the trend components.",with zero mean and unit variance.,2022-08-29 13:26:20+00:00,Spatio-Temporal Wind Speed Forecasting using Graph Networks and Novel Transformer Architectures,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Lars Ødegaard Bentsen'), arxiv.Result.Author('Narada Dilp Warakagoda'), arxiv.Result.Author('Roy Stenbro'), arxiv.Result.Author('Paal Engelstad')]","To improve the security and reliability of wind energy production, short-term
forecasting has become of utmost importance. This study focuses on multi-step
spatio-temporal wind speed forecasting for the Norwegian continental shelf. A
graph neural network (GNN) architecture was used to extract spatial
dependencies, with different update functions to learn temporal correlations.
These update functions were implemented using different neural network
architectures. One such architecture, the Transformer, has become increasingly
popular for sequence modelling in recent years. Various alterations of the
original architecture have been proposed to better facilitate time-series
forecasting, of which this study focused on the Informer, LogSparse Transformer
and Autoformer. This is the first time the LogSparse Transformer and Autoformer
have been applied to wind forecasting and the first time any of these or the
Informer have been formulated in a spatio-temporal setting for wind
forecasting. By comparing against spatio-temporal Long Short-Term Memory (LSTM)
and Multi-Layer Perceptron (MLP) models, the study showed that the models using
the altered Transformer architectures as update functions in GNNs were able to
outperform these. Furthermore, we propose the Fast Fourier Transformer
(FFTransformer), which is a novel Transformer architecture based on signal
decomposition and consists of two separate streams that analyse trend and
periodic components separately. The FFTransformer and Autoformer were found to
achieve superior results for the 10-minute and 1-hour ahead forecasts, with the
FFTransformer significantly outperforming all other models for the 4-hour ahead
forecasts. Finally, by varying the degree of connectivity for the graph
representations, the study explicitly demonstrates how all models were able to
leverage spatial dependencies to improve local short-term wind speed
forecasting."
10526,"We
latter also converged slightly later, which was likely due to  therefore hope that this study will spark further research
the longer term forecasts taking advantage of information      into modiﬁcations and other applications of the FFTrans-
from nodes further away from the target.","Comparing the 6- and 24-step forecasts, the         mented and tested to facilitate different applications.","The percentage        former, as well as investigation into the applicability of
change in MAEs was also greater for the 24-step setting        different Transformer-based architectures for use in wind
than for the 6-step, even though the difference was not        forecasting.",2022-08-29 13:26:20+00:00,Spatio-Temporal Wind Speed Forecasting using Graph Networks and Novel Transformer Architectures,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Lars Ødegaard Bentsen'), arxiv.Result.Author('Narada Dilp Warakagoda'), arxiv.Result.Author('Roy Stenbro'), arxiv.Result.Author('Paal Engelstad')]","To improve the security and reliability of wind energy production, short-term
forecasting has become of utmost importance. This study focuses on multi-step
spatio-temporal wind speed forecasting for the Norwegian continental shelf. A
graph neural network (GNN) architecture was used to extract spatial
dependencies, with different update functions to learn temporal correlations.
These update functions were implemented using different neural network
architectures. One such architecture, the Transformer, has become increasingly
popular for sequence modelling in recent years. Various alterations of the
original architecture have been proposed to better facilitate time-series
forecasting, of which this study focused on the Informer, LogSparse Transformer
and Autoformer. This is the first time the LogSparse Transformer and Autoformer
have been applied to wind forecasting and the first time any of these or the
Informer have been formulated in a spatio-temporal setting for wind
forecasting. By comparing against spatio-temporal Long Short-Term Memory (LSTM)
and Multi-Layer Perceptron (MLP) models, the study showed that the models using
the altered Transformer architectures as update functions in GNNs were able to
outperform these. Furthermore, we propose the Fast Fourier Transformer
(FFTransformer), which is a novel Transformer architecture based on signal
decomposition and consists of two separate streams that analyse trend and
periodic components separately. The FFTransformer and Autoformer were found to
achieve superior results for the 10-minute and 1-hour ahead forecasts, with the
FFTransformer significantly outperforming all other models for the 4-hour ahead
forecasts. Finally, by varying the degree of connectivity for the graph
representations, the study explicitly demonstrates how all models were able to
leverage spatial dependencies to improve local short-term wind speed
forecasting."
10534,"For example, algorithms to construct response operators
to further study how ﬂuctuations, natural variability, and response to perturbation relate to each other (Ruelle, 1998) can be

                                                                          12
implemented with a differentiable model.","Aside from this, differentiable ESMs also enable further studies on the sensitivity, response and stability of the Earth’s
climate, which previously mostly had to rely on gradient-free methods.","Advances in this direction would greatly improve our understanding of climate
sensitivity and climate change (Lucarini et al., 2017).",2022-08-29 18:32:24+00:00,Differentiable Programming for Earth System Modeling,cs.LG,"['cs.LG', 'physics.ao-ph']","[arxiv.Result.Author('Maximilian Gelbrecht'), arxiv.Result.Author('Alistair White'), arxiv.Result.Author('Sebastian Bathiany'), arxiv.Result.Author('Niklas Boers')]","Earth System Models (ESMs) are the primary tools for investigating future
Earth system states at time scales from decades to centuries, especially in
response to anthropogenic greenhouse gas release. State-of-the-art ESMs can
reproduce the observational global mean temperature anomalies of the last 150
years. Nevertheless, ESMs need further improvements, most importantly regarding
(i) the large spread in their estimates of climate sensitivity, i.e., the
temperature response to increases in atmospheric greenhouse gases, (ii) the
modeled spatial patterns of key variables such as temperature and
precipitation, (iii) their representation of extreme weather events, and (iv)
their representation of multistable Earth system components and their ability
to predict associated abrupt transitions. Here, we argue that making ESMs
automatically differentiable has huge potential to advance ESMs, especially
with respect to these key shortcomings. First, automatic differentiability
would allow objective calibration of ESMs, i.e., the selection of optimal
values with respect to a cost function for a large number of free parameters,
which are currently tuned mostly manually. Second, recent advances in Machine
Learning (ML) and in the amount, accuracy, and resolution of observational data
promise to be helpful with at least some of the above aspects because ML may be
used to incorporate additional information from observations into ESMs.
Automatic differentiability is an essential ingredient in the construction of
such hybrid models, combining process-based ESMs with ML components. We
document recent work showcasing the potential of automatic differentiation for
a new generation of substantially improved, data-informed ESMs."
10552,"These
SRL goals align with ExAI, and further research should be conducted to strengthen this connection.","SRL can be seen as a way to simplify the problem of decision-making by separating
feature extraction and policy learning by identifying the low-dimensional set of important variables for control.","6 Evaluation and Benchmark

6.1 Evaluation Metrics

Evaluating the learned representations is crucial for assessing and comparing new approaches, especially in DRL, where
most of the results are empirical.",2022-08-27 09:38:56+00:00,Unsupervised Representation Learning in Deep Reinforcement Learning: A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nicolò Botteghi'), arxiv.Result.Author('Mannes Poel'), arxiv.Result.Author('Christoph Brune')]","This review addresses the problem of learning abstract representations of the
measurement data in the context of Deep Reinforcement Learning (DRL). While the
data are often ambiguous, high-dimensional, and complex to interpret, many
dynamical systems can be effectively described by a low-dimensional set of
state variables. Discovering these state variables from the data is a crucial
aspect for improving the data efficiency, robustness and generalization of DRL
methods, tackling the curse of dimensionality, and bringing interpretability
and insights into black-box DRL. This review provides a comprehensive and
complete overview of unsupervised representation learning in DRL by describing
the main Deep Learning tools used for learning representations of the world,
providing a systematic view of the method and principles, summarizing
applications, benchmarks and evaluation strategies, and discussing open
challenges and future directions."
10553,"Therefore, we believe that further research must be done to understand what the optimal state representation is, how to
learn it, and how to interpret and explain representations and agents’ behaviors.","High human-interpretability of the representations does not necessarily mean optimal behaviors of
the agents.","8 Conclusion

This paper reviewed the most important and newest research trends on unsupervised SRL in DRL.",2022-08-27 09:38:56+00:00,Unsupervised Representation Learning in Deep Reinforcement Learning: A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nicolò Botteghi'), arxiv.Result.Author('Mannes Poel'), arxiv.Result.Author('Christoph Brune')]","This review addresses the problem of learning abstract representations of the
measurement data in the context of Deep Reinforcement Learning (DRL). While the
data are often ambiguous, high-dimensional, and complex to interpret, many
dynamical systems can be effectively described by a low-dimensional set of
state variables. Discovering these state variables from the data is a crucial
aspect for improving the data efficiency, robustness and generalization of DRL
methods, tackling the curse of dimensionality, and bringing interpretability
and insights into black-box DRL. This review provides a comprehensive and
complete overview of unsupervised representation learning in DRL by describing
the main Deep Learning tools used for learning representations of the world,
providing a systematic view of the method and principles, summarizing
applications, benchmarks and evaluation strategies, and discussing open
challenges and future directions."
10554,"These
SRL goals align with ExAI, and further research should be conducted to strengthen this connection.","SRL can be seen as a way to simplify the problem of decision-making by separating
feature extraction and policy learning by identifying the low-dimensional set of important variables for control.","5.3 Summary

In Figure 22, we show the use of the SRL methods for the different groups of applications.",2022-08-27 09:38:56+00:00,Unsupervised Representation Learning in Deep Reinforcement Learning: A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nicolò Botteghi'), arxiv.Result.Author('Mannes Poel'), arxiv.Result.Author('Christoph Brune')]","This review addresses the problem of learning abstract representations of the
measurement data in the context of Deep Reinforcement Learning (DRL). While the
data are often ambiguous, high-dimensional, and complex to interpret, many
dynamical systems can be effectively described by a low-dimensional set of
state variables. Discovering these state variables from the data is a crucial
aspect for improving the data efficiency, robustness and generalization of DRL
methods, tackling the \textit{curse of dimensionality}, and bringing
interpretability and insights into black-box DRL. This review provides a
comprehensive and complete overview of unsupervised representation learning in
DRL by describing the main Deep Learning tools used for learning
representations of the world, providing a systematic view of the method and
principles, summarizing applications, benchmarks and evaluation strategies, and
discussing open challenges and future directions."
10555,"Therefore, we believe that further research must be done to understand what the optimal state representation is, how to
learn it, and how to interpret and explain representations and agents’ behaviors.","High human-interpretability of the representations does not necessarily mean optimal behaviors of
the agents.","8 Conclusion

This paper reviewed the most important and newest research trends on unsupervised SRL in DRL.",2022-08-27 09:38:56+00:00,Unsupervised Representation Learning in Deep Reinforcement Learning: A Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nicolò Botteghi'), arxiv.Result.Author('Mannes Poel'), arxiv.Result.Author('Christoph Brune')]","This review addresses the problem of learning abstract representations of the
measurement data in the context of Deep Reinforcement Learning (DRL). While the
data are often ambiguous, high-dimensional, and complex to interpret, many
dynamical systems can be effectively described by a low-dimensional set of
state variables. Discovering these state variables from the data is a crucial
aspect for improving the data efficiency, robustness and generalization of DRL
methods, tackling the \textit{curse of dimensionality}, and bringing
interpretability and insights into black-box DRL. This review provides a
comprehensive and complete overview of unsupervised representation learning in
DRL by describing the main Deep Learning tools used for learning
representations of the world, providing a systematic view of the method and
principles, summarizing applications, benchmarks and evaluation strategies, and
discussing open challenges and future directions."
10557,"This leads to several interesting
consequences and opportunities for further research:

CL comparability A direct consequence is the diﬃculty to directly compare
results of diﬀerent articles.","The so-called default scenario,
see Sec 3, is the nearest thing to a commonly agreed scenario, yet many de-
tails ﬂuctuate strongly between contributions.","This underscores the need, in CL more than in other
domains of machine learning, to precisely describe evaluation procedures and,
where possible, make use of existing libraries (see Sec 3 and 6.1) and evaluation
procedures.",2022-08-30 14:44:41+00:00,Beyond Supervised Continual Learning: a Review,cs.LG,['cs.LG'],"[arxiv.Result.Author('Benedikt Bagus'), arxiv.Result.Author('Alexander Gepperth'), arxiv.Result.Author('Timothée Lesort')]","Continual Learning (CL, sometimes also termed incremental learning) is a
flavor of machine learning where the usual assumption of stationary data
distribution is relaxed or omitted. When naively applying, e.g., DNNs in CL
problems, changes in the data distribution can cause the so-called catastrophic
forgetting (CF) effect: an abrupt loss of previous knowledge. Although many
significant contributions to enabling CL have been made in recent years, most
works address supervised (classification) problems. This article reviews
literature that study CL in other settings, such as learning with reduced
supervision, fully unsupervised learning, and reinforcement learning. Besides
proposing a simple schema for classifying CL approaches w.r.t. their level of
autonomy and supervision, we discuss the specific challenges associated with
each setting and the potential contributions to the field of CL in general."
10586,"The extent to which this is
possible is a topic for further research.","Ideally, we would like to obtain a gap factor γ that is as close
as possible to 1, so that d = d (see Deﬁnition 2.16).",Throughout this paper we use γ = 800.,2022-08-31 03:29:21+00:00,Fine-Grained Distribution-Dependent Learning Curves,cs.LG,"['cs.LG', 'cs.CC', 'stat.ML']","[arxiv.Result.Author('Olivier Bousquet'), arxiv.Result.Author('Steve Hanneke'), arxiv.Result.Author('Shay Moran'), arxiv.Result.Author('Jonathan Shafer'), arxiv.Result.Author('Ilya Tolstikhin')]","Learning curves plot the expected error of a learning algorithm as a function
of the number of labeled input samples. They are widely used by machine
learning practitioners as a measure of an algorithm's performance, but classic
PAC learning theory cannot explain their behavior. In this paper we introduce a
new combinatorial characterization called the VCL dimension that improves and
refines the recent results of Bousquet et al. (2021). Our characterization
sheds new light on the structure of learning curves by providing fine-grained
bounds, and showing that for classes with finite VCL, the rate of decay can be
decomposed into a linear component that depends only on the hypothesis class
and an exponential component that depends also on the target distribution. In
particular, the finer nuance of the VCL dimension implies lower bounds that are
quantitatively stronger than the bounds of Bousquet et al. (2021) and
qualitatively stronger than classic 'no free lunch' lower bounds. The VCL
characterization solves an open problem studied by Antos and Lugosi (1998), who
asked in what cases such lower bounds exist. As a corollary, we recover their
lower bound for half-spaces in $\mathbb{R}^d$, and we do so in a principled way
that should be applicable to other cases as well. Finally, to provide another
viewpoint on our work and how it compares to traditional PAC learning bounds,
we also present an alternative formulation of our results in a language that is
closer to the PAC setting."
10587,"The extent to
which this is possible is a topic for further research.","Ideally, we would like to obtain a gap factor γ that is as
close as possible to 1, so that d = d (see Deﬁnition 2.16).","Throughout this paper
we use γ = 800.",2022-08-31 03:29:21+00:00,Fine-Grained Distribution-Dependent Learning Curves,cs.LG,"['cs.LG', 'cs.CC', 'stat.ML']","[arxiv.Result.Author('Olivier Bousquet'), arxiv.Result.Author('Steve Hanneke'), arxiv.Result.Author('Shay Moran'), arxiv.Result.Author('Jonathan Shafer'), arxiv.Result.Author('Ilya Tolstikhin')]","Learning curves plot the expected error of a learning algorithm as a function
of the number of labeled samples it receives from a target distribution. They
are widely used as a measure of an algorithm's performance, but classic PAC
learning theory cannot explain their behavior.
  As observed by Antos and Lugosi (1996 , 1998), the classic `No Free Lunch'
lower bounds only trace the upper envelope above all learning curves of
specific target distributions. For a concept class with VC dimension $d$ the
classic bound decays like $d/n$, yet it is possible that the learning curve for
\emph{every} specific distribution decays exponentially. In this case, for each
$n$ there exists a different `hard' distribution requiring $d/n$ samples. Antos
and Lugosi asked which concept classes admit a `strong minimax lower bound' --
a lower bound of $d'/n$ that holds for a fixed distribution for infinitely many
$n$.
  We solve this problem in a principled manner, by introducing a combinatorial
dimension called VCL that characterizes the best $d'$ for which $d'/n$ is a
strong minimax lower bound. Our characterization strengthens the lower bounds
of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines
their theory of learning curves, by showing that for classes with finite VCL
the learning rate can be decomposed into a linear component that depends only
on the hypothesis class and an exponential component that depends also on the
target distribution. As a corollary, we recover the lower bound of Antos and
Lugosi (1996 , 1998) for half-spaces in $\mathbb{R}^d$.
  Finally, to provide another viewpoint on our work and how it compares to
traditional PAC learning bounds, we also present an alternative formulation of
our results in a language that is closer to the PAC setting."
10608,"8.1 Future Work

Based on this work, we identiﬁed two potential areas of further study

8.1.1 Extensions of Invariant Dropout

This work has implemented Invariant Dropout with three datasets and experimented with ﬁve mobile
clients.","Our technique, called Invariant Dropout, mitigates performance overheads while providing a 1.4%
point higher accuracy than the state-of-the-art technique, called Ordered Dropout, across three dif-
ferent datasets and ML models.","In future works based on Invariant Dropout, we could investigate how the technique could
be extended to support a wider variety of datasets and models.",2022-08-30 00:39:06+00:00,Reducing Impacts of System Heterogeneity in Federated Learning using Weight Update Magnitudes,cs.LG,['cs.LG'],[arxiv.Result.Author('Irene Wang')],"The widespread adoption of handheld devices have fueled rapid growth in new
applications. Several of these new applications employ machine learning models
to train on user data that is typically private and sensitive. Federated
Learning enables machine learning models to train locally on each handheld
device while only synchronizing their neuron updates with a server. While this
enables user privacy, technology scaling and software advancements have
resulted in handheld devices with varying performance capabilities. This
results in the training time of federated learning tasks to be dictated by a
few low-performance straggler devices, essentially becoming a bottleneck to the
entire training process. In this work, we aim to mitigate the performance
bottleneck of federated learning by dynamically forming sub-models for
stragglers based on their performance and accuracy feedback. To this end, we
offer the Invariant Dropout, a dynamic technique that forms a sub-model based
on the neuron update threshold. Invariant Dropout uses neuron updates from the
non-straggler clients to develop a tailored sub-models for each straggler
during each training iteration. All corresponding weights which have a
magnitude less than the threshold are dropped for the iteration. We evaluate
Invariant Dropout using five real-world mobile clients. Our evaluations show
that Invariant Dropout obtains a maximum accuracy gain of 1.4% points over
state-of-the-art Ordered Dropout while mitigating performance bottlenecks of
stragglers."
10610,"We
hope that this comparison has shed some light on the relative performance
of multi-omics integration methods, has produced valuable insights for their
application, and that it encourages further research.","Our experiments also suggest
that a fair experimental is necessary to see the strengths and weaknesses of
various algorithms, which were not visible from the publications alone.","17
Declarations

Acknowledgment

Funding

This work was funded by the German Federal Ministry for Education and
Research as part of the DIASyM project under grant number [031L0217A].",2022-08-31 12:46:08+00:00,A Fair Experimental Comparison of Neural Network Architectures for Latent Representations of Multi-Omics for Drug Response Prediction,cs.LG,"['cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Tony Hauptmann'), arxiv.Result.Author('Stefan Kramer')]","Recent years have seen a surge of novel neural network architectures for the
integration of multi-omics data for prediction. Most of the architectures
include either encoders alone or encoders and decoders, i.e., autoencoders of
various sorts, to transform multi-omics data into latent representations. One
important parameter is the depth of integration: the point at which the latent
representations are computed or merged, which can be either early,
intermediate, or late. The literature on integration methods is growing
steadily, however, close to nothing is known about the relative performance of
these methods under fair experimental conditions and under consideration of
different use cases. We developed a comparison framework that trains and
optimizes multi-omics integration methods under equal conditions. We
incorporated early integration and four recently published deep learning
methods: MOLI, Super.FELT, OmiEmbed, and MOMA. Further, we devised a novel
method, Omics Stacking, that combines the advantages of intermediate and late
integration. Experiments were conducted on a public drug response data set with
multiple omics data (somatic point mutations, somatic copy number profiles and
gene expression profiles) that was obtained from cell lines, patient-derived
xenografts, and patient samples. Our experiments confirmed that early
integration has the lowest predictive performance. Overall, architectures that
integrate triplet loss achieved the best results. Statistical differences can,
overall, rarely be observed, however, in terms of the average ranks of methods,
Super.FELT is consistently performing best in a cross-validation setting and
Omics Stacking best in an external test set setting. The source code of all
experiments is available under
\url{https://github.com/kramerlab/Multi-Omics_analysis}"
10612,"VARMA      EXP        SQ

VARMA        1.00±0.03  3.35±0.69  1.90±0.14
ShallowARMA  1.00±0.03  3.14±0.74  1.75±0.12
DeepARMA     1.01±0.03  3.10±0.75  1.76±0.11
LSTM         1.01±0.04  3.26±0.73  1.83±0.16
DeepLSTM     1.03±0.04  3.35±0.68  1.86±0.14
GRU          1.02±0.04  3.19±0.75  1.80±0.13
DeepGRU      1.01±0.04  3.22±0.80  1.82±0.18
SIMPLE       1.02±0.03  3.29±0.70  1.80±0.12
DeepSimple   1.02±0.03  3.30±0.84  1.83±0.13

we further study the empirical convergence of a single unit single hidden layer ARMA cell, which is mathe-
matically equivalent to an ARMA model for given values of p and q.","The best
performing method is highlighted in bold, the second-best in italics.","5.2 Benchmarks

In order to investigate the performance of our approach for real-world time series with a potentially more
complex generating process, we compare the previously deﬁned models on various time series benchmark
datasets.",2022-08-31 15:23:10+00:00,ARMA Cell: A Modular and Effective Approach for Neural Autoregressive Modeling,cs.LG,"['cs.LG', 'cs.NE', 'stat.ML', 'G.3']","[arxiv.Result.Author('Philipp Schiele'), arxiv.Result.Author('Christoph Berninger'), arxiv.Result.Author('David Rügamer')]","The autoregressive moving average (ARMA) model is a classical, and arguably
one of the most studied approaches to model time series data. It has compelling
theoretical properties and is widely used among practitioners. More recent deep
learning approaches popularize recurrent neural networks (RNNs) and, in
particular, long short-term memory (LSTM) cells that have become one of the
best performing and most common building blocks in neural time series modeling.
While advantageous for time series data or sequences with long-term effects,
complex RNN cells are not always a must and can sometimes even be inferior to
simpler recurrent approaches. In this work, we introduce the ARMA cell, a
simpler, modular, and effective approach for time series modeling in neural
networks. This cell can be used in any neural network architecture where
recurrent structures are present and naturally handles multivariate time series
using vector autoregression. We also introduce the ConvARMA cell as a natural
successor for spatially-correlated time series. Our experiments show that the
proposed methodology is competitive with popular alternatives in terms of
performance while being more robust and compelling due to its simplicity."
10662,"A
question that arose from this which requires further research was, under which attribute/s
would data distribution drift/shift be included?","This does not imply manufacturers are not making use of their own techniques and
the concepts were accepted as being extremely important and integral to any ML project.","Currently, both manufacturers obtain their data sets from real-world settings (partnered
healthcare organisations) which come with associated biases and limitations.",2022-09-01 13:00:36+00:00,Review of the AMLAS Methodology for Application in Healthcare,cs.LG,"['cs.LG', 'cs.SE']","[arxiv.Result.Author('Shakir Laher'), arxiv.Result.Author('Carla Brackstone'), arxiv.Result.Author('Sara Reis'), arxiv.Result.Author('An Nguyen'), arxiv.Result.Author('Sean White'), arxiv.Result.Author('Ibrahim Habli')]","In recent years, the number of machine learning (ML) technologies gaining
regulatory approval for healthcare has increased significantly allowing them to
be placed on the market. However, the regulatory frameworks applied to them
were originally devised for traditional software, which has largely rule-based
behaviour, compared to the data-driven and learnt behaviour of ML. As the
frameworks are in the process of reformation, there is a need to proactively
assure the safety of ML to prevent patient safety being compromised. The
Assurance of Machine Learning for use in Autonomous Systems (AMLAS) methodology
was developed by the Assuring Autonomy International Programme based on
well-established concepts in system safety. This review has appraised the
methodology by consulting ML manufacturers to understand if it converges or
diverges from their current safety assurance practices, whether there are gaps
and limitations in its structure and if it is fit for purpose when applied to
the healthcare domain. Through this work we offer the view that there is clear
utility for AMLAS as a safety assurance methodology when applied to healthcare
machine learning technologies, although development of healthcare specific
supplementary guidance would benefit those implementing the methodology."
10676,"As an attempt to identify the limitations in the current Neural Process-based modeling frameworks
          and fuel further research advances, we discuss perspectives on several such directions that are plau-
          sible of bringing far-sighted impacts to the ﬁeld.",4.,The rest of the paper is organized as follows.,2022-09-01 15:01:32+00:00,"The Neural Process Family: Survey, Applications and Perspectives",cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Saurav Jha'), arxiv.Result.Author('Dong Gong'), arxiv.Result.Author('Xuesong Wang'), arxiv.Result.Author('Richard E. Turner'), arxiv.Result.Author('Lina Yao')]","The standard approaches to neural network implementation yield powerful
function approximation capabilities but are limited in their abilities to learn
meta representations and reason probabilistic uncertainties in their
predictions. Gaussian processes, on the other hand, adopt the Bayesian learning
scheme to estimate such uncertainties but are constrained by their efficiency
and approximation capacity. The Neural Processes Family (NPF) intends to offer
the best of both worlds by leveraging neural networks for meta-learning
predictive uncertainties. Such potential has brought substantial research
activity to the family in recent years. Therefore, a comprehensive survey of
NPF models is needed to organize and relate their motivation, methodology, and
experiments. This paper intends to address this gap while digging deeper into
the formulation, research themes, and applications concerning the family
members. We shed light on their potential to bring several recent advances in
other deep learning domains under one umbrella. We then provide a rigorous
taxonomy of the family and empirically demonstrate their capabilities for
modeling data generating functions operating on 1-d, 2-d, and 3-d input
domains. We conclude by discussing our perspectives on the promising directions
that can fuel the research advances in the field. Code for our experiments will
be made available at https://github.com/srvCodes/neural-processes-survey."
10677,"In this section, we outline some of the major issues faced by the contemporary NPF branches and
highlight the directions for further research.","7 Future Research Directions

The domain of uncertainty-aware deep modeling of stochastic processes carries a huge potential for im-
provement despite the eﬀectiveness of the current Neural Process variants over static kernel-based Gaussian
processes.",Cost-eﬃcient generalization.,2022-09-01 15:01:32+00:00,"The Neural Process Family: Survey, Applications and Perspectives",cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Saurav Jha'), arxiv.Result.Author('Dong Gong'), arxiv.Result.Author('Xuesong Wang'), arxiv.Result.Author('Richard E. Turner'), arxiv.Result.Author('Lina Yao')]","The standard approaches to neural network implementation yield powerful
function approximation capabilities but are limited in their abilities to learn
meta representations and reason probabilistic uncertainties in their
predictions. Gaussian processes, on the other hand, adopt the Bayesian learning
scheme to estimate such uncertainties but are constrained by their efficiency
and approximation capacity. The Neural Processes Family (NPF) intends to offer
the best of both worlds by leveraging neural networks for meta-learning
predictive uncertainties. Such potential has brought substantial research
activity to the family in recent years. Therefore, a comprehensive survey of
NPF models is needed to organize and relate their motivation, methodology, and
experiments. This paper intends to address this gap while digging deeper into
the formulation, research themes, and applications concerning the family
members. We shed light on their potential to bring several recent advances in
other deep learning domains under one umbrella. We then provide a rigorous
taxonomy of the family and empirically demonstrate their capabilities for
modeling data generating functions operating on 1-d, 2-d, and 3-d input
domains. We conclude by discussing our perspectives on the promising directions
that can fuel the research advances in the field. Code for our experiments will
be made available at https://github.com/srvCodes/neural-processes-survey."
10678,"As an attempt to identify the limitations in the current Neural Process-based modeling frameworks
          and fuel further research advances, we discuss perspectives on several such directions that are plau-
          sible of bringing far-sighted impacts to the ﬁeld.",4.,The rest of the paper is organized as follows.,2022-09-01 15:01:32+00:00,"The Neural Process Family: Survey, Applications and Perspectives",cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Saurav Jha'), arxiv.Result.Author('Dong Gong'), arxiv.Result.Author('Xuesong Wang'), arxiv.Result.Author('Richard E. Turner'), arxiv.Result.Author('Lina Yao')]","The standard approaches to neural network implementation yield powerful
function approximation capabilities but are limited in their abilities to learn
meta representations and reason probabilistic uncertainties in their
predictions. Gaussian processes, on the other hand, adopt the Bayesian learning
scheme to estimate such uncertainties but are constrained by their efficiency
and approximation capacity. The Neural Processes Family (NPF) intends to offer
the best of both worlds by leveraging neural networks for meta-learning
predictive uncertainties. Such potential has brought substantial research
activity to the family in recent years. Therefore, a comprehensive survey of
NPF models is needed to organize and relate their motivation, methodology, and
experiments. This paper intends to address this gap while digging deeper into
the formulation, research themes, and applications concerning the family
members. We shed light on their potential to bring several recent advances in
other deep learning domains under one umbrella. We then provide a rigorous
taxonomy of the family and empirically demonstrate their capabilities for
modeling data generating functions operating on 1-d, 2-d, and 3-d input
domains. We conclude by discussing our perspectives on the promising directions
that can fuel the research advances in the field. Code for our experiments will
be made available at https://github.com/srvCodes/neural-processes-survey."
10679,"In this section, we outline some of the major issues faced by the contemporary NPF branches and
highlight the directions for further research.","52
7 Future Research Directions

The domain of uncertainty-aware deep modeling of stochastic processes carries a huge potential for im-
provement despite the eﬀectiveness of the current Neural Process variants over static kernel-based Gaussian
processes.",Cost-eﬃcient generalization.,2022-09-01 15:01:32+00:00,"The Neural Process Family: Survey, Applications and Perspectives",cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Saurav Jha'), arxiv.Result.Author('Dong Gong'), arxiv.Result.Author('Xuesong Wang'), arxiv.Result.Author('Richard E. Turner'), arxiv.Result.Author('Lina Yao')]","The standard approaches to neural network implementation yield powerful
function approximation capabilities but are limited in their abilities to learn
meta representations and reason probabilistic uncertainties in their
predictions. Gaussian processes, on the other hand, adopt the Bayesian learning
scheme to estimate such uncertainties but are constrained by their efficiency
and approximation capacity. The Neural Processes Family (NPF) intends to offer
the best of both worlds by leveraging neural networks for meta-learning
predictive uncertainties. Such potential has brought substantial research
activity to the family in recent years. Therefore, a comprehensive survey of
NPF models is needed to organize and relate their motivation, methodology, and
experiments. This paper intends to address this gap while digging deeper into
the formulation, research themes, and applications concerning the family
members. We shed light on their potential to bring several recent advances in
other deep learning domains under one umbrella. We then provide a rigorous
taxonomy of the family and empirically demonstrate their capabilities for
modeling data generating functions operating on 1-d, 2-d, and 3-d input
domains. We conclude by discussing our perspectives on the promising directions
that can fuel the research advances in the field. Code for our experiments will
be made available at https://github.com/srvCodes/neural-processes-survey."
10680,"Finally, the source code for our algorithm is publicly
available at our GitHub repository1 for easy reproducibility and to support further research in non-uniform sampling
methods.","Therefore, we ﬁrmly believe that issues with PER in continuous control are corrected by the presented
modiﬁcations supported by a comprehensive theoretical analysis.","References

Long ji Lin.",2022-09-01 15:27:46+00:00,Actor Prioritized Experience Replay,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Baturay Saglam'), arxiv.Result.Author('Furkan B. Mutlu'), arxiv.Result.Author('Dogan C. Cicek'), arxiv.Result.Author('Suleyman S. Kozat')]","A widely-studied deep reinforcement learning (RL) technique known as
Prioritized Experience Replay (PER) allows agents to learn from transitions
sampled with non-uniform probability proportional to their temporal-difference
(TD) error. Although it has been shown that PER is one of the most crucial
components for the overall performance of deep RL methods in discrete action
domains, many empirical studies indicate that it considerably underperforms
actor-critic algorithms in continuous control. We theoretically show that actor
networks cannot be effectively trained with transitions that have large TD
errors. As a result, the approximate policy gradient computed under the
Q-network diverges from the actual gradient computed under the optimal
Q-function. Motivated by this, we introduce a novel experience replay sampling
framework for actor-critic methods, which also regards issues with stability
and recent findings behind the poor empirical performance of PER. The
introduced algorithm suggests a new branch of improvements to PER and schedules
effective and efficient training for both actor and critic networks. An
extensive set of experiments verifies our theoretical claims and demonstrates
that the introduced method significantly outperforms the competing approaches
and obtains state-of-the-art results over the standard off-policy actor-critic
algorithms."
10688,"2.2.5 The AlgoVision Library

                                              We published the proposed method in the form of the   li-

                                              brary to facilitate easy application and further research in this direction.","A collection of possible distributions is discussed in
                                              Supplementary Material A.","The

                                              library is based on Python and PyTorch [26].",2022-09-01 17:30:00+00:00,Learning with Differentiable Algorithms,cs.LG,['cs.LG'],[arxiv.Result.Author('Felix Petersen')],"Classic algorithms and machine learning systems like neural networks are both
abundant in everyday life. While classic computer science algorithms are
suitable for precise execution of exactly defined tasks such as finding the
shortest path in a large graph, neural networks allow learning from data to
predict the most likely answer in more complex tasks such as image
classification, which cannot be reduced to an exact algorithm. To get the best
of both worlds, this thesis explores combining both concepts leading to more
robust, better performing, more interpretable, more computationally efficient,
and more data efficient architectures. The thesis formalizes the idea of
algorithmic supervision, which allows a neural network to learn from or in
conjunction with an algorithm. When integrating an algorithm into a neural
architecture, it is important that the algorithm is differentiable such that
the architecture can be trained end-to-end and gradients can be propagated back
through the algorithm in a meaningful way. To make algorithms differentiable,
this thesis proposes a general method for continuously relaxing algorithms by
perturbing variables and approximating the expectation value in closed form,
i.e., without sampling. In addition, this thesis proposes differentiable
algorithms, such as differentiable sorting networks, differentiable renderers,
and differentiable logic gate networks. Finally, this thesis presents
alternative training strategies for learning with algorithms."
10692,"The further research direction could be to expand
the corpus with more patients, extend the scope of diagnosis with heart pathologies, experiment with the latest sound
corpus HF_Lung_V2 [18], and apply deep learning to spectrogram-based image representations of sounds with the aim
of improving detection success.","Our pilot research shows that supervised machine learning using the standard audio feature extractor can be successfully
applied to differentiate between pathological and normal lung sounds.","Acknowledgements

This work was supported by a student summer research grant provided to Lukas Drukteinis and Evaldas Vaicˇiukynas
by the Research Council of Lithuania (agreement No.",2022-09-01 18:03:21+00:00,Exploring traditional machine learning for identification of pathological auscultations,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Haroldas Razvadauskas'), arxiv.Result.Author('Evaldas Vaiciukynas'), arxiv.Result.Author('Kazimieras Buskus'), arxiv.Result.Author('Lukas Drukteinis'), arxiv.Result.Author('Lukas Arlauskas'), arxiv.Result.Author('Saulius Sadauskas'), arxiv.Result.Author('Albinas Naudziunas')]","Today, data collection has improved in various areas, and the medical domain
is no exception. Auscultation, as an important diagnostic technique for
physicians, due to the progress and availability of digital stethoscopes, lends
itself well to applications of machine learning. Due to the large number of
auscultations performed, the availability of data opens up an opportunity for
more effective analysis of sounds where prognostic accuracy even among experts
remains low. In this study, digital 6-channel auscultations of 45 patients were
used in various machine learning scenarios, with the aim of distinguishing
between normal and anomalous pulmonary sounds. Audio features (such as
fundamental frequencies F0-4, loudness, HNR, DFA, as well as descriptive
statistics of log energy, RMS and MFCC) were extracted using the Python library
Surfboard. Windowing and feature aggregation and concatenation strategies were
used to prepare data for tree-based ensemble models in unsupervised (fair-cut
forest) and supervised (random forest) machine learning settings. The
evaluation was carried out using 9-fold stratified cross-validation repeated 30
times. Decision fusion by averaging outputs for a subject was tested and found
to be useful. Supervised models showed a consistent advantage over unsupervised
ones, achieving mean AUC ROC of 0.691 (accuracy 71.11%, Kappa 0.416, F1-score
0.771) in side-based detection and mean AUC ROC of 0.721 (accuracy 68.89%,
Kappa 0.371, F1-score 0.650) in patient-based detection."
10715,"Additionally, the sheer volume of work can
obscure major trends and hinder further research progress.","Over
the past two years, the body of research on diffusion models has grown significantly, making it increasingly challenging
for new researchers to stay abreast of the recent developments in the field.","This survey aims to address these problems by providing a
comprehensive overview of the state of diffusion model research, categorizing various approaches, and highlighting
key advances.",2022-09-02 02:59:10+00:00,Diffusion Models: A Comprehensive Survey of Methods and Applications,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Zhilong Zhang'), arxiv.Result.Author('Yang Song'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Runsheng Xu'), arxiv.Result.Author('Yue Zhao'), arxiv.Result.Author('Yingxia Shao'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Bin Cui'), arxiv.Result.Author('Ming-Hsuan Yang')]","Diffusion models have emerged as a powerful new family of deep generative
models with record-breaking performance in many applications, including image
synthesis, video generation, and molecule design. In this survey, we provide an
overview of the rapidly expanding body of work on diffusion models,
categorizing the research into three key areas: efficient sampling, improved
likelihood estimation, and handling data with special structures. We also
discuss the potential for combining diffusion models with other generative
models for enhanced results. We further review the wide-ranging applications of
diffusion models in fields spanning from computer vision, natural language
processing, temporal data modeling, to interdisciplinary applications in other
scientific disciplines. This survey aims to provide a contextualized, in-depth
look at the state of diffusion models, identifying the key areas of focus and
pointing to potential areas for further exploration. Github:
https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy."
10716,"Additionally, the sheer volume of work can
obscure major trends and hinder further research progress.","Over
the past two years, the body of research on diffusion models has grown significantly, making it increasingly challenging
for new researchers to stay abreast of the recent developments in the field.","This survey aims to address these problems by providing a
comprehensive overview of the state of diffusion model research, categorizing various approaches, and highlighting
key advances.",2022-09-02 02:59:10+00:00,Diffusion Models: A Comprehensive Survey of Methods and Applications,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Ling Yang'), arxiv.Result.Author('Zhilong Zhang'), arxiv.Result.Author('Yang Song'), arxiv.Result.Author('Shenda Hong'), arxiv.Result.Author('Runsheng Xu'), arxiv.Result.Author('Yue Zhao'), arxiv.Result.Author('Yingxia Shao'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Bin Cui'), arxiv.Result.Author('Ming-Hsuan Yang')]","Diffusion models have emerged as a powerful new family of deep generative
models with record-breaking performance in many applications, including image
synthesis, video generation, and molecule design. In this survey, we provide an
overview of the rapidly expanding body of work on diffusion models,
categorizing the research into three key areas: efficient sampling, improved
likelihood estimation, and handling data with special structures. We also
discuss the potential for combining diffusion models with other generative
models for enhanced results. We further review the wide-ranging applications of
diffusion models in fields spanning from computer vision, natural language
processing, temporal data modeling, to interdisciplinary applications in other
scientific disciplines. This survey aims to provide a contextualized, in-depth
look at the state of diffusion models, identifying the key areas of focus and
pointing to potential areas for further exploration. Github:
https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy."
10728,"Despite the MTA-UNet neural network we constructed for the regression problems of multiple physical
quantities, further research is still required.","Compared with the data-driven only strategy, the physics-informed method exhibits better performance in
training sets with diﬀerent sample sizes and obtains solutions more consistent with the laws of physics.","As our model structure is strongly general, it is worth exploring
strategies to enhance the balance between multiple tasks further.",2022-09-01 12:00:33+00:00,Physics-informed MTA-UNet: Prediction of Thermal Stress and Thermal Deformation of Satellites,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zeyu Cao'), arxiv.Result.Author('Wei Peng'), arxiv.Result.Author('Xiaoya Zhang'), arxiv.Result.Author('Kairui Bao'), arxiv.Result.Author('Wen Yao')]","The rapid analysis of thermal stress and deformation plays a pivotal role in
the thermal control measures and optimization of the structural design of
satellites. For achieving real-time thermal stress and thermal deformation
analysis of satellite motherboards, this paper proposes a novel Multi-Task
Attention UNet (MTA-UNet) neural network which combines the advantages of both
Multi-Task Learning (MTL) and U-Net with attention mechanism. Besides, a
physics-informed strategy is used in the training process, where partial
differential equations (PDEs) are integrated into the loss functions as
residual terms. Finally, an uncertainty-based loss balancing approach is
applied to weight different loss functions of multiple training tasks.
Experimental results show that the proposed MTA-UNet effectively improves the
prediction accuracy of multiple physics tasks compared with Single-Task
Learning (STL) models. In addition, the physics-informed method brings less
error in the prediction of each task, especially on small data sets. The code
can be downloaded at: \url{https://github.com/KomorebiTso/MTA-UNet}."
10729,"Despite the MTA-UNet neural network we constructed for the regression problems of multiple physical
quantities, further research is still required.","Compared with the data-driven only strategy, the physics-informed method exhibits better performance in
training sets with diﬀerent sample sizes and obtains solutions more consistent with the laws of physics.","As our model structure is strongly general, it is worth exploring
strategies to enhance the balance between multiple tasks further.",2022-09-01 12:00:33+00:00,Physics-informed MTA-UNet: Prediction of Thermal Stress and Thermal Deformation of Satellites,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zeyu Cao'), arxiv.Result.Author('Wen Yao'), arxiv.Result.Author('Wei Peng'), arxiv.Result.Author('Xiaoya Zhang'), arxiv.Result.Author('Kairui Bao')]","The rapid analysis of thermal stress and deformation plays a pivotal role in
the thermal control measures and optimization of the structural design of
satellites. For achieving real-time thermal stress and thermal deformation
analysis of satellite motherboards, this paper proposes a novel Multi-Task
Attention UNet (MTA-UNet) neural network which combines the advantages of both
Multi-Task Learning (MTL) and U-Net with attention mechanism. Besides, a
physics-informed strategy is used in the training process, where partial
differential equations (PDEs) are integrated into the loss functions as
residual terms. Finally, an uncertainty-based loss balancing approach is
applied to weight different loss functions of multiple training tasks.
Experimental results show that the proposed MTA-UNet effectively improves the
prediction accuracy of multiple physics tasks compared with Single-Task
Learning (STL) models. In addition, the physics-informed method brings less
error in the prediction of each task, especially on small data sets. The code
can be downloaded at: \url{https://github.com/KomorebiTso/MTA-UNet}."
10736,"The second goal is to ensure that anyone in the same
laboratory or further researchers can reproduce the experiments.","The
ﬁrst goal is to make an accurate translation from bioprocess problems to ap-
propriate machine learning tasks that can produce a correct prediction on the
experimental datasets.","Therefore, we
will also provide an in-depth investigation on the reproducibility capability of
these mentioned research that we either believe in the results or build up the
conﬁdence of reproducing the whole experiments and improving further from
there.",2022-09-02 14:30:49+00:00,When Bioprocess Engineering Meets Machine Learning: A Survey from the Perspective of Automated Bioprocess Development,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nghia Duong-Trung'), arxiv.Result.Author('Stefan Born'), arxiv.Result.Author('Jong Woo Kim'), arxiv.Result.Author('Marie-Therese Schermeyer'), arxiv.Result.Author('Katharina Paulick'), arxiv.Result.Author('Maxim Borisyak'), arxiv.Result.Author('Ernesto Martinez'), arxiv.Result.Author('Mariano Nicolas Cruz-Bournazou'), arxiv.Result.Author('Thorben Werner'), arxiv.Result.Author('Randolf Scholz'), arxiv.Result.Author('Lars Schmidt-Thieme'), arxiv.Result.Author('Peter Neubauer')]","Machine learning (ML) has significantly contributed to the development of
bioprocess engineering, but its application is still limited, hampering the
enormous potential for bioprocess automation. ML for model building automation
can be seen as a way of introducing another level of abstraction to focus
expert humans in the most cognitive tasks of bioprocess development. First,
probabilistic programming is used for the autonomous building of predictive
models. Second, machine learning automatically assesses alternative decisions
by planning experiments to test hypotheses and conducting investigations to
gather informative data that focus on model selection based on the uncertainty
of model predictions. This review provides a comprehensive overview of ML-based
automation in bioprocess development. On the one hand, the biotech and
bioengineering community should be aware of the potential and, most
importantly, the limitation of existing ML solutions for their application in
biotechnology and biopharma. On the other hand, it is essential to identify the
missing links to enable the easy implementation of ML and Artificial
Intelligence (AI) solutions in valuable solutions for the bio-community. We
summarize recent ML implementation across several important subfields of
bioprocess systems and raise two crucial challenges remaining the bottleneck of
bioprocess automation and reducing uncertainty in biotechnology development.
There is no one-fits-all procedure; however, this review should help identify
the potential automation combining biotechnology and ML domains."
10737,"The second goal is to ensure that anyone in the same
laboratory or further researchers can reproduce the experiments.","The ﬁrst goal is to make an accurate translation from bioprocess problems to
appropriate machine learning tasks that can produce a correct prediction on the

                                                  34
experimental datasets.","Therefore, we
will also provide an in-depth investigation of the reproducibility capability of
these mentioned research so that we either believe in the results or build up
conﬁdence in reproducing the whole experiments and improving further from
there.",2022-09-02 14:30:49+00:00,When Bioprocess Engineering Meets Machine Learning: A Survey from the Perspective of Automated Bioprocess Development,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nghia Duong-Trung'), arxiv.Result.Author('Stefan Born'), arxiv.Result.Author('Jong Woo Kim'), arxiv.Result.Author('Marie-Therese Schermeyer'), arxiv.Result.Author('Katharina Paulick'), arxiv.Result.Author('Maxim Borisyak'), arxiv.Result.Author('Mariano Nicolas Cruz-Bournazou'), arxiv.Result.Author('Thorben Werner'), arxiv.Result.Author('Randolf Scholz'), arxiv.Result.Author('Lars Schmidt-Thieme'), arxiv.Result.Author('Peter Neubauer'), arxiv.Result.Author('Ernesto Martinez')]","Machine learning (ML) is becoming increasingly crucial in many fields of
engineering but has not yet played out its full potential in bioprocess
engineering. While experimentation has been accelerated by increasing levels of
lab automation, experimental planning and data modeling are still largerly
depend on human intervention. ML can be seen as a set of tools that contribute
to the automation of the whole experimental cycle, including model building and
practical planning, thus allowing human experts to focus on the more demanding
and overarching cognitive tasks. First, probabilistic programming is used for
the autonomous building of predictive models. Second, machine learning
automatically assesses alternative decisions by planning experiments to test
hypotheses and conducting investigations to gather informative data that focus
on model selection based on the uncertainty of model predictions. This review
provides a comprehensive overview of ML-based automation in bioprocess
development. On the one hand, the biotech and bioengineering community should
be aware of the potential and, most importantly, the limitation of existing ML
solutions for their application in biotechnology and biopharma. On the other
hand, it is essential to identify the missing links to enable the easy
implementation of ML and Artificial Intelligence (AI) tools in valuable
solutions for the bio-community."
10762,"Source codes           label scarcity where, unlike conventional multistream clas-
                                       of LEOPARD are shared in https://github.com/wengweng001/                   siﬁcation problems, the source stream and the target stream
                                       LEOPARD.git to enable further study.","Our numerical study demonstrates the            Practical Scenario: This paper puts into perspective a cross-
                                       efﬁcacy of LEOPARD where it delivers improved performances                 domain multistream classiﬁcation problem under extreme
                                       compared to prominent algorithms in 15 of 24 cases.","are generated from different feature spaces but share the same
                                                                                                                  target attributes.",2022-09-04 07:17:17+00:00,Autonomous Cross Domain Adaptation under Extreme Label Scarcity,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Weiwei Weng'), arxiv.Result.Author('Mahardhika Pratama'), arxiv.Result.Author(""Choiru Za'in""), arxiv.Result.Author('Marcus De Carvalho'), arxiv.Result.Author('Rakaraddi Appan'), arxiv.Result.Author('Andri Ashfahani'), arxiv.Result.Author('Edward Yapp Kien Yee')]","A cross domain multistream classification is a challenging problem calling
for fast domain adaptations to handle different but related streams in
never-ending and rapidly changing environments. Notwithstanding that existing
multistream classifiers assume no labelled samples in the target stream, they
still incur expensive labelling cost since they require fully labelled samples
of the source stream. This paper aims to attack the problem of extreme label
shortage in the cross domain multistream classification problems where only
very few labelled samples of the source stream are provided before process
runs. Our solution, namely Learning Streaming Process from Partial Ground Truth
(LEOPARD), is built upon a flexible deep clustering network where its hidden
nodes, layers and clusters are added and removed dynamically in respect to
varying data distributions. A deep clustering strategy is underpinned by a
simultaneous feature learning and clustering technique leading to
clustering-friendly latent spaces. A domain adaptation strategy relies on the
adversarial domain adaptation technique where a feature extractor is trained to
fool a domain classifier classifying source and target streams. Our numerical
study demonstrates the efficacy of LEOPARD where it delivers improved
performances compared to prominent algorithms in 15 of 24 cases. Source codes
of LEOPARD are shared in \url{https://github.com/wengweng001/LEOPARD.git} to
enable further study."
10763,com/wengweng001/LEOPARD.git to enable further study.,"Another
along with all datasets are made public in https://github.","Our
                                                                                     stream respectively where KS, KT are respectively the number
                                                                                     of source stream and target stream unknown in practise.",2022-09-04 07:17:17+00:00,Autonomous Cross Domain Adaptation under Extreme Label Scarcity,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Weiwei Weng'), arxiv.Result.Author('Mahardhika Pratama'), arxiv.Result.Author(""Choiru Za'in""), arxiv.Result.Author('Marcus De Carvalho'), arxiv.Result.Author('Rakaraddi Appan'), arxiv.Result.Author('Andri Ashfahani'), arxiv.Result.Author('Edward Yapp Kien Yee')]","A cross domain multistream classification is a challenging problem calling
for fast domain adaptations to handle different but related streams in
never-ending and rapidly changing environments. Notwithstanding that existing
multistream classifiers assume no labelled samples in the target stream, they
still incur expensive labelling cost since they require fully labelled samples
of the source stream. This paper aims to attack the problem of extreme label
shortage in the cross domain multistream classification problems where only
very few labelled samples of the source stream are provided before process
runs. Our solution, namely Learning Streaming Process from Partial Ground Truth
(LEOPARD), is built upon a flexible deep clustering network where its hidden
nodes, layers and clusters are added and removed dynamically in respect to
varying data distributions. A deep clustering strategy is underpinned by a
simultaneous feature learning and clustering technique leading to
clustering-friendly latent spaces. A domain adaptation strategy relies on the
adversarial domain adaptation technique where a feature extractor is trained to
fool a domain classifier classifying source and target streams. Our numerical
study demonstrates the efficacy of LEOPARD where it delivers improved
performances compared to prominent algorithms in 15 of 24 cases. Source codes
of LEOPARD are shared in \url{https://github.com/wengweng001/LEOPARD.git} to
enable further study."
10768,"This warrants further study into the value and
positioning of expert layers.","But a deeper analysis of the full encoder-decoder ST-MoE architecture found clearer evidence of
specialization in the encoder, rather than the decoder.","Lack of evident specialization may either signal a difﬁcult to discern
patterns or no useful patterns.",2022-09-04 18:00:29+00:00,A Review of Sparse Expert Models in Deep Learning,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('William Fedus'), arxiv.Result.Author('Jeff Dean'), arxiv.Result.Author('Barret Zoph')]","Sparse expert models are a thirty-year old concept re-emerging as a popular
architecture in deep learning. This class of architecture encompasses
Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and
others, all with the unifying idea that each example is acted on by a subset of
the parameters. By doing so, the degree of sparsity decouples the parameter
count from the compute per example allowing for extremely large, but efficient
models. The resulting models have demonstrated significant improvements across
diverse domains such as natural language processing, computer vision, and
speech recognition. We review the concept of sparse expert models, provide a
basic description of the common algorithms, contextualize the advances in the
deep learning era, and conclude by highlighting areas for future work."
10801,up helpful suggestions and triggers for further research.,"header information to research if machine learning approaches are
   The open questions and the discussion after the evaluation brought       able to classify encrypted data with high accuracy.","In the               Another challenges we encountered was the unbalanced
following we summarize the improvement feedback.",2022-09-05 16:34:43+00:00,Visualization Of Class Activation Maps To Explain AI Classification Of Network Packet Captures,cs.LG,"['cs.LG', 'cs.HC', 'cs.NI']","[arxiv.Result.Author('Igor Cherepanov'), arxiv.Result.Author('Alex Ulmer'), arxiv.Result.Author('Jonathan Geraldi Joewono'), arxiv.Result.Author('Jörn Kohlhammer')]","The classification of internet traffic has become increasingly important due
to the rapid growth of today's networks and applications. The number of
connections and the addition of new applications in our networks causes a vast
amount of log data and complicates the search for common patterns by experts.
Finding such patterns among specific classes of applications is necessary to
fulfill various requirements in network analytics. Deep learning methods
provide both feature extraction and classification from data in a single
system. However, these networks are very complex and are used as black-box
models, which weakens the experts' trust in the classifications. Moreover, by
using them as a black-box, new knowledge cannot be obtained from the model
predictions despite their excellent performance. Therefore, the explainability
of the classifications is crucial. Besides increasing trust, the explanation
can be used for model evaluation gaining new insights from the data and
improving the model. In this paper, we present a visual interactive tool that
combines the classification of network data with an explanation technique to
form an interface between experts, algorithms, and data."
10802,up helpful suggestions and triggers for further research.,"header information to research if machine learning approaches are
   The open questions and the discussion after the evaluation brought    able to classify encrypted data with high accuracy.","In the            Another challenges we encountered was the unbalanced
following we summarize the improvement feedback.",2022-09-05 16:34:43+00:00,Visualization Of Class Activation Maps To Explain AI Classification Of Network Packet Captures,cs.LG,"['cs.LG', 'cs.HC', 'cs.NI']","[arxiv.Result.Author('Igor Cherepanov'), arxiv.Result.Author('Alex Ulmer'), arxiv.Result.Author('Jonathan Geraldi Joewono'), arxiv.Result.Author('Jörn Kohlhammer')]","The classification of internet traffic has become increasingly important due
to the rapid growth of today's networks and applications. The number of
connections and the addition of new applications in our networks causes a vast
amount of log data and complicates the search for common patterns by experts.
Finding such patterns among specific classes of applications is necessary to
fulfill various requirements in network analytics. Deep learning methods
provide both feature extraction and classification from data in a single
system. However, these networks are very complex and are used as black-box
models, which weakens the experts' trust in the classifications. Moreover, by
using them as a black-box, new knowledge cannot be obtained from the model
predictions despite their excellent performance. Therefore, the explainability
of the classifications is crucial. Besides increasing trust, the explanation
can be used for model evaluation gaining new insights from the data and
improving the model. In this paper, we present a visual interactive tool that
combines the classification of network data with an explanation technique to
form an interface between experts, algorithms, and data."
10893,"The results on the time-
XXX Pimg                   2          4        5                   series data using RNN are inconclusive and further research
 ModelsXXX            (non-IID)  (non-IID)   (IID)                 on more challenging datasets is needed.","FedPer when the individual clients do not have sufﬁcient
                                                                   data to train the personalisation layers.","Further work on more
                  XX   97.08%     88.75%    91.00%                 challenging time-series data is desired to understand better the
                       97.08%     87.92%    90.00%                 advantages and limitations of the proposed framework in this
      FedX hapt        97.08%     86.25%    85.00%                 kind of data.",2022-09-07 11:54:55+00:00,Modular Federated Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kuo-Yun Liang'), arxiv.Result.Author('Abhishek Srinivasan'), arxiv.Result.Author('Juan Carlos Andresen')]","Federated learning is an approach to train machine learning models on the
edge of the networks, as close as possible where the data is produced,
motivated by the emerging problem of the inability to stream and centrally
store the large amount of data produced by edge devices as well as by data
privacy concerns. This learning paradigm is in need of robust algorithms to
device heterogeneity and data heterogeneity. This paper proposes ModFL as a
federated learning framework that splits the models into a configuration module
and an operation module enabling federated learning of the individual modules.
This modular approach makes it possible to extract knowlege from a group of
heterogeneous devices as well as from non-IID data produced from its users.
This approach can be viewed as an extension of the federated learning with
personalisation layers FedPer framework that addresses data heterogeneity. We
show that ModFL outperforms FedPer for non-IID data partitions of CIFAR-10 and
STL-10 using CNNs. Our results on time-series data with HAPT, RWHAR, and WISDM
datasets using RNNs remain inconclusive, we argue that the chosen datasets do
not highlight the advantages of ModFL, but in the worst case scenario it
performs as well as FedPer."
10896,"Finally, we can do further research in the future.","participants have
only one data category.",1.,2022-09-05 22:26:57+00:00,Federated Transfer Learning with Multimodal Data,cs.LG,"['cs.LG', 'cs.AI', '94-05', 'H.4.3']",[arxiv.Result.Author('Yulian Sun')],"Smart cars, smartphones and other devices in the Internet of Things (IoT),
which usually have more than one sensors, produce multimodal data. Federated
Learning supports collecting a wealth of multimodal data from different devices
without sharing raw data. Transfer Learning methods help transfer knowledge
from some devices to others. Federated Transfer Learning methods benefit both
Federated Learning and Transfer Learning. This newly proposed Federated
Transfer Learning framework aims at connecting data islands with privacy
protection. Our construction is based on Federated Learning and Transfer
Learning. Compared with previous Federated Transfer Learnings, where each user
should have data with identical modalities (either all unimodal or all
multimodal), our new framework is more generic, it allows a hybrid distribution
of user data. The core strategy is to use two different but inherently
connected training methods for our two types of users. Supervised Learning is
adopted for users with only unimodal data (Type 1), while Self-Supervised
Learning is applied to user with multimodal data (Type 2) for both the feature
of each modality and the connection between them. This connection knowledge of
Type 2 will help Type 1 in later stages of training. Training in the new
framework can be divided in three steps. In the first step, users who have data
with the identical modalities are grouped together. For example, user with only
sound signals are in group one, and those with only images are in group two,
and users with multimodal data are in group three, and so on. In the second
step, Federated Learning is executed within the groups, where Supervised
Learning and Self-Supervised Learning are used depending on the group's nature.
Most of the Transfer Learning happens in the third step, where the related
parts in the network obtained from the previous steps are aggregated
(federated)."
10930,"Consequently, we highlight that the proposed algorithms
consistently outperform the existing ones, making them valuable candidates for further research and
implementation.","In its simplest form, it does not have any parameter to tune and does not require explicit
construction of the conﬁdence bound.","Our work addresses practically relevant RL issues and, therefore, we ﬁrmly believe that it may help
design algorithms for real-life reinforcement learning applications.",2022-09-08 06:52:49+00:00,An Empirical Evaluation of Posterior Sampling for Constrained Reinforcement Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Danil Provodin'), arxiv.Result.Author('Pratik Gajane'), arxiv.Result.Author('Mykola Pechenizkiy'), arxiv.Result.Author('Maurits Kaptein')]","We study a posterior sampling approach to efficient exploration in
constrained reinforcement learning. Alternatively to existing algorithms, we
propose two simple algorithms that are more efficient statistically, simpler to
implement and computationally cheaper. The first algorithm is based on a linear
formulation of CMDP, and the second algorithm leverages the saddle-point
formulation of CMDP. Our empirical results demonstrate that, despite its
simplicity, posterior sampling achieves state-of-the-art performance and, in
some cases, significantly outperforms optimistic algorithms."
10936,"Possible future directions Speaking about the potential prospects for further research, three direc-
tions can be distinguished.","We showed that these regimes are present in conventional
neural network training as well and can provide intuition on how to achieve better optima.","First, it would be interesting to develop more solid and general theoretical
groundings for the observed phenomena.",2022-09-08 10:30:05+00:00,Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Maxim Kodryan'), arxiv.Result.Author('Ekaterina Lobacheva'), arxiv.Result.Author('Maksim Nakhodnov'), arxiv.Result.Author('Dmitry Vetrov')]","A fundamental property of deep learning normalization techniques, such as
batch normalization, is making the pre-normalization parameters scale
invariant. The intrinsic domain of such parameters is the unit sphere, and
therefore their gradient optimization dynamics can be represented via spherical
optimization with varying effective learning rate (ELR), which was studied
previously. In this work, we investigate the properties of training
scale-invariant neural networks directly on the sphere using a fixed ELR. We
discover three regimes of such training depending on the ELR value:
convergence, chaotic equilibrium, and divergence. We study these regimes in
detail both on a theoretical examination of a toy example and on a thorough
empirical analysis of real scale-invariant deep learning models. Each regime
has unique features and reflects specific properties of the intrinsic loss
landscape, some of which have strong parallels with previous research on both
regular and scale-invariant neural networks training. Finally, we demonstrate
how the discovered regimes are reflected in conventional training of normalized
networks and how they can be leveraged to achieve better optima."
10937,"Possible future directions Speaking about the potential prospects for further research, three direc-
tions can be distinguished.","We showed that these regimes are present in conventional
neural network training as well and can provide intuition on how to achieve better optima.","First, it would be interesting to develop more solid and general theoretical
groundings for the observed phenomena.",2022-09-08 10:30:05+00:00,Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Maxim Kodryan'), arxiv.Result.Author('Ekaterina Lobacheva'), arxiv.Result.Author('Maksim Nakhodnov'), arxiv.Result.Author('Dmitry Vetrov')]","A fundamental property of deep learning normalization techniques, such as
batch normalization, is making the pre-normalization parameters scale
invariant. The intrinsic domain of such parameters is the unit sphere, and
therefore their gradient optimization dynamics can be represented via spherical
optimization with varying effective learning rate (ELR), which was studied
previously. However, the varying ELR may obscure certain characteristics of the
intrinsic loss landscape structure. In this work, we investigate the properties
of training scale-invariant neural networks directly on the sphere using a
fixed ELR. We discover three regimes of such training depending on the ELR
value: convergence, chaotic equilibrium, and divergence. We study these regimes
in detail both on a theoretical examination of a toy example and on a thorough
empirical analysis of real scale-invariant deep learning models. Each regime
has unique features and reflects specific properties of the intrinsic loss
landscape, some of which have strong parallels with previous research on both
regular and scale-invariant neural networks training. Finally, we demonstrate
how the discovered regimes are reflected in conventional training of normalized
networks and how they can be leveraged to achieve better optima."
10989,"So, the research work of graph neural networks started with
how to ﬁx the number of neighbor nodes and how to sort neighbor nodes, such as PATCHY-SAN
[4], large-scale learnable graph convolutional networks (LGCN) [5], and diﬀusion-convolutional neural
networks(DCNN) [6].And now, with the further study on graph neural networks, researchers begin
to turn to the research on graph neural networks in various ﬁelds of deep learning, such as graph
reinforcement learning [7], graph transfer learning [8] and the explanation for graph neural networks
[9].","The essence of graph neural networks is utilizing neighbor nodes to update the
feature representation of central node.","However, no matter which direction graph neural networks develops, it will always be a powerful
tool for handling graph structure data.",2022-09-09 10:06:18+00:00,Self-supervised Learning for Heterogeneous Graph via Structure Information based on Metapath,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Shuai Ma'), arxiv.Result.Author('Jian-wei Liu'), arxiv.Result.Author('Xin Zuo')]","graph neural networks (GNNs) are the dominant paradigm for modeling and
handling graph structure data by learning universal node representation. The
traditional way of training GNNs depends on a great many labeled data, which
results in high requirements on cost and time. In some special scene, it is
even unavailable and impracticable. Self-supervised representation learning,
which can generate labels by graph structure data itself, is a potential
approach to tackle this problem. And turning to research on self-supervised
learning problem for heterogeneous graphs is more challenging than dealing with
homogeneous graphs, also there are fewer studies about it. In this paper, we
propose a SElfsupervised learning method for heterogeneous graph via Structure
Information based on Metapath (SESIM). The proposed model can construct pretext
tasks by predicting jump number between nodes in each metapath to improve the
representation ability of primary task. In order to predict jump number, SESIM
uses data itself to generate labels, avoiding time-consuming manual labeling.
Moreover, predicting jump number in each metapath can effectively utilize graph
structure information, which is the essential property between nodes.
Therefore, SESIM deepens the understanding of models for graph structure. At
last, we train primary task and pretext tasks jointly, and use meta-learning to
balance the contribution of pretext tasks for primary task. Empirical results
validate the performance of SESIM method and demonstrate that this method can
improve the representation ability of traditional neural networks on link
prediction task and node classification task."
10992,"Therefore, we consider this an area of further research.","However, one may argue that these strategies
are sub-optimal, and better strategies are possible.","So far, we have demonstrated the use of Shapley values to explain the contribution of each feature towards
the robustness of classiﬁers.",2022-09-09 11:28:17+00:00,Shapley value-based approaches to explain the robustness of classifiers in machine learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Guilherme Dean Pelegrina'), arxiv.Result.Author('Sajid Siraj')]","In machine learning, the use of algorithm-agnostic approaches is an emerging
area of research for explaining the contribution of individual features towards
the predicted outcome. Whilst there is a focus on explaining the prediction
itself, a little has been done on explaining the robustness of these models,
that is, how each feature contributes towards achieving that robustness. In
this paper, we propose the use of Shapley values to explain the contribution of
each feature towards the model's robustness, measured in terms of
Receiver-operating Characteristics (ROC) curve and the Area under the ROC curve
(AUC). With the help of an illustrative example, we demonstrate the proposed
idea of explaining the ROC curve, and visualising the uncertainties in these
curves. For imbalanced datasets, the use of Precision-Recall Curve (PRC) is
considered more appropriate, therefore we also demonstrate how to explain the
PRCs with the help of Shapley values."
10993,"Therefore, we consider this an area of further research.","However, one may argue that these strategies are sub-
optimal, and better strategies are possible.","The visualisation of ShapROC (as shown in Figure 5a) can assist data analysts in assessing the contri-
bution of each feature across a range of FPR values.",2022-09-09 11:28:17+00:00,Shapley value-based approaches to explain the robustness of classifiers in machine learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Guilherme Dean Pelegrina'), arxiv.Result.Author('Sajid Siraj')]","The use of algorithm-agnostic approaches is an emerging area of research for
explaining the contribution of individual features towards the predicted
outcome. Whilst there is a focus on explaining the prediction itself, a little
has been done on explaining the robustness of these models, that is, how each
feature contributes towards achieving that robustness. In this paper, we
propose the use of Shapley values to explain the contribution of each feature
towards the model's robustness, measured in terms of Receiver-operating
Characteristics (ROC) curve and the Area under the ROC curve (AUC). With the
help of an illustrative example, we demonstrate the proposed idea of explaining
the ROC curve, and visualising the uncertainties in these curves. For
imbalanced datasets, the use of Precision-Recall Curve (PRC) is considered more
appropriate, therefore we also demonstrate how to explain the PRCs with the
help of Shapley values. The explanation of robustness can help analysts in a
number of ways, for example, it can help in feature selection by identifying
the irrelevant features that can be removed to reduce the computational
complexity. It can also help in identifying the features having critical
contributions or negative contributions towards robustness."
11001,"Based upon these ﬁrst results, we believe that the combination of GP
and TPE is promising enough to warrant further research.","In addition, it showed promising reliability prop-
erties (small changes in hypervolume when the ML algorithm is evaluated on the
test set).","The observation that
it outperforms the pure GP algorithm (which used PSO to maximize the inﬁll
criterion) is useful in its own right, as the optimization of inﬁll criteria is known
to be challenging.",2022-09-09 14:58:43+00:00,Multi-objective hyperparameter optimization with performance uncertainty,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Alejandro Morales-Hernández'), arxiv.Result.Author('Inneke Van Nieuwenhuyse'), arxiv.Result.Author('Gonzalo Nápoles')]","The performance of any Machine Learning (ML) algorithm is impacted by the
choice of its hyperparameters. As training and evaluating a ML algorithm is
usually expensive, the hyperparameter optimization (HPO) method needs to be
computationally efficient to be useful in practice. Most of the existing
approaches on multi-objective HPO use evolutionary strategies and
metamodel-based optimization. However, few methods have been developed to
account for uncertainty in the performance measurements. This paper presents
results on multi-objective hyperparameter optimization with uncertainty on the
evaluation of ML algorithms. We combine the sampling strategy of
Tree-structured Parzen Estimators (TPE) with the metamodel obtained after
training a Gaussian Process Regression (GPR) with heterogeneous noise.
Experimental results on three analytical test functions and three ML problems
show the improvement over multi-objective TPE and GPR, achieved with respect to
the hypervolume indicator."
11049,"For instance,        to identify the most important parameters on the performance
the increase of the value of num_words leads to higher model          of the model to inform reference for further study.","metrics of the model: the accuracy of the trained model, the gap
between the accuracies of the model before and after simulation,          In this investigation, we have done extensive experiemtns
and the time to use up the balance of bad agent.","Our main
accuracy and eliminates the negative influence of malicious           focus is to incentive collaborative data sharing for ML model
agents in a shorter time from our observation of the experiment       training through the utilization of blockchain.",2022-09-12 04:25:01+00:00,An Investigation of Smart Contract for Collaborative Machine Learning Model Training,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shengwen Ding'), arxiv.Result.Author('Chenhui Hu')]","Machine learning (ML) has penetrated various fields in the era of big data.
The advantage of collaborative machine learning (CML) over most conventional ML
lies in the joint effort of decentralized nodes or agents that results in
better model performance and generalization. As the training of ML models
requires a massive amount of good quality data, it is necessary to eliminate
concerns about data privacy and ensure high-quality data. To solve this
problem, we cast our eyes on the integration of CML and smart contracts. Based
on blockchain, smart contracts enable automatic execution of data preserving
and validation, as well as the continuity of CML model training. In our
simulation experiments, we define incentive mechanisms on the smart contract,
investigate the important factors such as the number of features in the dataset
(num_words), the size of the training data, the cost for the data holders to
submit data, etc., and conclude how these factors impact the performance
metrics of the model: the accuracy of the trained model, the gap between the
accuracies of the model before and after simulation, and the time to use up the
balance of bad agent. For instance, the increase of the value of num_words
leads to higher model accuracy and eliminates the negative influence of
malicious agents in a shorter time from our observation of the experiment
results. Statistical analyses show that with the help of smart contracts, the
influence of invalid data is efficiently diminished and model robustness is
maintained. We also discuss the gap in existing research and put forward
possible future directions for further works."
11050,"Hence, we carry out a series of comparative
        can be utilized for further research.","However, we notice that the explanation of the selected values
    • Total transparency: All the related data is truly backup                   (default values) of the parameters of the incentive mechanism
        on blockchain and any participant with the key can                       is missing, which may be depended on the developers’
        have access to the recorded and hashed data, which                       experience.","experiments to study the relationships between variables and
                                                                                 in-depth analyses on the effect of important factors of the
    • Automated execution: There is no need for a third                          smart contract and try to fill this gap on basis of this
        party to implement the protocol.",2022-09-12 04:25:01+00:00,An Investigation of Smart Contract for Collaborative Machine Learning Model Training,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shengwen Ding'), arxiv.Result.Author('Chenhui Hu')]","Machine learning (ML) has penetrated various fields in the era of big data.
The advantage of collaborative machine learning (CML) over most conventional ML
lies in the joint effort of decentralized nodes or agents that results in
better model performance and generalization. As the training of ML models
requires a massive amount of good quality data, it is necessary to eliminate
concerns about data privacy and ensure high-quality data. To solve this
problem, we cast our eyes on the integration of CML and smart contracts. Based
on blockchain, smart contracts enable automatic execution of data preserving
and validation, as well as the continuity of CML model training. In our
simulation experiments, we define incentive mechanisms on the smart contract,
investigate the important factors such as the number of features in the dataset
(num_words), the size of the training data, the cost for the data holders to
submit data, etc., and conclude how these factors impact the performance
metrics of the model: the accuracy of the trained model, the gap between the
accuracies of the model before and after simulation, and the time to use up the
balance of bad agent. For instance, the increase of the value of num_words
leads to higher model accuracy and eliminates the negative influence of
malicious agents in a shorter time from our observation of the experiment
results. Statistical analyses show that with the help of smart contracts, the
influence of invalid data is efficiently diminished and model robustness is
maintained. We also discuss the gap in existing research and put forward
possible future directions for further works."
11051,"Through our further research, these different           300, 400, 500, 600, 700, 800, and 900 and keep the default
  values on the parameters do have a different impact on the           values of all the other parameters.","In this
  adjusted and the result of the new model is compared to the          simulation, we separately set its new values equal to 100, 200,
  initial one.",Fig.,2022-09-12 04:25:01+00:00,An Investigation of Smart Contract for Collaborative Machine Learning Model Training,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shengwen Ding'), arxiv.Result.Author('Chenhui Hu')]","Machine learning (ML) has penetrated various fields in the era of big data.
The advantage of collaborative machine learning (CML) over most conventional ML
lies in the joint effort of decentralized nodes or agents that results in
better model performance and generalization. As the training of ML models
requires a massive amount of good quality data, it is necessary to eliminate
concerns about data privacy and ensure high-quality data. To solve this
problem, we cast our eyes on the integration of CML and smart contracts. Based
on blockchain, smart contracts enable automatic execution of data preserving
and validation, as well as the continuity of CML model training. In our
simulation experiments, we define incentive mechanisms on the smart contract,
investigate the important factors such as the number of features in the dataset
(num_words), the size of the training data, the cost for the data holders to
submit data, etc., and conclude how these factors impact the performance
metrics of the model: the accuracy of the trained model, the gap between the
accuracies of the model before and after simulation, and the time to use up the
balance of bad agent. For instance, the increase of the value of num_words
leads to higher model accuracy and eliminates the negative influence of
malicious agents in a shorter time from our observation of the experiment
results. Statistical analyses show that with the help of smart contracts, the
influence of invalid data is efficiently diminished and model robustness is
maintained. We also discuss the gap in existing research and put forward
possible future directions for further works."
11082,"With further research, we believe that using
                   RoBERTa     6.733±0.753            4.447±0.066     pre-trained language models would achieve better and more robust
                               7.556±0.036            4.823±0.023     performance in the task of human mobility forecasting.","Al-
                    Informer   16.747±0.150           5.149±0.019     though using the language foundation models in our AuxMobLCast
                  Autoformer   15.546±0.241           5.723±0.224     does not demonstrate superior performance for all settings, it still
                     XLNet     20.920±1.245           5.202±0.048     shows that AuxMobLCast has a relatively good and promising gen-
                      BERT                                            eralisation ability.","Transformer  6.766±0.078            4.497±0.075
                   Reformer    6.939±0.204            4.855±0.167     5 DISCUSSION
                    Informer   7.202±0.371            4.702±0.225
                  Autoformer   6.231±0.066            4.162±0.017     In this work, we studied the application of language models espe-
                     XLNet     6.291±0.144            4.249±0.090     cially pre-trained language foundation models for mining temporal
                      BERT     10.904±0.129           5.995±0.037     sequential patterns to predict human mobility.",2022-09-11 01:15:16+00:00,Leveraging Language Foundation Models for Human Mobility Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Hao Xue'), arxiv.Result.Author('Bhanu Prakash Voutharoj'), arxiv.Result.Author('Flora D. Salim')]","In this paper, we propose a novel pipeline that leverages language foundation
models for temporal sequential pattern mining, such as for human mobility
forecasting tasks. For example, in the task of predicting Place-of-Interest
(POI) customer flows, typically the number of visits is extracted from
historical logs, and only the numerical data are used to predict visitor flows.
In this research, we perform the forecasting task directly on the natural
language input that includes all kinds of information such as numerical values
and contextual semantic information. Specific prompts are introduced to
transform numerical temporal sequences into sentences so that existing language
models can be directly applied. We design an AuxMobLCast pipeline for
predicting the number of visitors in each POI, integrating an auxiliary POI
category classification task with the encoder-decoder architecture. This
research provides empirical evidence of the effectiveness of the proposed
AuxMobLCast pipeline to discover sequential patterns in mobility forecasting
tasks. The results, evaluated on three real-world datasets, demonstrate that
pre-trained language foundation models also have good performance in
forecasting temporal sequences. This study could provide visionary insights and
lead to new research directions for predicting human mobility."
11083,"With further research, we believe that using
                   RoBERTa     6.733±0.753            4.447±0.066     pre-trained language models would achieve better and more robust
                               7.556±0.036            4.823±0.023     performance in the task of human mobility forecasting.","Al-
                    Informer   16.747±0.150           5.149±0.019     though using the language foundation models in our AuxMobLCast
                  Autoformer   15.546±0.241           5.723±0.224     does not demonstrate superior performance for all settings, it still
                     XLNet     20.920±1.245           5.202±0.048     shows that AuxMobLCast has a relatively good and promising gen-
                      BERT                                            eralisation ability.","Transformer  6.766±0.078            4.497±0.075
                   Reformer    6.939±0.204            4.855±0.167     5 DISCUSSION
                    Informer   7.202±0.371            4.702±0.225
                  Autoformer   6.231±0.066            4.162±0.017     In this work, we studied the application of language models espe-
                     XLNet     6.291±0.144            4.249±0.090     cially pre-trained language foundation models for mining temporal
                      BERT     10.904±0.129           5.995±0.037     sequential patterns to predict human mobility.",2022-09-11 01:15:16+00:00,Leveraging Language Foundation Models for Human Mobility Forecasting,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Hao Xue'), arxiv.Result.Author('Bhanu Prakash Voutharoja'), arxiv.Result.Author('Flora D. Salim')]","In this paper, we propose a novel pipeline that leverages language foundation
models for temporal sequential pattern mining, such as for human mobility
forecasting tasks. For example, in the task of predicting Place-of-Interest
(POI) customer flows, typically the number of visits is extracted from
historical logs, and only the numerical data are used to predict visitor flows.
In this research, we perform the forecasting task directly on the natural
language input that includes all kinds of information such as numerical values
and contextual semantic information. Specific prompts are introduced to
transform numerical temporal sequences into sentences so that existing language
models can be directly applied. We design an AuxMobLCast pipeline for
predicting the number of visitors in each POI, integrating an auxiliary POI
category classification task with the encoder-decoder architecture. This
research provides empirical evidence of the effectiveness of the proposed
AuxMobLCast pipeline to discover sequential patterns in mobility forecasting
tasks. The results, evaluated on three real-world datasets, demonstrate that
pre-trained language foundation models also have good performance in
forecasting temporal sequences. This study could provide visionary insights and
lead to new research directions for predicting human mobility."
11084,"Regarding prob-
abilities calibration, we will conduct further research regarding the AHPC
technique to understand how it can be improved and if using a larger amount
of predicted scores can enhance its performance to achieve competitive results
against other probability calibration techniques.","Regarding active
learning, we are interested in enriching our current setup by adopting diﬀer-
ent strategies to decide how interesting an upcoming image is (e.g., learning
distance metrics for each class) and enhancing the calibration techniques to
display the desired behavior for high-conﬁdence thresholds.","Finally, we will explore how
robust the proposed technique is against concept drift and whether it provides
advantages against other calibration techniques in such a setting.",2022-09-12 15:00:29+00:00,Active Learning and Approximate Model Calibration for Automated Visual Inspection in Manufacturing,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Jože M. Rožanec'), arxiv.Result.Author('Luka Bizjak'), arxiv.Result.Author('Elena Trajkova'), arxiv.Result.Author('Patrik Zajec'), arxiv.Result.Author('Jelle Keizer'), arxiv.Result.Author('Blaž Fortuna'), arxiv.Result.Author('Dunja Mladenić')]","Quality control is a crucial activity performed by manufacturing enterprises
to ensure that their products meet quality standards and avoid potential damage
to the brand's reputation. The decreased cost of sensors and connectivity
enabled increasing digitalization of manufacturing. In addition, artificial
intelligence enables higher degrees of automation, reducing overall costs and
time required for defect inspection. This research compares three active
learning approaches (with single and multiple oracles) to visual inspection. We
propose a novel approach to probabilities calibration of classification models
and two new metrics to assess the performance of the calibration without the
need for ground truth. We performed experiments on real-world data provided by
Philips Consumer Lifestyle BV. Our results show that explored active learning
settings can reduce the data labeling effort by between three and four percent
without detriment to the overall quality goals, considering a threshold of
p=0.95. Furthermore, we show that the proposed metrics successfully capture
relevant information otherwise available to metrics used up to date only
through ground truth data. Therefore, the proposed metrics can be used to
estimate the quality of models' probability calibration without committing to a
labeling effort to obtain ground truth data."
11085,"We will conduct
further research on probabilities calibration to understand how the proposed
metrics behave when concept drift occurs.","Regarding active
learning, we are interested in enriching our current setup by adopting diﬀerent
strategies to decide how interesting an upcoming image is (e.g., learning dis-
tance metrics for each class or learning to predict which piece of data would
enhance the classiﬁer the most) and enhancing the calibration techniques to
display the desired behavior for high-conﬁdence thresholds.","Finally, we will explore new approx-
imate probability calibration approaches leading to enhanced calibrators when
no ground truth is available.",2022-09-12 15:00:29+00:00,Active Learning and Novel Model Calibration Measurements for Automated Visual Inspection in Manufacturing,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Jože M. Rožanec'), arxiv.Result.Author('Luka Bizjak'), arxiv.Result.Author('Elena Trajkova'), arxiv.Result.Author('Patrik Zajec'), arxiv.Result.Author('Jelle Keizer'), arxiv.Result.Author('Blaž Fortuna'), arxiv.Result.Author('Dunja Mladenić')]","Quality control is a crucial activity performed by manufacturing enterprises
to ensure that their products meet quality standards and avoid potential damage
to the brand's reputation. The decreased cost of sensors and connectivity
enabled increasing digitalization of manufacturing. In addition, artificial
intelligence enables higher degrees of automation, reducing overall costs and
time required for defect inspection. This research compares three active
learning approaches, having single and multiple oracles, to visual inspection.
Six new metrics are proposed to assess the quality of calibration without the
need for ground truth. Furthermore, this research explores whether existing
calibrators can improve their performance by leveraging an approximate ground
truth to enlarge the calibration set. The experiments were performed on
real-world data provided by Philips Consumer Lifestyle BV. Our results show
that the explored active learning settings can reduce the data labeling effort
by between three and four percent without detriment to the overall quality
goals, considering a threshold of p=0.95. Furthermore, the results show that
the proposed calibration metrics successfully capture relevant information
otherwise available to metrics used up to date only through ground truth data.
Therefore, the proposed metrics can be used to estimate the quality of models'
probability calibration without committing to a labeling effort to obtain
ground truth data."
11086,"We believe the application of the CNT framework presented in this paper can
stimulate further research in general-purpose Deep Learning, e.g., enhancing our
understanding of different architectures such as CNNs, RNNs, attention, etc.","Finally, we release a fast and extensible package to replicate the experiments and
further extend CNT metrics beyond the application of this paper.3

Concisely, CNT neural network’s dynamics are capable of: (i) identifying spe-
ciﬁc task-dependent patterns in MNIST and CIFAR10, in case of classiﬁcation
as well as image reconstruction; (ii) Discriminating the between different DNNs
activation functions (i.e., linear, ReLU and sigmoid) of shallow and deep archi-
tectures; (iii) Revealing the performance gaps of trained vs. untrained networks.",2.,2022-09-12 16:26:04+00:00,Deep Neural Networks as Complex Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Emanuele La Malfa'), arxiv.Result.Author('Gabriele La Malfa'), arxiv.Result.Author('Claudio Caprioli'), arxiv.Result.Author('Giuseppe Nicosia'), arxiv.Result.Author('Vito Latora')]","Deep Neural Networks are, from a physical perspective, graphs whose `links`
and `vertices` iteratively process data and solve tasks sub-optimally. We use
Complex Network Theory (CNT) to represents Deep Neural Networks (DNNs) as
directed weighted graphs: within this framework, we introduce metrics to study
DNNs as dynamical systems, with a granularity that spans from weights to
layers, including neurons. CNT discriminates networks that differ in the number
of parameters and neurons, the type of hidden layers and activations, and the
objective task. We further show that our metrics discriminate low vs. high
performing networks. CNT is a comprehensive method to reason about DNNs and a
complementary approach to explain a model's behavior that is physically
grounded to networks theory and goes beyond the well-studied input-output
relation."
11130,"It should be noted that the reported performance of the three classiﬁers can be
tuned for further optimisation, which we aim at providing in a further study.","With the exception of SVM on One-Max which
seems to be underperforming that on SUKP, the predictability of success oper-
ators from both individual as well as population domain features is consistent.","In this study, however, the aim is to examine whether predictability of success
operators can be achieved with a subset of input features learnt in diﬀerent
search problem(s).",2022-09-11 20:04:17+00:00,Analysing the Predictivity of Features to Characterise the Search Space,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE']","[arxiv.Result.Author('Rafet Durgut'), arxiv.Result.Author('Mehmet Emin Aydin'), arxiv.Result.Author('Hisham Ihshaish'), arxiv.Result.Author('Abdur Rakib')]","Exploring search spaces is one of the most unpredictable challenges that has
attracted the interest of researchers for decades. One way to handle
unpredictability is to characterise the search spaces and take actions
accordingly. A well-characterised search space can assist in mapping the
problem states to a set of operators for generating new problem states. In this
paper, a landscape analysis-based set of features has been analysed using the
most renown machine learning approaches to determine the optimal feature set.
However, in order to deal with problem complexity and induce commonality for
transferring experience across domains, the selection of the most
representative features remains crucial. The proposed approach analyses the
predictivity of a set of features in order to determine the best
categorization."
11162,By introducing this     choose to further study its effects.,"Therefore, we
good parameter initialization.","To the best of
algorithm, classical methods act as simple but com-    our knowledge, it is seldom studied in transformer-
petitive few-shot one-class learners.",2022-09-14 03:21:47+00:00,Classical Sequence Match is a Competitive Few-Shot One-Class Learner,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Mengting Hu'), arxiv.Result.Author('Hang Gao'), arxiv.Result.Author('Yinhao Bai'), arxiv.Result.Author('Mingming Liu')]","Nowadays, transformer-based models gradually become the default choice for
artificial intelligence pioneers. The models also show superiority even in the
few-shot scenarios. In this paper, we revisit the classical methods and propose
a new few-shot alternative. Specifically, we investigate the few-shot one-class
problem, which actually takes a known sample as a reference to detect whether
an unknown instance belongs to the same class. This problem can be studied from
the perspective of sequence match. It is shown that with meta-learning, the
classical sequence match method, i.e. Compare-Aggregate, significantly
outperforms transformer ones. The classical approach requires much less
training cost. Furthermore, we perform an empirical comparison between two
kinds of sequence match approaches under simple fine-tuning and meta-learning.
Meta-learning causes the transformer models' features to have high-correlation
dimensions. The reason is closely related to the number of layers and heads of
transformer models. Experimental codes and data are available at
https://github.com/hmt2014/FewOne"
11163,By introducing this     choose to further study its effects.,"Therefore, we
good parameter initialization.","To the best of
algorithm, classical methods act as simple but com-    our knowledge, it is seldom studied in transformer-
petitive few-shot one-class learners.",2022-09-14 03:21:47+00:00,Classical Sequence Match is a Competitive Few-Shot One-Class Learner,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Mengting Hu'), arxiv.Result.Author('Hang Gao'), arxiv.Result.Author('Yinhao Bai'), arxiv.Result.Author('Mingming Liu')]","Nowadays, transformer-based models gradually become the default choice for
artificial intelligence pioneers. The models also show superiority even in the
few-shot scenarios. In this paper, we revisit the classical methods and propose
a new few-shot alternative. Specifically, we investigate the few-shot one-class
problem, which actually takes a known sample as a reference to detect whether
an unknown instance belongs to the same class. This problem can be studied from
the perspective of sequence match. It is shown that with meta-learning, the
classical sequence match method, i.e. Compare-Aggregate, significantly
outperforms transformer ones. The classical approach requires much less
training cost. Furthermore, we perform an empirical comparison between two
kinds of sequence match approaches under simple fine-tuning and meta-learning.
Meta-learning causes the transformer models' features to have high-correlation
dimensions. The reason is closely related to the number of layers and heads of
transformer models. Experimental codes and data are available at
https://github.com/hmt2014/FewOne"
11180,"The goal is to make         with the other aspects of trustworthy machine
   the model’s training focus on general patterns          learning and require further study.","These concerns also interact
   completely ignore outliers.",and avoid being inﬂuenced by distinct points.,2022-09-14 10:07:44+00:00,Data Privacy and Trustworthy Machine Learning,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Martin Strobel'), arxiv.Result.Author('Reza Shokri')]","The privacy risks of machine learning models is a major concern when training
them on sensitive and personal data. We discuss the tradeoffs between data
privacy and the remaining goals of trustworthy machine learning (notably,
fairness, robustness, and explainability)."
11187,"The code and dataset for the remaining tasks used in this evaluation are made publicly available
to facilitate further research in this domain3.","The dataset for neural machine translation is available
at [3].","In all experiments, we divide the learning curve into two
splits: (1) one split used for training the scaling law estimators, and (2) one split used for evaluating
extrapolation.",2022-09-13 09:41:51+00:00,Revisiting Neural Scaling Laws in Language and Vision,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ibrahim Alabdulmohsin'), arxiv.Result.Author('Behnam Neyshabur'), arxiv.Result.Author('Xiaohua Zhai')]","The remarkable progress in deep learning in recent years is largely driven by
improvements in scale, where bigger models are trained on larger datasets for
longer schedules. To predict the benefit of scale empirically, we argue for a
more rigorous methodology based on the extrapolation loss, instead of reporting
the best-fitting (interpolating) parameters. We then present a recipe for
estimating scaling law parameters reliably from learning curves. We demonstrate
that it extrapolates more accurately than previous methods in a wide range of
architecture families across several domains, including image classification,
neural machine translation (NMT) and language modeling, in addition to tasks
from the BIG-Bench evaluation benchmark. Finally, we release a benchmark
dataset comprising of 90 evaluation tasks to facilitate research in this
domain."
11188,"The code and dataset for the remaining tasks used in this evaluation are made publicly available
to facilitate further research in this domain3.","The dataset for neural machine translation is available
at [3].","In all experiments, we divide the learning curve into two
splits: (1) one split used for training the scaling law estimators, and (2) one split used for evaluating
extrapolation.",2022-09-13 09:41:51+00:00,Revisiting Neural Scaling Laws in Language and Vision,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Ibrahim Alabdulmohsin'), arxiv.Result.Author('Behnam Neyshabur'), arxiv.Result.Author('Xiaohua Zhai')]","The remarkable progress in deep learning in recent years is largely driven by
improvements in scale, where bigger models are trained on larger datasets for
longer schedules. To predict the benefit of scale empirically, we argue for a
more rigorous methodology based on the extrapolation loss, instead of reporting
the best-fitting (interpolating) parameters. We then present a recipe for
estimating scaling law parameters reliably from learning curves. We demonstrate
that it extrapolates more accurately than previous methods in a wide range of
architecture families across several domains, including image classification,
neural machine translation (NMT) and language modeling, in addition to tasks
from the BIG-Bench evaluation benchmark. Finally, we release a benchmark
dataset comprising of 90 evaluation tasks to facilitate research in this
domain."
11191,"We hope that these findings
and measure model performance on the remaining 46,842 holdout          motivate further research and explorations of use cases for rule-
records, that were not part of the 2,000 sampled records, and thus     adhering synthetic data, as it shows promise to serve as a lingua
                                                                       franca of learning.","For that, we        alike, to learn the statistical information of the original data, as
pursue a train-synthetic-test-real (TSTR) evaluation scheme [3],       well as the rules by domain experts.","REFERENCES

                                                                       [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-
                                                                            fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
                                                                            et al.",2022-09-12 20:01:13+00:00,Rule-adhering synthetic data -- the lingua franca of learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Michael Platzer'), arxiv.Result.Author('Ivona Krchova')]","AI-generated synthetic data allows to distill the general patterns of
existing data, that can then be shared safely as granular-level representative,
yet novel data samples within the original semantics. In this work we explore
approaches of incorporating domain expertise into the data synthesis, to have
the statistical properties as well as pre-existing domain knowledge of rules be
represented. The resulting synthetic data generator, that can be probed for any
number of new samples, can then serve as a common source of intelligence, as a
lingua franca of learning, consumable by humans and machines alike. We
demonstrate the concept for a publicly available data set, and evaluate its
benefits via descriptive analysis as well as a downstream ML model."
11216,"We further study the generated images by GWAE and state-of-the-art VAE-based generative models
in Fig.","This difference highlights the difference in
their objectives, i.e., the GW objective aims at distribution matching in the latent space, while the
β-VAE (Higgins et al., 2017a) objective with β < 1 puts weight on reconstruction.",9.,2022-09-15 02:34:39+00:00,Gromov-Wasserstein Autoencoders,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Nao Nakagawa'), arxiv.Result.Author('Ren Togo'), arxiv.Result.Author('Takahiro Ogawa'), arxiv.Result.Author('Miki Haseyama')]","Learning concise data representations without supervisory signals is a
fundamental challenge in machine learning. A prominent approach to this goal is
likelihood-based models such as variational autoencoders (VAE) to learn latent
representations based on a meta-prior, which is a general premise assumed
beneficial for downstream tasks (e.g., disentanglement). However, such
approaches often deviate from the original likelihood architecture to apply the
introduced meta-prior, causing undesirable changes in their training. In this
paper, we propose a novel representation learning method, Gromov-Wasserstein
Autoencoders (GWAE), which directly matches the latent and data distributions.
Instead of a likelihood-based objective, GWAE models have a trainable prior
optimized by minimizing the Gromov-Wasserstein (GW) metric. The GW metric
measures the distance structure-oriented discrepancy between distributions
supported on incomparable spaces, e.g., with different dimensionalities. By
restricting the family of the trainable prior, we can introduce meta-priors to
control latent representations for downstream tasks. The empirical comparison
with the existing VAE-based methods shows that GWAE models can learn
representations based on meta-priors by changing the prior family without
further modifying the GW objective."
11225,"We further study the
generalization performance of GAGA with competing methods, so as to show its ability to select
optimal model during the learning process.","In experiments, we ﬁrst verify the performance of GAGA and traditional ACS
algorithm under different noise intensity to reﬂect the robustness of GAGA.","Meanwhile, we also evaluate the running efﬁciency
between GAGA and existing SPL implementations in different settings, which examines the speedup
of GAGA as well as its practicability.",2022-09-15 05:49:00+00:00,GAGA: Deciphering Age-path of Generalized Self-paced Regularizer,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Xingyu Qu'), arxiv.Result.Author('Diyang Li'), arxiv.Result.Author('Xiaohan Zhao'), arxiv.Result.Author('Bin Gu')]","Nowadays self-paced learning (SPL) is an important machine learning paradigm
that mimics the cognitive process of humans and animals. The SPL regime
involves a self-paced regularizer and a gradually increasing age parameter,
which plays a key role in SPL but where to optimally terminate this process is
still non-trivial to determine. A natural idea is to compute the solution path
w.r.t. age parameter (i.e., age-path). However, current age-path algorithms are
either limited to the simplest regularizer, or lack solid theoretical
understanding as well as computational efficiency. To address this challenge,
we propose a novel \underline{G}eneralized \underline{Ag}e-path
\underline{A}lgorithm (GAGA) for SPL with various self-paced regularizers based
on ordinary differential equations (ODEs) and sets control, which can learn the
entire solution spectrum w.r.t. a range of age parameters. To the best of our
knowledge, GAGA is the first exact path-following algorithm tackling the
age-path for general self-paced regularizer. Finally the algorithmic steps of
classic SVM and Lasso are described in detail. We demonstrate the performance
of GAGA on real-world datasets, and find considerable speedup between our
algorithm and competing baselines."
11226,"We further study the
generalization performance of GAGA with competing methods, so as to show its ability to select
optimal model during the learning process.","In experiments, we ﬁrst verify the performance of GAGA and traditional ACS
algorithm under different noise intensity to reﬂect the robustness of GAGA.","Meanwhile, we also evaluate the running efﬁciency
between GAGA and existing SPL implementations in different settings, which examines the speedup
of GAGA as well as its practicability.",2022-09-15 05:49:00+00:00,GAGA: Deciphering Age-path of Generalized Self-paced Regularizer,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Xingyu Qu'), arxiv.Result.Author('Diyang Li'), arxiv.Result.Author('Xiaohan Zhao'), arxiv.Result.Author('Bin Gu')]","Nowadays self-paced learning (SPL) is an important machine learning paradigm
that mimics the cognitive process of humans and animals. The SPL regime
involves a self-paced regularizer and a gradually increasing age parameter,
which plays a key role in SPL but where to optimally terminate this process is
still non-trivial to determine. A natural idea is to compute the solution path
w.r.t. age parameter (i.e., age-path). However, current age-path algorithms are
either limited to the simplest regularizer, or lack solid theoretical
understanding as well as computational efficiency. To address this challenge,
we propose a novel \underline{G}eneralized \underline{Ag}e-path
\underline{A}lgorithm (GAGA) for SPL with various self-paced regularizers based
on ordinary differential equations (ODEs) and sets control, which can learn the
entire solution spectrum w.r.t. a range of age parameters. To the best of our
knowledge, GAGA is the first exact path-following algorithm tackling the
age-path for general self-paced regularizer. Finally the algorithmic steps of
classic SVM and Lasso are described in detail. We demonstrate the performance
of GAGA on real-world datasets, and find considerable speedup between our
algorithm and competing baselines."
11243,"To further study
the stability, we take a deeper look into MIXRTs itself in terms of the assigned feature importance
and the action distributions.",The stability of the interpretable model is an important factor reﬂecting the reliability.,"We conduct analyses with the violin plot to study the distribution of
action among several episodes.",2022-09-15 11:39:59+00:00,MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via Mixing Recurrent Soft Decision Trees,cs.LG,"['cs.LG', 'cs.MA']","[arxiv.Result.Author('Zichuan Liu'), arxiv.Result.Author('Yuanyang Zhu'), arxiv.Result.Author('Zhi Wang'), arxiv.Result.Author('Chunlin Chen')]","Multi-agent reinforcement learning (MARL) recently has achieved tremendous
success in a wide range of fields. However, with a black-box neural network
architecture, existing MARL methods make decisions in an opaque fashion that
hinders humans from understanding the learned knowledge and how input
observations influence decisions. Our solution is MIXing Recurrent soft
decision Trees (MIXRTs), a novel interpretable architecture that can represent
explicit decision processes via the root-to-leaf path of decision trees. We
introduce a novel recurrent structure in soft decision trees to address partial
observability, and estimate joint action values via linearly mixing outputs of
recurrent trees based on local observations only. Theoretical analysis shows
that MIXRTs guarantees the structural constraint with additivity and
monotonicity in factorization. We evaluate MIXRTs on a range of challenging
StarCraft II tasks. Experimental results show that our interpretable learning
framework obtains competitive performance compared to widely investigated
baselines, and delivers more straightforward explanations and domain knowledge
of the decision processes."
11266,"Furthermore, the improvements that we propose do not necessarily
only apply to Agent57, and further study could analyze the impact of these components in other
state-of-the-art agents.","An interesting research direction to pursue would be to devise a training scheme such that the
agent with the same set of weights can achieve similar performance and sample-eﬃciency as the
proposed agent on all games.",We also expect that the generality of the agent could be expanded further.,2022-09-15 18:08:48+00:00,Human-level Atari 200x faster,cs.LG,['cs.LG'],"[arxiv.Result.Author('Steven Kapturowski'), arxiv.Result.Author('Víctor Campos'), arxiv.Result.Author('Ray Jiang'), arxiv.Result.Author('Nemanja Rakićević'), arxiv.Result.Author('Hado van Hasselt'), arxiv.Result.Author('Charles Blundell'), arxiv.Result.Author('Adrià Puigdomènech Badia')]","The task of building general agents that perform well over a wide range of
tasks has been an important goal in reinforcement learning since its inception.
The problem has been subject of research of a large body of work, with
performance frequently measured by observing scores over the wide range of
environments contained in the Atari 57 benchmark. Agent57 was the first agent
to surpass the human benchmark on all 57 games, but this came at the cost of
poor data-efficiency, requiring nearly 80 billion frames of experience to
achieve. Taking Agent57 as a starting point, we employ a diverse set of
strategies to achieve a 200-fold reduction of experience needed to out perform
the human baseline. We investigate a range of instabilities and bottlenecks we
encountered while reducing the data regime, and propose effective solutions to
build a more robust and efficient agent. We also demonstrate competitive
performance with high-performing methods such as Muesli and MuZero. The four
key components to our approach are (1) an approximate trust region method which
enables stable bootstrapping from the online network, (2) a normalisation
scheme for the loss and priorities which improves robustness when learning a
set of value functions with a wide range of scales, (3) an improved
architecture employing techniques from NFNets in order to leverage deeper
networks without the need for normalization layers, and (4) a policy
distillation method which serves to smooth out the instantaneous greedy policy
overtime."
11288,"The analysis of the corresponding NTK lays a theoretical
foundation for the interested practitioner to further study other priorities of NN-Hp such as con-
vergence and generalization.","34
H Societal impact

This work studies a cutting-edge network architecture, i.e., neural network with Hadamard product
(NN-Hp), from a theoretical perspective.","Furthermore, our current analysis mainly focuses on the theoretical
side of extrapolation.",2022-09-16 06:36:06+00:00,Extrapolation and Spectral Bias of Neural Nets with Hadamard Product: a Polynomial Net Study,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yongtao Wu'), arxiv.Result.Author('Zhenyu Zhu'), arxiv.Result.Author('Fanghui Liu'), arxiv.Result.Author('Grigorios G Chrysos'), arxiv.Result.Author('Volkan Cevher')]","Neural tangent kernel (NTK) is a powerful tool to analyze training dynamics
of neural networks and their generalization bounds. The study on NTK has been
devoted to typical neural network architectures, but is incomplete for neural
networks with Hadamard products (NNs-Hp), e.g., StyleGAN and polynomial neural
networks. In this work, we derive the finite-width NTK formulation for a
special class of NNs-Hp, i.e., polynomial neural networks. We prove their
equivalence to the kernel regression predictor with the associated NTK, which
expands the application scope of NTK. Based on our results, we elucidate the
separation of PNNs over standard neural networks with respect to extrapolation
and spectral bias. Our two key insights are that when compared to standard
neural networks, PNNs are able to fit more complicated functions in the
extrapolation regime and admit a slower eigenvalue decay of the respective NTK.
Besides, our theoretical results can be extended to other types of NNs-Hp,
which expand the scope of our work. Our empirical results validate the
separations in broader classes of NNs-Hp, which provide a good justification
for a deeper understanding of neural architectures."
11289,"The analysis of the corresponding NTK lays a theoretical
foundation for the interested practitioner to further study other priorities of NN-Hp such as con-
vergence and generalization.","H Societal impact

This work studies a cutting-edge network architecture, i.e., neural network with Hadamard product
(NN-Hp), from a theoretical perspective.","Furthermore, our current analysis mainly focuses on the theoretical
side of extrapolation.",2022-09-16 06:36:06+00:00,Extrapolation and Spectral Bias of Neural Nets with Hadamard Product: a Polynomial Net Study,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yongtao Wu'), arxiv.Result.Author('Zhenyu Zhu'), arxiv.Result.Author('Fanghui Liu'), arxiv.Result.Author('Grigorios G Chrysos'), arxiv.Result.Author('Volkan Cevher')]","Neural tangent kernel (NTK) is a powerful tool to analyze training dynamics
of neural networks and their generalization bounds. The study on NTK has been
devoted to typical neural network architectures, but it is incomplete for
neural networks with Hadamard products (NNs-Hp), e.g., StyleGAN and polynomial
neural networks (PNNs). In this work, we derive the finite-width NTK
formulation for a special class of NNs-Hp, i.e., polynomial neural networks. We
prove their equivalence to the kernel regression predictor with the associated
NTK, which expands the application scope of NTK. Based on our results, we
elucidate the separation of PNNs over standard neural networks with respect to
extrapolation and spectral bias. Our two key insights are that when compared to
standard neural networks, PNNs can fit more complicated functions in the
extrapolation regime and admit a slower eigenvalue decay of the respective NTK,
leading to a faster learning towards high-frequency functions. Besides, our
theoretical results can be extended to other types of NNs-Hp, which expand the
scope of our work. Our empirical results validate the separations in broader
classes of NNs-Hp, which provide a good justification for a deeper
understanding of neural architectures."
11293,This issue limits these models’ practice usage and further research.,"Therefore, their
results are not directly comparable and difﬁcult to reproduce.","Besides, the prediction tasks in most existing works still face the task practicality issue discussed in the introduction.",2022-09-16 09:09:15+00:00,A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care: Choosing the Best Model for COVID-19 Prognosis,cs.LG,['cs.LG'],"[arxiv.Result.Author('Junyi Gao'), arxiv.Result.Author('Yinghao Zhu'), arxiv.Result.Author('Wenqing Wang'), arxiv.Result.Author('Yasha Wang'), arxiv.Result.Author('Wen Tang'), arxiv.Result.Author('Liantao Ma')]","The COVID-19 pandemic has posed a heavy burden to the healthcare system
worldwide and caused huge social disruption and economic loss. Many deep
learning models have been proposed to conduct clinical predictive tasks such as
mortality prediction for COVID-19 patients in intensive care units using
Electronic Health Record (EHR) data. Despite their initial success in certain
clinical applications, there is currently a lack of benchmarking results to
achieve a fair comparison so that we can select the optimal model for clinical
use. Furthermore, there is a discrepancy between the formulation of traditional
prediction tasks and real-world clinical practice in intensive care. To fill
these gaps, we propose two clinical prediction tasks, Outcome-specific
length-of-stay prediction and Early mortality prediction for COVID-19 patients
in intensive care units. The two tasks are adapted from the naive
length-of-stay and mortality prediction tasks to accommodate the clinical
practice for COVID-19 patients. We propose fair, detailed, open-source
data-preprocessing pipelines and evaluate 17 state-of-the-art predictive models
on two tasks, including 5 machine learning models, 6 basic deep learning models
and 6 deep learning predictive models specifically designed for EHR data. We
provide benchmarking results using data from two real-world COVID-19 EHR
datasets. Both datasets are publicly available without needing any inquiry and
one dataset can be accessed on request. We provide fair, reproducible
benchmarking results for two tasks. We deploy all experiment results and models
on an online platform. We also allow clinicians and researchers to upload their
data to the platform and get quick prediction results using our trained models.
We hope our efforts can further facilitate deep learning and machine learning
research for COVID-19 predictive modeling."
11294,"Thus, we need to further study how the payment and privacy cost
vary under diﬀerent reporting strategies to obtain a more satisfactory result.","However, the lack of a closer look at the distribution
of an agent’s utility under diﬀerent reporting strategies prevents us concluding that each agent will
have the highest utility in average if she truthfully reports the data (We only establish that the sym-
metric threshold strategy is an η Bayesian Nash equilibrium, which means an agent may increase η
utility by misreporting the data).","Acknowledgement

Di Wang is supported in part by the baseline funding BAS/1/1689-01-01, funding from the CRG
grand URF/1/4663-01-01, FCC/1/1976-49-01 from CBRC and funding from the AI Initiative
REI/1/4811-10-01 of King Abdullah University of Science and Technology (KAUST).",2022-09-16 09:28:08+00:00,Truthful Generalized Linear Models,cs.LG,"['cs.LG', 'cs.CY', 'cs.GT']","[arxiv.Result.Author('Yuan Qiu'), arxiv.Result.Author('Jinyan Liu'), arxiv.Result.Author('Di Wang')]","In this paper we study estimating Generalized Linear Models (GLMs) in the
case where the agents (individuals) are strategic or self-interested and they
concern about their privacy when reporting data. Compared with the classical
setting, here we aim to design mechanisms that can both incentivize most agents
to truthfully report their data and preserve the privacy of individuals'
reports, while their outputs should also close to the underlying parameter. In
the first part of the paper, we consider the case where the covariates are
sub-Gaussian and the responses are heavy-tailed where they only have the finite
fourth moments. First, motivated by the stationary condition of the maximizer
of the likelihood function, we derive a novel private and closed form
estimator. Based on the estimator, we propose a mechanism which has the
following properties via some appropriate design of the computation and payment
scheme for several canonical models such as linear regression, logistic
regression and Poisson regression: (1) the mechanism is $o(1)$-jointly
differentially private (with probability at least $1-o(1)$); (2) it is an
$o(\frac{1}{n})$-approximate Bayes Nash equilibrium for a $(1-o(1))$-fraction
of agents to truthfully report their data, where $n$ is the number of agents;
(3) the output could achieve an error of $o(1)$ to the underlying parameter;
(4) it is individually rational for a $(1-o(1))$ fraction of agents in the
mechanism ; (5) the payment budget required from the analyst to run the
mechanism is $o(1)$. In the second part, we consider the linear regression
model under more general setting where both covariates and responses are
heavy-tailed and only have finite fourth moments. By using an $\ell_4$-norm
shrinkage operator, we propose a private estimator and payment scheme which
have similar properties as in the sub-Gaussian case."
11302,"Therefore, automated feature engineering is a critical but
challenging subject that needs further research.","Appropriate fea-
ture engineering often requires specialized domain knowledge or a signiﬁcant
amount of eﬀort.",11.2.3.,2022-09-16 16:02:56+00:00,IoT Data Analytics in Dynamic Environments: From An Automated Machine Learning Perspective,cs.LG,"['cs.LG', 'cs.CR', 'cs.NI', 'cs.SY', 'eess.SY', '68T01, 90C31', 'I.2.0; I.2.2; C.2.0']","[arxiv.Result.Author('Li Yang'), arxiv.Result.Author('Abdallah Shami')]","With the wide spread of sensors and smart devices in recent years, the data
generation speed of the Internet of Things (IoT) systems has increased
dramatically. In IoT systems, massive volumes of data must be processed,
transformed, and analyzed on a frequent basis to enable various IoT services
and functionalities. Machine Learning (ML) approaches have shown their capacity
for IoT data analytics. However, applying ML models to IoT data analytics tasks
still faces many difficulties and challenges, specifically, effective model
selection, design/tuning, and updating, which have brought massive demand for
experienced data scientists. Additionally, the dynamic nature of IoT data may
introduce concept drift issues, causing model performance degradation. To
reduce human efforts, Automated Machine Learning (AutoML) has become a popular
field that aims to automatically select, construct, tune, and update machine
learning models to achieve the best performance on specified tasks. In this
paper, we conduct a review of existing methods in the model selection, tuning,
and updating procedures in the area of AutoML in order to identify and
summarize the optimal solutions for every step of applying ML algorithms to IoT
data analytics. To justify our findings and help industrial users and
researchers better implement AutoML approaches, a case study of applying AutoML
to IoT anomaly detection problems is conducted in this work. Lastly, we discuss
and classify the challenges and research directions for this domain."
11303,"We hope that this survey will serve as a touch-point and
reference for scientists, engineers, and policymakers in the trustworthy RL domain and spur further research.","6 Conclusion

   In this survey, we aim to clarify the terminology of robustness, safety, and generalization of RL, analyze their intrinsic
vulnerabilities, introduce work tackling these problems, and summarize popular benchmarks, thus bringing together
disparate threads of studies together in a unified framework.",We summarise the key takeaways of this survey below.,2022-09-16 16:10:08+00:00,"Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities: Robustness, Safety, and Generalizability",cs.LG,['cs.LG'],"[arxiv.Result.Author('Mengdi Xu'), arxiv.Result.Author('Zuxin Liu'), arxiv.Result.Author('Peide Huang'), arxiv.Result.Author('Wenhao Ding'), arxiv.Result.Author('Zhepeng Cen'), arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Ding Zhao')]","A trustworthy reinforcement learning algorithm should be competent in solving
challenging real-world problems, including {robustly} handling uncertainties,
satisfying {safety} constraints to avoid catastrophic failures, and
{generalizing} to unseen scenarios during deployments. This study aims to
overview these main perspectives of trustworthy reinforcement learning
considering its intrinsic vulnerabilities on robustness, safety, and
generalizability. In particular, we give rigorous formulations, categorize
corresponding methodologies, and discuss benchmarks for each perspective.
Moreover, we provide an outlook section to spur promising future directions
with a brief discussion on extrinsic vulnerabilities considering human
feedback. We hope this survey could bring together separate threads of studies
together in a unified framework and promote the trustworthiness of
reinforcement learning."
11306,"With
this work and our the extension to robustness evaluation for the EvalNE software we hope to lay the
foundations for further research in this area.","Finally, we
have also shown that the number of labeled nodes plays a fundamental role in node classiﬁcation
robustness, that rewiring attacks are generally stronger than addition or deletion independently, and
that attacks under heterophily assumption can unexpectedly result in better model performance.","9
                               A Systematic Evaluation of Node Embedding Robustness

References

 [1] Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar.",2022-09-16 17:20:23+00:00,A Systematic Evaluation of Node Embedding Robustness,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Alexandru Mara'), arxiv.Result.Author('Jefrey Lijffijt'), arxiv.Result.Author('Stephan Günnemann'), arxiv.Result.Author('Tijl De Bie')]","Node embedding methods map network nodes to low dimensional vectors that can
be subsequently used in a variety of downstream prediction tasks. The
popularity of these methods has significantly increased in recent years, yet,
their robustness to perturbations of the input data is still poorly understood.
In this paper, we assess the empirical robustness of node embedding models to
random and adversarial poisoning attacks. Our systematic evaluation covers
representative embedding methods based on Skip-Gram, matrix factorization, and
deep neural networks. We compare edge addition, deletion and rewiring
strategies computed using network properties as well as node labels. We also
investigate the effect of label homophily and heterophily on robustness. We
report qualitative results via embedding visualization and quantitative results
in terms of downstream node classification and network reconstruction
performances. We found that node classification suffers from higher performance
degradation as opposed to network reconstruction, and that degree-based and
label-based attacks are on average the most damaging."
11307,"With
this work and our the extension to robustness evaluation for the EvalNE software we hope to lay the
foundations for further research in this area.","Finally, we
have also shown that the number of labeled nodes plays a fundamental role in node classiﬁcation
robustness, that rewiring attacks are generally stronger than addition or deletion independently, and
that attacks under heterophily assumption can unexpectedly result in better model performance.","9
                               A Systematic Evaluation of Node Embedding Robustness

References

 [1] Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar.",2022-09-16 17:20:23+00:00,A Systematic Evaluation of Node Embedding Robustness,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Alexandru Mara'), arxiv.Result.Author('Jefrey Lijffijt'), arxiv.Result.Author('Stephan Günnemann'), arxiv.Result.Author('Tijl De Bie')]","Node embedding methods map network nodes to low dimensional vectors that can
be subsequently used in a variety of downstream prediction tasks. The
popularity of these methods has significantly increased in recent years, yet,
their robustness to perturbations of the input data is still poorly understood.
In this paper, we assess the empirical robustness of node embedding models to
random and adversarial poisoning attacks. Our systematic evaluation covers
representative embedding methods based on Skip-Gram, matrix factorization, and
deep neural networks. We compare edge addition, deletion and rewiring
strategies computed using network properties as well as node labels. We also
investigate the effect of label homophily and heterophily on robustness. We
report qualitative results via embedding visualization and quantitative results
in terms of downstream node classification and network reconstruction
performances. We found that node classification suffers from higher performance
degradation as opposed to network reconstruction, and that degree-based and
label-based attacks are on average the most damaging."
11308,"With this work and our extension to robustness evaluation for the EvalNE software, we
hope to lay the foundations for further research in this area.","Finally,
we have also shown that the number of labeled nodes plays a fundamental role in node classiﬁcation
robustness, that rewiring attacks are generally stronger than addition or deletion independently, and
that attacks leveraging node label information can result in improved representations of heterophilic
networks.","Acknowledgements

The research leading to these results has received funding from the European Research Council under
the European Union’s Seventh Framework Programme (FP7/2007-2013) (ERC Grant Agreement no.",2022-09-16 17:20:23+00:00,A Systematic Evaluation of Node Embedding Robustness,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Alexandru Mara'), arxiv.Result.Author('Jefrey Lijffijt'), arxiv.Result.Author('Stephan Günnemann'), arxiv.Result.Author('Tijl De Bie')]","Node embedding methods map network nodes to low dimensional vectors that can
be subsequently used in a variety of downstream prediction tasks. The
popularity of these methods has grown significantly in recent years, yet, their
robustness to perturbations of the input data is still poorly understood. In
this paper, we assess the empirical robustness of node embedding models to
random and adversarial poisoning attacks. Our systematic evaluation covers
representative embedding methods based on Skip-Gram, matrix factorization, and
deep neural networks. We compare edge addition, deletion and rewiring attacks
computed using network properties as well as node labels. We also investigate
the performance of popular node classification attack baselines that assume
full knowledge of the node labels. We report qualitative results via embedding
visualization and quantitative results in terms of downstream node
classification and network reconstruction performances. We find that node
classification results are impacted more than network reconstruction ones, that
degree-based and label-based attacks are on average the most damaging and that
label heterophily can strongly influence attack performance."
11335,"The connection between
          simplex ETFs that naturally arise in DNNs and OoD performance permits an elegant analytical
          framework for further study of the underlying mechanisms that govern uncertainty and robustness
          in DNNs.","We show evidence that fast NC plays a role in achieving OoD detec-
          tion performance with less training, and that training directly on NC has a substantially diﬀerent
          eﬀect on OoD performance than standad cross entropy (CE) training.","2 Background

2.1 Problem Deﬁnition

A standard classiﬁcation model maps images to classes f : x → y, such that x ∈ X where X is the
set of images and y ∈ {1, ..., k} where there are k possible classes.",2022-09-17 17:46:06+00:00,Inducing Early Neural Collapse in Deep Neural Networks for Improved Out-of-Distribution Detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jarrod Haas'), arxiv.Result.Author('William Yolland'), arxiv.Result.Author('Bernhard Rabus')]","We propose a simple modification to standard ResNet architectures--L2
regularization over feature space--that substantially improves
out-of-distribution (OoD) performance on the previously proposed Deep
Deterministic Uncertainty (DDU) benchmark. This change also induces early
Neural Collapse (NC), which we show is an effect under which better OoD
performance is more probable. Our method achieves comparable or superior OoD
detection scores and classification accuracy in a small fraction of the
training time of the benchmark. Additionally, it substantially improves worst
case OoD performance over multiple, randomly initialized models. Though we do
not suggest that NC is the sole mechanism or comprehensive explanation for OoD
behaviour in deep neural networks (DNN), we believe NC's simple mathematical
and geometric structure can provide an framework for analysis of this complex
phenomenon in future work."
11336,"The connection between
          simplex ETFs that naturally arise in DNNs and OoD performance permits an elegant analytical
          framework for further study of the underlying mechanisms that govern uncertainty and robustness
          in DNNs.","We show evidence that fast NC plays a role in achieving OoD detec-
          tion performance with less training, and that training directly on NC has a substantially diﬀerent
          eﬀect on OoD performance than standad cross entropy (CE) training.","2 Background

2.1 Problem Deﬁnition

A standard classiﬁcation model maps images to classes f : x → y, such that x ∈ X where X is the set of
images and y ∈ {1, ..., k} where there are k possible classes.",2022-09-17 17:46:06+00:00,Inducing Early Neural Collapse in Deep Neural Networks for Improved Out-of-Distribution Detection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jarrod Haas'), arxiv.Result.Author('William Yolland'), arxiv.Result.Author('Bernhard Rabus')]","We propose a simple modification to standard ResNet architectures--L2
regularization over feature space--that substantially improves
out-of-distribution (OoD) performance on the previously proposed Deep
Deterministic Uncertainty (DDU) benchmark. This change also induces early
Neural Collapse (NC), which we show is an effect under which better OoD
performance is more probable. Our method achieves comparable or superior OoD
detection scores and classification accuracy in a small fraction of the
training time of the benchmark. Additionally, it substantially improves worst
case OoD performance over multiple, randomly initialized models. Though we do
not suggest that NC is the sole mechanism or a comprehensive explanation for
OoD behaviour in deep neural networks (DNN), we believe NC's simple
mathematical and geometric structure can provide a framework for analysis of
this complex phenomenon in future work."
11337,"The Nystr¨om-type approximation process
that uses low-rank matrices to approximate the asymmetric kernel matrix is
worthy of further research.","Another intererting avenue for
asymmetric kernel approximation is the Nystr¨om-type method (Williams and
Seeger, 2001; Drineas et al., 2005).","Appendix A Proof of Proposition 6

The proof strategy closely follows which of (Rahimi and Recht, 2007; Suther-

land and Schneider, 2015).",2022-09-18 03:39:18+00:00,Random Fourier Features for Asymmetric Kernels,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mingzhen He'), arxiv.Result.Author('Fan He'), arxiv.Result.Author('Fanghui Liu'), arxiv.Result.Author('Xiaolin Huang')]","The random Fourier features (RFFs) method is a powerful and popular technique
in kernel approximation for scalability of kernel methods. The theoretical
foundation of RFFs is based on the Bochner theorem that relates symmetric,
positive definite (PD) functions to probability measures. This condition
naturally excludes asymmetric functions with a wide range applications in
practice, e.g., directed graphs, conditional probability, and asymmetric
kernels. Nevertheless, understanding asymmetric functions (kernels) and its
scalability via RFFs is unclear both theoretically and empirically. In this
paper, we introduce a complex measure with the real and imaginary parts
corresponding to four finite positive measures, which expands the application
scope of the Bochner theorem. By doing so, this framework allows for handling
classical symmetric, PD kernels via one positive measure; symmetric,
non-positive definite kernels via signed measures; and asymmetric kernels via
complex measures, thereby unifying them into a general framework by RFFs, named
AsK-RFFs. Such approximation scheme via complex measures enjoys theoretical
guarantees in the perspective of the uniform convergence. In algorithmic
implementation, to speed up the kernel approximation process, which is
expensive due to the calculation of total mass, we employ a subset-based fast
estimation method that optimizes total masses on a sub-training set, which
enjoys computational efficiency in high dimensions. Our AsK-RFFs method is
empirically validated on several typical large-scale datasets and achieves
promising kernel approximation performance, which demonstrate the effectiveness
of AsK-RFFs."
11356,"A further study
to understand the mechanism is an interesting future direction.","While the current theory requires the inner loop iterations to scale in a
logarithmic order w.r.t to the outer loop iterations, we do not observe this empirically.","References

 [1] Michael Arbel and Julien Mairal.",2022-09-19 01:51:12+00:00,BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach,cs.LG,"['cs.LG', 'cs.AI', 'math.OC']","[arxiv.Result.Author('Mao Ye'), arxiv.Result.Author('Bo Liu'), arxiv.Result.Author('Stephen Wright'), arxiv.Result.Author('Peter Stone'), arxiv.Result.Author('Qiang Liu')]","Bilevel optimization (BO) is useful for solving a variety of important
machine learning problems including but not limited to hyperparameter
optimization, meta-learning, continual learning, and reinforcement learning.
Conventional BO methods need to differentiate through the low-level
optimization process with implicit differentiation, which requires expensive
calculations related to the Hessian matrix. There has been a recent quest for
first-order methods for BO, but the methods proposed to date tend to be
complicated and impractical for large-scale deep learning applications. In this
work, we propose a simple first-order BO algorithm that depends only on
first-order gradient information, requires no implicit differentiation, and is
practical and efficient for large-scale non-convex functions in deep learning.
We provide non-asymptotic convergence analysis of the proposed method to
stationary points for non-convex objectives and present empirical results that
show its superior practical performance."
11376,"Although some shortcomings need to be addressed
and further research should be followed-up, this new method has shown better performance in various
aspects compared to the existing methods.","To the best of our knowledge, this is the ﬁrst study that applies the techniques from topological data
analysis to the wafer map classiﬁcation problem.","We expect this TDA-based method to be utilized as a new
alternative for the defect pattern classiﬁcation problem, especially when the training data is insuﬃcient
and imbalanced.",2022-09-19 11:54:13+00:00,A novel approach for wafer defect pattern classification based on topological data analysis,cs.LG,"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']","[arxiv.Result.Author('Seungchan Ko'), arxiv.Result.Author('Dowan Koo')]","In semiconductor manufacturing, wafer map defect pattern provides critical
information for facility maintenance and yield management, so the
classification of defect patterns is one of the most important tasks in the
manufacturing process. In this paper, we propose a novel way to represent the
shape of the defect pattern as a finite-dimensional vector, which will be used
as an input for a neural network algorithm for classification. The main idea is
to extract the topological features of each pattern by using the theory of
persistent homology from topological data analysis (TDA). Through some
experiments with a simulated dataset, we show that the proposed method is
faster and much more efficient in training with higher accuracy, compared with
the method using convolutional neural networks (CNN) which is the most common
approach for wafer map defect pattern classification. Moreover, our method
outperforms the CNN-based method when the number of training data is not enough
and is imbalanced."
11384,"In further research it might be interesting to systematically compare our approach to graph
neural networks, since their message passing scheme is similar to the update strategy of the
WL algorithm.","We also investigated the application of our method to approximating the
graph edit distance, where we again outperformed the state-of-the-art methods.","Moreover, other renep functions can be explored, for example, by using other
clustering strategies, or by developing new concepts for inexact neighborhood comparison.",2022-09-19 14:37:35+00:00,Gradual Weisfeiler-Leman: Slow and Steady Wins the Race,cs.LG,"['cs.LG', 'cs.DS']","[arxiv.Result.Author('Franka Bause'), arxiv.Result.Author('Nils M. Kriege')]","The classical Weisfeiler-Leman algorithm aka color refinement is fundamental
for graph learning and central for successful graph kernels and graph neural
networks. Originally developed for graph isomorphism testing, the algorithm
iteratively refines vertex colors. On many datasets, the stable coloring is
reached after a few iterations and the optimal number of iterations for machine
learning tasks is typically even lower. This suggests that the colors diverge
too fast, defining a similarity that is too coarse. We generalize the concept
of color refinement and propose a framework for gradual neighborhood
refinement, which allows a slower convergence to the stable coloring and thus
provides a more fine-grained refinement hierarchy and vertex similarity. We
assign new colors by clustering vertex neighborhoods, replacing the original
injective color assignment function. Our approach is used to derive new
variants of existing graph kernels and to approximate the graph edit distance
via optimal assignments regarding vertex similarity. We show that in both
tasks, our method outperforms the original color refinement with only moderate
increase in running time advancing the state of the art."
11385,"In further research, other renep functions can be explored, for example, by using di erent clus-
tering strategies or developing new concepts for inexact neighborhood comparison.","We also investigated the application of our method to approximating the
graph edit distance, where we again outperformed the state-of-the-art methods.","Moreover,
we will systematically relate our approach to graph neural networks and investigate whether
similar ideas can be incorporated into their neighborhood aggregation step and to what ex-
tent common architectures and optimization methods are capable of learning certain renep
functions.",2022-09-19 14:37:35+00:00,Gradual Weisfeiler-Leman: Slow and Steady Wins the Race,cs.LG,"['cs.LG', 'cs.DS']","[arxiv.Result.Author('Franka Bause'), arxiv.Result.Author('Nils M. Kriege')]","The classical Weisfeiler-Leman algorithm aka color refinement is fundamental
for graph learning with kernels and neural networks. Originally developed for
graph isomorphism testing, the algorithm iteratively refines vertex colors. On
many datasets, the stable coloring is reached after a few iterations and the
optimal number of iterations for machine learning tasks is typically even
lower. This suggests that the colors diverge too fast, defining a similarity
that is too coarse. We generalize the concept of color refinement and propose a
framework for gradual neighborhood refinement, which allows a slower
convergence to the stable coloring and thus provides a more fine-grained
refinement hierarchy and vertex similarity. We assign new colors by clustering
vertex neighborhoods, replacing the original injective color assignment
function. Our approach is used to derive new variants of existing graph kernels
and to approximate the graph edit distance via optimal assignments regarding
vertex similarity. We show that in both tasks, our method outperforms the
original color refinement with only a moderate increase in running time
advancing the state of the art."
11395,"Thus, we further study the global landscape of Problem (4) by characterizing all the Rieman-
nian critical points (W , H) ∈ OB(d, K) × OB(d, N ) satisfying

                                 gradH f (W , H) = 0, gradW f (W , H) = 0.","3.2 Global Landscape Analysis

Due to the nonconvex nature of Problem (4), the characterization of global optimality in Theo-
rem 3.1 alone is not suﬃcient for guaranteeing eﬃcient optimization to those desired global solu-
tions.",We now state our major result below.,2022-09-19 17:26:32+00:00,Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold,cs.LG,"['cs.LG', 'cs.CV', 'cs.IT', 'eess.SP', 'math.IT', 'stat.ML']","[arxiv.Result.Author('Can Yaras'), arxiv.Result.Author('Peng Wang'), arxiv.Result.Author('Zhihui Zhu'), arxiv.Result.Author('Laura Balzano'), arxiv.Result.Author('Qing Qu')]","When training overparameterized deep networks for classification tasks, it
has been widely observed that the learned features exhibit a so-called ""neural
collapse"" phenomenon. More specifically, for the output features of the
penultimate layer, for each class the within-class features converge to their
means, and the means of different classes exhibit a certain tight frame
structure, which is also aligned with the last layer's classifier. As feature
normalization in the last layer becomes a common practice in modern
representation learning, in this work we theoretically justify the neural
collapse phenomenon for normalized features. Based on an unconstrained feature
model, we simplify the empirical loss function in a multi-class classification
task into a nonconvex optimization problem over the Riemannian manifold by
constraining all features and classifiers over the sphere. In this context, we
analyze the nonconvex landscape of the Riemannian optimization problem over the
product of spheres, showing a benign global landscape in the sense that the
only global minimizers are the neural collapse solutions while all other
critical points are strict saddles with negative curvature. Experimental
results on practical deep networks corroborate our theory and demonstrate that
better representations can be learned faster via feature normalization."
11405,"(Formalizing this with a lower bound, possibly in
the case of random initialization, or with an arbitrary initialization and a randomly chosen
regression function Θ⋆, is a potential topic for further research.)","Looking at our proofs, it appears that the only way that a deep linear parameterization
can promote benign over tting is for the function computed by the network at initialization
to approximate the regression function.","The bene ts of a good
approximation to the regression function at initialization has been explored in the case of
two-layer linear networks [CLB22].",2022-09-19 19:23:04+00:00,Deep Linear Networks can Benignly Overfit when Shallow Ones Do,cs.LG,"['cs.LG', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Niladri S. Chatterji'), arxiv.Result.Author('Philip M. Long')]","We bound the excess risk of interpolating deep linear networks trained using
gradient flow. In a setting previously used to establish risk bounds for the
minimum $\ell_2$-norm interpolant, we show that randomly initialized deep
linear networks can closely approximate or even match known bounds for the
minimum $\ell_2$-norm interpolant. Our analysis also reveals that interpolating
deep linear models have exactly the same conditional variance as the minimum
$\ell_2$-norm solution. Since the noise affects the excess risk only through
the conditional variance, this implies that depth does not improve the
algorithm's ability to ""hide the noise"". Our simulations verify that aspects of
our bounds reflect typical behavior for simple data distributions. We also find
that similar phenomena are seen in simulations with ReLU networks, although the
situation there is more nuanced."
11406,"Extending this analysis to deep networks is a potential
subject for further study.","The bene ts of a good
approximation to the regression function at initialization has been explored in the case of
two-layer linear networks [CLB22].","We focused on a particular random initialization scheme in this paper, it is possible to
study other initialization schemes as well.",2022-09-19 19:23:04+00:00,Deep Linear Networks can Benignly Overfit when Shallow Ones Do,cs.LG,"['cs.LG', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Niladri S. Chatterji'), arxiv.Result.Author('Philip M. Long')]","We bound the excess risk of interpolating deep linear networks trained using
gradient flow. In a setting previously used to establish risk bounds for the
minimum $\ell_2$-norm interpolant, we show that randomly initialized deep
linear networks can closely approximate or even match known bounds for the
minimum $\ell_2$-norm interpolant. Our analysis also reveals that interpolating
deep linear models have exactly the same conditional variance as the minimum
$\ell_2$-norm solution. Since the noise affects the excess risk only through
the conditional variance, this implies that depth does not improve the
algorithm's ability to ""hide the noise"". Our simulations verify that aspects of
our bounds reflect typical behavior for simple data distributions. We also find
that similar phenomena are seen in simulations with ReLU networks, although the
situation there is more nuanced."
11410,"To help further research and inclusion of these distributed representations we publicly
release all of the drug representations as learned and utilised in this study.","To the best of our knowledge this is
the ﬁrst application and study of distributed representations of molecular drug graphs for drug pair
scoring tasks.","To summarise our contributions are as follows:

   • We show that learning distributed representations of graphs as a source of additional features is
      reasonable within drug pair scoring pipelines.",2022-09-19 23:35:26+00:00,Distributed representations of graphs for drug pair scoring,cs.LG,"['cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Paul Scherer'), arxiv.Result.Author('Pietro Liò'), arxiv.Result.Author('Mateja Jamnik')]","In this paper we study the practicality and usefulness of incorporating
distributed representations of graphs into models within the context of drug
pair scoring. We argue that the real world growth and update cycles of drug
pair scoring datasets subvert the limitations of transductive learning
associated with distributed representations. Furthermore, we argue that the
vocabulary of discrete substructure patterns induced over drug sets is not
dramatically large due to the limited set of atom types and constraints on
bonding patterns enforced by chemistry. Under this pretext, we explore the
effectiveness of distributed representations of the molecular graphs of drugs
in drug pair scoring tasks such as drug synergy, polypharmacy, and drug-drug
interaction prediction. To achieve this, we present a methodology for learning
and incorporating distributed representations of graphs within a unified
framework for drug pair scoring. Subsequently, we augment a number of recent
and state-of-the-art models to utilise our embeddings. We empirically show that
the incorporation of these embeddings improves downstream performance of almost
every model across different drug pair scoring tasks, even those the original
model was not designed for. We publicly release all of our drug embeddings for
the DrugCombDB, DrugComb, DrugbankDDI, and TwoSides datasets."
11411,"To help further research and inclusion of these distributed representations we publicly
release all of the drug representations as learned and utilised in this study.","To the best of our knowledge this is
the ﬁrst application and study of distributed representations of molecular drug graphs for drug pair
scoring tasks.","To summarise our contributions are as follows:

   • We show that learning distributed representations of graphs as a source of additional features is
      reasonable within drug pair scoring pipelines.",2022-09-19 23:35:26+00:00,Distributed representations of graphs for drug pair scoring,cs.LG,"['cs.LG', 'q-bio.QM']","[arxiv.Result.Author('Paul Scherer'), arxiv.Result.Author('Pietro Liò'), arxiv.Result.Author('Mateja Jamnik')]","In this paper we study the practicality and usefulness of incorporating
distributed representations of graphs into models within the context of drug
pair scoring. We argue that the real world growth and update cycles of drug
pair scoring datasets subvert the limitations of transductive learning
associated with distributed representations. Furthermore, we argue that the
vocabulary of discrete substructure patterns induced over drug sets is not
dramatically large due to the limited set of atom types and constraints on
bonding patterns enforced by chemistry. Under this pretext, we explore the
effectiveness of distributed representations of the molecular graphs of drugs
in drug pair scoring tasks such as drug synergy, polypharmacy, and drug-drug
interaction prediction. To achieve this, we present a methodology for learning
and incorporating distributed representations of graphs within a unified
framework for drug pair scoring. Subsequently, we augment a number of recent
and state-of-the-art models to utilise our embeddings. We empirically show that
the incorporation of these embeddings improves downstream performance of almost
every model across different drug pair scoring tasks, even those the original
model was not designed for. We publicly release all of our drug embeddings for
the DrugCombDB, DrugComb, DrugbankDDI, and TwoSides datasets."
11424,"Since the module essentially learns to assign a weight to each feature, further research could be made to investi-
gate and build a framework that monitors the reweighting layer of the model and identiﬁes features with higher and
lower inﬂuence.","Our experiments present the embedding reweighting module as the clear winner in terms of performance boost-
ing.",This information could then be utilized to perform feature selection in a production setting.,2022-09-20 05:48:30+00:00,Feature embedding in click-through rate prediction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Samo Pahor'), arxiv.Result.Author('Davorin Kopič'), arxiv.Result.Author('Jure Demšar')]","We tackle the challenge of feature embedding for the purposes of improving
the click-through rate prediction process. We select three models: logistic
regression, factorization machines and deep factorization machines, as our
baselines and propose five different feature embedding modules: embedding
scaling, FM embedding, embedding encoding, NN embedding and the embedding
reweighting module. The embedding modules act as a way to improve baseline
model feature embeddings and are trained alongside the rest of the model
parameters in an end-to-end manner. Each module is individually added to a
baseline model to obtain a new augmented model. We test the predictive
performance of our augmented models on a publicly accessible dataset used for
benchmarking click-through rate prediction models. Our results show that
several proposed embedding modules provide an important increase in predictive
performance without a drastic increase in training time."
11425,"Finally, there remain two major areas of further research: investigating how diﬀerent combinations of proposed
modules aﬀect performance when combined into a single model and applying the proposed feature embedding ﬁnd-
24  Pahor et al.",This information could then be utilized to perform feature selection in a production setting.,ings to other machine learning areas.,2022-09-20 05:48:30+00:00,Feature embedding in click-through rate prediction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Samo Pahor'), arxiv.Result.Author('Davorin Kopič'), arxiv.Result.Author('Jure Demšar')]","We tackle the challenge of feature embedding for the purposes of improving
the click-through rate prediction process. We select three models: logistic
regression, factorization machines and deep factorization machines, as our
baselines and propose five different feature embedding modules: embedding
scaling, FM embedding, embedding encoding, NN embedding and the embedding
reweighting module. The embedding modules act as a way to improve baseline
model feature embeddings and are trained alongside the rest of the model
parameters in an end-to-end manner. Each module is individually added to a
baseline model to obtain a new augmented model. We test the predictive
performance of our augmented models on a publicly accessible dataset used for
benchmarking click-through rate prediction models. Our results show that
several proposed embedding modules provide an important increase in predictive
performance without a drastic increase in training time."
11439,"4 Baseline model performance

We run extensive experiments on the proposed benchmark as a starting point for further research.","We consider only multi-shot unmasking and choose top-k RMSE as the
evaluation metric.","We
consider classical machine learning methods as well as large neural networks.",2022-09-20 08:20:03+00:00,FACT: Learning Governing Abstractions Behind Integer Sequences,cs.LG,"['cs.LG', 'cs.AI', 'cs.SC']","[arxiv.Result.Author('Peter Belcák'), arxiv.Result.Author('Ard Kastrati'), arxiv.Result.Author('Flavio Schenker'), arxiv.Result.Author('Roger Wattenhofer')]","Integer sequences are of central importance to the modeling of concepts
admitting complete finitary descriptions. We introduce a novel view on the
learning of such concepts and lay down a set of benchmarking tasks aimed at
conceptual understanding by machine learning models. These tasks indirectly
assess model ability to abstract, and challenge them to reason both
interpolatively and extrapolatively from the knowledge gained by observing
representative examples. To further aid research in knowledge representation
and reasoning, we present FACT, the Finitary Abstraction Comprehension Toolkit.
The toolkit surrounds a large dataset of integer sequences comprising both
organic and synthetic entries, a library for data pre-processing and
generation, a set of model performance evaluation tools, and a collection of
baseline model implementations, enabling the making of the future advancements
with ease."
11465,"One can also learn a mapping of nominal          and test diﬀerent CAD methods to further study their
data from a high-dimensional space to a compact re-         eﬃciency and generalization.","We also create synthesized tasks on CIFAR10,
lous data.","Concluding remarks are
gion concentrated around a pre-deﬁned centroid, us-         presented in Section 6.
ing kernel functions (Sch¨olkopf et al., 2001; Tax and
Duin, 2004) or deep neural networks (Ruﬀ et al., 2018).",2022-09-20 18:01:07+00:00,Collaborative Anomaly Detection,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Ke Bai'), arxiv.Result.Author('Aonan Zhang'), arxiv.Result.Author('Zhizhong Li'), arxiv.Result.Author('Ricardo Heano'), arxiv.Result.Author('Chong Wang'), arxiv.Result.Author('Lawrence Carin')]","In recommendation systems, items are likely to be exposed to various users
and we would like to learn about the familiarity of a new user with an existing
item. This can be formulated as an anomaly detection (AD) problem
distinguishing between ""common users"" (nominal) and ""fresh users"" (anomalous).
Considering the sheer volume of items and the sparsity of user-item paired
data, independently applying conventional single-task detection methods on each
item quickly becomes difficult, while correlations between items are ignored.
To address this multi-task anomaly detection problem, we propose collaborative
anomaly detection (CAD) to jointly learn all tasks with an embedding encoding
correlations among tasks. We explore CAD with conditional density estimation
and conditional likelihood ratio estimation. We found that: $i$) estimating a
likelihood ratio enjoys more efficient learning and yields better results than
density estimation. $ii$) It is beneficial to select a small number of tasks in
advance to learn a task embedding model, and then use it to warm-start all task
embeddings. Consequently, these embeddings can capture correlations between
tasks and generalize to new correlated tasks."
11479,"The UDN and the dynamic variational inference offer sev-
These categories were selected from an independently gen-           eral avenues for further research.","10: an easy subsample containing only deer and car images,
and a hard subsample containing only cat and dog images.","First, the unbounded
erated CIFAR-10 confusion matrix, in which (deer,car) were          neural network could be applied to transformers, where very
found to be the least confused labels, whereas (cat,dog) were       deep models have shown successful results (Liu et al., 2020).",2022-09-21 03:54:34+00:00,Variational Inference for Infinitely Deep Neural Networks,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Achille Nazaret'), arxiv.Result.Author('David Blei')]","We introduce the unbounded depth neural network (UDN), an infinitely deep
probabilistic model that adapts its complexity to the training data. The UDN
contains an infinite sequence of hidden layers and places an unbounded prior on
a truncation L, the layer from which it produces its data. Given a dataset of
observations, the posterior UDN provides a conditional distribution of both the
parameters of the infinite neural network and its truncation. We develop a
novel variational inference algorithm to approximate this posterior, optimizing
a distribution of the neural network weights and of the truncation depth L, and
without any upper limit on L. To this end, the variational family has a special
structure: it models neural network weights of arbitrary depth, and it
dynamically creates or removes free variational parameters as its distribution
of the truncation is optimized. (Unlike heuristic approaches to model search,
it is solely through gradient-based optimization that this algorithm explores
the space of truncations.) We study the UDN on real and synthetic data. We find
that the UDN adapts its posterior depth to the dataset complexity; it
outperforms standard neural networks of similar computational complexity; and
it outperforms other approaches to infinite-depth neural networks."
11491,"6, are labelled accordingly so that the resulting dataset of clean and anomalous jobs can be used for
further research and validation of developments.","Synthetic images,
as shown in Fig.","Figure 7: Confusion chart of the predicted class membership for real striation images: 512 real striation images are
predicted correctly as striations, whereas 158 images are misclassiﬁed as foreign objects.",2022-09-21 08:14:34+00:00,A data-centric approach to anomaly detection in layer-based additive manufacturing,cs.LG,['cs.LG'],"[arxiv.Result.Author('Alexander Zeiser'), arxiv.Result.Author('Bekir Özcan'), arxiv.Result.Author('Bas van Stein'), arxiv.Result.Author('Thomas Bäck')]","Anomaly detection describes methods of finding abnormal states, instances or
data points that differ from a normal value space. Industrial processes are a
domain where predicitve models are needed for finding anomalous data instances
for quality enhancement. A main challenge, however, is absence of labels in
this environment. This paper contributes to a data-centric way of approaching
artificial intelligence in industrial production. With a use case from additive
manufacturing for automotive components we present a deep-learning-based image
processing pipeline. We integrate the concept of domain randomisation and
synthetic data in the loop that shows promising results for bridging advances
in deep learning and its application to real-world, industrial production
processes."
11494,"V),                                    of neural networks, rather than to propose neurosymbolic or
   • the establishment of an expectation baseline on the pos-       evolutionary methods to further research in structure discovery.","recently proposed for learning of periodic signals with
      neural networks (Sect.",sible performance of neural models for the periodic ex-              III.,2022-09-21 11:47:30+00:00,Periodic Extrapolative Generalisation in Neural Networks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Peter Belcák'), arxiv.Result.Author('Roger Wattenhofer')]","The learning of the simplest possible computational pattern -- periodicity --
is an open problem in the research of strong generalisation in neural networks.
We formalise the problem of extrapolative generalisation for periodic signals
and systematically investigate the generalisation abilities of classical,
population-based, and recently proposed periodic architectures on a set of
benchmarking tasks. We find that periodic and ""snake"" activation functions
consistently fail at periodic extrapolation, regardless of the trainability of
their periodicity parameters. Further, our results show that traditional
sequential models still outperform the novel architectures designed
specifically for extrapolation, and that these are in turn trumped by
population-based training. We make our benchmarking and evaluation toolkit,
PerKit, available and easily accessible to facilitate future work in the area."
11511,"In terms of comprehensive ability, while improving the convergence ability of the
algorithm, we should also consider reducing the system overhead of the algorithm, which is a direction that needs
further research in the future.","We also believe that merging notions like quantum computing with our suggested
approach will improve results.","EA generally have prominent biological and social behavior characteristics and weak
mathematical theoretical support in theoretical analysis.",2022-09-20 15:12:10+00:00,A Tent Lévy Flying Sparrow Search Algorithm for Feature Selection: A COVID-19 Case Study,cs.LG,"['cs.LG', 'F.2; I.2']","[arxiv.Result.Author('Qinwen Yang'), arxiv.Result.Author('Yuelin Gao'), arxiv.Result.Author('Yanjie Song')]","The ""Curse of Dimensionality"" induced by the rapid development of information
science, might have a negative impact when dealing with big datasets. In this
paper, we propose a variant of the sparrow search algorithm (SSA), called Tent
L\'evy flying sparrow search algorithm (TFSSA), and use it to select the best
subset of features in the packing pattern for classification purposes. SSA is a
recently proposed algorithm that has not been systematically applied to feature
selection problems. After verification by the CEC2020 benchmark function, TFSSA
is used to select the best feature combination to maximize classification
accuracy and minimize the number of selected features. The proposed TFSSA is
compared with nine algorithms in the literature. Nine evaluation metrics are
used to properly evaluate and compare the performance of these algorithms on
twenty-one datasets from the UCI repository. Furthermore, the approach is
applied to the coronavirus disease (COVID-19) dataset, yielding the best
average classification accuracy and the average number of feature selections,
respectively, of 93.47% and 2.1. Experimental results confirm the advantages of
the proposed algorithm in improving classification accuracy and reducing the
number of selected features compared to other wrapper-based algorithms."
11540,"For
from network instability, which warrants further research.","Despite                                                      trained on ImageNet data, learn to identify edges and
the broad appeal of generative models, they require sub-                                                        corners in the image, and later layers build on top of
stantially large amounts of training data and also suﬀer                                                        these features to learn more complicated structures.","any new problem whose data looks similar to ImageNet,
However, there has been some interesting recent work on                                                         we can start with pre-trained ImageNet models, change
self-validating classiﬁers that allow a user to determine if                                                    the ﬁnal layers, and ﬁne-tune it to our dataset.",2022-09-22 16:08:34+00:00,Modern Machine Learning Tools for Monitoring and Control of Industrial Processes: A Survey,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('R. Bhushan Gopaluni'), arxiv.Result.Author('Aditya Tulsyan'), arxiv.Result.Author('Benoit Chachuat'), arxiv.Result.Author('Biao Huang'), arxiv.Result.Author('Jong Min Lee'), arxiv.Result.Author('Faraz Amjad'), arxiv.Result.Author('Seshu Kumar Damarla'), arxiv.Result.Author('Jong Woo Kim'), arxiv.Result.Author('Nathan P. Lawrence')]","Over the last ten years, we have seen a significant increase in industrial
data, tremendous improvement in computational power, and major theoretical
advances in machine learning. This opens up an opportunity to use modern
machine learning tools on large-scale nonlinear monitoring and control
problems. This article provides a survey of recent results with applications in
the process industry."
11541,"Addressing the above concerns
requires further research in providing robustness guarantees for generative models as well as close
collaborations with researchers in socio-technical disciplines.","Generative models are also brittle and susceptible to backdoor adversarial attacks
on publicly available training data, causing unanticipated failure.","28
Figure 13: CIFAR-10 samples from PFGM (RK45)
                             29
Figure 14: CelebA 64 × 64 samples from PFGM (RK45)
                                30
Figure 15: LSUN bedroom 256 × 256 samples from PFGM (RK45) using DDPM channel conﬁgura-
tion.",2022-09-22 17:26:58+00:00,Poisson Flow Generative Models,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Yilun Xu'), arxiv.Result.Author('Ziming Liu'), arxiv.Result.Author('Max Tegmark'), arxiv.Result.Author('Tommi Jaakkola')]","We propose a new ""Poisson flow"" generative model (PFGM) that maps a uniform
distribution on a high-dimensional hemisphere into any data distribution. We
interpret the data points as electrical charges on the $z=0$ hyperplane in a
space augmented with an additional dimension $z$, generating a high-dimensional
electric field (the gradient of the solution to Poisson equation). We prove
that if these charges flow upward along electric field lines, their initial
distribution in the $z=0$ plane transforms into a distribution on the
hemisphere of radius $r$ that becomes uniform in the $r \to\infty$ limit. To
learn the bijective transformation, we estimate the normalized field in the
augmented space. For sampling, we devise a backward ODE that is anchored by the
physically meaningful additional dimension: the samples hit the unaugmented
data manifold when the $z$ reaches zero. Experimentally, PFGM achieves current
state-of-the-art performance among the normalizing flow models on CIFAR-10,
with an Inception score of $9.68$ and a FID score of $2.48$. It also performs
on par with the state-of-the-art SDE approaches while offering $10\times $ to
$20 \times$ acceleration on image generation tasks. Additionally, PFGM appears
more tolerant of estimation errors on a weaker network architecture and robust
to the step size in the Euler method. The code is available at
https://github.com/Newbeeer/poisson_flow ."
11542,"Addressing the above concerns
requires further research in providing robustness guarantees for generative models as well as close
collaborations with researchers in socio-technical disciplines.","Generative models are also brittle and susceptible to backdoor adversarial attacks
on publicly available training data, causing unanticipated failure.","29
Figure 12: CIFAR-10 samples from PFGM (RK45)
                             30
Figure 13: CelebA 64 × 64 samples from PFGM (RK45, NCSNv2 architecture)
                                              31
Figure 14: LSUN bedroom 256 × 256 samples from PFGM (RK45) using DDPM channel conﬁgura-
tion.",2022-09-22 17:26:58+00:00,Poisson Flow Generative Models,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Yilun Xu'), arxiv.Result.Author('Ziming Liu'), arxiv.Result.Author('Max Tegmark'), arxiv.Result.Author('Tommi Jaakkola')]","We propose a new ""Poisson flow"" generative model (PFGM) that maps a uniform
distribution on a high-dimensional hemisphere into any data distribution. We
interpret the data points as electrical charges on the $z=0$ hyperplane in a
space augmented with an additional dimension $z$, generating a high-dimensional
electric field (the gradient of the solution to Poisson equation). We prove
that if these charges flow upward along electric field lines, their initial
distribution in the $z=0$ plane transforms into a distribution on the
hemisphere of radius $r$ that becomes uniform in the $r \to\infty$ limit. To
learn the bijective transformation, we estimate the normalized field in the
augmented space. For sampling, we devise a backward ODE that is anchored by the
physically meaningful additional dimension: the samples hit the unaugmented
data manifold when the $z$ reaches zero. Experimentally, PFGM achieves current
state-of-the-art performance among the normalizing flow models on CIFAR-10,
with an Inception score of $9.68$ and a FID score of $2.35$. It also performs
on par with the state-of-the-art SDE approaches while offering $10\times $ to
$20 \times$ acceleration on image generation tasks. Additionally, PFGM appears
more tolerant of estimation errors on a weaker network architecture and robust
to the step size in the Euler method. The code is available at
https://github.com/Newbeeer/poisson_flow ."
11543,"Addressing the above concerns
requires further research in providing robustness guarantees for generative models as well as close
collaborations with researchers in socio-technical disciplines.","Generative models are also brittle and susceptible to backdoor adversarial attacks
on publicly available training data, causing unanticipated failure.","29
Figure 12: CIFAR-10 samples from PFGM (RK45)
                             30
Figure 13: CelebA 64 × 64 samples from PFGM (RK45, NCSNv2 architecture)
                                              31
Figure 14: LSUN bedroom 256 × 256 samples from PFGM (RK45) using DDPM channel conﬁgura-
tion.",2022-09-22 17:26:58+00:00,Poisson Flow Generative Models,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Yilun Xu'), arxiv.Result.Author('Ziming Liu'), arxiv.Result.Author('Max Tegmark'), arxiv.Result.Author('Tommi Jaakkola')]","We propose a new ""Poisson flow"" generative model (PFGM) that maps a uniform
distribution on a high-dimensional hemisphere into any data distribution. We
interpret the data points as electrical charges on the $z=0$ hyperplane in a
space augmented with an additional dimension $z$, generating a high-dimensional
electric field (the gradient of the solution to Poisson equation). We prove
that if these charges flow upward along electric field lines, their initial
distribution in the $z=0$ plane transforms into a distribution on the
hemisphere of radius $r$ that becomes uniform in the $r \to\infty$ limit. To
learn the bijective transformation, we estimate the normalized field in the
augmented space. For sampling, we devise a backward ODE that is anchored by the
physically meaningful additional dimension: the samples hit the unaugmented
data manifold when the $z$ reaches zero. Experimentally, PFGM achieves current
state-of-the-art performance among the normalizing flow models on CIFAR-10,
with an Inception score of $9.68$ and a FID score of $2.35$. It also performs
on par with the state-of-the-art SDE approaches while offering $10\times $ to
$20 \times$ acceleration on image generation tasks. Additionally, PFGM appears
more tolerant of estimation errors on a weaker network architecture and robust
to the step size in the Euler method. The code is available at
https://github.com/Newbeeer/poisson_flow ."
11544,"Addressing the above concerns
requires further research in providing robustness guarantees for generative models as well as close
collaborations with researchers in socio-technical disciplines.","Generative models are also brittle and susceptible to backdoor adversarial attacks
on publicly available training data, causing unanticipated failure.","29
Figure 12: CIFAR-10 samples from PFGM (RK45)
                             30
Figure 13: CelebA 64 × 64 samples from PFGM (RK45, NCSNv2 architecture)
                                              31
Figure 14: LSUN bedroom 256 × 256 samples from PFGM (RK45) using DDPM channel conﬁgura-
tion.",2022-09-22 17:26:58+00:00,Poisson Flow Generative Models,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Yilun Xu'), arxiv.Result.Author('Ziming Liu'), arxiv.Result.Author('Max Tegmark'), arxiv.Result.Author('Tommi Jaakkola')]","We propose a new ""Poisson flow"" generative model (PFGM) that maps a uniform
distribution on a high-dimensional hemisphere into any data distribution. We
interpret the data points as electrical charges on the $z=0$ hyperplane in a
space augmented with an additional dimension $z$, generating a high-dimensional
electric field (the gradient of the solution to Poisson equation). We prove
that if these charges flow upward along electric field lines, their initial
distribution in the $z=0$ plane transforms into a distribution on the
hemisphere of radius $r$ that becomes uniform in the $r \to\infty$ limit. To
learn the bijective transformation, we estimate the normalized field in the
augmented space. For sampling, we devise a backward ODE that is anchored by the
physically meaningful additional dimension: the samples hit the unaugmented
data manifold when the $z$ reaches zero. Experimentally, PFGM achieves current
state-of-the-art performance among the normalizing flow models on CIFAR-10,
with an Inception score of $9.68$ and a FID score of $2.35$. It also performs
on par with the state-of-the-art SDE approaches while offering $10\times $ to
$20 \times$ acceleration on image generation tasks. Additionally, PFGM appears
more tolerant of estimation errors on a weaker network architecture and robust
to the step size in the Euler method. The code is available at
https://github.com/Newbeeer/poisson_flow ."
11580,"Parameters
                                                             6.8 Case Study
      HGT       35.06 ± 0.17   35.44 ± 0.36  94,479,069
    R-GCN                                                    Here, we analyze the learned embeddings of a 4-layered RE-
    R-GSN       49.89 ± 0.47   49.27 ± 0.61  21,173,389      GCN on the DBLP dataset to further study the mechanism
   R-HGNN       49.12 ± 0.50   47.89 ± 0.53  154,366,772     of our RE-GNN framework.",Test Acc.,As presented in Tab.,2022-09-23 05:24:18+00:00,Enabling Homogeneous GNNs to Handle Heterogeneous Graphs via Relation Embedding,cs.LG,['cs.LG'],"[arxiv.Result.Author('Junfu Wang'), arxiv.Result.Author('Yuanfang Guo'), arxiv.Result.Author('Liang Yang'), arxiv.Result.Author('Yunhong Wang')]","Graph Neural Networks (GNNs) have been generalized to process the
heterogeneous graphs by various approaches. Unfortunately, these approaches
usually model the heterogeneity via various complicated modules. This paper
aims to propose a simple yet effective framework to assign adequate ability to
the homogeneous GNNs to handle the heterogeneous graphs. Specifically, we
propose Relation Embedding based Graph Neural Network (RE-GNN), which employs
only one parameter per relation to embed the importance of distinct types of
relations and node-type-specific self-loop connections. To optimize these
relation embeddings and the model parameters simultaneously, a gradient scaling
factor is proposed to constrain the embeddings to converge to suitable values.
Besides, we interpret the proposed RE-GNN from two perspectives, and
theoretically demonstrate that our RE-GCN possesses more expressive power than
GTN (which is a typical heterogeneous GNN, and it can generate meta-paths
adaptively). Extensive experiments demonstrate that our RE-GNN can effectively
and efficiently handle the heterogeneous graphs and can be applied to various
homogeneous GNNs."
11598,"W ITH the fast growth of sensor applications, the amount                     First, the measurement indexes of missing value imputation
                                                 of sensor data has increased tremendously in recent              warrant further study [3].",INTRODUCTION                                 have three types of issues.,"To measure the imputation effect of
                                        years [1].",2022-09-22 14:17:24+00:00,Multistage Large Segment Imputation Framework Based on Deep Learning and Statistic Metrics,cs.LG,['cs.LG'],"[arxiv.Result.Author('JinSheng Yang'), arxiv.Result.Author('YuanHai Shao'), arxiv.Result.Author('ChunNa Li'), arxiv.Result.Author('Wensi Wang')]","Missing value is a very common and unavoidable problem in sensors, and
researchers have made numerous attempts for missing value imputation,
particularly in deep learning models. However, for real sensor data, the
specific data distribution and data periods are rarely considered, making it
difficult to choose the appropriate evaluation indexes and models for different
sensors. To address this issue, this study proposes a multistage imputation
framework based on deep learning with adaptability for missing value
imputation. The model presents a mixture measurement index of low- and
higher-order statistics for data distribution and a new perspective on data
imputation performance metrics, which is more adaptive and effective than the
traditional mean squared error. A multistage imputation strategy and dynamic
data length are introduced into the imputation process for data periods.
Experimental results on different types of sensor data show that the multistage
imputation strategy and the mixture index are superior and that the effect of
missing value imputation has been improved to some extent, particularly for the
large segment imputation problem. The codes and experimental results have been
uploaded to GitHub."
11601,"They further study the pricing problem with demand fairness that are unknown
and needs learn√ing.","[2021] considers both group (price) fairness and individual (time) fairness,
and their algorithm FaPU solves this problem with sublinear regret while guaranteeing
fairness.","In this setting, they propose another FaPD algorithm that achieves
the optimal O˜( T ) regret and guarantees the demand fairness “almost surely”, i.e., upper
bounded by δ · T as a budget.",2022-09-23 20:02:09+00:00,Doubly Fair Dynamic Pricing,cs.LG,"['cs.LG', 'econ.EM', 'stat.ML', '91B06 (Primary) 91B24, 62P20, 62C20, 90B50 (Secondary)', 'I.2.6']","[arxiv.Result.Author('Jianyu Xu'), arxiv.Result.Author('Dan Qiao'), arxiv.Result.Author('Yu-Xiang Wang')]","We study the problem of online dynamic pricing with two types of fairness
constraints: a ""procedural fairness"" which requires the proposed prices to be
equal in expectation among different groups, and a ""substantive fairness"" which
requires the accepted prices to be equal in expectation among different groups.
A policy that is simultaneously procedural and substantive fair is referred to
as ""doubly fair"". We show that a doubly fair policy must be random to have
higher revenue than the best trivial policy that assigns the same price to
different groups. In a two-group setting, we propose an online learning
algorithm for the 2-group pricing problems that achieves $\tilde{O}(\sqrt{T})$
regret, zero procedural unfairness and $\tilde{O}(\sqrt{T})$ substantive
unfairness over $T$ rounds of learning. We also prove two lower bounds showing
that these results on regret and unfairness are both information-theoretically
optimal up to iterated logarithmic factors. To the best of our knowledge, this
is the first dynamic pricing algorithm that learns to price while satisfying
two fairness constraints at the same time."
11609,"Figure 2:
The ﬂuctuating performance on source model VGG-19 mo-                                                                                       The number of adversaries
tivates us to further study the impact of model components         that successfully attack the correspond-
                                                                   ing number of the defense models.","They provides a performance increase of
                                                                                         00 1 2 3 4 5 6 7
                                                                                                                                                   # Defense Models
1.6% - 45.4% compared with the other attacks in general.","Most
on attack performance in future.",2022-09-24 08:57:10+00:00,"Approximate better, Attack stronger: Adversarial Example Generation via Asymptotically Gaussian Mixture Distribution",cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Zhengwei Fang'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Tao Huang'), arxiv.Result.Author('Liping Jing')]","Strong adversarial examples are the keys to evaluating and enhancing the
robustness of deep neural networks. The popular adversarial attack algorithms
maximize the non-concave loss function using the gradient ascent. However, the
performance of each attack is usually sensitive to, for instance, minor image
transformations due to insufficient information (only one input example, few
white-box source models and unknown defense strategies). Hence, the crafted
adversarial examples are prone to overfit the source model, which limits their
transferability to unidentified architectures. In this paper, we propose
Multiple Asymptotically Normal Distribution Attacks (MultiANDA), a novel method
that explicitly characterizes adversarial perturbations from a learned
distribution. Specifically, we approximate the posterior distribution over the
perturbations by taking advantage of the asymptotic normality property of
stochastic gradient ascent (SGA), then apply the ensemble strategy on this
procedure to estimate a Gaussian mixture model for a better exploration of the
potential optimization space. Drawing perturbations from the learned
distribution allow us to generate any number of adversarial examples for each
input. The approximated posterior essentially describes the stationary
distribution of SGA iterations, which captures the geometric information around
the local optimum. Thus, the samples drawn from the distribution reliably
maintain the transferability. Our proposed method outperforms nine
state-of-the-art black-box attacks on deep learning models with or without
defenses through extensive experiments on seven normally trained and seven
defence models."
11610,"We further study the effect on attack performance of the approximation of perturbation distribution,
i.e., accumulating historical gradients on the optimization trajectory.","This is probably due to the absent of the original image in the
augmented batches.","The experimental results are
shown in Figure 8.",2022-09-24 08:57:10+00:00,"Approximate better, Attack stronger: Adversarial Example Generation via Asymptotically Gaussian Mixture Distribution",cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Zhengwei Fang'), arxiv.Result.Author('Rui Wang'), arxiv.Result.Author('Tao Huang'), arxiv.Result.Author('Liping Jing')]","Strong adversarial examples are the keys to evaluating and enhancing the
robustness of deep neural networks. The popular adversarial attack algorithms
maximize the non-concave loss function using the gradient ascent. However, the
performance of each attack is usually sensitive to, for instance, minor image
transformations due to insufficient information (only one input example, few
white-box source models and unknown defense strategies). Hence, the crafted
adversarial examples are prone to overfit the source model, which limits their
transferability to unidentified architectures. In this paper, we propose
Multiple Asymptotically Normal Distribution Attacks (MultiANDA), a novel method
that explicitly characterizes adversarial perturbations from a learned
distribution. Specifically, we approximate the posterior distribution over the
perturbations by taking advantage of the asymptotic normality property of
stochastic gradient ascent (SGA), then apply the ensemble strategy on this
procedure to estimate a Gaussian mixture model for a better exploration of the
potential optimization space. Drawing perturbations from the learned
distribution allow us to generate any number of adversarial examples for each
input. The approximated posterior essentially describes the stationary
distribution of SGA iterations, which captures the geometric information around
the local optimum. Thus, the samples drawn from the distribution reliably
maintain the transferability. Our proposed method outperforms nine
state-of-the-art black-box attacks on deep learning models with or without
defenses through extensive experiments on seven normally trained and seven
defence models."
11654,"With the help of the voltage data of the
FiN-2 dataset, we want to enable further research in this area and provide
data that reﬂects the widest possible range of a real environment.","As for example
in [12], where the use of PLC modems as voltage sensors for smart grids and
grid automation was investigated.","While the
previous ﬁelds of application primarily involve voltage data, the SNR mea-
surements in particular oﬀer additional ﬁelds of application.",2022-09-23 11:37:02+00:00,Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Christoph Balada'), arxiv.Result.Author('Max Bondorf'), arxiv.Result.Author('Sheraz Ahmed'), arxiv.Result.Author('Andreas Dengela'), arxiv.Result.Author('Markus Zdrallek')]","Electricity grids have become an essential part of daily life, even if they
are often not noticed in everyday life. We usually only become particularly
aware of this dependence by the time the electricity grid is no longer
available. However, significant changes, such as the transition to renewable
energy (photovoltaic, wind turbines, etc.) and an increasing number of energy
consumers with complex load profiles (electric vehicles, home battery systems,
etc.), pose new challenges for the electricity grid. To address these
challenges, we propose two first-of-its-kind datasets based on measurements in
a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1
and FiN-2, were collected during real practical use in a part of the German
low-voltage grid that supplies around 4.4 million people and show more than 13
billion datapoints collected by more than 5100 sensors. In addition, we present
different use cases in asset management, grid state visualization, forecasting,
predictive maintenance, and novelty detection to highlight the benefits of
these types of data. For these applications, we particularly highlight the use
of novel machine learning architectures to extract rich information from
real-world data that cannot be captured using traditional approaches. By
publishing the first large-scale real-world dataset, we aim to shed light on
the previously largely unrecognized potential of PLC data and emphasize
machine-learning-based research in low-voltage distribution networks by
presenting a variety of different use cases."
11655,"However, since we only provide this application
as an example, we leave these extensions open for further research.","Furthermore, it is also possible to include
other data, such as voltage.","16
Figure 10: Result of the neighborhood estimation system for a local neighborhood.",2022-09-23 11:37:02+00:00,Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Christoph Balada'), arxiv.Result.Author('Max Bondorf'), arxiv.Result.Author('Sheraz Ahmed'), arxiv.Result.Author('Andreas Dengela'), arxiv.Result.Author('Markus Zdrallek')]","Electricity grids have become an essential part of daily life, even if they
are often not noticed in everyday life. We usually only become particularly
aware of this dependence by the time the electricity grid is no longer
available. However, significant changes, such as the transition to renewable
energy (photovoltaic, wind turbines, etc.) and an increasing number of energy
consumers with complex load profiles (electric vehicles, home battery systems,
etc.), pose new challenges for the electricity grid. To address these
challenges, we propose two first-of-its-kind datasets based on measurements in
a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1
and FiN-2, were collected during real practical use in a part of the German
low-voltage grid that supplies around 4.4 million people and show more than 13
billion datapoints collected by more than 5100 sensors. In addition, we present
different use cases in asset management, grid state visualization, forecasting,
predictive maintenance, and novelty detection to highlight the benefits of
these types of data. For these applications, we particularly highlight the use
of novel machine learning architectures to extract rich information from
real-world data that cannot be captured using traditional approaches. By
publishing the first large-scale real-world dataset, we aim to shed light on
the previously largely unrecognized potential of PLC data and emphasize
machine-learning-based research in low-voltage distribution networks by
presenting a variety of different use cases."
11669,"taneously, graphs have been leveraged to represent design data
and to capture various relationships, including joint relationships                     To enable further research and reproducibility, we share the
between parts in assemblies [11, 12], semantic relationships be-                   code for feature extraction, training, and experiments 1.
tween engineering and design concepts [4], and geometric rela-
tionships between faces of BREPs [13, 14].",Simul-                          evaluate the framework within three design scenarios.,"2 RELATED WORK
                                                                                        We review the prior work related to material selection, graph
     In product design, efforts to consolidate design knowledge
in knowledge graphs have resulted in robust graph representa-                      representation learning in design automation, and graph neural
tions of domain-speciﬁc semantic relationships.",2022-09-26 15:49:35+00:00,Material Prediction for Design Automation Using Graph Representation Learning,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Shijie Bian'), arxiv.Result.Author('Daniele Grandi'), arxiv.Result.Author('Kaveh Hassani'), arxiv.Result.Author('Elliot Sadler'), arxiv.Result.Author('Bodia Borijin'), arxiv.Result.Author('Axel Fernandes'), arxiv.Result.Author('Andrew Wang'), arxiv.Result.Author('Thomas Lu'), arxiv.Result.Author('Richard Otis'), arxiv.Result.Author('Nhut Ho'), arxiv.Result.Author('Bingbing Li')]","Successful material selection is critical in designing and manufacturing
products for design automation. Designers leverage their knowledge and
experience to create high-quality designs by selecting the most appropriate
materials through performance, manufacturability, and sustainability
evaluation. Intelligent tools can help designers with varying expertise by
providing recommendations learned from prior designs. To enable this, we
introduce a graph representation learning framework that supports the material
prediction of bodies in assemblies. We formulate the material selection task as
a node-level prediction task over the assembly graph representation of CAD
models and tackle it using Graph Neural Networks (GNNs). Evaluations over three
experimental protocols performed on the Fusion 360 Gallery dataset indicate the
feasibility of our approach, achieving a 0.75 top-3 micro-f1 score. The
proposed framework can scale to large datasets and incorporate designers'
knowledge into the learning process. These capabilities allow the framework to
serve as a recommendation system for design automation and a baseline for
future work, narrowing the gap between human designers and intelligent design
agents."
11670,We hope to support further research                bottom-up assembly of parametric cad joints.,"Joinable: Learning
from design requirements.","to bridge the gap between the understanding of human designers         [12] Jones, B., Hildreth, D., Chen, D., Baran, I., Kim, V., and
and that of intelligent design agents.",2022-09-26 15:49:35+00:00,Material Prediction for Design Automation Using Graph Representation Learning,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Shijie Bian'), arxiv.Result.Author('Daniele Grandi'), arxiv.Result.Author('Kaveh Hassani'), arxiv.Result.Author('Elliot Sadler'), arxiv.Result.Author('Bodia Borijin'), arxiv.Result.Author('Axel Fernandes'), arxiv.Result.Author('Andrew Wang'), arxiv.Result.Author('Thomas Lu'), arxiv.Result.Author('Richard Otis'), arxiv.Result.Author('Nhut Ho'), arxiv.Result.Author('Bingbing Li')]","Successful material selection is critical in designing and manufacturing
products for design automation. Designers leverage their knowledge and
experience to create high-quality designs by selecting the most appropriate
materials through performance, manufacturability, and sustainability
evaluation. Intelligent tools can help designers with varying expertise by
providing recommendations learned from prior designs. To enable this, we
introduce a graph representation learning framework that supports the material
prediction of bodies in assemblies. We formulate the material selection task as
a node-level prediction task over the assembly graph representation of CAD
models and tackle it using Graph Neural Networks (GNNs). Evaluations over three
experimental protocols performed on the Fusion 360 Gallery dataset indicate the
feasibility of our approach, achieving a 0.75 top-3 micro-f1 score. The
proposed framework can scale to large datasets and incorporate designers'
knowledge into the learning process. These capabilities allow the framework to
serve as a recommendation system for design automation and a baseline for
future work, narrowing the gap between human designers and intelligent design
agents."
11673,"It can be reused with other IDE techniques
          or models for further research in the domain.","(ii) We have released the library created for this experiment (https://github.com/
          bonheml/VAE_learning_dynamics).","(iii) During our analysis of VAEs, we found that (1) the deeper the layer of the encoder, the
          lower the estimated IDs, whereas the layers of the decoder all have the same IDE; (2) the
          extrinsic dimensionality of the latent representations is generally higher than its IDE; (3)
          the layers reach a stable ID very early in the training; and (4) the IDE of mean and sampled
          representations is different when some latent variables collapse.",2022-09-26 15:59:54+00:00,FONDUE: an algorithm to find the optimal dimensionality of the latent representations of variational autoencoders,cs.LG,"['cs.LG', 'I.2.6; G.3']","[arxiv.Result.Author('Lisa Bonheme'), arxiv.Result.Author('Marek Grzes')]","When training a variational autoencoder (VAE) on a given dataset, determining
the optimal number of latent variables is mostly done by grid search: a costly
process in terms of computational time and carbon footprint. In this paper, we
explore the intrinsic dimension estimation (IDE) of the data and latent
representations learned by VAEs. We show that the discrepancies between the IDE
of the mean and sampled representations of a VAE after only a few steps of
training reveal the presence of passive variables in the latent space, which,
in well-behaved VAEs, indicates a superfluous number of dimensions. Using this
property, we propose FONDUE: an algorithm which quickly finds the number of
latent dimensions after which the mean and sampled representations start to
diverge (i.e., when passive variables are introduced), providing a principled
method for selecting the number of latent dimensions for VAEs and autoencoders."
11697,"We hope this work will encourage further research around this new connection, and help to
better understand current best practices in model training as well as discover new ones.","In conclusion, altogether, geometric complexity provides a useful lens for understanding deep learning
and sheds light into why neural networks are able to achieve low test error with highly expressive
models.","Acknowledgments and Disclosure of Funding

We would like to thank Chongli Qin, Samuel Smith, Soham De, Yan Wu, and the reviewers for
helpful discussions and feedback as well as Patrick Cole, Xavi Gonzalvo, and Shakir Mohamed for
their support.",2022-09-27 00:16:38+00:00,Why neural networks find simple solutions: the many regularizers of geometric complexity,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Benoit Dherin'), arxiv.Result.Author('Michael Munn'), arxiv.Result.Author('Mihaela Rosca'), arxiv.Result.Author('David G. T. Barrett')]","In many contexts, simpler models are preferable to more complex models and
the control of this model complexity is the goal for many methods in machine
learning such as regularization, hyperparameter tuning and architecture design.
In deep learning, it has been difficult to understand the underlying mechanisms
of complexity control, since many traditional measures are not naturally
suitable for deep neural networks. Here we develop the notion of geometric
complexity, which is a measure of the variability of the model function,
computed using a discrete Dirichlet energy. Using a combination of theoretical
arguments and empirical results, we show that many common training heuristics
such as parameter norm regularization, spectral norm regularization, flatness
regularization, implicit gradient regularization, noise regularization and the
choice of parameter initialization all act to control geometric complexity,
providing a unifying framework in which to characterize the behavior of deep
learning models."
11698,"We hope this work will encourage further research around this new connection, and help to
better understand current best practices in model training as well as discover new ones.","In conclusion, altogether, geometric complexity provides a useful lens for understanding deep learning
and sheds light into why neural networks are able to achieve low test error with highly expressive
models.","Acknowledgments and Disclosure of Funding

We would like to thank Chongli Qin, Samuel Smith, Soham De, Yan Wu, and the reviewers for
helpful discussions and feedback as well as Patrick Cole, Xavi Gonzalvo, and Shakir Mohamed for
their support.",2022-09-27 00:16:38+00:00,Why neural networks find simple solutions: the many regularizers of geometric complexity,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Benoit Dherin'), arxiv.Result.Author('Michael Munn'), arxiv.Result.Author('Mihaela Rosca'), arxiv.Result.Author('David G. T. Barrett')]","In many contexts, simpler models are preferable to more complex models and
the control of this model complexity is the goal for many methods in machine
learning such as regularization, hyperparameter tuning and architecture design.
In deep learning, it has been difficult to understand the underlying mechanisms
of complexity control, since many traditional measures are not naturally
suitable for deep neural networks. Here we develop the notion of geometric
complexity, which is a measure of the variability of the model function,
computed using a discrete Dirichlet energy. Using a combination of theoretical
arguments and empirical results, we show that many common training heuristics
such as parameter norm regularization, spectral norm regularization, flatness
regularization, implicit gradient regularization, noise regularization and the
choice of parameter initialization all act to control geometric complexity,
providing a unifying framework in which to characterize the behavior of deep
learning models."
11719,Next we further study the interpolation error.,These results are consistent with the above remark.,"For the sake of presentation, denote p : [0, 1] → R to be
an increasing score function describing hw(S+), where p(x) is the score in the bottom x-quantile of
hw(S+).",2022-09-27 09:06:37+00:00,Exploring the Algorithm-Dependent Generalization of AUPRC Optimization with List Stability,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Peisong Wen'), arxiv.Result.Author('Qianqian Xu'), arxiv.Result.Author('Zhiyong Yang'), arxiv.Result.Author('Yuan He'), arxiv.Result.Author('Qingming Huang')]","Stochastic optimization of the Area Under the Precision-Recall Curve (AUPRC)
is a crucial problem for machine learning. Although various algorithms have
been extensively studied for AUPRC optimization, the generalization is only
guaranteed in the multi-query case. In this work, we present the first trial in
the single-query generalization of stochastic AUPRC optimization. For sharper
generalization bounds, we focus on algorithm-dependent generalization. There
are both algorithmic and theoretical obstacles to our destination. From an
algorithmic perspective, we notice that the majority of existing stochastic
estimators are biased only when the sampling strategy is biased, and is
leave-one-out unstable due to the non-decomposability. To address these issues,
we propose a sampling-rate-invariant unbiased stochastic estimator with
superior stability. On top of this, the AUPRC optimization is formulated as a
composition optimization problem, and a stochastic algorithm is proposed to
solve this problem. From a theoretical perspective, standard techniques of the
algorithm-dependent generalization analysis cannot be directly applied to such
a listwise compositional optimization problem. To fill this gap, we extend the
model stability from instancewise losses to listwise losses and bridge the
corresponding generalization and stability. Additionally, we construct state
transition matrices to describe the recurrence of the stability, and simplify
calculations by matrix spectrum. Practically, experimental results on three
image retrieval datasets on speak to the effectiveness and soundness of our
framework."
11726,"Then by further studying the clipping impact, we discover that the inﬂuence of ﬁnal
performance when clipping the outliers varies greatly, where some more aggressive outliers covering
a large area can be clipped safely without accuracy degradation, but the accuracy can drop suddenly
when the important outliers are clipped.","By extracting it, the activation becomes more robust for
quantization.","More interestingly, though those less important outliers
might present in a long tail form, they are only provided by a few tokens.",2022-09-27 12:05:59+00:00,Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xiuying Wei'), arxiv.Result.Author('Yunchen Zhang'), arxiv.Result.Author('Xiangguo Zhang'), arxiv.Result.Author('Ruihao Gong'), arxiv.Result.Author('Shanghang Zhang'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Fengwei Yu'), arxiv.Result.Author('Xianglong Liu')]","Transformer architecture has become the fundamental element of the widespread
natural language processing~(NLP) models. With the trends of large NLP models,
the increasing memory and computation costs hinder their efficient deployment
on resource-limited devices. Therefore, transformer quantization attracts wide
research interest. Recent work recognizes that structured outliers are the
critical bottleneck for quantization performance. However, their proposed
methods increase the computation overhead and still leave the outliers there.
To fundamentally address this problem, this paper delves into the inherent
inducement and importance of the outliers. We discover that $\boldsymbol
\gamma$ in LayerNorm (LN) acts as a sinful amplifier for the outliers, and the
importance of outliers varies greatly where some outliers provided by a few
tokens cover a large area but can be clipped sharply without negative impacts.
Motivated by these findings, we propose an outlier suppression framework
including two components: Gamma Migration and Token-Wise Clipping. The Gamma
Migration migrates the outlier amplifier to subsequent modules in an equivalent
transformation, contributing to a more quantization-friendly model without any
extra burden. The Token-Wise Clipping takes advantage of the large variance of
token range and designs a token-wise coarse-to-fine pipeline, obtaining a
clipping range with minimal final quantization loss in an efficient way. This
framework effectively suppresses the outliers and can be used in a
plug-and-play mode. Extensive experiments prove that our framework surpasses
the existing works and, for the first time, pushes the 6-bit post-training BERT
quantization to the full-precision (FP) level. Our code is available at
https://github.com/wimh966/outlier_suppression."
11727,"Then by further studying the clipping impact, we discover that the inﬂuence of ﬁnal
performance when clipping the outliers varies greatly, where some more aggressive outliers covering
a large area can be clipped safely without accuracy degradation, but the accuracy can drop suddenly
when the important outliers are clipped.","By extracting it, the activation becomes more robust for
quantization.","More interestingly, though those less important outliers
might present in a long tail form, they are only provided by a few tokens.",2022-09-27 12:05:59+00:00,Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xiuying Wei'), arxiv.Result.Author('Yunchen Zhang'), arxiv.Result.Author('Xiangguo Zhang'), arxiv.Result.Author('Ruihao Gong'), arxiv.Result.Author('Shanghang Zhang'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Fengwei Yu'), arxiv.Result.Author('Xianglong Liu')]","Transformer architecture has become the fundamental element of the widespread
natural language processing~(NLP) models. With the trends of large NLP models,
the increasing memory and computation costs hinder their efficient deployment
on resource-limited devices. Therefore, transformer quantization attracts wide
research interest. Recent work recognizes that structured outliers are the
critical bottleneck for quantization performance. However, their proposed
methods increase the computation overhead and still leave the outliers there.
To fundamentally address this problem, this paper delves into the inherent
inducement and importance of the outliers. We discover that $\boldsymbol
\gamma$ in LayerNorm (LN) acts as a sinful amplifier for the outliers, and the
importance of outliers varies greatly where some outliers provided by a few
tokens cover a large area but can be clipped sharply without negative impacts.
Motivated by these findings, we propose an outlier suppression framework
including two components: Gamma Migration and Token-Wise Clipping. The Gamma
Migration migrates the outlier amplifier to subsequent modules in an equivalent
transformation, contributing to a more quantization-friendly model without any
extra burden. The Token-Wise Clipping takes advantage of the large variance of
token range and designs a token-wise coarse-to-fine pipeline, obtaining a
clipping range with minimal final quantization loss in an efficient way. This
framework effectively suppresses the outliers and can be used in a
plug-and-play mode. Extensive experiments prove that our framework surpasses
the existing works and, for the first time, pushes the 6-bit post-training BERT
quantization to the full-precision (FP) level. Our code is available at
https://github.com/wimh966/outlier_suppression."
11769,"A more detailed comparison on instances with
more than 10 objectives will be investigated in further research.","Although no statistical signiﬁcant diﬀerences were achieved, it is shown that
many-objective algorithms can help with performance increases on instances
with more than 3 objectives.","Acknowledgement

The ﬁnancial support by the Christian Doppler Research Association, the Aus-
trian Federal Ministry for Digital and Economic Aﬀairs and the National Foun-
dation for Research, Technology and Development is gratefully acknowledged.",2022-09-28 06:10:34+00:00,Shape-constrained Symbolic Regression with NSGA-III,cs.LG,['cs.LG'],[arxiv.Result.Author('Christian Haider')],"Shape-constrained symbolic regression (SCSR) allows to include prior
knowledge into data-based modeling. This inclusion allows to ensure that
certain expected behavior is better reflected by the resulting models. The
expected behavior is defined via constraints, which refer to the function form
e.g. monotonicity, concavity, convexity or the models image boundaries. In
addition to the advantage of obtaining more robust and reliable models due to
defining constraints over the functions shape, the use of SCSR allows to find
models which are more robust to noise and have a better extrapolation behavior.
This paper presents a mutlicriterial approach to minimize the approximation
error as well as the constraint violations. Explicitly the two algorithms
NSGA-II and NSGA-III are implemented and compared against each other in terms
of model quality and runtime. Both algorithms are capable of dealing with
multiple objectives, whereas NSGA-II is a well established multi-objective
approach performing well on instances with up-to 3 objectives. NSGA-III is an
extension of the NSGA-II algorithm and was developed to handle problems with
""many"" objectives (more than 3 objectives). Both algorithms are executed on a
selected set of benchmark instances from physics textbooks. The results
indicate that both algorithms are able to find largely feasible solutions and
NSGA-III provides slight improvements in terms of model quality. Moreover, an
improvement in runtime can be observed using the many-objective approach."
11777,"A density-
consider further research is required to understand whether this                               based algorithm for discovering clusters in large spatial databases with noise..",1996.,"In
kind of feature can be useful across a wide range of attacks and                               kdd, Vol.",2022-09-28 09:47:34+00:00,Machine Beats Machine: Machine Learning Models to Defend Against Adversarial Attacks,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jože M. Rožanec'), arxiv.Result.Author('Dimitrios Papamartzivanos'), arxiv.Result.Author('Entso Veliou'), arxiv.Result.Author('Theodora Anastasiou'), arxiv.Result.Author('Jelle Keizer'), arxiv.Result.Author('Blaž Fortuna'), arxiv.Result.Author('Dunja Mladenić')]","We propose using a two-layered deployment of machine learning models to
prevent adversarial attacks. The first layer determines whether the data was
tampered, while the second layer solves a domain-specific problem. We explore
three sets of features and three dataset variations to train machine learning
models. Our results show clustering algorithms achieved promising results. In
particular, we consider the best results were obtained by applying the DBSCAN
algorithm to the structured structural similarity index measure computed
between the images and a white reference image."
11805,"Our results ﬁnally contribute strong evidence towards the strength and efﬁciency of
bidirectional pre-training objectives and motivate further research into bidirectional architectures,
pre-training objectives, and language models designed and optimized for prompting and few-shot
learning.","Importantly, these results demonstrate bidirectional models possess few-shot in-context learning and
zero-shot instruction following capabilities innately, without the post-hoc modiﬁcations required by
prior work.","We hypothesize these future bidirectional training schemes could yield an approach that
overcomes the efﬁciency limitations of SAP, while maintaining the performance and parameter size
reduction beneﬁts.",2022-09-29 01:35:57+00:00,Bidirectional Language Models Are Also Few-shot Learners,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Ajay Patel'), arxiv.Result.Author('Bryan Li'), arxiv.Result.Author('Mohammad Sadegh Rasooli'), arxiv.Result.Author('Noah Constant'), arxiv.Result.Author('Colin Raffel'), arxiv.Result.Author('Chris Callison-Burch')]","Large language models such as GPT-3 (Brown et al., 2020) can perform
arbitrary tasks without undergoing fine-tuning after being prompted with only a
few labeled examples. An arbitrary task can be reformulated as a natural
language prompt, and a language model can be asked to generate the completion,
indirectly performing the task in a paradigm known as prompt-based learning. To
date, emergent prompt-based learning capabilities have mainly been demonstrated
for unidirectional language models. However, bidirectional language models
pre-trained on denoising objectives such as masked language modeling produce
stronger learned representations for transfer learning. This motivates the
possibility of prompting bidirectional models, but their pre-training
objectives have made them largely incompatible with the existing prompting
paradigm. We present SAP (Sequential Autoregressive Prompting), a technique
that enables the prompting of bidirectional models. Utilizing the machine
translation task as a case study, we prompt the bidirectional mT5 model (Xue et
al., 2021) with SAP and demonstrate its few-shot and zero-shot translations
outperform the few-shot translations of unidirectional models like GPT-3 and
XGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We
further show SAP is effective on question answering and summarization. For the
first time, our results demonstrate prompt-based learning is an emergent
property of a broader class of language models, rather than only unidirectional
models."
11806,"We hypothesize
that further research into pre-training objectives and language model design following Wang et al.","Nevertheless, SAP uncovers an impor-
tant result: prompt-based learning is an emergent property of bidirectional models.","(2022), Tay et al.",2022-09-29 01:35:57+00:00,Bidirectional Language Models Are Also Few-shot Learners,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Ajay Patel'), arxiv.Result.Author('Bryan Li'), arxiv.Result.Author('Mohammad Sadegh Rasooli'), arxiv.Result.Author('Noah Constant'), arxiv.Result.Author('Colin Raffel'), arxiv.Result.Author('Chris Callison-Burch')]","Large language models such as GPT-3 (Brown et al., 2020) can perform
arbitrary tasks without undergoing fine-tuning after being prompted with only a
few labeled examples. An arbitrary task can be reformulated as a natural
language prompt, and a language model can be asked to generate the completion,
indirectly performing the task in a paradigm known as prompt-based learning. To
date, emergent prompt-based learning capabilities have mainly been demonstrated
for unidirectional language models. However, bidirectional language models
pre-trained on denoising objectives such as masked language modeling produce
stronger learned representations for transfer learning. This motivates the
possibility of prompting bidirectional models, but their pre-training
objectives have made them largely incompatible with the existing prompting
paradigm. We present SAP (Sequential Autoregressive Prompting), a technique
that enables the prompting of bidirectional models. Utilizing the machine
translation task as a case study, we prompt the bidirectional mT5 model (Xue et
al., 2021) with SAP and demonstrate its few-shot and zero-shot translations
outperform the few-shot translations of unidirectional models like GPT-3 and
XGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We
further show SAP is effective on question answering and summarization. For the
first time, our results demonstrate prompt-based learning is an emergent
property of a broader class of language models, rather than only unidirectional
models."
11809,We reserve further research on this topic for the future.,(2019).,"4 EXPERIMENTAL EVALUATION

4.1 COMPARISON WITH FL METHODS

We run the baselines (see Section E) and compare with our F2L.",2022-09-29 02:49:13+00:00,Label driven Knowledge Distillation for Federated Learning with non-IID Data,cs.LG,"['cs.LG', 'cs.AI', '19A22', 'I.2.11']","[arxiv.Result.Author('Minh-Duong Nguyen'), arxiv.Result.Author('Quoc-Viet Pham'), arxiv.Result.Author('Dinh Thai Hoang'), arxiv.Result.Author('Long Tran-Thanh'), arxiv.Result.Author('Diep N. Nguyen'), arxiv.Result.Author('Won-Joo Hwang')]","In real-world applications, Federated Learning (FL) meets two challenges: (1)
scalability, especially when applied to massive IoT networks; and (2) how to be
robust against an environment with heterogeneous data. Realizing the first
problem, we aim to design a novel FL framework named Full-stack FL (F2L). More
specifically, F2L utilizes a hierarchical network architecture, making
extending the FL network accessible without reconstructing the whole network
system. Moreover, leveraging the advantages of hierarchical network design, we
propose a new label-driven knowledge distillation (LKD) technique at the global
server to address the second problem. As opposed to current knowledge
distillation techniques, LKD is capable of training a student model, which
consists of good knowledge from all teachers' models. Therefore, our proposed
algorithm can effectively extract the knowledge of the regions' data
distribution (i.e., the regional aggregated models) to reduce the divergence
between clients' models when operating under the FL system with non-independent
identically distributed data. Extensive experiment results reveal that: (i) our
F2L method can significantly improve the overall FL efficiency in all global
distillations, and (ii) F2L rapidly achieves convergence as global distillation
stages occur instead of increasing on each communication cycle."
11810,We reserve further research on this topic for the future.,(2019).,"4 EXPERIMENTAL EVALUATION

4.1 COMPARISON WITH FL METHODS

We run the baselines (see Section E) and compare with our F2L.",2022-09-29 02:49:13+00:00,Label driven Knowledge Distillation for Federated Learning with non-IID Data,cs.LG,"['cs.LG', 'cs.AI', '19A22', 'I.2.11']","[arxiv.Result.Author('Minh-Duong Nguyen'), arxiv.Result.Author('Quoc-Viet Pham'), arxiv.Result.Author('Dinh Thai Hoang'), arxiv.Result.Author('Long Tran-Thanh'), arxiv.Result.Author('Diep N. Nguyen'), arxiv.Result.Author('Won-Joo Hwang')]","In real-world applications, Federated Learning (FL) meets two challenges: (1)
scalability, especially when applied to massive IoT networks; and (2) how to be
robust against an environment with heterogeneous data. Realizing the first
problem, we aim to design a novel FL framework named Full-stack FL (F2L). More
specifically, F2L utilizes a hierarchical network architecture, making
extending the FL network accessible without reconstructing the whole network
system. Moreover, leveraging the advantages of hierarchical network design, we
propose a new label-driven knowledge distillation (LKD) technique at the global
server to address the second problem. As opposed to current knowledge
distillation techniques, LKD is capable of training a student model, which
consists of good knowledge from all teachers' models. Therefore, our proposed
algorithm can effectively extract the knowledge of the regions' data
distribution (i.e., the regional aggregated models) to reduce the divergence
between clients' models when operating under the FL system with non-independent
identically distributed data. Extensive experiment results reveal that: (i) our
F2L method can significantly improve the overall FL efficiency in all global
distillations, and (ii) F2L rapidly achieves convergence as global distillation
stages occur instead of increasing on each communication cycle."
11829,"Unfortunately, the lack of standardized model zoos and available benchmarks
                                                   signiﬁcantly increases the friction for further research about populations of NNs.","With such model
                                                   zoos, one could investigate novel approaches for (i) model analysis, (ii) discover
                                                   unknown learning dynamics, (iii) learn rich representations of such populations,
                                                   or (iv) exploit the model zoos for generative modelling of NN weights and biases.","With this work, we publish a novel dataset of model zoos containing systematically
                                                   generated and diverse populations of NN models for further research.",2022-09-29 13:20:42+00:00,Model Zoos: A Dataset of Diverse Populations of Neural Network Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Konstantin Schürholt'), arxiv.Result.Author('Diyar Taskiran'), arxiv.Result.Author('Boris Knyazev'), arxiv.Result.Author('Xavier Giró-i-Nieto'), arxiv.Result.Author('Damian Borth')]","In the last years, neural networks (NN) have evolved from laboratory
environments to the state-of-the-art for many real-world problems. It was shown
that NN models (i.e., their weights and biases) evolve on unique trajectories
in weight space during training. Following, a population of such neural network
models (referred to as model zoo) would form structures in weight space. We
think that the geometry, curvature and smoothness of these structures contain
information about the state of training and can reveal latent properties of
individual models. With such model zoos, one could investigate novel approaches
for (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn
rich representations of such populations, or (iv) exploit the model zoos for
generative modelling of NN weights and biases. Unfortunately, the lack of
standardized model zoos and available benchmarks significantly increases the
friction for further research about populations of NNs. With this work, we
publish a novel dataset of model zoos containing systematically generated and
diverse populations of NN models for further research. In total the proposed
model zoo dataset is based on eight image datasets, consists of 27 model zoos
trained with varying hyperparameter combinations and includes 50'360 unique NN
models as well as their sparsified twins, resulting in over 3'844'360 collected
model states. Additionally, to the model zoo data we provide an in-depth
analysis of the zoos and provide benchmarks for multiple downstream tasks. The
dataset can be found at www.modelzoos.cc."
11830,"With this work, we publish a novel dataset of model zoos containing systematically
                                                   generated and diverse populations of NN models for further research.","Unfortunately, the lack of standardized model zoos and available benchmarks
                                                   signiﬁcantly increases the friction for further research about populations of NNs.","In total
                                                   the proposed model zoo dataset is based on eight image datasets, consists of 27
                                                   model zoos trained with varying hyperparameter combinations and includes 50’360
                                                   unique NN models as well as their sparsiﬁed twins, resulting in over 3’844’360
                                                   collected model states.",2022-09-29 13:20:42+00:00,Model Zoos: A Dataset of Diverse Populations of Neural Network Models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Konstantin Schürholt'), arxiv.Result.Author('Diyar Taskiran'), arxiv.Result.Author('Boris Knyazev'), arxiv.Result.Author('Xavier Giró-i-Nieto'), arxiv.Result.Author('Damian Borth')]","In the last years, neural networks (NN) have evolved from laboratory
environments to the state-of-the-art for many real-world problems. It was shown
that NN models (i.e., their weights and biases) evolve on unique trajectories
in weight space during training. Following, a population of such neural network
models (referred to as model zoo) would form structures in weight space. We
think that the geometry, curvature and smoothness of these structures contain
information about the state of training and can reveal latent properties of
individual models. With such model zoos, one could investigate novel approaches
for (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn
rich representations of such populations, or (iv) exploit the model zoos for
generative modelling of NN weights and biases. Unfortunately, the lack of
standardized model zoos and available benchmarks significantly increases the
friction for further research about populations of NNs. With this work, we
publish a novel dataset of model zoos containing systematically generated and
diverse populations of NN models for further research. In total the proposed
model zoo dataset is based on eight image datasets, consists of 27 model zoos
trained with varying hyperparameter combinations and includes 50'360 unique NN
models as well as their sparsified twins, resulting in over 3'844'360 collected
model states. Additionally, to the model zoo data we provide an in-depth
analysis of the zoos and provide benchmarks for multiple downstream tasks. The
dataset can be found at www.modelzoos.cc."
11831,"We further study the possibility for mounting attacks under a
more strict scenario, where only OOD (out-of-distribution, OOD) data are available.",Consideration of Out-of-distribution.,"Speciﬁcally, we
utilize the model pre-trained on OOD samples to mount lightweight black-box attacks.",2022-09-29 14:43:03+00:00,Towards Lightweight Black-Box Attacks against Deep Neural Networks,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Chenghao Sun'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Wan Chaoqun'), arxiv.Result.Author('Qizhou Wang'), arxiv.Result.Author('Ya Li'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Xinmei Tian')]","Black-box attacks can generate adversarial examples without accessing the
parameters of target model, largely exacerbating the threats of deployed deep
neural networks (DNNs). However, previous works state that black-box attacks
fail to mislead target models when their training data and outputs are
inaccessible. In this work, we argue that black-box attacks can pose practical
attacks in this extremely restrictive scenario where only several test samples
are available. Specifically, we find that attacking the shallow layers of DNNs
trained on a few test samples can generate powerful adversarial examples. As
only a few samples are required, we refer to these attacks as lightweight
black-box attacks. The main challenge to promoting lightweight attacks is to
mitigate the adverse impact caused by the approximation error of shallow
layers. As it is hard to mitigate the approximation error with few available
samples, we propose Error TransFormer (ETF) for lightweight attacks. Namely,
ETF transforms the approximation error in the parameter space into a
perturbation in the feature space and alleviates the error by disturbing
features. In experiments, lightweight black-box attacks with the proposed ETF
achieve surprising results. For example, even if only 1 sample per category
available, the attack success rate in lightweight black-box attacks is only
about 3% lower than that of the black-box attacks with complete training data."
11832,"We further study the possibility for mounting attacks under a
more strict scenario, where only OOD (out-of-distribution, OOD) data are available.",Consideration of Out-of-distribution.,"Speciﬁcally, we
utilize the model pre-trained on OOD samples to mount lightweight black-box attacks.",2022-09-29 14:43:03+00:00,Towards Lightweight Black-Box Attacks against Deep Neural Networks,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Chenghao Sun'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Wan Chaoqun'), arxiv.Result.Author('Qizhou Wang'), arxiv.Result.Author('Ya Li'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Xinmei Tian')]","Black-box attacks can generate adversarial examples without accessing the
parameters of target model, largely exacerbating the threats of deployed deep
neural networks (DNNs). However, previous works state that black-box attacks
fail to mislead target models when their training data and outputs are
inaccessible. In this work, we argue that black-box attacks can pose practical
attacks in this extremely restrictive scenario where only several test samples
are available. Specifically, we find that attacking the shallow layers of DNNs
trained on a few test samples can generate powerful adversarial examples. As
only a few samples are required, we refer to these attacks as lightweight
black-box attacks. The main challenge to promoting lightweight attacks is to
mitigate the adverse impact caused by the approximation error of shallow
layers. As it is hard to mitigate the approximation error with few available
samples, we propose Error TransFormer (ETF) for lightweight attacks. Namely,
ETF transforms the approximation error in the parameter space into a
perturbation in the feature space and alleviates the error by disturbing
features. In experiments, lightweight black-box attacks with the proposed ETF
achieve surprising results. For example, even if only 1 sample per category
available, the attack success rate in lightweight black-box attacks is only
about 3% lower than that of the black-box attacks with complete training data."
11833,"We further study the possibility for mounting attacks under a
more strict scenario, where only OOD (out-of-distribution, OOD) data are available.","Intuitively, we can also
employ other self-supervised learning method, e.g., rotation prediction, to train shallow layers, see
details of experimental settings and results in Appendix G.

Consideration of Out-of-distribution.","Speciﬁcally, we
utilize the model pre-trained on OOD samples to mount lightweight black-box attacks.",2022-09-29 14:43:03+00:00,Towards Lightweight Black-Box Attacks against Deep Neural Networks,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Chenghao Sun'), arxiv.Result.Author('Yonggang Zhang'), arxiv.Result.Author('Wan Chaoqun'), arxiv.Result.Author('Qizhou Wang'), arxiv.Result.Author('Ya Li'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Bo Han'), arxiv.Result.Author('Xinmei Tian')]","Black-box attacks can generate adversarial examples without accessing the
parameters of target model, largely exacerbating the threats of deployed deep
neural networks (DNNs). However, previous works state that black-box attacks
fail to mislead target models when their training data and outputs are
inaccessible. In this work, we argue that black-box attacks can pose practical
attacks in this extremely restrictive scenario where only several test samples
are available. Specifically, we find that attacking the shallow layers of DNNs
trained on a few test samples can generate powerful adversarial examples. As
only a few samples are required, we refer to these attacks as lightweight
black-box attacks. The main challenge to promoting lightweight attacks is to
mitigate the adverse impact caused by the approximation error of shallow
layers. As it is hard to mitigate the approximation error with few available
samples, we propose Error TransFormer (ETF) for lightweight attacks. Namely,
ETF transforms the approximation error in the parameter space into a
perturbation in the feature space and alleviates the error by disturbing
features. In experiments, lightweight black-box attacks with the proposed ETF
achieve surprising results. For example, even if only 1 sample per category
available, the attack success rate in lightweight black-box attacks is only
about 3% lower than that of the black-box attacks with complete training data."
11838,"To further study this generalization gap among diﬀerent algorithms, Table 8 shows the generalization gap of
diﬀerent algorithms.","While META-STORM-SG and META-STORM
achieve low training loss, their generalization performance seems worse than their heuristic counterparts.",META-STORM with heuristics and Adam achieve the smallest gap among all the algorithms.,2022-09-29 15:12:54+00:00,META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for Unbounded Functions,cs.LG,"['cs.LG', 'cs.DS']","[arxiv.Result.Author('Zijian Liu'), arxiv.Result.Author('Ta Duy Nguyen'), arxiv.Result.Author('Thien Hang Nguyen'), arxiv.Result.Author('Alina Ene'), arxiv.Result.Author('Huy L. Nguyen')]","We study the application of variance reduction (VR) techniques to general
non-convex stochastic optimization problems. In this setting, the recent work
STORM [Cutkosky-Orabona '19] overcomes the drawback of having to compute
gradients of ""mega-batches"" that earlier VR methods rely on. There, STORM
utilizes recursive momentum to achieve the VR effect and is then later made
fully adaptive in STORM+ [Levy et al., '21], where full-adaptivity removes the
requirement for obtaining certain problem-specific parameters such as the
smoothness of the objective and bounds on the variance and norm of the
stochastic gradients in order to set the step size. However, STORM+ crucially
relies on the assumption that the function values are bounded, excluding a
large class of useful functions. In this work, we propose META-STORM, a
generalized framework of STORM+ that removes this bounded function values
assumption while still attaining the optimal convergence rate for non-convex
optimization. META-STORM not only maintains full-adaptivity, removing the need
to obtain problem specific parameters, but also improves the convergence rate's
dependency on the problem parameters. Furthermore, META-STORM can utilize a
large range of parameter settings that subsumes previous methods allowing for
more flexibility in a wider range of settings. Finally, we demonstrate the
effectiveness of META-STORM through experiments across common deep learning
tasks. Our algorithm improves upon the previous work STORM+ and is competitive
with widely used algorithms after the addition of per-coordinate update and
exponential moving average heuristics."
11840,"However, neither work further study these
                                        intriguing phenomena.","(2021) observe that wider projectors further improve the representation learned
                                        by the encoder f and the former alludes to the possibility that decorrelating a non-linear projection of the
                                        representation improves the quality of the learned representation.","1
    This study sheds a new light on the role of the projector by demonstrating that VC regularization of
the projector’s output precisely enforces pairwise independence for the components of the projector’s input
i.e.",2022-09-29 16:13:10+00:00,Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations,cs.LG,['cs.LG'],"[arxiv.Result.Author('Grégoire Mialon'), arxiv.Result.Author('Randall Balestriero'), arxiv.Result.Author('Yann LeCun')]","Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE
avoid collapse of their joint embedding architectures by constraining or
regularizing the covariance matrix of their projector's output. This study
highlights important properties of such strategy, which we coin
Variance-Covariance regularization (VCReg). More precisely, we show that VCReg
enforces pairwise independence between the features of the learned
representation. This result emerges by bridging VCReg applied on the
projector's output to kernel independence criteria applied on the projector's
input. This provides the first theoretical motivations and explanations of
VCReg. We empirically validate our findings where (i) we observe that SSL
methods employing VCReg learn visual representations with greater pairwise
independence than other methods, (i) we put in evidence which projector's
characteristics favor pairwise independence, and show it to emerge
independently from learning the projector, (ii) we use these findings to obtain
nontrivial performance gains for VICReg, (iii) we demonstrate that the scope of
VCReg goes beyond SSL by using it to solve Independent Component Analysis. We
hope that our findings will support the adoption of VCReg in SSL and beyond."
11842,"Currently, there is relatively little work  opportunities and challenges for further research in this area.","Additionally, we presented several
for anomaly detection.","A
on designing explainable GNN models for graph anomaly              majority of research efforts have been concentrated on detect-
detection (e.g., Deng et al.",2022-09-29 16:47:57+00:00,Graph Anomaly Detection with Graph Neural Networks: Current Status and Challenges,cs.LG,"['cs.LG', 'cs.AI', 'cs.SI']","[arxiv.Result.Author('Hwan Kim'), arxiv.Result.Author('Byung Suk Lee'), arxiv.Result.Author('Won-Yong Shin'), arxiv.Result.Author('Sungsu Lim')]","Graphs are used widely to model complex systems, and detecting anomalies in a
graph is an important task in the analysis of complex systems. Graph anomalies
are patterns in a graph that do not conform to normal patterns expected of the
attributes and/or structures of the graph. In recent years, graph neural
networks (GNNs) have been studied extensively and have successfully performed
difficult machine learning tasks in node classification, link prediction, and
graph classification thanks to the highly expressive capability via message
passing in effectively learning graph representations. To solve the graph
anomaly detection problem, GNN-based methods leverage information about the
graph attributes (or features) and/or structures to learn to score anomalies
appropriately. In this survey, we review the recent advances made in detecting
graph anomalies using GNN models. Specifically, we summarize GNN-based methods
according to the graph type (i.e., static and dynamic), the anomaly type (i.e.,
node, edge, subgraph, and whole graph), and the network architecture (e.g.,
graph autoencoder, graph convolutional network). To the best of our knowledge,
this survey is the first comprehensive review of graph anomaly detection
methods based on GNNs."
11843,"Additionally, we presented several
interpretable than traditional machine learning approaches, it     opportunities and challenges for further research in this area.",Since GNN-based methods are inherently less        built upon GNN models.,"A
is important to resolve the issue along with explainable models    majority of research efforts have been concentrated on detect-
for anomaly detection.",2022-09-29 16:47:57+00:00,Graph Anomaly Detection with Graph Neural Networks: Current Status and Challenges,cs.LG,"['cs.LG', 'cs.AI', 'cs.SI']","[arxiv.Result.Author('Hwan Kim'), arxiv.Result.Author('Byung Suk Lee'), arxiv.Result.Author('Won-Yong Shin'), arxiv.Result.Author('Sungsu Lim')]","Graphs are used widely to model complex systems, and detecting anomalies in a
graph is an important task in the analysis of complex systems. Graph anomalies
are patterns in a graph that do not conform to normal patterns expected of the
attributes and/or structures of the graph. In recent years, graph neural
networks (GNNs) have been studied extensively and have successfully performed
difficult machine learning tasks in node classification, link prediction, and
graph classification thanks to the highly expressive capability via message
passing in effectively learning graph representations. To solve the graph
anomaly detection problem, GNN-based methods leverage information about the
graph attributes (or features) and/or structures to learn to score anomalies
appropriately. In this survey, we review the recent advances made in detecting
graph anomalies using GNN models. Specifically, we summarize GNN-based methods
according to the graph type (i.e., static and dynamic), the anomaly type (i.e.,
node, edge, subgraph, and whole graph), and the network architecture (e.g.,
graph autoencoder, graph convolutional network). To the best of our knowledge,
this survey is the first comprehensive review of graph anomaly detection
methods based on GNNs."
11847,Our method has some aspects that would beneﬁt from further research.,"And,
by using diversity sampling with tuples of level properties as cluster keys, the
expressive range signiﬁcantly expands.","It would be beneﬁcial to ﬁnd methods that would improve the model’s gen-
eralization to out-of-training sizes.",2022-09-29 18:52:54+00:00,Start Small: Training Game Level Generators from Nothing by Learning at Multiple Sizes,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yahia Zakaria'), arxiv.Result.Author('Magda Fayek'), arxiv.Result.Author('Mayada Hadhoud')]","A procedural level generator is a tool that generates levels from noise. One
approach to build generators is using machine learning, but given the training
data rarity, multiple methods have been proposed to train generators from
nothing. However, level generation tasks tend to have sparse feedback, which is
commonly mitigated using game-specific supplemental rewards. This paper
proposes a novel approach to train generators from nothing by learning at
multiple level sizes starting from a small size up to the desired sizes. This
approach employs the observed phenomenon that feedback is denser at smaller
sizes to avoid supplemental rewards. It also presents the benefit of training
generators to output levels at various sizes. We apply this approach to train
controllable generators using generative flow networks. We also modify
diversity sampling to be compatible with generative flow networks and to expand
the expressive range. The results show that our methods can generate
high-quality diverse levels for Sokoban, Zelda and Danger Dave for a variety of
sizes, after only 3h 29min up to 6h 11min (depending on the game) of training
on a single commodity machine. Also, the results show that our generators can
output levels for sizes that were unavailable during training."
11853,"This paper’s in-depth study of machine
learning model generalization for stress detection provides an important foun-

Preprint submitted to Journal of Biomedical Informatics  October 3, 2022
dation for the further study of stress response measurement using sensor
biomarkers, recorded with wearable technologies.","In favor of re-
producible research and to assist the community advance the ﬁled, we make
all our experimental data and code publicly available through Github at
https://github.com/xalentis/Stress.",Methods.,2022-09-30 00:20:57+00:00,Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices,cs.LG,"['cs.LG', 'cs.AI', 'eess.SP', 'I.2.0']","[arxiv.Result.Author('Gideon Vos'), arxiv.Result.Author('Kelly Trinh'), arxiv.Result.Author('Zoltan Sarnyai'), arxiv.Result.Author('Mostafa Rahimi Azghadi')]","Introduction. We investigate the generalization ability of models built on
datasets containing a small number of subjects, recorded in single study
protocols. Next, we propose and evaluate methods combining these datasets into
a single, large dataset. Finally, we propose and evaluate the use of ensemble
techniques by combining gradient boosting with an artificial neural network to
measure predictive power on new, unseen data.
  Methods. Sensor biomarker data from six public datasets were utilized in this
study. To test model generalization, we developed a gradient boosting model
trained on one dataset (SWELL), and tested its predictive power on two datasets
previously used in other studies (WESAD, NEURO). Next, we merged four small
datasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of
99 subjects,. In addition, we utilized random sampling combined with another
dataset (EXAM) to build a larger training dataset consisting of 200 synthesized
subjects,. Finally, we developed an ensemble model that combines our gradient
boosting model with an artificial neural network, and tested it on two
additional, unseen publicly available stress datasets (WESAD and Toadstool).
  Results. Our method delivers a robust stress measurement system capable of
achieving 85% predictive accuracy on new, unseen validation data, achieving a
25% performance improvement over single models trained on small datasets.
  Conclusion. Models trained on small, single study protocol datasets do not
generalize well for use on new, unseen data and lack statistical power.
Ma-chine learning models trained on a dataset containing a larger number of
varied study subjects capture physiological variance better, resulting in more
robust stress detection."
11885,"This paper focuses on a practical algorithm that can plug and play on exist-
     ing DL-based survival models, and we will further study its theoretical properties
     in our future work.","In a wide variety of experiments, including

     two semi-synthetic datasets, one toy dataset, and three clinical and/or genetic

                    25
     datasets, we demonstrate the eﬀectiveness of our proposed explainable censored
375 learning or ﬁnding critical features and making survival predictions.","Funding

380  This work was partially supported by the NIH grants R21AG070909, R56NS117587,

     R01HD101508, P30 AG072496, and ARO W911NF-17-1-0040.",2022-09-30 12:56:29+00:00,Explainable Censored Learning: Finding Critical Features with Long Term Prognostic Values for Survival Prediction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xinxing Wu'), arxiv.Result.Author('Chong Peng'), arxiv.Result.Author('Richard Charnigo'), arxiv.Result.Author('Qiang Cheng')]","Interpreting critical variables involved in complex biological processes
related to survival time can help understand prediction from survival models,
evaluate treatment efficacy, and develop new therapies for patients. Currently,
the predictive results of deep learning (DL)-based models are better than or as
good as standard survival methods, they are often disregarded because of their
lack of transparency and little interpretability, which is crucial to their
adoption in clinical applications. In this paper, we introduce a novel, easily
deployable approach, called EXplainable CEnsored Learning (EXCEL), to
iteratively exploit critical variables and simultaneously implement (DL) model
training based on these variables. First, on a toy dataset, we illustrate the
principle of EXCEL; then, we mathematically analyze our proposed method, and we
derive and prove tight generalization error bounds; next, on two semi-synthetic
datasets, we show that EXCEL has good anti-noise ability and stability;
finally, we apply EXCEL to a variety of real-world survival datasets including
clinical data and genetic data, demonstrating that EXCEL can effectively
identify critical features and achieve performance on par with or better than
the original models. It is worth pointing out that EXCEL is flexibly deployed
in existing or emerging models for explainable survival data in the presence of
right censoring."
11889,"This motivates further research into effective, ODE-driven, approaches
to learning the map.","Namely, the SDE must evolve for inﬁnite time
to connect the distributions, the parameterization of the time steps remains heuristic (Feller, 1949;
Xiao et al., 2022), and the criticality of noise, as well as the score, is not absolutely apparent (Bansal
et al., 2022; Lu et al., 2022).","Indeed, under certain conditions, the primary SDE in score-based diffusion can
already be associated to its ODE probability ﬂow through a lower bound (Song et al., 2021a).",2022-09-30 16:30:31+00:00,Building Normalizing Flows with Stochastic Interpolants,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Michael S. Albergo'), arxiv.Result.Author('Eric Vanden-Eijnden')]","A simple generative model based on a continuous-time normalizing flow between
any pair of base and target distributions is proposed. The velocity field of
this flow is inferred from the probability current of a time-dependent
distribution that interpolates between the base and the target in finite time.
Unlike conventional normalizing flow inference methods based the maximum
likelihood principle, which require costly backpropagation through ODE solvers,
our interpolant approach leads to a simple quadratic loss for the velocity
itself which is expressed in terms of expectations that are readily amenable to
empirical estimation. The flow can be used to generate samples from either the
base or target, and can be used to estimate the likelihood at any time along
the interpolant. The approach is contextualized in its relation to diffusions.
In particular, in situations where the base is a Gaussian distribution, we show
that the velocity of our normalizing flow can also be used to construct a
diffusion model to sample the target as well as estimating its score. This
allows one to map methods based on stochastic differential equations to those
of ordinary differential equations, simplifying the mechanics of the model, but
capturing equivalent dynamics. Benchmarking on density estimation tasks
illustrates that the learned flow can match and surpass maximum likelihood
continuous flows at a fraction of the conventional ODE training costs."
11890,"This motivates further research into effective, ODE-driven, approaches to
learning the map.",(2022).,"Indeed, under certain conditions, the primary SDE in score-based diffusion can
already be associated to its ODE probability ﬂow through a lower bound (Song et al., 2021a).",2022-09-30 16:30:31+00:00,Building Normalizing Flows with Stochastic Interpolants,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Michael S. Albergo'), arxiv.Result.Author('Eric Vanden-Eijnden')]","A simple generative model based on a continuous-time normalizing flow between
any pair of base and target probability densities is proposed. The velocity
field of this flow is inferred from the probability current of a time-dependent
density that interpolates between the base and the target in finite time.
Unlike conventional normalizing flow inference methods based the maximum
likelihood principle, which require costly backpropagation through ODE solvers,
our interpolant approach leads to a simple quadratic loss for the velocity
itself which is expressed in terms of expectations that are readily amenable to
empirical estimation. The flow can be used to generate samples from either the
base or target, and to estimate the likelihood at any time along the
interpolant. In addition, the flow can be optimized to minimize the path length
of the interpolant density, thereby paving the way for building optimal
transport maps. The approach is also contextualized in its relation to
diffusions. In particular, in situations where the base is a Gaussian density,
we show that the velocity of our normalizing flow can also be used to construct
a diffusion model to sample the target as well as estimating its score. This
allows one to map methods based on stochastic differential equations to those
using ordinary differential equations, simplifying the mechanics of the model,
but capturing equivalent dynamics. Benchmarking on density estimation tasks
illustrates that the learned flow can match and surpass maximum likelihood
continuous flows at a fraction of the conventional ODE training costs."
11898,"The main advantage of using a ML-trained surrogate model over direct simulation is
that they require less time to generate a large volume of data for performing further research and
development (R&D) purpose (Davis, Cremaschi and Eden, 2018).","These statistical models utilize machine learning (ML) and AI
to derive approximation functions that can represent the actual physical system with good
accuracy.","The concept of surrogate
modeling is quite straightforward; data-driven fitting functions, known as approximation
                                                                                          Preprint & Uncorrected Proof

functions, are derived through ML-based training process.",2022-09-30 20:19:04+00:00,"Leveraging Industry 4.0 -- Deep Learning, Surrogate Model and Transfer Learning with Uncertainty Quantification Incorporated into Digital Twin for Nuclear System",cs.LG,"['cs.LG', 'stat.AP']","[arxiv.Result.Author('M. Rahman'), arxiv.Result.Author('Abid Khan'), arxiv.Result.Author('Sayeed Anowar'), arxiv.Result.Author('Md Al-Imran'), arxiv.Result.Author('Richa Verma'), arxiv.Result.Author('Dinesh Kumar'), arxiv.Result.Author('Kazuma Kobayashi'), arxiv.Result.Author('Syed Alam')]","Industry 4.0 targets the conversion of the traditional industries into
intelligent ones through technological revolution. This revolution is only
possible through innovation, optimization, interconnection, and rapid
decision-making capability. Numerical models are believed to be the key
components of Industry 4.0, facilitating quick decision-making through
simulations instead of costly experiments. However, numerical investigation of
precise, high-fidelity models for optimization or decision-making is usually
time-consuming and computationally expensive. In such instances, data-driven
surrogate models are excellent substitutes for fast computational analysis and
the probabilistic prediction of the output parameter for new input parameters.
The emergence of Internet of Things (IoT) and Machine Learning (ML) has made
the concept of surrogate modeling even more viable. However, these surrogate
models contain intrinsic uncertainties, originate from modeling defects, or
both. These uncertainties, if not quantified and minimized, can produce a
skewed result. Therefore, proper implementation of uncertainty quantification
techniques is crucial during optimization, cost reduction, or safety
enhancement processes analysis. This chapter begins with a brief overview of
the concept of surrogate modeling, transfer learning, IoT and digital twins.
After that, a detailed overview of uncertainties, uncertainty quantification
frameworks, and specifics of uncertainty quantification methodologies for a
surrogate model linked to a digital twin is presented. Finally, the use of
uncertainty quantification approaches in the nuclear industry has been
addressed."
11899,"To further study this potential, we only substituted the Momentum-SGD optimizer
with the ASAM optimizer (Kwon et al., 2021) and used the same hyperparameters used in previous
experiments.","The previously shown results suggest excellent potential for adversarial training with adaptive
weight decay.","To the best of our knowledge, and according to the RobustBench benchmark (Croce
et al., 2020), the state-of-the-art ∞ = 8.0 robust accuracy for CIFAR-100 without extra synthe-
sized or captured data using WRN28-10 is 29.80% (Rebufﬁ et al., 2021).",2022-09-30 21:13:00+00:00,Adaptive Weight Decay: On The Fly Weight Decay Tuning for Improving Robustness,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Amin Ghiasi'), arxiv.Result.Author('Ali Shafahi'), arxiv.Result.Author('Reza Ardekani')]","We introduce adaptive weight decay, which automatically tunes the
hyper-parameter for weight decay during each training iteration. For
classification problems, we propose changing the value of the weight decay
hyper-parameter on the fly based on the strength of updates from the
classification loss (i.e., gradient of cross-entropy), and the regularization
loss (i.e., $\ell_2$-norm of the weights). We show that this simple
modification can result in large improvements in adversarial robustness -- an
area which suffers from robust overfitting -- without requiring extra data.
Specifically, our reformulation results in 20% relative robustness improvement
for CIFAR-100, and 10% relative robustness improvement on CIFAR-10 comparing to
traditional weight decay. In addition, this method has other desirable
properties, such as less sensitivity to learning rate, and smaller weight
norms, which the latter contributes to robustness to overfitting to label
noise, and pruning."
11906,"Thus, there is scope for further research to evaluate the adversarial attacks pro-
posed in this thesis.","This is because the in-
ference patterns for benchmark datasets are not well deﬁned, and these datasets
may not be representative of the high-stakes domains.","A promising direction here is to design benchmark tasks and
datasets that measure the speciﬁc inductive abilities of models.",2022-09-30 22:41:22+00:00,Adversarial Robustness of Representation Learning for Knowledge Graphs,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']",[arxiv.Result.Author('Peru Bhardwaj')],"Knowledge graphs represent factual knowledge about the world as relationships
between concepts and are critical for intelligent decision making in enterprise
applications. New knowledge is inferred from the existing facts in the
knowledge graphs by encoding the concepts and relations into low-dimensional
feature vector representations. The most effective representations for this
task, called Knowledge Graph Embeddings (KGE), are learned through neural
network architectures. Due to their impressive predictive performance, they are
increasingly used in high-impact domains like healthcare, finance and
education. However, are the black-box KGE models adversarially robust for use
in domains with high stakes? This thesis argues that state-of-the-art KGE
models are vulnerable to data poisoning attacks, that is, their predictive
performance can be degraded by systematically crafted perturbations to the
training knowledge graph. To support this argument, two novel data poisoning
attacks are proposed that craft input deletions or additions at training time
to subvert the learned model's performance at inference time. These adversarial
attacks target the task of predicting the missing facts in knowledge graphs
using KGE models, and the evaluation shows that the simpler attacks are
competitive with or outperform the computationally expensive ones. The thesis
contributions not only highlight and provide an opportunity to fix the security
vulnerabilities of KGE models, but also help to understand the black-box
predictive behaviour of KGE models."
11907,"However, there is scope for further research
on encoding the attack unnoticeability in the design of adversarial attacks itself.","The notion of discreet or unno-
ticeable perturbations in this research is deﬁned by the attacker’s budget for the

112
                                                                             7.5 concluding statement

number of adversarial perturbations.","Lastly, the attacks in this thesis assume a white-box threat model.",2022-09-30 22:41:22+00:00,Adversarial Robustness of Representation Learning for Knowledge Graphs,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']",[arxiv.Result.Author('Peru Bhardwaj')],"Knowledge graphs represent factual knowledge about the world as relationships
between concepts and are critical for intelligent decision making in enterprise
applications. New knowledge is inferred from the existing facts in the
knowledge graphs by encoding the concepts and relations into low-dimensional
feature vector representations. The most effective representations for this
task, called Knowledge Graph Embeddings (KGE), are learned through neural
network architectures. Due to their impressive predictive performance, they are
increasingly used in high-impact domains like healthcare, finance and
education. However, are the black-box KGE models adversarially robust for use
in domains with high stakes? This thesis argues that state-of-the-art KGE
models are vulnerable to data poisoning attacks, that is, their predictive
performance can be degraded by systematically crafted perturbations to the
training knowledge graph. To support this argument, two novel data poisoning
attacks are proposed that craft input deletions or additions at training time
to subvert the learned model's performance at inference time. These adversarial
attacks target the task of predicting the missing facts in knowledge graphs
using KGE models, and the evaluation shows that the simpler attacks are
competitive with or outperform the computationally expensive ones. The thesis
contributions not only highlight and provide an opportunity to fix the security
vulnerabilities of KGE models, but also help to understand the black-box
predictive behaviour of KGE models."
11908,"The proposed
methods for adversarial attacks improve our understanding of the adversarial vul-
nerabilities and the predictive performance of representation learning algorithms
for knowledge graphs, providing a foundation for further research towards adver-
sarially robust and trustworthy graph Machine Learning.","7.5 concluding statement

To conclude, given the increasing effectiveness of graph ML systems, this the-
sis contributes towards their responsible integration in daily lives.","113
              A

ADDITIONAL DETAILS FOR INSTANCE ATTRIBUTION
AT TA C K S

a.1 training details

a.1.1 Training KGE models

In this research, four KGE models are implemented - DistMult, ComplEx, ConvE
and TransE.",2022-09-30 22:41:22+00:00,Adversarial Robustness of Representation Learning for Knowledge Graphs,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']",[arxiv.Result.Author('Peru Bhardwaj')],"Knowledge graphs represent factual knowledge about the world as relationships
between concepts and are critical for intelligent decision making in enterprise
applications. New knowledge is inferred from the existing facts in the
knowledge graphs by encoding the concepts and relations into low-dimensional
feature vector representations. The most effective representations for this
task, called Knowledge Graph Embeddings (KGE), are learned through neural
network architectures. Due to their impressive predictive performance, they are
increasingly used in high-impact domains like healthcare, finance and
education. However, are the black-box KGE models adversarially robust for use
in domains with high stakes? This thesis argues that state-of-the-art KGE
models are vulnerable to data poisoning attacks, that is, their predictive
performance can be degraded by systematically crafted perturbations to the
training knowledge graph. To support this argument, two novel data poisoning
attacks are proposed that craft input deletions or additions at training time
to subvert the learned model's performance at inference time. These adversarial
attacks target the task of predicting the missing facts in knowledge graphs
using KGE models, and the evaluation shows that the simpler attacks are
competitive with or outperform the computationally expensive ones. The thesis
contributions not only highlight and provide an opportunity to fix the security
vulnerabilities of KGE models, but also help to understand the black-box
predictive behaviour of KGE models."
11912,This assists in analysis and provides a foundation for further research.,"We show that it can be naturally modeled via a geometric and combinatoric object
                                                  known as a zonotope with its vertex set isomorphic to the set of feasible activation
                                                  patterns.","We
                                                  demonstrate its usefulness when we explore the sensitivity of the optimal loss to
                                                  perturbations of the training data.",2022-10-01 03:09:02+00:00,A Combinatorial Perspective on the Optimization of Shallow ReLU Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Michael Matena'), arxiv.Result.Author('Colin Raffel')]","The NP-hard problem of optimizing a shallow ReLU network can be characterized
as a combinatorial search over each training example's activation pattern
followed by a constrained convex problem given a fixed set of activation
patterns. We explore the implications of this combinatorial aspect of ReLU
optimization in this work. We show that it can be naturally modeled via a
geometric and combinatoric object known as a zonotope with its vertex set
isomorphic to the set of feasible activation patterns. This assists in analysis
and provides a foundation for further research. We demonstrate its usefulness
when we explore the sensitivity of the optimal loss to perturbations of the
training data. Later we discuss methods of zonotope vertex selection and its
relevance to optimization. Overparameterization assists in training by making a
randomly chosen vertex more likely to contain a good solution. We then
introduce a novel polynomial-time vertex selection procedure that provably
picks a vertex containing the global optimum using only double the minimum
number of parameters required to fit the data. We further introduce a local
greedy search heuristic over zonotope vertices and demonstrate that it
outperforms gradient descent on underparameterized problems."
11913,"We hope that further research along these avenues
will deepen our understanding of neural network training and enable improvements to training in
practice.","However, the concepts of
activation patterns are still meaningful for deep ReLU networks but require real algebraic geometry
for analysis (Basu, 2014; Bochnak et al., 2013).","9
References

Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., and Kolter, Z. Differentiable convex
   optimization layers.",2022-10-01 03:09:02+00:00,A Combinatorial Perspective on the Optimization of Shallow ReLU Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Michael Matena'), arxiv.Result.Author('Colin Raffel')]","The NP-hard problem of optimizing a shallow ReLU network can be characterized
as a combinatorial search over each training example's activation pattern
followed by a constrained convex problem given a fixed set of activation
patterns. We explore the implications of this combinatorial aspect of ReLU
optimization in this work. We show that it can be naturally modeled via a
geometric and combinatoric object known as a zonotope with its vertex set
isomorphic to the set of feasible activation patterns. This assists in analysis
and provides a foundation for further research. We demonstrate its usefulness
when we explore the sensitivity of the optimal loss to perturbations of the
training data. Later we discuss methods of zonotope vertex selection and its
relevance to optimization. Overparameterization assists in training by making a
randomly chosen vertex more likely to contain a good solution. We then
introduce a novel polynomial-time vertex selection procedure that provably
picks a vertex containing the global optimum using only double the minimum
number of parameters required to fit the data. We further introduce a local
greedy search heuristic over zonotope vertices and demonstrate that it
outperforms gradient descent on underparameterized problems."
11914,"It motivates us to further study more
                                       of a state’s novelty and the associated beneﬁt of exploring the              advanced strategies for boosting exploration in off-policy AC
                                       state (with regards to policy optimization), altogether called               algorithms.","Under this strategy, we propose a new method to boost                   they do no always perform well in high dimensional and sparse
                                       exploration through an intrinsic reward, based on measurement                reward environments.",plausible novelty.,2022-10-01 07:07:11+00:00,Boosting Exploration in Actor-Critic Algorithms by Incentivizing Plausible Novel States,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Chayan Banerjee'), arxiv.Result.Author('Zhiyong Chen'), arxiv.Result.Author('Nasimul Noman')]","Actor-critic (AC) algorithms are a class of model-free deep reinforcement
learning algorithms, which have proven their efficacy in diverse domains,
especially in solving continuous control problems. Improvement of exploration
(action entropy) and exploitation (expected return) using more efficient
samples is a critical issue in AC algorithms. A basic strategy of a learning
algorithm is to facilitate indiscriminately exploring all of the environment
state space, as well as to encourage exploring rarely visited states rather
than frequently visited one. Under this strategy, we propose a new method to
boost exploration through an intrinsic reward, based on measurement of a
state's novelty and the associated benefit of exploring the state (with regards
to policy optimization), altogether called plausible novelty. With incentivized
exploration of plausible novel states, an AC algorithm is able to improve its
sample efficiency and hence training performance. The new method is verified by
extensive simulations of continuous control tasks of MuJoCo environments on a
variety of prominent off-policy AC algorithms."
11931,"In addition, the               of the 24th ACM international on conference on information and
optimal transportation measure for the similarity between              knowledge management, pages 891–900, 2015.
the evolving data with respect to varying times would be
another attractive topic for further study.","In Proceedings
ment in the calculation in future work.",[2] Mikhail Belkin and Partha Niyogi.,2022-10-02 03:18:30+00:00,Metric Distribution to Vector: Constructing Data Representation via Broad-Scale Discrepancies,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Xue Liu'), arxiv.Result.Author('Dan Sun'), arxiv.Result.Author('Xiaobo Cao'), arxiv.Result.Author('Hao Ye'), arxiv.Result.Author('Wei Wei')]","Graph embedding provides a feasible methodology to conduct pattern
classification for graph-structured data by mapping each data into the
vectorial space. Various pioneering works are essentially coding method that
concentrates on a vectorial representation about the inner properties of a
graph in terms of the topological constitution, node attributions, link
relations, etc. However, the classification for each targeted data is a
qualitative issue based on understanding the overall discrepancies within the
dataset scale. From the statistical point of view, these discrepancies manifest
a metric distribution over the dataset scale if the distance metric is adopted
to measure the pairwise similarity or dissimilarity. Therefore, we present a
novel embedding strategy named $\mathbf{MetricDistribution2vec}$ to extract
such distribution characteristics into the vectorial representation for each
data. We demonstrate the application and effectiveness of our representation
method in the supervised prediction tasks on extensive real-world structural
graph datasets. The results have gained some unexpected increases compared with
a surge of baselines on all the datasets, even if we take the lightweight
models as classifiers. Moreover, the proposed methods also conducted
experiments in Few-Shot classification scenarios, and the results still show
attractive discrimination in rare training samples based inference."
11961,aggregates at the bus stop level for further study.,"The raw location data are pre-processed to remove             that have preceding trips with less than 30 minutes difference
missing and erroneous entries and are transformed into                 from the current trip start time are considered for the study.","The travel         Based on the spatial patterns, the sections of the route are
time between bus stops depends on factors such as Land Use             divided as follows:
patterns (LUP), signalized intersections, un-signalized
intersections, section length, etc.",2022-10-03 06:35:03+00:00,A Dynamic Model for Bus Arrival Time Estimation based on Spatial Patterns using Machine Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('B. P. Ashwini'), arxiv.Result.Author('R. Sumathi'), arxiv.Result.Author('H. S. Sudhira')]","The notion of smart cities is being adapted globally to provide a better
quality of living. A smart city's smart mobility component focuses on providing
smooth and safe commuting for its residents and promotes eco-friendly and
sustainable alternatives such as public transit (bus). Among several smart
applications, a system that provides up-to-the-minute information like bus
arrival, travel duration, schedule, etc., improves the reliability of public
transit services. Still, this application needs live information on traffic
flow, accidents, events, and the location of the buses. Most cities lack the
infrastructure to provide these data. In this context, a bus arrival prediction
model is proposed for forecasting the arrival time using limited data sets. The
location data of public transit buses and spatial characteristics are used for
the study. One of the routes of Tumakuru city service, Tumakuru, India, is
selected and divided into two spatial patterns: sections with intersections and
sections without intersections. The machine learning model XGBoost is modeled
for both spatial patterns individually. A model to dynamically predict bus
arrival time is developed using the preceding trip information and the machine
learning model to estimate the arrival time at a downstream bus stop. The
performance of models is compared based on the R-squared values of the
predictions made, and the proposed model established superior results. It is
suggested to predict bus arrival in the study area. The proposed model can also
be extended to other similar cities with limited traffic-related
infrastructure."
11995,"Accordingly, we intend to perform further research about this
model.","First, direct application of this model to the detailed
design used in the actual product has a limitation because it is trained with the concept design data of
road wheel, as mentioned in Section 4.3.","Considering that sufficient amount of detailed design data is difficult to obtain, we would like
to conduct a future study to expand it to detailed roadwheel design by applying the domain adaptation
of Ganin and Lempitsky (2015) based on the proposed model trained from the concept wheel design.",2022-10-03 12:57:45+00:00,Wheel Impact Test by Deep Learning: Prediction of Location and Magnitude of Maximum Stress,cs.LG,['cs.LG'],"[arxiv.Result.Author('Seungyeon Shin'), arxiv.Result.Author('Ah-hyeon Jin'), arxiv.Result.Author('Soyoung Yoo'), arxiv.Result.Author('Sunghee Lee'), arxiv.Result.Author('ChangGon Kim'), arxiv.Result.Author('Sungpil Heo'), arxiv.Result.Author('Namwoo Kang')]","The impact performance of the wheel during wheel development must be ensured
through a wheel impact test for vehicle safety. However, manufacturing and
testing a real wheel take a significant amount of time and money because
developing an optimal wheel design requires numerous iterative processes of
modifying the wheel design and verifying the safety performance. Accordingly,
the actual wheel impact test has been replaced by computer simulations, such as
Finite Element Analysis (FEA), but it still requires high computational costs
for modeling and analysis. Moreover, FEA experts are needed. This study
presents an aluminum road wheel impact performance prediction model based on
deep learning that replaces the computationally expensive and time-consuming 3D
FEA. For this purpose, 2D disk-view wheel image data, 3D wheel voxel data, and
barrier mass value used for wheel impact test are utilized as the inputs to
predict the magnitude of maximum von Mises stress, corresponding location, and
the stress distribution of 2D disk-view. The wheel impact performance
prediction model can replace the impact test in the early wheel development
stage by predicting the impact performance in real time and can be used without
domain knowledge. The time required for the wheel development process can be
shortened through this mechanism."
11996,"Accordingly, we
would like to conduct further research using this model.","First, the direct application of this model to the
detailed design used in the actual product was limited as mentioned in Section 4.3.","Considering that obtaining a sufficient amount
of detailed design data was difficult, we would like to conduct a future study to expand it to a detailed
road wheel design by applying the domain adaptation of Ganin and Lempitsky (2015) based on the
proposed model trained from the concept wheel design.",2022-10-03 12:57:45+00:00,Wheel Impact Test by Deep Learning: Prediction of Location and Magnitude of Maximum Stress,cs.LG,['cs.LG'],"[arxiv.Result.Author('Seungyeon Shin'), arxiv.Result.Author('Ah-hyeon Jin'), arxiv.Result.Author('Soyoung Yoo'), arxiv.Result.Author('Sunghee Lee'), arxiv.Result.Author('ChangGon Kim'), arxiv.Result.Author('Sungpil Heo'), arxiv.Result.Author('Namwoo Kang')]","For ensuring vehicle safety, the impact performance of wheels during wheel
development must be ensured through a wheel impact test. However, manufacturing
and testing a real wheel requires a significant time and money because
developing an optimal wheel design requires numerous iterative processes to
modify the wheel design and verify the safety performance. Accordingly, wheel
impact tests have been replaced by computer simulations such as finite element
analysis (FEA); however, it still incurs high computational costs for modeling
and analysis, and requires FEA experts. In this study, we present an aluminum
road wheel impact performance prediction model based on deep learning that
replaces computationally expensive and time-consuming 3D FEA. For this purpose,
2D disk-view wheel image data, 3D wheel voxel data, and barrier mass values
used for the wheel impact test were utilized as the inputs to predict the
magnitude of the maximum von Mises stress, corresponding location, and the
stress distribution of the 2D disk-view. The input data were first compressed
into a latent space with a 3D convolutional variational autoencoder (cVAE) and
2D convolutional autoencoder (cAE). Subsequently, the fully connected layers
were used to predict the impact performance, and a decoder was used to predict
the stress distribution heatmap of the 2D disk-view. The proposed model can
replace the impact test in the early wheel-development stage by predicting the
impact performance in real-time and can be used without domain knowledge. The
time required for the wheel development process can be reduced by using this
mechanism."
12026,"Therefore, we advocate for further research on advancing defenses necessary to mitigate
such privacy leaks.","As tabular
data is ubiquitous in privacy critical high-stakes applications, our results raise important concerns regarding practical
systems currently using FL.","6 Ethics Statement

As tabular data is often used in high-stakes applications and may contain sensitive data of natural or legal persons,
conﬁdential treatment is critical.",2022-10-04 17:55:20+00:00,Data Leakage in Tabular Federated Learning,cs.LG,"['cs.LG', 'cs.CR', 'cs.DC', 'I.2.11']","[arxiv.Result.Author('Mark Vero'), arxiv.Result.Author('Mislav Balunović'), arxiv.Result.Author('Dimitar I. Dimitrov'), arxiv.Result.Author('Martin Vechev')]","While federated learning (FL) promises to preserve privacy in distributed
training of deep learning models, recent work in the image and NLP domains
showed that training updates leak private data of participating clients. At the
same time, most high-stakes applications of FL (e.g., legal and financial) use
tabular data. Compared to the NLP and image domains, reconstruction of tabular
data poses several unique challenges: (i) categorical features introduce a
significantly more difficult mixed discrete-continuous optimization problem,
(ii) the mix of categorical and continuous features causes high variance in the
final reconstructions, and (iii) structured data makes it difficult for the
adversary to judge reconstruction quality. In this work, we tackle these
challenges and propose the first comprehensive reconstruction attack on tabular
data, called TabLeak. TabLeak is based on three key ingredients: (i) a softmax
structural prior, implicitly converting the mixed discrete-continuous
optimization problem into an easier fully continuous one, (ii) a way to reduce
the variance of our reconstructions through a pooled ensembling scheme
exploiting the structure of tabular data, and (iii) an entropy measure which
can successfully assess reconstruction quality. Our experimental evaluation
demonstrates the effectiveness of TabLeak, reaching a state-of-the-art on four
popular tabular datasets. For instance, on the Adult dataset, we improve attack
accuracy by 10% compared to the baseline on the practically relevant batch size
of 32 and further obtain non-trivial reconstructions for batch sizes as large
as 128. Our findings are important as they show that performing FL on tabular
data, which often poses high privacy risks, is highly vulnerable."
12030,"(2020) to
simulate real oﬃce spaces to support further research      detect anomalous behaviour from physical access data
on environmental monitoring in the built environment.",(2019) presented a living lab to     machine learning was proposed by Poh et al.,of employees about their job proﬁles.,2022-10-04 18:16:36+00:00,Detecting Anomalies within Smart Buildings using Do-It-Yourself Internet of Things,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yasar Majib'), arxiv.Result.Author('Mahmoud Barhamgi'), arxiv.Result.Author('Behzad Momahed Heravi'), arxiv.Result.Author('Sharadha Kariyawasam'), arxiv.Result.Author('Charith Perera')]","Detecting anomalies at the time of happening is vital in environments like
buildings and homes to identify potential cyber-attacks. This paper discussed
the various mechanisms to detect anomalies as soon as they occur. We shed light
on crucial considerations when building machine learning models. We constructed
and gathered data from multiple self-build (DIY) IoT devices with different
in-situ sensors and found effective ways to find the point, contextual and
combine anomalies. We also discussed several challenges and potential solutions
when dealing with sensing devices that produce data at different sampling rates
and how we need to pre-process them in machine learning models. This paper also
looks at the pros and cons of extracting sub-datasets based on environmental
conditions."
12034,"Finally, further research can push

                                                           9
the limits of multimodal meta-learning, e.g., by implementing non-parametric Bayesian methods
to automatically infer an optimal number of clusters, thereby eliminating a hyperparameter of the
current approach.","One could also generalize our approach to non-Gaussian
likelihoods, making UNLIMITD effective for classiﬁcation tasks.","ACKNOWLEDGEMENTS

The authors acknowledge the MIT SuperCloud (Reuther et al., 2018) and Lincoln Laboratory Super-
computing Center for providing HPC resources that have contributed to the research results reported
within this paper.",2022-10-04 20:02:25+00:00,Uncertainty-Aware Meta-Learning for Multimodal Task Distributions,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Cesar Almecija'), arxiv.Result.Author('Apoorva Sharma'), arxiv.Result.Author('Navid Azizan')]","Meta-learning or learning to learn is a popular approach for learning new
tasks with limited data (i.e., few-shot learning) by leveraging the
commonalities among different tasks. However, meta-learned models can perform
poorly when context data is limited, or when data is drawn from an
out-of-distribution (OoD) task. Especially in safety-critical settings, this
necessitates an uncertainty-aware approach to meta-learning. In addition, the
often multimodal nature of task distributions can pose unique challenges to
meta-learning methods. In this work, we present UnLiMiTD (uncertainty-aware
meta-learning for multimodal task distributions), a novel method for
meta-learning that (1) makes probabilistic predictions on in-distribution tasks
efficiently, (2) is capable of detecting OoD context data at test time, and (3)
performs on heterogeneous, multimodal task distributions. To achieve this goal,
we take a probabilistic perspective and train a parametric, tuneable
distribution over tasks on the meta-dataset. We construct this distribution by
performing Bayesian inference on a linearized neural network, leveraging
Gaussian process theory. We demonstrate that UnLiMiTD's predictions compare
favorably to, and outperform in most cases, the standard baselines, especially
in the low-data regime. Furthermore, we show that UnLiMiTD is effective in
detecting data from OoD tasks. Finally, we confirm that both of these findings
continue to hold in the multimodal task-distribution setting."
12058,"To simplify the derivation, we further study the case x = (1, 0, 0, 0, ...)T .","Thus, the minimum is attained for perfectly negative associations between x and
x˜ and thus x˜ = −x.","It follows that either φ(w1,j1) or φ(−w1,j1) is positive while the other one is zero.",2022-10-05 17:33:23+00:00,Dynamical Isometry for Residual Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Advait Gadhikar'), arxiv.Result.Author('Rebekka Burkholz')]","The training success, training speed and generalization ability of neural
networks rely crucially on the choice of random parameter initialization. It
has been shown for multiple architectures that initial dynamical isometry is
particularly advantageous. Known initialization schemes for residual blocks,
however, miss this property and suffer from degrading separability of different
inputs for increasing depth and instability without Batch Normalization or lack
feature diversity. We propose a random initialization scheme, RISOTTO, that
achieves perfect dynamical isometry for residual networks with ReLU activation
functions even for finite depth and width. It balances the contributions of the
residual and skip branches unlike other schemes, which initially bias towards
the skip connections. In experiments, we demonstrate that in most cases our
approach outperforms initialization schemes proposed to make Batch
Normalization obsolete, including Fixup and SkipInit, and facilitates stable
training. Also in combination with Batch Normalization, we find that RISOTTO
often achieves the overall best result."
12062,"4.4 Parameter sensitivity

We further study the parameter sensitivity of the proposed framework, including the number of victim
nodes η, the perturbation budget ε, and the batch size γ.","Moreover, we observe that the STPGD module plays a more important
role in the adversarial spatiotemporal attack.","Due to page limit, we report the result of
G-RMSE on the PeMS-BAY dataset.",2022-10-05 02:25:10+00:00,Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Fan Liu'), arxiv.Result.Author('Hao Liu'), arxiv.Result.Author('Wenzhao Jiang')]","Machine learning based traffic forecasting models leverage sophisticated
spatiotemporal auto-correlations to provide accurate predictions of city-wide
traffic states. However, existing methods assume a reliable and unbiased
forecasting environment, which is not always available in the wild. In this
work, we investigate the vulnerability of spatiotemporal traffic forecasting
models and propose a practical adversarial spatiotemporal attack framework.
Specifically, instead of simultaneously attacking all geo-distributed data
sources, an iterative gradient-guided node saliency method is proposed to
identify the time-dependent set of victim nodes. Furthermore, we devise a
spatiotemporal gradient descent based scheme to generate real-valued
adversarial traffic states under a perturbation constraint. Meanwhile, we
theoretically demonstrate the worst performance bound of adversarial traffic
forecasting attacks. Extensive experiments on two real-world datasets show that
the proposed two-step framework achieves up to $67.8\%$ performance degradation
on various advanced spatiotemporal forecasting models. Remarkably, we also show
that adversarial training with our proposed attacks can significantly improve
the robustness of spatiotemporal traffic forecasting models. Our code is
available in \url{https://github.com/luckyfan-cs/ASTFA}."
12071,The above improvements can be regarded as directions for further research.,"Sixth, the distance
       between the instance x and all instances, which fall in the same leaf as x, can be deﬁned
       diﬀerently.",3.,2022-10-05 20:58:57+00:00,Improved Anomaly Detection by Using the Attention-Based Isolation Forest,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrey Y. Ageev'), arxiv.Result.Author('Andrei V. Konstantinov')]","A new modification of Isolation Forest called Attention-Based Isolation
Forest (ABIForest) for solving the anomaly detection problem is proposed. It
incorporates the attention mechanism in the form of the Nadaraya-Watson
regression into the Isolation Forest for improving solution of the anomaly
detection problem. The main idea underlying the modification is to assign
attention weights to each path of trees with learnable parameters depending on
instances and trees themselves. The Huber's contamination model is proposed to
be used for defining the attention weights and their parameters. As a result,
the attention weights are linearly depend on the learnable attention parameters
which are trained by solving the standard linear or quadratic optimization
problem. ABIForest can be viewed as the first modification of Isolation Forest,
which incorporates the attention mechanism in a simple way without applying
gradient-based algorithms. Numerical experiments with synthetic and real
datasets illustrate outperforming results of ABIForest. The code of proposed
algorithms is available."
12072,"The following modiﬁcations resolving the above disadvantages are interesting directions for
further research.","In spite of the disadvantages, ABIForest can be viewed as the ﬁrst version for incor-
porating the attention mechanism into iForest which has illustrated outperforming results.","22
References

 [1] R Chalapathy and S Chawla.",2022-10-05 20:58:57+00:00,Improved Anomaly Detection by Using the Attention-Based Isolation Forest,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrey Y. Ageev'), arxiv.Result.Author('Andrei V. Konstantinov')]","A new modification of Isolation Forest called Attention-Based Isolation
Forest (ABIForest) for solving the anomaly detection problem is proposed. It
incorporates the attention mechanism in the form of the Nadaraya-Watson
regression into the Isolation Forest for improving solution of the anomaly
detection problem. The main idea underlying the modification is to assign
attention weights to each path of trees with learnable parameters depending on
instances and trees themselves. The Huber's contamination model is proposed to
be used for defining the attention weights and their parameters. As a result,
the attention weights are linearly depend on the learnable attention parameters
which are trained by solving the standard linear or quadratic optimization
problem. ABIForest can be viewed as the first modification of Isolation Forest,
which incorporates the attention mechanism in a simple way without applying
gradient-based algorithms. Numerical experiments with synthetic and real
datasets illustrate outperforming results of ABIForest. The code of proposed
algorithms is available."
12078,"Therefore, this paper presents a scikit-learn
compatible hyperbox-based machine learning library in Python to ﬁll this gap and serve as
a facilitator for further research and applications in this ﬁeld.","Although many hyperbox-based machine learning algorithms have been developed over
the years with many very recent examples (Khuat et al., 2021b), there is no comprehen-
sive software library gathering them in one convenient package allowing their easy usage,
benchmarking and further development.",2.,2022-10-06 06:40:07+00:00,hyperbox-brain: A Toolbox for Hyperbox-based Machine Learning Algorithms,cs.LG,"['cs.LG', '68T05, 68T30, 68T37, 68W27', 'I.2.1; I.2.4; I.2.5; I.2.6; I.5.1; I.5.3; I.5.4; I.5.5']","[arxiv.Result.Author('Thanh Tung Khuat'), arxiv.Result.Author('Bogdan Gabrys')]","Hyperbox-based machine learning algorithms are an important and popular
branch of machine learning in the construction of classifiers using fuzzy sets
and logic theory and neural network architectures. This type of learning is
characterised by many strong points of modern predictors such as a high
scalability, explainability, online adaptation, effective learning from a small
amount of data, native ability to deal with missing data and accommodating new
classes. Nevertheless, there is no comprehensive existing package for
hyperbox-based machine learning which can serve as a benchmark for research and
allow non-expert users to apply these algorithms easily. hyperbox-brain is an
open-source Python library implementing the leading hyperbox-based machine
learning algorithms. This library exposes a unified API which closely follows
and is compatible with the renowned scikit-learn and numpy toolboxes. The
library may be installed from Python Package Index (PyPI) and the conda package
manager and is distributed under the GPL-3 license. The source code,
documentation, detailed tutorials, and the full descriptions of the API are
available at https://uts-caslab.github.io/hyperbox-brain."
12079,"We further study the
proper nature of the obtained algorithms and give the ﬁrst proper learning methods that are robust
against instance-targeted poisoning attacks for natural hypothesis classes such as linear classiﬁers.","In this work, we make progress on all the directions above and achieve optimal error rates (up to
constant factors) for general η, both for the realizable and agnostic settings.","More precisely, we give a characterization of the optimal error rate of learning under instance-
targeted poisoning attacks with budget η · n as follows.",2022-10-06 06:49:48+00:00,On Optimal Learning Under Targeted Data Poisoning,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Steve Hanneke'), arxiv.Result.Author('Amin Karbasi'), arxiv.Result.Author('Mohammad Mahmoody'), arxiv.Result.Author('Idan Mehalel'), arxiv.Result.Author('Shay Moran')]","Consider the task of learning a hypothesis class $\mathcal{H}$ in the
presence of an adversary that can replace up to an $\eta$ fraction of the
examples in the training set with arbitrary adversarial examples. The adversary
aims to fail the learner on a particular target test point $x$ which is known
to the adversary but not to the learner. In this work we aim to characterize
the smallest achievable error $\epsilon=\epsilon(\eta)$ by the learner in the
presence of such an adversary in both realizable and agnostic settings. We
fully achieve this in the realizable setting, proving that
$\epsilon=\Theta(\mathtt{VC}(\mathcal{H})\cdot \eta)$, where
$\mathtt{VC}(\mathcal{H})$ is the VC dimension of $\mathcal{H}$. Remarkably, we
show that the upper bound can be attained by a deterministic learner. In the
agnostic setting we reveal a more elaborate landscape: we devise a
deterministic learner with a multiplicative regret guarantee of $\epsilon \leq
C\cdot\mathtt{OPT} + O(\mathtt{VC}(\mathcal{H})\cdot \eta)$, where $C > 1$ is a
universal numerical constant. We complement this by showing that for any
deterministic learner there is an attack which worsens its error to at least
$2\cdot \mathtt{OPT}$. This implies that a multiplicative deterioration in the
regret is unavoidable in this case. Finally, the algorithms we develop for
achieving the optimal rates are inherently improper. Nevertheless, we show that
for a variety of natural concept classes, such as linear classifiers, it is
possible to retain the dependence $\epsilon=\Theta_{\mathcal{H}}(\eta)$ by a
proper algorithm in the realizable setting. Here $\Theta_{\mathcal{H}}$
conceals a polynomial dependence on $\mathtt{VC}(\mathcal{H})$."
12080,"We further study the proper
nature of the obtained algorithms and give the ﬁrst proper learning methods that are robust against
instance-targeted poisoning attacks for natural hypothesis classes such as linear classiﬁers.","In this work, we make progress on all the directions above and achieve optimal error rates (up to con-
stant factors) for general η, both for the realizable and agnostic settings.","More
precisely, we give a characterization of the optimal error rate of learning under instance-targeted
poisoning attacks with budget η · n as follows.",2022-10-06 06:49:48+00:00,On Optimal Learning Under Targeted Data Poisoning,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Steve Hanneke'), arxiv.Result.Author('Amin Karbasi'), arxiv.Result.Author('Mohammad Mahmoody'), arxiv.Result.Author('Idan Mehalel'), arxiv.Result.Author('Shay Moran')]","Consider the task of learning a hypothesis class $\mathcal{H}$ in the
presence of an adversary that can replace up to an $\eta$ fraction of the
examples in the training set with arbitrary adversarial examples. The adversary
aims to fail the learner on a particular target test point $x$ which is known
to the adversary but not to the learner. In this work we aim to characterize
the smallest achievable error $\epsilon=\epsilon(\eta)$ by the learner in the
presence of such an adversary in both realizable and agnostic settings. We
fully achieve this in the realizable setting, proving that
$\epsilon=\Theta(\mathtt{VC}(\mathcal{H})\cdot \eta)$, where
$\mathtt{VC}(\mathcal{H})$ is the VC dimension of $\mathcal{H}$. Remarkably, we
show that the upper bound can be attained by a deterministic learner. In the
agnostic setting we reveal a more elaborate landscape: we devise a
deterministic learner with a multiplicative regret guarantee of $\epsilon \leq
C\cdot\mathtt{OPT} + O(\mathtt{VC}(\mathcal{H})\cdot \eta)$, where $C > 1$ is a
universal numerical constant. We complement this by showing that for any
deterministic learner there is an attack which worsens its error to at least
$2\cdot \mathtt{OPT}$. This implies that a multiplicative deterioration in the
regret is unavoidable in this case. Finally, the algorithms we develop for
achieving the optimal rates are inherently improper. Nevertheless, we show that
for a variety of natural concept classes, such as linear classifiers, it is
possible to retain the dependence $\epsilon=\Theta_{\mathcal{H}}(\eta)$ by a
proper algorithm in the realizable setting. Here $\Theta_{\mathcal{H}}$
conceals a polynomial dependence on $\mathtt{VC}(\mathcal{H})$."
12095,"It would be fruitful to pursue further research about the KDE-based estimation of
          the Frobenius–Perron operator for high-dimensional maps to analyze this method.","As a result of conducting this research, we propose the possibility of introducing any density estimation
          methods to this ﬁeld.","October 10, 2022 0:1 ws-ijbc

         16 REFERENCES

      Acknowledgments

       Erik Bollt gratefully acknowledges funding from the Army Research Oﬃce (ARO), the Defense Advanced
       Research Projects Agency (DARPA), the National Science Foundation (NSF) and National Institutes of
       Health (NIH) CRNS program, and the Oﬃce of Naval Research (ONR) during the period of this work.",2022-08-01 14:28:10+00:00,Learning Transfer Operators by Kernel Density Estimation,cs.LG,"['cs.LG', 'cs.IT', 'math.DS', 'math.IT', 'math.ST', 'stat.CO', 'stat.TH']","[arxiv.Result.Author('Sudam Surasinghe'), arxiv.Result.Author('Jeremie Fish'), arxiv.Result.Author('Erik M. Bollt')]","Inference of transfer operators from data is often formulated as a classical
problem that hinges on the Ulam method. The usual description, which we will
call the Ulam-Galerkin method, is in terms of projection onto basis functions
that are characteristic functions supported over a fine grid of rectangles. In
these terms, the usual Ulam-Galerkin approach can be understood as density
estimation by the histogram method. Here we show that the problem can be recast
in statistical density estimation formalism. This recasting of the classical
problem, is a perspective that allows for an explicit and rigorous analysis of
bias and variance, and therefore toward a discussion of the mean square error.
  Keywords: Transfer Operators; Frobenius-Perron operator; probability density
estimation; Ulam-Galerkin method;Kernel Density Estimation."
12098,"In the next
chapter, we will further study how to adapt the network on edge devices
when the resource constraint is varied along the lifetime.","This chapter studied how to compress the network for eﬃcient
inference given the ﬁxed on-device resource constraints.","Although we
may deploy multiple ALQ-quantized multi-bit networks with diﬀerent
average bitwidth to execute under diﬀerent resource budgets, this naive
solution can only result in a subpar performance, as it requires several
times more storage consumption in comparison to a single (multi-bit)
network.",2022-10-06 20:52:57+00:00,Enabling Deep Learning on Edge Devices,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']",[arxiv.Result.Author('Zhongnan Qu')],"Deep neural networks (DNNs) have succeeded in many different perception
tasks, e.g., computer vision, natural language processing, reinforcement
learning, etc. The high-performed DNNs heavily rely on intensive resource
consumption. For example, training a DNN requires high dynamic memory, a
large-scale dataset, and a large number of computations (a long training time);
even inference with a DNN also demands a large amount of static storage,
computations (a long inference time), and energy. Therefore, state-of-the-art
DNNs are often deployed on a cloud server with a large number of
super-computers, a high-bandwidth communication bus, a shared storage
infrastructure, and a high power supplement.
  Recently, some new emerging intelligent applications, e.g., AR/VR, mobile
assistants, Internet of Things, require us to deploy DNNs on
resource-constrained edge devices. Compare to a cloud server, edge devices
often have a rather small amount of resources. To deploy DNNs on edge devices,
we need to reduce the size of DNNs, i.e., we target a better trade-off between
resource consumption and model accuracy.
  In this dissertation, we studied four edge intelligence scenarios, i.e.,
Inference on Edge Devices, Adaptation on Edge Devices, Learning on Edge
Devices, and Edge-Server Systems, and developed different methodologies to
enable deep learning in each scenario. Since current DNNs are often
over-parameterized, our goal is to find and reduce the redundancy of the DNNs
in each scenario."
12099,"To further study the impact of adopting diﬀerent rewinding
metrics, we show the distribution of updated weights across layers in this
section.","5.5.4.3 Number of Updated Weights across Layers under Diﬀerent
           Rewinding Metrics

Settings.","We implement partial updating methods with three rewinding
metrics (i.e., the global contribution, the local contribution, and the
combined contribution, see in Section 5.4.1) on VGGNet using CIFAR10
dataset.",2022-10-06 20:52:57+00:00,Enabling Deep Learning on Edge Devices,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']",[arxiv.Result.Author('Zhongnan Qu')],"Deep neural networks (DNNs) have succeeded in many different perception
tasks, e.g., computer vision, natural language processing, reinforcement
learning, etc. The high-performed DNNs heavily rely on intensive resource
consumption. For example, training a DNN requires high dynamic memory, a
large-scale dataset, and a large number of computations (a long training time);
even inference with a DNN also demands a large amount of static storage,
computations (a long inference time), and energy. Therefore, state-of-the-art
DNNs are often deployed on a cloud server with a large number of
super-computers, a high-bandwidth communication bus, a shared storage
infrastructure, and a high power supplement.
  Recently, some new emerging intelligent applications, e.g., AR/VR, mobile
assistants, Internet of Things, require us to deploy DNNs on
resource-constrained edge devices. Compare to a cloud server, edge devices
often have a rather small amount of resources. To deploy DNNs on edge devices,
we need to reduce the size of DNNs, i.e., we target a better trade-off between
resource consumption and model accuracy.
  In this dissertation, we studied four edge intelligence scenarios, i.e.,
Inference on Edge Devices, Adaptation on Edge Devices, Learning on Edge
Devices, and Edge-Server Systems, and developed different methodologies to
enable deep learning in each scenario. Since current DNNs are often
over-parameterized, our goal is to find and reduce the redundancy of the DNNs
in each scenario."
12101,"The goal of our work is to make it faster and easier for researchers to run
reproducible, generalizable ZC proxy experiments and to motivate further study on exploiting the
complementary strengths of ZC proxies.",Broader impact.,"By pre-computing ZC proxies across many benchmarks,
researchers can run many trials of NAS experiments cheaply on a CPU, reducing the carbon footprint
of the experiments [11, 26].",2022-10-06 21:56:26+00:00,NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Arjun Krishnakumar'), arxiv.Result.Author('Colin White'), arxiv.Result.Author('Arber Zela'), arxiv.Result.Author('Renbo Tu'), arxiv.Result.Author('Mahmoud Safari'), arxiv.Result.Author('Frank Hutter')]","Zero-cost proxies (ZC proxies) are a recent architecture performance
prediction technique aiming to significantly speed up algorithms for neural
architecture search (NAS). Recent work has shown that these techniques show
great promise, but certain aspects, such as evaluating and exploiting their
complementary strengths, are under-studied. In this work, we create
NAS-Bench-Suite: we evaluate 13 ZC proxies across 28 tasks, creating by far the
largest dataset (and unified codebase) for ZC proxies, enabling
orders-of-magnitude faster experiments on ZC proxies, while avoiding
confounding factors stemming from different implementations. To demonstrate the
usefulness of NAS-Bench-Suite, we run a large-scale analysis of ZC proxies,
including a bias analysis, and the first information-theoretic analysis which
concludes that ZC proxies capture substantial complementary information.
Motivated by these findings, we present a procedure to improve the performance
of ZC proxies by reducing biases such as cell size, and we also show that
incorporating all 13 ZC proxies into the surrogate models used by NAS
algorithms can improve their predictive performance by up to 42%. Our code and
datasets are available at https://github.com/automl/naslib/tree/zerocost."
12115,"on Neural Networks (IJCNN), pages 1–8, 2022.
problem in general and the latter suggestion need further study.",This                          Conf.,"[19] Q. Hong, J. W. Siegel, and J. Xu.",2022-10-07 09:49:18+00:00,Certified machine learning: Rigorous a posteriori error bounds for PDE defined PINNs,cs.LG,"['cs.LG', 'cs.NA', 'math.NA']","[arxiv.Result.Author('Birgit Hillebrecht'), arxiv.Result.Author('Benjamin Unger')]","Prediction error quantification in machine learning has been left out of most
methodological investigations of neural networks, for both purely data-driven
and physics-informed approaches. Beyond statistical investigations and generic
results on the approximation capabilities of neural networks, we present a
rigorous upper bound on the prediction error of physics-informed neural
networks. This bound can be calculated without the knowledge of the true
solution and only with a priori available information about the characteristics
of the underlying dynamical system governed by a partial differential equation.
We apply this a posteriori error bound exemplarily to four problems: the
transport equation, the heat equation, the Navier-Stokes equation and the
Klein-Gordon equation."
12143,"To promote further research on LLMs for HTML
                                                 understanding, we create and open-source a large-scale HTML dataset distilled
                                                 and auto-labeled from CommonCrawl.1

                                       1 INTRODUCTION

                                       Web crawling (Olston et al., 2010), form-ﬁlling (Diaz et al., 2013; Gur et al., 2021), or information
                                       retrieving web agents (Nogueira & Cho, 2016) are important for both automating and assisting
                                       users in web-based tasks.","Out of the LLMs we evalu-
                                                 ate, we show evidence that T5-based models are ideal due to their bidirectional
                                                 encoder-decoder architecture.","These and similar applications rely on models that can search for speciﬁc
                                       content or controls on a web page as well as navigate a website autonomously.",2022-10-08 07:27:17+00:00,Understanding HTML with Large Language Models,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Izzeddin Gur'), arxiv.Result.Author('Ofir Nachum'), arxiv.Result.Author('Yingjie Miao'), arxiv.Result.Author('Mustafa Safdari'), arxiv.Result.Author('Austin Huang'), arxiv.Result.Author('Aakanksha Chowdhery'), arxiv.Result.Author('Sharan Narang'), arxiv.Result.Author('Noah Fiedel'), arxiv.Result.Author('Aleksandra Faust')]","Large language models (LLMs) have shown exceptional performance on a variety
of natural language tasks. Yet, their capabilities for HTML understanding --
i.e., parsing the raw HTML of a webpage, with applications to automation of
web-based tasks, crawling, and browser-assisted retrieval -- have not been
fully explored. We contribute HTML understanding models (fine-tuned LLMs) and
an in-depth analysis of their capabilities under three tasks: (i) Semantic
Classification of HTML elements, (ii) Description Generation for HTML inputs,
and (iii) Autonomous Web Navigation of HTML pages. While previous work has
developed dedicated architectures and training procedures for HTML
understanding, we show that LLMs pretrained on standard natural language
corpora transfer remarkably well to HTML understanding tasks. For instance,
fine-tuned LLMs are 12% more accurate at semantic classification compared to
models trained exclusively on the task dataset. Moreover, when fine-tuned on
data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks
using 192x less data compared to the previous best supervised model. Out of the
LLMs we evaluate, we show evidence that T5-based models are ideal due to their
bidirectional encoder-decoder architecture. To promote further research on LLMs
for HTML understanding, we create and open-source a large-scale HTML dataset
distilled and auto-labeled from CommonCrawl."
12144,"of deep learning need further study, it can be employed to improve
classiﬁcation performance.",Although the theoretical foundations          each node.,"Label Tree Label tree methods usually assume that labels are
                                                                       organized in a tree structure.",2022-10-08 08:31:34+00:00,A Survey on Extreme Multi-label Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tong Wei'), arxiv.Result.Author('Zhen Mao'), arxiv.Result.Author('Jiang-Xin Shi'), arxiv.Result.Author('Yu-Feng Li'), arxiv.Result.Author('Min-Ling Zhang')]","Multi-label learning has attracted significant attention from both academic
and industry field in recent decades. Although existing multi-label learning
algorithms achieved good performance in various tasks, they implicitly assume
the size of target label space is not huge, which can be restrictive for
real-world scenarios. Moreover, it is infeasible to directly adapt them to
extremely large label space because of the compute and memory overhead.
Therefore, eXtreme Multi-label Learning (XML) is becoming an important task and
many effective approaches are proposed. To fully understand XML, we conduct a
survey study in this paper. We first clarify a formal definition for XML from
the perspective of supervised learning. Then, based on different model
architectures and challenges of the problem, we provide a thorough discussion
of the advantages and disadvantages of each category of methods. For the
benefit of conducting empirical studies, we collect abundant resources
regarding XML, including code implementations, and useful tools. Lastly, we
propose possible research directions in XML, such as new evaluation metrics,
the tail label problem, and weakly supervised XML."
12145,"However, it still needs further study if such clustering is             EAGLE can be seen a baseline for this problem while leaves
      optimal.",one of the sample points in every positively labeled group.,big room for performance improvement.,2022-10-08 08:31:34+00:00,A Survey on Extreme Multi-label Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tong Wei'), arxiv.Result.Author('Zhen Mao'), arxiv.Result.Author('Jiang-Xin Shi'), arxiv.Result.Author('Yu-Feng Li'), arxiv.Result.Author('Min-Ling Zhang')]","Multi-label learning has attracted significant attention from both academic
and industry field in recent decades. Although existing multi-label learning
algorithms achieved good performance in various tasks, they implicitly assume
the size of target label space is not huge, which can be restrictive for
real-world scenarios. Moreover, it is infeasible to directly adapt them to
extremely large label space because of the compute and memory overhead.
Therefore, eXtreme Multi-label Learning (XML) is becoming an important task and
many effective approaches are proposed. To fully understand XML, we conduct a
survey study in this paper. We first clarify a formal definition for XML from
the perspective of supervised learning. Then, based on different model
architectures and challenges of the problem, we provide a thorough discussion
of the advantages and disadvantages of each category of methods. For the
benefit of conducting empirical studies, we collect abundant resources
regarding XML, including code implementations, and useful tools. Lastly, we
propose possible research directions in XML, such as new evaluation metrics,
the tail label problem, and weakly supervised XML."
12146,Her further research                  University of Florence in 1994.,"In her research, she focuses            honours in Computer Science from the University
                              on Machine Learning on structural-dynamic graphs,               of Pisa in 1989 and the Ph.D. degree in Computer
                              considering graph preprocessing, representation                 Science and Automation Engineering from the
                              and embedding techniques.","After the Ph.D. he
                              interests cover topics from Timeseries Analysis,                has been supported by foundations of private and
                              Graph Theory, Machine Learning and Deep Neural                  public companies and by a postdoc of the University
                              Networks.",2022-10-08 10:14:41+00:00,Weisfeiler--Lehman goes Dynamic: An Analysis of the Expressive Power of Graph Neural Networks for Attributed and Dynamic Graphs,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Silvia Beddar-Wiesing'), arxiv.Result.Author(""Giuseppe Alessio D'Inverno""), arxiv.Result.Author('Caterina Graziani'), arxiv.Result.Author('Veronica Lachi'), arxiv.Result.Author('Alice Moallemy-Oureh'), arxiv.Result.Author('Franco Scarselli'), arxiv.Result.Author('Josephine Maria Thomas')]","Graph Neural Networks (GNNs) are a large class of relational models for graph
processing. Recent theoretical studies on the expressive power of GNNs have
focused on two issues. On the one hand, it has been proven that GNNs are as
powerful as the Weisfeiler-Lehman test (1-WL) in their ability to distinguish
graphs. Moreover, it has been shown that the equivalence enforced by 1-WL
equals unfolding equivalence. On the other hand, GNNs turned out to be
universal approximators on graphs modulo the constraints enforced by
1-WL/unfolding equivalence. However, these results only apply to Static
Undirected Homogeneous Graphs with node attributes. In contrast, real-life
applications often involve a variety of graph properties, such as, e.g.,
dynamics or node and edge attributes. In this paper, we conduct a theoretical
analysis of the expressive power of GNNs for these two graph types that are
particularly of interest. Dynamic graphs are widely used in modern
applications, and its theoretical analysis requires new approaches. The
attributed type acts as a standard form for all graph types since it has been
shown that all graph types can be transformed without loss to Static Undirected
Homogeneous Graphs with attributes on nodes and edges (SAUHG). The study
considers generic GNN models and proposes appropriate 1-WL tests for those
domains. Then, the results on the expressive power of GNNs are extended by
proving that GNNs have the same capability as the 1-WL test in distinguishing
dynamic and attributed graphs, the 1-WL equivalence equals unfolding
equivalence and that GNNs are universal approximators modulo 1-WL/unfolding
equivalence. Moreover, the proof of the approximation capability holds for
SAUHGs, which include most of those used in practical applications, and it is
constructive in nature allowing to deduce hints on the architecture of GNNs
that can achieve the desired accuracy."
12150,"Moreover, we hope that this
work inspires further research into unsupervised robustness, which can contribute to more robust and
secure machine learning systems.","Therefore, we believe that it would empower the developers of
safety-, reliability- and fairness-critical systems to develop safer models.","REPRODUCIBILITY STATEMENT

The experiments in this paper were implemented using open-source software packages (Harris et al.,
2020; Virtanen et al., 2020; McKinney, 2010; Paszke et al., 2019), as well as the publicly available
MOCOv2 and MOCOv3 models (He et al., 2020; Chen et al., 2020d; 2021).",2022-10-08 18:03:28+00:00,Robustness of Unsupervised Representation Learning without Labels,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Aleksandar Petrov'), arxiv.Result.Author('Marta Kwiatkowska')]","Unsupervised representation learning leverages large unlabeled datasets and
is competitive with supervised learning. But non-robust encoders may affect
downstream task robustness. Recently, robust representation encoders have
become of interest. Still, all prior work evaluates robustness using a
downstream classification task. Instead, we propose a family of unsupervised
robustness measures, which are model- and task-agnostic and label-free. We
benchmark state-of-the-art representation encoders and show that none dominates
the rest. We offer unsupervised extensions to the FGSM and PGD attacks. When
used in adversarial training, they improve most unsupervised robustness
measures, including certified robustness. We validate our results against a
linear probe and show that, for MOCOv2, adversarial training results in 3 times
higher certified accuracy, a 2-fold decrease in impersonation attack success
rate and considerable improvements in certified robustness."
12156,"This is mainly kind of data structures: single-
is of great signiﬁcance for guiding further research in this ﬁeld.","In this paper, we introduce recent advances in the ﬁeld of
So, exploring the theoretical basis of deep clustering optimization    deep clustering.","view, semi-supervised, multi-view, and transfer learning.",2022-10-09 02:31:32+00:00,Deep Clustering: A Comprehensive Survey,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yazhou Ren'), arxiv.Result.Author('Jingyu Pu'), arxiv.Result.Author('Zhimeng Yang'), arxiv.Result.Author('Jie Xu'), arxiv.Result.Author('Guofeng Li'), arxiv.Result.Author('Xiaorong Pu'), arxiv.Result.Author('Philip S. Yu'), arxiv.Result.Author('Lifang He')]","Cluster analysis plays an indispensable role in machine learning and data
mining. Learning a good data representation is crucial for clustering
algorithms. Recently, deep clustering, which can learn clustering-friendly
representations using deep neural networks, has been broadly applied in a wide
range of clustering tasks. Existing surveys for deep clustering mainly focus on
the single-view fields and the network architectures, ignoring the complex
application scenarios of clustering. To address this issue, in this paper we
provide a comprehensive survey for deep clustering in views of data sources.
With different data sources and initial conditions, we systematically
distinguish the clustering methods in terms of methodology, prior knowledge,
and architecture. Concretely, deep clustering methods are introduced according
to four categories, i.e., traditional single-view deep clustering,
semi-supervised deep clustering, deep multi-view clustering, and deep transfer
clustering. Finally, we discuss the open challenges and potential future
opportunities in different fields of deep clustering."
12178,"Therefore, a
further study is required to have lower computation costs while preventing the deterioration in the

performance.","We observe that the performance may degrade
by using RˆFP, which may due to the inaccurate approximation to the exact score FPE.","11
E Proofs and discussion

E.1 Proof of Corollary 1

We prove the result with a more general forward SDE

                                            dx = F (x, t)dt + G(x, t)dwt,                                           (15)

where F (·, t) : RD → RD and G(·, t) : RD → RD×D.",2022-10-09 16:27:25+00:00,Regularizing Score-based Models with Score Fokker-Planck Equations,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Chieh-Hsin Lai'), arxiv.Result.Author('Yuhta Takida'), arxiv.Result.Author('Naoki Murata'), arxiv.Result.Author('Toshimitsu Uesaka'), arxiv.Result.Author('Yuki Mitsufuji'), arxiv.Result.Author('Stefano Ermon')]","Score-based generative models learn a family of noise-conditional score
functions corresponding to the data density perturbed with increasingly large
amounts of noise. These pertubed data densities are tied together by the
Fokker-Planck equation (FPE), a PDE governing the spatial-temporal evolution of
a density undergoing a diffusion process. In this work, we derive a
corresponding equation characterizing the noise-conditional scores of the
perturbed data densities (i.e., their gradients), termed the score FPE.
Surprisingly, despite impressive empirical performance, we observe that scores
learned via denoising score matching (DSM) do not satisfy the underlying score
FPE. We mathematically analyze two implications of satisfying the score FPE and
a potential explanation for why the score FPE is not satisfied in practice. At
last, we propose to regularize the DSM objective to enforce satisfaction of the
score FPE, and show its effectiveness on synthetic data and MNIST."
12179,"Therefore, a
further study is required to have lower computation costs while preventing the deterioration in the

performance.","We observe that the performance may degrade
by using RˆFP, which may due to the inaccurate approximation to the exact score FPE.","13
D Explanation of Implementation

In Fig.",2022-10-09 16:27:25+00:00,Regularizing Score-based Models with Score Fokker-Planck Equations,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Chieh-Hsin Lai'), arxiv.Result.Author('Yuhta Takida'), arxiv.Result.Author('Naoki Murata'), arxiv.Result.Author('Toshimitsu Uesaka'), arxiv.Result.Author('Yuki Mitsufuji'), arxiv.Result.Author('Stefano Ermon')]","Score-based generative models learn a family of noise-conditional score
functions corresponding to the data density perturbed with increasingly large
amounts of noise. These perturbed data densities are tied together by the
Fokker-Planck equation (FPE), a PDE governing the spatial-temporal evolution of
a density undergoing a diffusion process. In this work, we derive a
corresponding equation characterizing the noise-conditional scores of the
perturbed data densities (i.e., their gradients), termed the score FPE.
Surprisingly, despite impressive empirical performance, we observe that scores
learned via denoising score matching (DSM) do not satisfy the underlying score
FPE. We mathematically analyze three implications of satisfying the score FPE
and a potential explanation for why the score FPE is not satisfied in practice.
At last, we propose to regularize the DSM objective to enforce satisfaction of
the score FPE, and show its effectiveness on synthetic data and MNIST."
12202,"Analysis of the activations suggested the
model might be learning to recognise contrast between land and sea, and further research could
examine performance on satellite images with no coastline.","This suggested the model might be inﬂuenced by an unbalanced dataset, and that an augmented
dataset or a different loss function might improve results.","A more in depth analysis of the current strategies implemented by grid operators used to meet
energy demands in case of disruption to the renewable solar supply would be needed to better put in
perspective the climate change mitigation potential of nowcasting models.",2022-10-10 10:48:34+00:00,Comparing the carbon costs and benefits of low-resource solar nowcasting,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Dixon'), arxiv.Result.Author('María Pérez-Ortiz'), arxiv.Result.Author('Jacob Bieker')]","Solar PV yield nowcasting is used to help anticipate peaks and troughs in
demand to support grid integration. This paper compares multiple low-resource
approaches to nowcasting solar PV yield, using a dataset of UK satellite
imagery and solar PV energy readings over a 1 to 4-hour time range. The paper
also estimates the carbon emissions generated and averted by deploying models,
and finds that even small models that could be deployable in low-resource
settings may have a benefit several orders of magnitude greater than its carbon
cost. The paper also examines prediction errors and the activations in a CNN."
12203,"A useful area of further research would be
examining the activations in the more complex, time series models used in the rest of the paper.","This might also be true for our
main models - are they also just recognising contrast?","References

 [1] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun.",2022-10-10 10:48:34+00:00,Comparing the carbon costs and benefits of low-resource solar nowcasting,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ben Dixon'), arxiv.Result.Author('María Pérez-Ortiz'), arxiv.Result.Author('Jacob Bieker')]","Solar PV yield nowcasting is used to help anticipate peaks and troughs in
demand to support grid integration. This paper compares multiple low-resource
approaches to nowcasting solar PV yield, using a dataset of UK satellite
imagery and solar PV energy readings over a 1 to 4-hour time range. The paper
also estimates the carbon emissions generated and averted by deploying models,
and finds that even small models that could be deployable in low-resource
settings may have a benefit several orders of magnitude greater than its carbon
cost. The paper also examines prediction errors and the activations in a CNN."
12204,"While this model
                                                                   is commonly adopted in the literature, we believe that
In this article we studied the impact of IV, an oft neglected      further research should explore the relaxation of this as-
type of uncertainty affecting data, on the performance and         sumption, by considering more general models of IV ac-
robustness of ML models.","• In our experiments we assumed the IV distributions to
                      Conclusion                                   be Gaussian with diagonal covariance.","Crucially, through a realistic ex-        counting for causal relationships among features;
periment on COVID-19 diagnosis, we showed that standard
ML algorithms can be strongly impacted by the presence of        • While we focused on the impact of IV in ML and brieﬂy
IV, failing to generalize properly.",2022-10-10 10:49:06+00:00,Everything is Varied: The Surprising Impact of Individual Variation on ML Robustness in Medicine,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andra Campagner'), arxiv.Result.Author('Lorenzo Famiglini'), arxiv.Result.Author('Anna Carobene'), arxiv.Result.Author('Federico Cabitza')]","In medical settings, Individual Variation (IV) refers to variation that is
due not to population differences or errors, but rather to within-subject
variation, that is the intrinsic and characteristic patterns of variation
pertaining to a given instance or the measurement process. While taking into
account IV has been deemed critical for proper analysis of medical data, this
source of uncertainty and its impact on robustness have so far been neglected
in Machine Learning (ML). To fill this gap, we look at how IV affects ML
performance and generalization and how its impact can be mitigated.
Specifically, we provide a methodological contribution to formalize the problem
of IV in the statistical learning framework and, through an experiment based on
one of the largest real-world laboratory medicine datasets for the problem of
COVID-19 diagnosis, we show that: 1) common state-of-the-art ML models are
severely impacted by the presence of IV in data; and 2) advanced learning
strategies, based on data augmentation and data imprecisiation, and proper
study designs can be effective at improving robustness to IV. Our findings
demonstrate the critical relevance of correctly accounting for IV to enable
safe deployment of ML in clinical settings."
12205,"Such an issue can severely     discussed IV in SLT, we believe the theoretical side of
limit the applicability and safety of ML methods in tasks          this issue merits further study: even though the prob-
where data are expected to be affected by IV, that is most         lem of learning from distributional data has recently been
applications in clinical settings and more generally in real-      investigated in SLT (Campagner 2021; Ma et al.","Crucially, through a realistic ex-        counting for causal relationships among features;
periment on COVID-19 diagnosis, we showed that standard
ML algorithms can be strongly impacted by the presence of        • While we focused on the impact of IV in ML and brieﬂy
IV, failing to generalize properly.","2021;
world domains where the manifestations of the phenomena            Muandet et al.",2022-10-10 10:49:06+00:00,Everything is Varied: The Surprising Impact of Individual Variation on ML Robustness in Medicine,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andra Campagner'), arxiv.Result.Author('Lorenzo Famiglini'), arxiv.Result.Author('Anna Carobene'), arxiv.Result.Author('Federico Cabitza')]","In medical settings, Individual Variation (IV) refers to variation that is
due not to population differences or errors, but rather to within-subject
variation, that is the intrinsic and characteristic patterns of variation
pertaining to a given instance or the measurement process. While taking into
account IV has been deemed critical for proper analysis of medical data, this
source of uncertainty and its impact on robustness have so far been neglected
in Machine Learning (ML). To fill this gap, we look at how IV affects ML
performance and generalization and how its impact can be mitigated.
Specifically, we provide a methodological contribution to formalize the problem
of IV in the statistical learning framework and, through an experiment based on
one of the largest real-world laboratory medicine datasets for the problem of
COVID-19 diagnosis, we show that: 1) common state-of-the-art ML models are
severely impacted by the presence of IV in data; and 2) advanced learning
strategies, based on data augmentation and data imprecisiation, and proper
study designs can be effective at improving robustness to IV. Our findings
demonstrate the critical relevance of correctly accounting for IV to enable
safe deployment of ML in clinical settings."
12206,"While this model
                                                                   is commonly adopted in the literature, we believe that
In this article we studied the impact of IV, an oft neglected      further research should explore the relaxation of this as-
type of uncertainty affecting data, on the performance and         sumption, by considering more general models of IV ac-
robustness of ML models.","• In our experiments we assumed the IV distributions to
                      Conclusion                                   be Gaussian with diagonal covariance.","Crucially, through a realistic ex-        counting for causal relationships among features;
periment on COVID-19 diagnosis, we showed that standard
ML algorithms can be strongly impacted by the presence of        • While we focused on the impact of IV in ML and brieﬂy
IV, failing to generalize properly.",2022-10-10 10:49:06+00:00,Everything is Varied: The Surprising Impact of Individual Variation on ML Robustness in Medicine,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andrea Campagner'), arxiv.Result.Author('Lorenzo Famiglini'), arxiv.Result.Author('Anna Carobene'), arxiv.Result.Author('Federico Cabitza')]","In medical settings, Individual Variation (IV) refers to variation that is
due not to population differences or errors, but rather to within-subject
variation, that is the intrinsic and characteristic patterns of variation
pertaining to a given instance or the measurement process. While taking into
account IV has been deemed critical for proper analysis of medical data, this
source of uncertainty and its impact on robustness have so far been neglected
in Machine Learning (ML). To fill this gap, we look at how IV affects ML
performance and generalization and how its impact can be mitigated.
Specifically, we provide a methodological contribution to formalize the problem
of IV in the statistical learning framework and, through an experiment based on
one of the largest real-world laboratory medicine datasets for the problem of
COVID-19 diagnosis, we show that: 1) common state-of-the-art ML models are
severely impacted by the presence of IV in data; and 2) advanced learning
strategies, based on data augmentation and data imprecisiation, and proper
study designs can be effective at improving robustness to IV. Our findings
demonstrate the critical relevance of correctly accounting for IV to enable
safe deployment of ML in clinical settings."
12207,"Such an issue can severely     discussed IV in SLT, we believe the theoretical side of
limit the applicability and safety of ML methods in tasks          this issue merits further study: even though the prob-
where data are expected to be affected by IV, that is most         lem of learning from distributional data has recently been
applications in clinical settings and more generally in real-      investigated in SLT (Campagner 2021; Ma et al.","Crucially, through a realistic ex-        counting for causal relationships among features;
periment on COVID-19 diagnosis, we showed that standard
ML algorithms can be strongly impacted by the presence of        • While we focused on the impact of IV in ML and brieﬂy
IV, failing to generalize properly.","2021;
world domains where the manifestations of the phenomena            Muandet et al.",2022-10-10 10:49:06+00:00,Everything is Varied: The Surprising Impact of Individual Variation on ML Robustness in Medicine,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andrea Campagner'), arxiv.Result.Author('Lorenzo Famiglini'), arxiv.Result.Author('Anna Carobene'), arxiv.Result.Author('Federico Cabitza')]","In medical settings, Individual Variation (IV) refers to variation that is
due not to population differences or errors, but rather to within-subject
variation, that is the intrinsic and characteristic patterns of variation
pertaining to a given instance or the measurement process. While taking into
account IV has been deemed critical for proper analysis of medical data, this
source of uncertainty and its impact on robustness have so far been neglected
in Machine Learning (ML). To fill this gap, we look at how IV affects ML
performance and generalization and how its impact can be mitigated.
Specifically, we provide a methodological contribution to formalize the problem
of IV in the statistical learning framework and, through an experiment based on
one of the largest real-world laboratory medicine datasets for the problem of
COVID-19 diagnosis, we show that: 1) common state-of-the-art ML models are
severely impacted by the presence of IV in data; and 2) advanced learning
strategies, based on data augmentation and data imprecisiation, and proper
study designs can be effective at improving robustness to IV. Our findings
demonstrate the critical relevance of correctly accounting for IV to enable
safe deployment of ML in clinical settings."
12211,"We hope that through our focus on ﬂexibility and customiza-
                                                                        tion, ParaDime inspires further research about the potential of para-
7.3.",ideas.,Non-parametric Routines                                            metric dimensionality reduction.,2022-10-10 11:36:56+00:00,ParaDime: A Framework for Parametric Dimensionality Reduction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andreas Hinterreiter'), arxiv.Result.Author('Christina Humer'), arxiv.Result.Author('Bernhard Kainz'), arxiv.Result.Author('Marc Streit')]","ParaDime is a framework for parametric dimensionality reduction (DR). In
parametric DR, neural networks are trained to embed high-dimensional data items
in a low-dimensional space while minimizing an objective function. ParaDime
builds on the idea that the objective functions of several modern DR techniques
result from transformed inter-item relationships. It provides a common
interface to specify the way these relations and transformations are computed
and how they are used within the losses that govern the training process.
Through this interface, ParaDime unifies parametric versions of DR techniques
such as metric MDS, t-SNE, and UMAP. Furthermore, it allows users to fully
customize each aspect of the DR process. We show how this ease of customization
makes ParaDime suitable for experimenting with interesting techniques, such as
hybrid classification/embedding models or supervised DR, which opens up new
possibilities for visualizing high-dimensional data."
12212,"Non-parametric Routines                                             tion, ParaDime inspires further research about the potential of para-
                                                                         metric dimensionality reduction.","We hope that through our focus on ﬂexibility and customiza-
6.3.","We realize that—except for the model entry—each ParaDime spec-
iﬁcation could also be valid for a non-parametric DR routine.",2022-10-10 11:36:56+00:00,ParaDime: A Framework for Parametric Dimensionality Reduction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andreas Hinterreiter'), arxiv.Result.Author('Christina Humer'), arxiv.Result.Author('Bernhard Kainz'), arxiv.Result.Author('Marc Streit')]","ParaDime is a framework for parametric dimensionality reduction (DR). In
parametric DR, neural networks are trained to embed high-dimensional data items
in a low-dimensional space while minimizing an objective function. ParaDime
builds on the idea that the objective functions of several modern DR techniques
result from transformed inter-item relationships. It provides a common
interface to specify these relations and transformations and to define how they
are used within the losses that govern the training process. Through this
interface, ParaDime unifies parametric versions of DR techniques such as metric
MDS, t-SNE, and UMAP. Furthermore, it allows users to fully customize each
aspect of the DR process. We show how this ease of customization makes ParaDime
suitable for experimenting with interesting techniques, such as hybrid
classification/embedding models or supervised DR, which opens up new
possibilities for visualizing high-dimensional data."
12221,"We leave
to future work to further study the role played by the asymmetry between sources (e.g., different
informativeness or ease).","of the learning dynamics, we focused on tasks with homogeneous sources (stereo, video).","Our theoretical and empirical analysis leads to several suggestions: Pre-
training different backbones separately on each modality, as advocated in some foundational model,
may yield representations that ultimately fail to encode synergistic information.",2022-10-06 23:50:38+00:00,Critical Learning Periods for Multisensory Integration in Deep Networks,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'q-bio.NC']","[arxiv.Result.Author('Michael Kleinman'), arxiv.Result.Author('Alessandro Achille'), arxiv.Result.Author('Stefano Soatto')]","We show that the ability of a neural network to integrate information from
diverse sources hinges critically on being exposed to properly correlated
signals during the early phases of training. Interfering with the learning
process during this initial stage can permanently impair the development of a
skill, both in artificial and biological systems where the phenomenon is known
as critical learning period. We show that critical periods arise from the
complex and unstable early transient dynamics, which are decisive of final
performance of the trained system and their learned representations. This
evidence challenges the view, engendered by analysis of wide and shallow
networks, that early learning dynamics of neural networks are simple, akin to
those of a linear model. Indeed, we show that even deep linear networks exhibit
critical learning periods for multi-source integration, while shallow networks
do not. To better understand how the internal representations change according
to disturbances or sensory deficits, we introduce a new measure of source
sensitivity, which allows us to track the inhibition and integration of sources
during training. Our analysis of inhibition suggests cross-source
reconstruction as a natural auxiliary training objective, and indeed we show
that architectures trained with cross-sensor reconstruction objectives are
remarkably more resilient to critical periods. Our findings suggest that the
recent success in self-supervised multi-modal training compared to previous
supervised efforts may be in part due to more robust learning dynamics and not
solely due to better architectures and/or more data."
12257,"When regressing the anomalous diﬀusion exponent, the sensitivity of machine
learning models to added noise in SBM trajectories warrants further study.","However, there were a few instances where the
                     ConvTransformer predicted values marginally less than 0 and greater than 2.

eduN) shown in Table 3, where SBM performance is the most sensitive to additional
noise in the trajectory.","Recently,
Szarek [36] has encountered a similar lack in resiliency to noise using an RNN-based
model, like UPV-Mat, HNU, and eduN models.",2022-10-10 18:53:13+00:00,Characterization of anomalous diffusion through convolutional transformers,cs.LG,"['cs.LG', '68T07', 'I.2']","[arxiv.Result.Author('Nicolás Firbas'), arxiv.Result.Author('Òscar Garibo-i-Orts'), arxiv.Result.Author('Miguel Ángel Garcia-March'), arxiv.Result.Author('J. Alberto Conejero')]","The results of the Anomalous Diffusion Challenge (AnDi Challenge) have shown
that machine learning methods can outperform classical statistical methodology
at the characterization of anomalous diffusion in both the inference of the
anomalous diffusion exponent alpha associated with each trajectory (Task 1),
and the determination of the underlying diffusive regime which produced such
trajectories (Task 2). Furthermore, of the five teams that finished in the top
three across both tasks of the AnDi challenge, three of those teams used
recurrent neural networks (RNNs). While RNNs, like the long short-term memory
(LSTM) network, are effective at learning long-term dependencies in sequential
data, their key disadvantage is that they must be trained sequentially. In
order to facilitate training with larger data sets, by training in parallel, we
propose a new transformer based neural network architecture for the
characterization of anomalous diffusion. Our new architecture, the
Convolutional Transformer (ConvTransformer) uses a bi-layered convolutional
neural network to extract features from our diffusive trajectories that can be
thought of as being words in a sentence. These features are then fed to two
transformer encoding blocks that perform either regression or classification.
To our knowledge, this is the first time transformers have been used for
characterizing anomalous diffusion. Moreover, this may be the first time that a
transformer encoding block has been used with a convolutional neural network
and without the need for a transformer decoding block or positional encoding.
Apart from being able to train in parallel, we show that the ConvTransformer is
able to outperform the previous state of the art at determining the underlying
diffusive regime in short trajectories (length 10-50 steps), which are the most
important for experimental researchers."
12274,"This is a direction
for further research.","An idea of the “leaf” attention and the optimization
over parameters of kernels can be directly transferred to the gradient boosting machine.","One of the important results presented in the paper is the usage of a speciﬁc mixture of contamination
models which can be regarded as a variant of the well-known multi-head attention [16], where each “head”
is deﬁned by the kernel parameter.",2022-10-11 06:14:12+00:00,LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrei V. Konstantinov'), arxiv.Result.Author('Lev V. Utkin')]","New models of the attention-based random forests called LARF (Leaf
Attention-based Random Forest) are proposed. The first idea behind the models
is to introduce a two-level attention, where one of the levels is the ""leaf""
attention and the attention mechanism is applied to every leaf of trees. The
second level is the tree attention depending on the ""leaf"" attention. The
second idea is to replace the softmax operation in the attention with the
weighted sum of the softmax operations with different parameters. It is
implemented by applying a mixture of the Huber's contamination models and can
be regarded as an analog of the multi-head attention with ""heads"" defined by
selecting a value of the softmax parameter. Attention parameters are simply
trained by solving the quadratic optimization problem. To simplify the tuning
process of the models, it is proposed to make the tuning contamination
parameters to be training and to compute them by solving the quadratic
optimization problem. Many numerical experiments with real datasets are
performed for studying LARFs. The code of proposed algorithms can be found in
https://github.com/andruekonst/leaf-attention-forest."
12275,"The corresponding procedures of the interpretation is also
a direction for further research.","The introduced two-level attention mechanisms may also improve interpretability of
RFs by taking into account additional factors.","Finally, we have developed the proposed modiﬁcations by using the Huber’s -contamination model and
the mixture of the models.",2022-10-11 06:14:12+00:00,LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrei V. Konstantinov'), arxiv.Result.Author('Lev V. Utkin')]","New models of the attention-based random forests called LARF (Leaf
Attention-based Random Forest) are proposed. The first idea behind the models
is to introduce a two-level attention, where one of the levels is the ""leaf""
attention and the attention mechanism is applied to every leaf of trees. The
second level is the tree attention depending on the ""leaf"" attention. The
second idea is to replace the softmax operation in the attention with the
weighted sum of the softmax operations with different parameters. It is
implemented by applying a mixture of the Huber's contamination models and can
be regarded as an analog of the multi-head attention with ""heads"" defined by
selecting a value of the softmax parameter. Attention parameters are simply
trained by solving the quadratic optimization problem. To simplify the tuning
process of the models, it is proposed to make the tuning contamination
parameters to be training and to compute them by solving the quadratic
optimization problem. Many numerical experiments with real datasets are
performed for studying LARFs. The code of proposed algorithms can be found in
https://github.com/andruekonst/leaf-attention-forest."
12276,This is also a direction for further research.,"A proper choice of the mixture components may signiﬁcantly improve the whole
attention-based RF.","Acknowledgement

The research is partially funded by the Ministry of Science and Higher Education of the Russian Federa-
tion under the strategic academic leadership program ’Priority 2030’ (Agreement N 075-15-2021-1333 dd
30.09.2021).",2022-10-11 06:14:12+00:00,LARF: Two-level Attention-based Random Forests with a Mixture of Contamination Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Andrei V. Konstantinov'), arxiv.Result.Author('Lev V. Utkin')]","New models of the attention-based random forests called LARF (Leaf
Attention-based Random Forest) are proposed. The first idea behind the models
is to introduce a two-level attention, where one of the levels is the ""leaf""
attention and the attention mechanism is applied to every leaf of trees. The
second level is the tree attention depending on the ""leaf"" attention. The
second idea is to replace the softmax operation in the attention with the
weighted sum of the softmax operations with different parameters. It is
implemented by applying a mixture of the Huber's contamination models and can
be regarded as an analog of the multi-head attention with ""heads"" defined by
selecting a value of the softmax parameter. Attention parameters are simply
trained by solving the quadratic optimization problem. To simplify the tuning
process of the models, it is proposed to make the tuning contamination
parameters to be training and to compute them by solving the quadratic
optimization problem. Many numerical experiments with real datasets are
performed for studying LARFs. The code of proposed algorithms can be found in
https://github.com/andruekonst/leaf-attention-forest."
12287,We hope that our work paves the way for further research.,"In experimental
terms, we showed the following properties of GVM in the general case: i) applicability to multi-input data,
ii) stability wrt to diﬀerent ways of computing the Periodogram, iii) consistency under diﬀerent realisations
of the GP unlike ML, iv) computational eﬃciency wrt ML and sparse GPs, v) a realistic alternative to
compute initial conditions for ML resulting in considerable reduction of ML iterations, and lastly, vi) ability
to train kernels of large number of components that are challenging to train from random initial conditions
using ML.","In theoretical terms, we envision extensions
towards non-stationary data using, e.g., time-frequency representations or mini-batches.",2022-10-11 12:13:21+00:00,Computationally-efficient initialisation of GPs: The generalised variogram method,cs.LG,"['cs.LG', 'eess.SP']","[arxiv.Result.Author('Felipe Tobar'), arxiv.Result.Author('Elsa Cazelles'), arxiv.Result.Author('Taco de Wolff')]","We present a computationally-efficient strategy to find the hyperparameters
of a Gaussian process (GP) avoiding the computation of the likelihood function.
The found hyperparameters can then be used directly for regression or passed as
initial conditions to maximum-likelihood (ML) training. Motivated by the fact
that training a GP via ML is equivalent (on average) to minimising the
KL-divergence between the true and learnt model, we set to explore different
metrics/divergences among GPs that are computationally inexpensive and provide
estimates close to those of ML. In particular, we identify the GP
hyperparameters by matching the empirical covariance to a parametric candidate,
proposing and studying various measures of discrepancy. Our proposal extends
the Variogram method developed by the geostatistics literature and thus is
referred to as the Generalised Variogram method (GVM). In addition to the
theoretical presentation of GVM, we provide experimental validation in terms of
accuracy, consistency with ML and computational complexity for different
kernels using synthetic and real-world data."
12295,"The equivariance relaxation morphism and the [G]-mixed equivariant
layer that enable the evolutionary and differentiable search methods respectively are the main focus; the other char-
acteristics of the NAS methods are adapted from existing methods, but further study could advance specialization of
equivariance-aware NAS.","These two NAS approaches present adaptations of two standard types of NAS, evolutionary and differentiable, to
the search for optimal partial equivariance.","We next study empirically the two EquiNAS methods on three datasets, one with known
rotational symmetry and two with unknown but visually signiﬁcant rotational and reﬂectional symmetry.",2022-10-11 14:37:29+00:00,Architectural Optimization over Subgroups for Equivariant Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kaitlin Maile'), arxiv.Result.Author('Dennis G. Wilson'), arxiv.Result.Author('Patrick Forré')]","Incorporating equivariance to symmetry groups as a constraint during neural
network training can improve performance and generalization for tasks
exhibiting those symmetries, but such symmetries are often not perfectly nor
explicitly present. This motivates algorithmically optimizing the architectural
constraints imposed by equivariance. We propose the equivariance relaxation
morphism, which preserves functionality while reparameterizing a group
equivariant layer to operate with equivariance constraints on a subgroup, as
well as the $[G]$-mixed equivariant layer, which mixes layers constrained to
different groups to enable within-layer equivariance optimization. We further
present evolutionary and differentiable neural architecture search (NAS)
algorithms that utilize these mechanisms respectively for equivariance-aware
architectural optimization. Experiments across a variety of datasets show the
benefit of dynamically constrained equivariance to find effective architectures
with approximate equivariance."
12296,"The equivariance relaxation morphism and the [G]-mixed equivariant
layer that enable the evolutionary and differentiable search methods respectively are the main focus; the other char-
acteristics of the NAS methods are adapted from existing methods, but further study could advance specialization of
equivariance-aware NAS.","These two NAS approaches present adaptations of two standard types of NAS, evolutionary and differentiable, to
the search for optimal partial equivariance.","We next study empirically the two EquiNAS methods on three datasets, one with known
rotational symmetry and two with unknown but visually signiﬁcant rotational and reﬂectional symmetry.",2022-10-11 14:37:29+00:00,Architectural Optimization over Subgroups for Equivariant Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kaitlin Maile'), arxiv.Result.Author('Dennis G. Wilson'), arxiv.Result.Author('Patrick Forré')]","Incorporating equivariance to symmetry groups as a constraint during neural
network training can improve performance and generalization for tasks
exhibiting those symmetries, but such symmetries are often not perfectly nor
explicitly present. This motivates algorithmically optimizing the architectural
constraints imposed by equivariance. We propose the equivariance relaxation
morphism, which preserves functionality while reparameterizing a group
equivariant layer to operate with equivariance constraints on a subgroup, as
well as the [G]-mixed equivariant layer, which mixes layers constrained to
different groups to enable within-layer equivariance optimization. We further
present evolutionary and differentiable neural architecture search (NAS)
algorithms that utilize these mechanisms respectively for equivariance-aware
architectural optimization. Experiments across a variety of datasets show the
benefit of dynamically constrained equivariance to find effective architectures
with approximate equivariance."
12306,"It is an initial attempt to convert
results of deep learning research into an online tool that may stir further research interests in this direction.","Conclusion: The iMedBot web application provides a user-friendly interface for user-
agent interaction in conducting personalized prediction and model training.","Keywords: Deep learning, Breast Cancer, Web application, Model training.",2022-10-07 19:27:05+00:00,iMedBot: A Web-based Intelligent Agent for Healthcare Related Prediction and Deep Learning,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Author('Chuhan Xu'), arxiv.Result.Author('Xia Jiang')]","Background: Breast cancer is a multifactorial disease, genetic and
environmental factors will affect its incidence probability. Breast cancer
metastasis is one of the main cause of breast cancer related deaths reported by
the American Cancer Society (ACS). Method: the iMedBot is a web application
that we developed using the python Flask web framework and deployed on Amazon
Web Services. It contains a frontend and a backend. The backend is supported by
a python program we developed using the python Keras and scikit-learn packages,
which can be used to learn deep feedforward neural network (DFNN) models.
Result: the iMedBot can provide two main services: 1. it can predict 5-, 10-,
or 15-year breast cancer metastasis based on a set of clinical information
provided by a user. The prediction is done by using a set of DFNN models that
were pretrained, and 2. It can train DFNN models for a user using user-provided
dataset. The model trained will be evaluated using AUC and both the AUC value
and the AUC ROC curve will be provided. Conclusion: The iMedBot web application
provides a user-friendly interface for user-agent interaction in conducting
personalized prediction and model training. It is an initial attempt to convert
results of deep learning research into an online tool that may stir further
research interests in this direction. Keywords: Deep learning, Breast Cancer,
Web application, Model training."
12307,"It is an initial attempt to convert results of deep learning research into an
online tool that may stir further research interests in this direction.","The iMedBot
web application provides a user-friendly interface for user-agent interaction in conducting personalized
prediction and model training.","We plan on further expanding the
backend of iMedBot to include other services such as risk factor learning (both single and interactive),
causing learning, and clinical decision support.",2022-10-07 19:27:05+00:00,iMedBot: A Web-based Intelligent Agent for Healthcare Related Prediction and Deep Learning,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Author('Chuhan Xu'), arxiv.Result.Author('Xia Jiang')]","Background: Breast cancer is a multifactorial disease, genetic and
environmental factors will affect its incidence probability. Breast cancer
metastasis is one of the main cause of breast cancer related deaths reported by
the American Cancer Society (ACS). Method: the iMedBot is a web application
that we developed using the python Flask web framework and deployed on Amazon
Web Services. It contains a frontend and a backend. The backend is supported by
a python program we developed using the python Keras and scikit-learn packages,
which can be used to learn deep feedforward neural network (DFNN) models.
Result: the iMedBot can provide two main services: 1. it can predict 5-, 10-,
or 15-year breast cancer metastasis based on a set of clinical information
provided by a user. The prediction is done by using a set of DFNN models that
were pretrained, and 2. It can train DFNN models for a user using user-provided
dataset. The model trained will be evaluated using AUC and both the AUC value
and the AUC ROC curve will be provided. Conclusion: The iMedBot web application
provides a user-friendly interface for user-agent interaction in conducting
personalized prediction and model training. It is an initial attempt to convert
results of deep learning research into an online tool that may stir further
research interests in this direction. Keywords: Deep learning, Breast Cancer,
Web application, Model training."
12325,"It is worth mentioning that the risk measurement
space in proposed paradigm is diﬀerent from the traditional one, so further study
the more appropriate loss measurement is our main research directions in the future.","Experimental results show the eﬀectiveness of the proposed ε-L1VSVM and
the proposed indicator on validity and interpretability of classiﬁcation, especially in
case of insuﬃcient training data.","35
             85                                                                                                                                                                                           92

                                                                                                                                                                                                                                                                                     90.86

         84.5                                                                                                                                                                                             90

           84                                                                                                                                                                                                   88.67
         83.5
                                                                                                 83.95                                                                                                    88    87.60
           83                                                                                    83.66
         82.5                               83.70                                   83.34                                                                                                                                                              87.31  86.55      86.39
G-means                                     83.40                                                                                                                                                G-means  86                                           85.73  85.43     85.31
           82                                                         83.21
                       83.10

                                                                      82.81                                                                                                                               84
                                                                                                                                                                                                                                                83.20
                                                                                    82.63
                                                                                                                                                                                                          82
                       82.45         82.22
                                     82.27

         81.5                                                                                                                                                                                             80           79.82
           81
                                                                                                 Acc                                                                                                                                                                                 Acc
                                                                                                 Vac
                                                                                                                                                                                                                                                                                     Vac

                                                                                                                                                                                                          78

                 CSVM         LSSVM         VSVM                      IDLSSVM -L SVM -L VSVM                                                                                                                    CSVM   LSSVM                           VSVM IDLSSVM -L SVM -L VSVM
                                                                                1             1                                                                                                                                                                      1            1

                 (a) Cleveland-HeartS                                                                                    (b) Cleveland0v2                                                                       (c) Cleveland0v3

         85                                                                                                          85

         80                                                                                      83.26           84.5
                                                               79.46                             75.96             84

                 76.60           77.39      78.39                                                                83.5                                84.35                            84.28
                                                                                                                   83                                84.22                            83.98

         75            75.12                                                                                     82.5
                                                                                                                   82
                                                                      74.12                                                    83.89
                                                                                                                               83.71
G-means                                                                                                 G-means                                                          83.77

         70                                                                        70.09

                                                                                                                                                            83.28

         65                                                                        65.18                                                                    82.85

                                                                      64.83                                                                          82.68
                                                                                                                                                     82.33
         60                      60.29                                                           Acc                                          LSSVM VSVM                 82.55
                                                                                                                                                                                            Acc
                                                                                                 Vac                                                                                        Vac

                 CSVM         LSSVM     VSVM IDLSSVM -L SVM -L VSVM                                                      CSVM                               IDLSSVM -L SVM -L VSVM
                                                                                1             1                                                                       1            1

                 (d) Cleveland0v4                                                                                        (e) Cleverland0vr                                                                                    (f) Echo

         95                                                                                                      82                                                                                         98
                                                                                                                                                                                                          97.8
                                                                                                                 81                                                                                       97.6
                                                                                                                                                                                                          97.4
                              91.75                                          90.53               92.67           80                                                                   79.82               97.2
                              91.08                                                              92.17                                 79.43
                                                                                                                                                                                                            97
                                                                      89.43                                      79                                                                   78.98               96.8
                                                                                                                                                                                                          96.6
         90 88.68                                                                  90.24                                 78.47                              78.48                                         96.4                                         97.26                         97.27
                                                                                                                                                                                                          96.2                                         97.15                         97.15
                                                                                                                 78                                                                                                                                                     97.25
                                                                                                                                                                                                            96                                                          97.10
G-means                                                               88.69                             G-means                                                          77.52                   G-means               97.12
                                                                                                                                                                                                                       96.83
                                                                                                                 77                                         77.29

                   87.31                                                                                                                      76.81

                                                                                                                                                     76.35                                                      96.86                                         96.81
                                                                                                                                                                                                                96.73
                                                                                                                 76                           75.91

         85                                                                                                      75
                                                            84.62

                                                                                                                                                                                                                                                              96.47

                                                                                                                 74

                                            82.42                                                                                                    73.53               73.59

                                                                                                 Acc             73                                                                   Acc                                                                                            Acc
                                                                                                 Vac                                                                                                                                                                                 Vac
                                                                                                                                                                                      Vac

         80                                                                                                      72

                 CSVM     LSSVM      VSVM IDLSSVM -L SVM -L VSVM                                                         CSVM                 LSSVM  VSVM IDLSSVM -L SVM -L VSVM                                CSVM   LSSVM                           VSVM  IDLSSVM -L SVM -L VSVM
                                                                             1             1                                                                       1            1                                                                                    1         1

                              (g) Monks3                                                                                                      (h) Ecoli                                                                (i) Ecoli0vr

Figure 5: Nonlinear results based on Acc (45) and Vac (46) as evaluation indicators for six classiﬁers
on nine small datasets.",2022-10-12 06:36:14+00:00,Classification by estimating the cumulative distribution function for small data,cs.LG,['cs.LG'],"[arxiv.Result.Author('Meng-Xian Zhua'), arxiv.Result.Author('Yuan-Hai Shao')]","In this paper, we study the classification problem by estimating the
conditional probability function of the given data. Different from the
traditional expected risk estimation theory on empirical data, we calculate the
probability via Fredholm equation, this leads to estimate the distribution of
the data. Based on the Fredholm equation, a new expected risk estimation theory
by estimating the cumulative distribution function is presented. The main
characteristics of the new expected risk estimation is to measure the risk on
the distribution of the input space. The corresponding empirical risk
estimation is also presented, and an $\varepsilon$-insensitive $L_{1}$
cumulative support vector machines ($\varepsilon$-$L_{1}$VSVM) is proposed by
introducing an insensitive loss. It is worth mentioning that the classification
models and the classification evaluation indicators based on the new mechanism
are different from the traditional one. Experimental results show the
effectiveness of the proposed $\varepsilon$-$L_{1}$VSVM and the corresponding
cumulative distribution function indicator on validity and interpretability of
small data classification."
12326,"It is worth mentioning that the risk measurement
space in proposed paradigm is diﬀerent from the traditional one, so further study
the more appropriate loss measurement is our main research directions in the future.","Experimental results show the eﬀectiveness of the proposed ε-L1VSVM and
the proposed indicator on validity and interpretability of classiﬁcation, especially in
case of insuﬃcient training data.","35
             85                                                                                                                                                                                           92

                                                                                                                                                                                                                                                                                     90.86

         84.5                                                                                                                                                                                             90

           84                                                                                                                                                                                                   88.67
         83.5
                                                                                                 83.95                                                                                                    88    87.60
           83                                                                                    83.66
         82.5                               83.70                                   83.34                                                                                                                                                              87.31  86.55      86.39
G-means                                     83.40                                                                                                                                                G-means  86                                           85.73  85.43     85.31
           82                                                         83.21
                       83.10

                                                                      82.81                                                                                                                               84
                                                                                                                                                                                                                                                83.20
                                                                                    82.63
                                                                                                                                                                                                          82
                       82.45         82.22
                                     82.27

         81.5                                                                                                                                                                                             80           79.82
           81
                                                                                                 Acc                                                                                                                                                                                 Acc
                                                                                                 Vac
                                                                                                                                                                                                                                                                                     Vac

                                                                                                                                                                                                          78

                 CSVM         LSSVM         VSVM                      IDLSSVM -L SVM -L VSVM                                                                                                                    CSVM   LSSVM                           VSVM IDLSSVM -L SVM -L VSVM
                                                                                1             1                                                                                                                                                                      1            1

                 (a) Cleveland-HeartS                                                                                    (b) Cleveland0v2                                                                       (c) Cleveland0v3

         85                                                                                                          85

         80                                                                                      83.26           84.5
                                                               79.46                             75.96             84

                 76.60           77.39      78.39                                                                83.5                                84.35                            84.28
                                                                                                                   83                                84.22                            83.98

         75            75.12                                                                                     82.5
                                                                                                                   82
                                                                      74.12                                                    83.89
                                                                                                                               83.71
G-means                                                                                                 G-means                                                          83.77

         70                                                                        70.09

                                                                                                                                                            83.28

         65                                                                        65.18                                                                    82.85

                                                                      64.83                                                                          82.68
                                                                                                                                                     82.33
         60                      60.29                                                           Acc                                          LSSVM VSVM                 82.55
                                                                                                                                                                                            Acc
                                                                                                 Vac                                                                                        Vac

                 CSVM         LSSVM     VSVM IDLSSVM -L SVM -L VSVM                                                      CSVM                               IDLSSVM -L SVM -L VSVM
                                                                                1             1                                                                       1            1

                 (d) Cleveland0v4                                                                                        (e) Cleverland0vr                                                                                    (f) Echo

         95                                                                                                      82                                                                                         98
                                                                                                                                                                                                          97.8
                                                                                                                 81                                                                                       97.6
                                                                                                                                                                                                          97.4
                              91.75                                          90.53               92.67           80                                                                   79.82               97.2
                              91.08                                                              92.17                                 79.43
                                                                                                                                                                                                            97
                                                                      89.43                                      79                                                                   78.98               96.8
                                                                                                                                                                                                          96.6
         90 88.68                                                                  90.24                                 78.47                              78.48                                         96.4                                         97.26                         97.27
                                                                                                                                                                                                          96.2                                         97.15                         97.15
                                                                                                                 78                                                                                                                                                     97.25
                                                                                                                                                                                                            96                                                          97.10
G-means                                                               88.69                             G-means                                                          77.52                   G-means               97.12
                                                                                                                                                                                                                       96.83
                                                                                                                 77                                         77.29

                   87.31                                                                                                                      76.81

                                                                                                                                                     76.35                                                      96.86                                         96.81
                                                                                                                                                                                                                96.73
                                                                                                                 76                           75.91

         85                                                                                                      75
                                                            84.62

                                                                                                                                                                                                                                                              96.47

                                                                                                                 74

                                            82.42                                                                                                    73.53               73.59

                                                                                                 Acc             73                                                                   Acc                                                                                            Acc
                                                                                                 Vac                                                                                                                                                                                 Vac
                                                                                                                                                                                      Vac

         80                                                                                                      72

                 CSVM     LSSVM      VSVM IDLSSVM -L SVM -L VSVM                                                         CSVM                 LSSVM  VSVM IDLSSVM -L SVM -L VSVM                                CSVM   LSSVM                           VSVM  IDLSSVM -L SVM -L VSVM
                                                                             1             1                                                                       1            1                                                                                    1         1

                              (g) Monks3                                                                                                      (h) Ecoli                                                                (i) Ecoli0vr

Figure 5: Nonlinear results based on Acc (45) and Vac (46) as evaluation indicators for six classiﬁers
on nine small datasets.",2022-10-12 06:36:14+00:00,Classification by estimating the cumulative distribution function for small data,cs.LG,['cs.LG'],"[arxiv.Result.Author('Meng-Xian Zhu'), arxiv.Result.Author('Yuan-Hai Shao')]","In this paper, we study the classification problem by estimating the
conditional probability function of the given data. Different from the
traditional expected risk estimation theory on empirical data, we calculate the
probability via Fredholm equation, this leads to estimate the distribution of
the data. Based on the Fredholm equation, a new expected risk estimation theory
by estimating the cumulative distribution function is presented. The main
characteristics of the new expected risk estimation is to measure the risk on
the distribution of the input space. The corresponding empirical risk
estimation is also presented, and an $\varepsilon$-insensitive $L_{1}$
cumulative support vector machines ($\varepsilon$-$L_{1}VSVM$) is proposed by
introducing an insensitive loss. It is worth mentioning that the classification
models and the classification evaluation indicators based on the new mechanism
are different from the traditional one. Experimental results show the
effectiveness of the proposed $\varepsilon$-$L_{1}VSVM$ and the corresponding
cumulative distribution function indicator on validity and interpretability of
small data classification."
12327,"We hope our work will
serve as a powerful and reproducible agent and motivate further research in model-based ofﬂine
reinforcement learning.","The empirical
investigation veriﬁed our hypotheses and the benchmark results demonstrate that our proposed al-
gorithm can achieve state-of-the-art results with low experimentation cost.","9
6 ETHICS STATEMENT

This paper does not raise any ethical concerns.",2022-10-12 07:41:04+00:00,Efficient Offline Policy Optimization with a Learned Model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zichen Liu'), arxiv.Result.Author('Siyi Li'), arxiv.Result.Author('Wee Sun Lee'), arxiv.Result.Author('Shuicheng Yan'), arxiv.Result.Author('Zhongwen Xu')]","MuZero Unplugged presents a promising approach for offline policy learning
from logged data. It conducts Monte-Carlo Tree Search (MCTS) with a learned
model and leverages Reanalyze algorithm to learn purely from offline data. For
good performance, MCTS requires accurate learned models and a large number of
simulations, thus costing huge computing time. This paper investigates a few
hypotheses where MuZero Unplugged may not work well under the offline RL
settings, including 1) learning with limited data coverage; 2) learning from
offline data of stochastic environments; 3) improperly parameterized models
given the offline data; 4) with a low compute budget. We propose to use a
regularized one-step look-ahead approach to tackle the above issues. Instead of
planning with the expensive MCTS, we use the learned model to construct an
advantage estimation based on a one-step rollout. Policy improvements are
towards the direction that maximizes the estimated advantage with
regularization of the dataset. We conduct extensive empirical studies with
BSuite environments to verify the hypotheses and then run our algorithm on the
RL Unplugged Atari benchmark. Experimental results show that our proposed
approach achieves stable performance even with an inaccurate learned model. On
the large-scale Atari benchmark, the proposed method outperforms MuZero
Unplugged by 43%. Most significantly, it uses only 5.6% wall-clock time (i.e.,
1 hour) compared to MuZero Unplugged (i.e., 17.8 hours) to achieve a 150% IQM
normalized score with the same hardware and software stacks."
12338,"While the relation between generalization
and hypercontractivity is under explored, with the small-ball theorem as a notable exception
[Lecué and Mendelson, 2017, Mendelson, 2014], we believe it is a fundamental concept in con-
temporary deep nets that require further research.","Our generalization bounds also imply that deep nets
hyper-contracts their input and therefore generalize.","Additionally, our framework focuses on bounding
the complexity term.",2022-10-12 12:49:20+00:00,On the Importance of Gradient Norm in PAC-Bayesian Bounds,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Itai Gat'), arxiv.Result.Author('Yossi Adi'), arxiv.Result.Author('Alexander Schwing'), arxiv.Result.Author('Tamir Hazan')]","Generalization bounds which assess the difference between the true risk and
the empirical risk, have been studied extensively. However, to obtain bounds,
current techniques use strict assumptions such as a uniformly bounded or a
Lipschitz loss function. To avoid these assumptions, in this paper, we follow
an alternative approach: we relax uniform bounds assumptions by using
on-average bounded loss and on-average bounded gradient norm assumptions.
Following this relaxation, we propose a new generalization bound that exploits
the contractivity of the log-Sobolev inequalities. These inequalities add an
additional loss-gradient norm term to the generalization bound, which is
intuitively a surrogate of the model complexity. We apply the proposed bound on
Bayesian deep nets and empirically analyze the effect of this new loss-gradient
norm term on different neural architectures."
12339,"C.2 Gradient statistics

We further study the gradient norm statistics, we report the max and mean values of the gradient
norm (not squared) using the same networks described in Section.","However, it is important to note that when using any generalization bound, one should care
about the training loss as well as the complexity term.","4 in the main paper:

# LAYERS  MNIST (MEAN)  MNIST (MAX)              CIFAR (MEAN)  CIFAR (MAX)

     1         0.00224      0.00267                   0.0258       0.0314
     2         0.00088       0.0012                    0.011        0.015
     3         0.00036      0.00056                   0.0049        0.008

We observe that the maximum value of the gradient norm is not higher than twice the mean value.",2022-10-12 12:49:20+00:00,On the Importance of Gradient Norm in PAC-Bayesian Bounds,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Itai Gat'), arxiv.Result.Author('Yossi Adi'), arxiv.Result.Author('Alexander Schwing'), arxiv.Result.Author('Tamir Hazan')]","Generalization bounds which assess the difference between the true risk and
the empirical risk, have been studied extensively. However, to obtain bounds,
current techniques use strict assumptions such as a uniformly bounded or a
Lipschitz loss function. To avoid these assumptions, in this paper, we follow
an alternative approach: we relax uniform bounds assumptions by using
on-average bounded loss and on-average bounded gradient norm assumptions.
Following this relaxation, we propose a new generalization bound that exploits
the contractivity of the log-Sobolev inequalities. These inequalities add an
additional loss-gradient norm term to the generalization bound, which is
intuitively a surrogate of the model complexity. We apply the proposed bound on
Bayesian deep nets and empirically analyze the effect of this new loss-gradient
norm term on different neural architectures."
12340,"While the relation between generalization
and hypercontractivity is under explored, with the small-ball theorem as a notable exception
[Lecué and Mendelson, 2017, Mendelson, 2014], we believe it is a fundamental concept in con-
temporary deep nets that require further research.","Our generalization bounds also imply that deep nets
hyper-contracts their input and therefore generalize.","Additionally, our framework focuses on bounding
the complexity term.",2022-10-12 12:49:20+00:00,On the Importance of Gradient Norm in PAC-Bayesian Bounds,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Itai Gat'), arxiv.Result.Author('Yossi Adi'), arxiv.Result.Author('Alexander Schwing'), arxiv.Result.Author('Tamir Hazan')]","Generalization bounds which assess the difference between the true risk and
the empirical risk, have been studied extensively. However, to obtain bounds,
current techniques use strict assumptions such as a uniformly bounded or a
Lipschitz loss function. To avoid these assumptions, in this paper, we follow
an alternative approach: we relax uniform bounds assumptions by using
on-average bounded loss and on-average bounded gradient norm assumptions.
Following this relaxation, we propose a new generalization bound that exploits
the contractivity of the log-Sobolev inequalities. These inequalities add an
additional loss-gradient norm term to the generalization bound, which is
intuitively a surrogate of the model complexity. We apply the proposed bound on
Bayesian deep nets and empirically analyze the effect of this new loss-gradient
norm term on different neural architectures."
12341,"C.2 Gradient statistics

We further study the gradient norm statistics, we report the max and mean values of the gradient
norm (not squared) using the same networks described in Section.","However, it is important to note that when using any generalization bound, one should care
about the training loss as well as the complexity term.","4 in the main paper:

# LAYERS  MNIST (MEAN)  MNIST (MAX)              CIFAR (MEAN)  CIFAR (MAX)

     1         0.00224      0.00267                   0.0258       0.0314
     2         0.00088       0.0012                    0.011        0.015
     3         0.00036      0.00056                   0.0049        0.008

We observe that the maximum value of the gradient norm is not higher than twice the mean value.",2022-10-12 12:49:20+00:00,On the Importance of Gradient Norm in PAC-Bayesian Bounds,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Itai Gat'), arxiv.Result.Author('Yossi Adi'), arxiv.Result.Author('Alexander Schwing'), arxiv.Result.Author('Tamir Hazan')]","Generalization bounds which assess the difference between the true risk and
the empirical risk, have been studied extensively. However, to obtain bounds,
current techniques use strict assumptions such as a uniformly bounded or a
Lipschitz loss function. To avoid these assumptions, in this paper, we follow
an alternative approach: we relax uniform bounds assumptions by using
on-average bounded loss and on-average bounded gradient norm assumptions.
Following this relaxation, we propose a new generalization bound that exploits
the contractivity of the log-Sobolev inequalities. These inequalities add an
additional loss-gradient norm term to the generalization bound, which is
intuitively a surrogate of the model complexity. We apply the proposed bound on
Bayesian deep nets and empirically analyze the effect of this new loss-gradient
norm term on different neural architectures."
12345,"This is    trade-off in the context of online task transfer in reinforcement
                                        powerful because it dramatically reduces the dependence on          learning, and question what exploration approaches should be
                                        large amounts of task-speciﬁc data to produce usable perfor-        further researched when considering transfer efﬁciency and
                                        mance and makes training new models much more efﬁcient              performance in reinforcement learning .","Transfer learning is built on the assumption that knowledge         In this paper, we re-examine RL’s exploration-exploitation
                                        learned in an initial task can generalize to a new task.","We deﬁne a taxonomy
                                        than training tabula rasa, or without prior knowledge [4].",2022-10-11 01:23:21+00:00,The Role of Exploration for Task Transfer in Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jonathan C Balloch'), arxiv.Result.Author('Julia Kim'), arxiv.Result.Author('and Jessica L Inman'), arxiv.Result.Author('Mark O Riedl')]","The exploration--exploitation trade-off in reinforcement learning (RL) is a
well-known and much-studied problem that balances greedy action selection with
novel experience, and the study of exploration methods is usually only
considered in the context of learning the optimal policy for a single learning
task. However, in the context of online task transfer, where there is a change
to the task during online operation, we hypothesize that exploration strategies
that anticipate the need to adapt to future tasks can have a pronounced impact
on the efficiency of transfer. As such, we re-examine the
exploration--exploitation trade-off in the context of transfer learning. In
this work, we review reinforcement learning exploration methods, define a
taxonomy with which to organize them, analyze these methods' differences in the
context of task transfer, and suggest avenues for future investigation."
12346,"Due to our use of L2O, LODO also has ﬂexibility in the design of its
linear neural network, which makes it amenable to further research and reﬁnement.","Our image generation task
demonstrates that LODO succeeds in a semi-realistic stochastic nonconvex task where other quasi-
Newton optimizers diverge.","Relative to L2O, LODO offers advantages associated with the restructuring of the outer and inner
loop into a single loop.",2022-10-11 03:47:14+00:00,Learning to Optimize Quasi-Newton Methods,cs.LG,"['cs.LG', 'I.2.6']","[arxiv.Result.Author('Isaac Liao'), arxiv.Result.Author('Rumen R. Dangovski'), arxiv.Result.Author('Jakob N. Foerster'), arxiv.Result.Author('Marin Soljačić')]","We introduce a novel machine learning optimizer called LODO, which online
meta-learns an implicit inverse Hessian of the loss as a subroutine of
quasi-Newton optimization. Our optimizer merges Learning to Optimize (L2O)
techniques with quasi-Newton methods to learn neural representations of
symmetric matrix vector products, which are more flexible than those in other
quasi-Newton methods. Unlike other L2O methods, ours does not require any
meta-training on a training task distribution, and instead learns to optimize
on the fly while optimizing on the test task, adapting to the local
characteristics of the loss landscape while traversing it. Theoretically, we
show that our optimizer approximates the inverse Hessian in noisy loss
landscapes and is capable of representing a wide range of inverse Hessians. We
experimentally verify our algorithm's performance in the presence of noise, and
show that simpler alternatives for representing the inverse Hessians worsen
performance. Lastly, we use our optimizer to train a semi-realistic deep neural
network with 95k parameters, and obtain competitive results against standard
neural network optimizers."
12381,"An intriguing
topic for further study is to clarify the connection between e-CMI and Gaussian complexity.","It should also be noted that
the complexity measures that we consider differ from the Gaussian complexity in [5].","Acknowledgements

This work was partly supported by the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation and the Chalmers AI Research Center
(CHAIR).",2022-10-12 18:10:59+00:00,Evaluated CMI Bounds for Meta Learning: Tightness and Expressiveness,cs.LG,"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']","[arxiv.Result.Author('Fredrik Hellström'), arxiv.Result.Author('Giuseppe Durisi')]","Recent work has established that the conditional mutual information (CMI)
framework of Steinke and Zakynthinou (2020) is expressive enough to capture
generalization guarantees in terms of algorithmic stability, VC dimension, and
related complexity measures for conventional learning (Harutyunyan et al.,
2021, Haghifam et al., 2021). Hence, it provides a unified method for
establishing generalization bounds. In meta learning, there has so far been a
divide between information-theoretic results and results from classical
learning theory. In this work, we take a first step toward bridging this
divide. Specifically, we present novel generalization bounds for meta learning
in terms of the evaluated CMI (e-CMI). To demonstrate the expressiveness of the
e-CMI framework, we apply our bounds to a representation learning setting, with
$n$ samples from $\hat n$ tasks parameterized by functions of the form $f_i
\circ h$. Here, each $f_i \in \mathcal F$ is a task-specific function, and $h
\in \mathcal H$ is the shared representation. For this setup, we show that the
e-CMI framework yields a bound that scales as $\sqrt{ \mathcal C(\mathcal
H)/(n\hat n) + \mathcal C(\mathcal F)/n} $, where $\mathcal C(\cdot)$ denotes a
complexity measure of the hypothesis class. This scaling behavior coincides
with the one reported in Tripuraneni et al. (2020) using Gaussian complexity."
12392,"We further study the effect of using calibrated pre-trained models coupled
                                        with calibration during training to guide sample prioritization, which again seems
                                        to improve the quality of samples selected.","We study the effect of popular calibration techniques in selecting
                                        better subsets of samples during training (also called sample prioritization) and
                                        observe that calibration can improve the quality of subsets, reduce the number of
                                        examples per epoch (by at least 70%), and can thereby speed up the overall training
                                        process.","1 Introduction

                                        Calibration is a widely used technique in machine learning to reduce overconﬁdence in predictions.",2022-10-12 21:11:08+00:00,Can Calibration Improve Sample Prioritization?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ganesh Tata'), arxiv.Result.Author('Gautham Krishna Gudur'), arxiv.Result.Author('Gopinath Chennupati'), arxiv.Result.Author('Mohammad Emtiyaz Khan')]","Calibration can reduce overconfident predictions of deep neural networks, but
can calibration also accelerate training by selecting the right samples? In
this paper, we show that it can. We study the effect of popular calibration
techniques in selecting better subsets of samples during training (also called
sample prioritization) and observe that calibration can improve the quality of
subsets, reduce the number of examples per epoch (by at least 70%), and can
thereby speed up the overall training process. We further study the effect of
using calibrated pre-trained models coupled with calibration during training to
guide sample prioritization, which again seems to improve the quality of
samples selected."
12393,"We further study the effect
                                        of using calibrated pre-trained models coupled with calibration during training to
                                        guide sample prioritization, which again seems to improve the quality of samples
                                        selected.","We study the effect of
                                        popular calibration techniques in selecting better subsets of samples during training
                                        (also called sample prioritization) and observe that calibration can improve the
                                        quality of subsets, reduce the number of examples per epoch (by at least 70%),
                                        and can thereby speed up the overall training process.","1 Introduction

                                        Calibration is a widely used technique in machine learning to reduce overconﬁdence in predictions.",2022-10-12 21:11:08+00:00,Can Calibration Improve Sample Prioritization?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Ganesh Tata'), arxiv.Result.Author('Gautham Krishna Gudur'), arxiv.Result.Author('Gopinath Chennupati'), arxiv.Result.Author('Mohammad Emtiyaz Khan')]","Calibration can reduce overconfident predictions of deep neural networks, but
can calibration also accelerate training? In this paper, we show that it can
when used to prioritize some examples for performing subset selection. We study
the effect of popular calibration techniques in selecting better subsets of
samples during training (also called sample prioritization) and observe that
calibration can improve the quality of subsets, reduce the number of examples
per epoch (by at least 70%), and can thereby speed up the overall training
process. We further study the effect of using calibrated pre-trained models
coupled with calibration during training to guide sample prioritization, which
again seems to improve the quality of samples selected."
12395,We believe WaM opens the door to extensive further research.,"It serves as a simple measure of fairness
that can be easily grasped by nontechnical people, and it is a solution to the X,S
correlation problem.","We mention two
possible areas:

    • Extension to other fairness and predictive power measures: In Section 8, we
       extended WaM to False Positive Rates.",2022-10-13 02:29:34+00:00,Walk a Mile in Their Shoes: a New Fairness Criterion for Machine Learning,cs.LG,"['cs.LG', 'cs.CY']",[arxiv.Result.Author('Norman Matloff')],"The old empathetic adage, ``Walk a mile in their shoes,'' asks that one
imagine the difficulties others may face. This suggests a new ML counterfactual
fairness criterion, based on a \textit{group} level: How would members of a
nonprotected group fare if their group were subject to conditions in some
protected group? Instead of asking what sentence would a particular Caucasian
convict receive if he were Black, take that notion to entire groups; e.g. how
would the average sentence for all White convicts change if they were Black,
but with their same White characteristics, e.g. same number of prior
convictions? We frame the problem and study it empirically, for different
datasets. Our approach also is a solution to the problem of covariate
correlation with sensitive attributes."
12396,"For such purposes, explicit formulas for estimator vari-
       ance such as (17) can be helpful, and worthy of further study.","of sample size,

                                                18
       needed in a study.","Extension of
       (17) to the generalized linear model, notably the logistic and Poisson regres-
       sion cases, should be pursued.",2022-10-13 02:29:34+00:00,Walk a Mile in Their Shoes: a New Fairness Criterion for Machine Learning,cs.LG,"['cs.LG', 'cs.CY']",[arxiv.Result.Author('Norman Matloff')],"The old empathetic adage, ``Walk a mile in their shoes,'' asks that one
imagine the difficulties others may face. This suggests a new ML counterfactual
fairness criterion, based on a \textit{group} level: How would members of a
nonprotected group fare if their group were subject to conditions in some
protected group? Instead of asking what sentence would a particular Caucasian
convict receive if he were Black, take that notion to entire groups; e.g. how
would the average sentence for all White convicts change if they were Black,
but with their same White characteristics, e.g. same number of prior
convictions? We frame the problem and study it empirically, for different
datasets. Our approach also is a solution to the problem of covariate
correlation with sensitive attributes."
12398,"We
expect our work can inspire further researches for both exploring network representative potential
and network compression.","We
conduct comprehensive experiments based on several popular network architectures to explore the
random weights potential for PEMN and test the compression performance of our new paradigm.","References

 [1] S. Bai, J.",2022-10-13 03:39:03+00:00,Parameter-Efficient Masking Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yue Bai'), arxiv.Result.Author('Huan Wang'), arxiv.Result.Author('Xu Ma'), arxiv.Result.Author('Yitian Zhang'), arxiv.Result.Author('Zhiqiang Tao'), arxiv.Result.Author('Yun Fu')]","A deeper network structure generally handles more complicated non-linearity
and performs more competitively. Nowadays, advanced network designs often
contain a large number of repetitive structures (e.g., Transformer). They
empower the network capacity to a new level but also increase the model size
inevitably, which is unfriendly to either model restoring or transferring. In
this study, we are the first to investigate the representative potential of
fixed random weights with limited unique values by learning diverse masks and
introduce the Parameter-Efficient Masking Networks (PEMN). It also naturally
leads to a new paradigm for model compression to diminish the model size.
Concretely, motivated by the repetitive structures in modern neural networks,
we utilize one random initialized layer, accompanied with different masks, to
convey different feature mappings and represent repetitive network modules.
Therefore, the model can be expressed as \textit{one-layer} with a bunch of
masks, which significantly reduce the model storage cost. Furthermore, we
enhance our strategy by learning masks for a model filled by padding a given
random weights vector. In this way, our method can further lower the space
complexity, especially for models without many repetitive architectures. We
validate the potential of PEMN learning masks on random weights with limited
unique values and test its effectiveness for a new compression paradigm based
on different network architectures. Code is available at
https://github.com/yueb17/PEMN"
12399,"To further study the sensitivity of random DA to the
cient, effective, and practical automated data augmentation      selection of transformations and magnitude, we run exper-
method for biobehavioral time series data.","Therefore, random DA is an efﬁ-          tude.","iments of random DA with different number of operations
                                                                 and magnitude on the PTB-XL dataset.",2022-10-13 03:40:12+00:00,Empirical Evaluation of Data Augmentations for Biobehavioral Time Series Data with Deep Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Huiyuan Yang'), arxiv.Result.Author('Han Yu'), arxiv.Result.Author('Akane Sano')]","Deep learning has performed remarkably well on many tasks recently. However,
the superior performance of deep models relies heavily on the availability of a
large number of training data, which limits the wide adaptation of deep models
on various clinical and affective computing tasks, as the labeled data are
usually very limited. As an effective technique to increase the data
variability and thus train deep models with better generalization, data
augmentation (DA) is a critical step for the success of deep learning models on
biobehavioral time series data. However, the effectiveness of various DAs for
different datasets with different tasks and deep models is understudied for
biobehavioral time series data. In this paper, we first systematically review
eight basic DA methods for biobehavioral time series data, and evaluate the
effects on seven datasets with three backbones. Next, we explore adapting more
recent DA techniques (i.e., automatic augmentation, random augmentation) to
biobehavioral time series data by designing a new policy architecture
applicable to time series data. Last, we try to answer the question of why a DA
is effective (or not) by first summarizing two desired attributes for
augmentations (challenging and faithful), and then utilizing two metrics to
quantitatively measure the corresponding attributes, which can guide us in the
search for more effective DA for biobehavioral time series data by designing
more challenging but still faithful transformations. Our code and results are
available at Link."
12403,"Based on the results from orthogonal equivariance, we further study the permutation equivariance
on the elements of the input sequence.","This reveals one possible reason behind the success of self-attention based models on language
problems.","Under this symmetry, we show that seq2seq functions with
knowledge have a further reduced form which only involves four different nonlinear functions.",2022-10-13 05:10:48+00:00,Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Chao Ma'), arxiv.Result.Author('Lexing Ying')]","In this paper, we show that structures similar to self-attention are natural
to learn many sequence-to-sequence problems from the perspective of symmetry.
Inspired by language processing applications, we study the orthogonal
equivariance of seq2seq functions with knowledge, which are functions taking
two inputs -- an input sequence and a ``knowledge'' -- and outputting another
sequence. The knowledge consists of a set of vectors in the same embedding
space as the input sequence, containing the information of the language used to
process the input sequence. We show that orthogonal equivariance in the
embedding space is natural for seq2seq functions with knowledge, and under such
equivariance the function must take the form close to the self-attention. This
shows that network structures similar to self-attention are the right
structures to represent the target function of many seq2seq problems. The
representation can be further refined if a ``finite information principle'' is
considered, or a permutation equivariance holds for the elements of the input
sequence."
12447,"We thus hope that our results encourage further research into the application
of disentanglement to the open problem of robust generalization in more realistic settings.","We ﬁnd this improvement in
disentanglement across correlation shifts to be also reﬂected in improved out-of-distribution gen-
eralization especially as these shifts become more severe; tackling a key promise for disentangled
representation learning.","ACKNOWLEDGEMENTS

The authors would like to thank Le´on Bottou for useful and encouraging early discussions, as well
as David Lopez-Paz, Mike Rabbat, and Badr Youbi Idrissi for their careful reading and feedback
that helped improve the paper.",2022-10-13 20:46:42+00:00,Disentanglement of Correlated Factors via Hausdorff Factorized Support,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Karsten Roth'), arxiv.Result.Author('Mark Ibrahim'), arxiv.Result.Author('Zeynep Akata'), arxiv.Result.Author('Pascal Vincent'), arxiv.Result.Author('Diane Bouchacourt')]","A grand goal in deep learning research is to learn representations capable of
generalizing across distribution shifts. Disentanglement is one promising
direction aimed at aligning a models representations with the underlying
factors generating the data (e.g. color or background). Existing
disentanglement methods, however, rely on an often unrealistic assumption: that
factors are statistically independent. In reality, factors (like object color
and shape) are correlated. To address this limitation, we propose a relaxed
disentanglement criterion - the Hausdorff Factorized Support (HFS) criterion -
that encourages a factorized support, rather than a factorial distribution, by
minimizing a Hausdorff distance. This allows for arbitrary distributions of the
factors over their support, including correlations between them. We show that
the use of HFS consistently facilitates disentanglement and recovery of
ground-truth factors across a variety of correlation settings and benchmarks,
even under severe training correlations and correlation shifts, with in parts
over +60% in relative improvement over existing disentanglement methods. In
addition, we find that leveraging HFS for representation learning can even
facilitate transfer to downstream tasks such as classification under
distribution shifts. We hope our original approach and positive empirical
results inspire further progress on the open problem of robust generalization."
12459,"The number of optimality criteria derived in
this paper can guide further research in this area and the presented approximate solvers may serve as
competitive baseline for new ML-based approaches.","Moreover, BOT combines both combinatorial and convex optimization and could be an inspiring
problem to be solved by machine learning techniques.","Acknowledgments and Disclosure of Funding

We would like to thank Jarosław Piersa for sharing his code with us for a comparison to his work.",2022-10-14 10:51:16+00:00,Theory and Approximate Solvers for Branched Optimal Transport with Multiple Sources,cs.LG,"['cs.LG', 'math.CO', 'math.OC']","[arxiv.Result.Author('Peter Lippmann'), arxiv.Result.Author('Enrique Fita Sanmartín'), arxiv.Result.Author('Fred A. Hamprecht')]","Branched Optimal Transport (BOT) is a generalization of optimal transport in
which transportation costs along an edge are subadditive. This subadditivity
models an increase in transport efficiency when shipping mass along the same
route, favoring branched transportation networks. We here study the NP-hard
optimization of BOT networks connecting a finite number of sources and sinks in
$\mathbb{R}^2$. First, we show how to efficiently find the best geometry of a
BOT network for many sources and sinks, given a topology. Second, we argue that
a topology with more than three edges meeting at a branching point is never
optimal. Third, we show that the results obtained for the Euclidean plane
generalize directly to optimal transportation networks on two-dimensional
Riemannian manifolds. Finally, we present a simple but effective approximate
BOT solver combining geometric optimization with a combinatorial optimization
of the network topology."
12465,"A possible approach to improving the prediction in the                                We further study the sensitivity of a few key parameters by varying
explorative setting is to model the dependencies between locations.","As shown in Table 42, the overall results in the explorative setting
are much lower than those in the main and recurring settings, which                              Hyperparameter Sensitivity Analysis
is intuitive because of the inherent difficulty of predicting unseen
locations.","In addition, due to the larger number of locations in the TKY dataset,                           each   parameter    while    keeping    others  constant.",2022-10-14 12:56:01+00:00,HGARN: Hierarchical Graph Attention Recurrent Network for Human Mobility Prediction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yihong Tang'), arxiv.Result.Author('Junlin He'), arxiv.Result.Author('Zhan Zhao')]","Human mobility prediction is a fundamental task essential for various
applications, including urban planning, transportation services, and location
recommendation. Existing approaches often ignore activity information crucial
for reasoning human preferences and routines, or adopt a simplified
representation of the dependencies between time, activities and locations. To
address these issues, we present Hierarchical Graph Attention Recurrent Network
(HGARN) for human mobility prediction. Specifically, we construct a
hierarchical graph based on all users' history mobility records and employ a
Hierarchical Graph Attention Module to capture complex time-activity-location
dependencies. This way, HGARN can learn representations with rich contextual
semantics to model user preferences at the global level. We also propose a
model-agnostic history-enhanced confidence (MaHec) label to focus our model on
each user's individual-level preferences. Finally, we introduce a Recurrent
Encoder-Decoder Module, which employs recurrent structures to jointly predict
users' next activities (as an auxiliary task) and locations. For model
evaluation, we test the performances of our Hgarn against existing SOTAs in
recurring and explorative settings. The recurring setting focuses more on
assessing models' capabilities to capture users' individual-level preferences.
In contrast, the results in the explorative setting tend to reflect the power
of different models to learn users' global-level preferences. Overall, our
model outperforms other baselines significantly in the main, recurring, and
explorative settings based on two real-world human mobility data benchmarks.
Source codes of HGARN are available at https://github.com/YihongT/HGARN."
12476,"No representations, war-
                                                                ranties or undertakings (express or implied) are given
gested for further research to compare αQBoost against          as to the accuracy or completeness of the information
                                                                in this communication, and none of DTTL, its member
other well-known and well-performing classiﬁers.","To fully test the eﬃcacy of this algorithm, with          Before making any decision or taking any action that may
a large enough data set that may be split into training,        aﬀect your ﬁnances or your business, you should consult
testing, backtesting, and out-of-sample sets, it is sug-        a qualiﬁed professional adviser.","ﬁrms, related entities, employees or agents shall be liable
                                                                or responsible for any loss or damage whatsoever arising
                        VII.",2022-10-14 17:28:08+00:00,$α$QBoost: An Iteratively Weighted Adiabatic Trained Classifier,cs.LG,"['cs.LG', 'quant-ph']","[arxiv.Result.Author('Salvatore Certo'), arxiv.Result.Author('Andrew Vlasic'), arxiv.Result.Author('Daniel Beaulieu')]","A new implementation of an adiabatically-trained ensemble model is derived
that shows significant improvements over classical methods. In particular,
empirical results of this new algorithm show that it offers not just higher
performance, but also more stability with less classifiers, an attribute that
is critically important in areas like explainability and speed-of-inference. In
all, the empirical analysis displays that the algorithm can provide an increase
in performance on unseen data by strengthening stability of the statistical
model through further minimizing and balancing variance and bias, while
decreasing the time to convergence over its predecessors."
12479,"Furthermore, we point out to the cost of estimating the gradient
of the regularization terms proposed here as another potential subject for further research.","Here, we focused our discussion and experiments on DARTS, however the analysis and ideas are
extendable to any other type of differentiable NAS model that performs cell search using weight sharing,
which we plan to study in future works.","ACKNOWLEDGMENTS

We thank colleagues and students at the University of Tehran for all the useful discussions.",2022-10-14 17:54:01+00:00,$Λ$-DARTS: Mitigating Performance Collapse by Harmonizing Operation Selection among Cells,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Sajad Movahedi'), arxiv.Result.Author('Melika Adabinejad'), arxiv.Result.Author('Ayyoob Imani'), arxiv.Result.Author('Arezou Keshavarz'), arxiv.Result.Author('Mostafa Dehghani'), arxiv.Result.Author('Azadeh Shakery'), arxiv.Result.Author('Babak N. Araabi')]","Differentiable neural architecture search (DARTS) is a popular method for
neural architecture search (NAS), which performs cell-search and utilizes
continuous relaxation to improve the search efficiency via gradient-based
optimization. The main shortcoming of DARTS is performance collapse, where the
discovered architecture suffers from a pattern of declining quality during
search. Performance collapse has become an important topic of research, with
many methods trying to solve the issue through either regularization or
fundamental changes to DARTS. However, the weight-sharing framework used for
cell-search in DARTS and the convergence of architecture parameters has not
been analyzed yet. In this paper, we provide a thorough and novel theoretical
and empirical analysis on DARTS and its point of convergence. We show that
DARTS suffers from a specific structural flaw due to its weight-sharing
framework that limits the convergence of DARTS to saturation points of the
softmax function. This point of convergence gives an unfair advantage to layers
closer to the output in choosing the optimal architecture, causing performance
collapse. We then propose two new regularization terms that aim to prevent
performance collapse by harmonizing operation selection via aligning gradients
of layers. Experimental results on six different search spaces and three
different datasets show that our method ($\Lambda$-DARTS) does indeed prevent
performance collapse, providing justification for our theoretical analysis and
the proposed remedy."
12500,2 and further study the effect of the L2 norm in Eq.,"Additionally, we provide
an ablation study for the terms in Eq.","6 on the
prototype reconstructions in the supplementary material in Sec.",2022-10-15 00:42:13+00:00,ProtoVAE: A Trustworthy Self-Explainable Prototypical Variational Model,cs.LG,['cs.LG'],"[arxiv.Result.Author('Srishti Gautam'), arxiv.Result.Author('Ahcene Boubekki'), arxiv.Result.Author('Stine Hansen'), arxiv.Result.Author('Suaiba Amina Salahuddin'), arxiv.Result.Author('Robert Jenssen'), arxiv.Result.Author('Marina MC Höhne'), arxiv.Result.Author('Michael Kampffmeyer')]","The need for interpretable models has fostered the development of
self-explainable classifiers. Prior approaches are either based on multi-stage
optimization schemes, impacting the predictive performance of the model, or
produce explanations that are not transparent, trustworthy or do not capture
the diversity of the data. To address these shortcomings, we propose ProtoVAE,
a variational autoencoder-based framework that learns class-specific prototypes
in an end-to-end manner and enforces trustworthiness and diversity by
regularizing the representation space and introducing an orthonormality
constraint. Finally, the model is designed to be transparent by directly
incorporating the prototypes into the decision process. Extensive comparisons
with previous self-explainable approaches demonstrate the superiority of
ProtoVAE, highlighting its ability to generate trustworthy and diverse
explanations, while not degrading predictive performance."
12503,"Due to its high complexity and NP-hard
characterization, further researches seek to improve on this or relax the constraint Yu et al.","[2018], in which a
smooth function quantiﬁes the “DAG-ness” of the graph.","[2019];
Lachapelle et al.",2022-10-15 04:07:39+00:00,GFlowCausal: Generative Flow Networks for Causal Discovery,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Wenqian Li'), arxiv.Result.Author('Yinchuan Li'), arxiv.Result.Author('Shengyu Zhu'), arxiv.Result.Author('Yunfeng Shao'), arxiv.Result.Author('Jianye Hao'), arxiv.Result.Author('Yan Pang')]","Causal discovery aims to uncover causal structure among a set of variables.
Score-based approaches mainly focus on searching for the best Directed Acyclic
Graph (DAG) based on a predefined score function. However, most of them are not
applicable on a large scale due to the limited searchability. Inspired by the
active learning in generative flow networks, we propose a novel approach to
learning a DAG from observational data called GFlowCausal. It converts the
graph search problem to a generation problem, in which direct edges are added
gradually. GFlowCausal aims to learn the best policy to generate high-reward
DAGs by sequential actions with probabilities proportional to predefined
rewards. We propose a plug-and-play module based on transitive closure to
ensure efficient sampling. Theoretical analysis shows that this module could
guarantee acyclicity properties effectively and the consistency between final
states and fully-connected graphs. We conduct extensive experiments on both
synthetic and real datasets, and results show the proposed approach to be
superior and also performs well in a large-scale setting."
12509,"Developing hybrid
models capable of handling hybrid data will open up possibilities for further research on
imputation in complex scenarios.","[81] advanced the
performance of imputation and classification in computer-aided diagnosis.","Rather than referring to mixed data types, some researchers use
the term ""multi-modality"" to describe datasets from different sources but of the same type (e.g.,
MRI and PET images[12, 68]).",2022-10-15 11:11:20+00:00,Handling missing values in healthcare data: A systematic review of deep learning-based imputation techniques,cs.LG,['cs.LG'],"[arxiv.Result.Author('Mingxuan Liu'), arxiv.Result.Author('Siqi Li'), arxiv.Result.Author('Han Yuan'), arxiv.Result.Author('Marcus Eng Hock Ong'), arxiv.Result.Author('Yilin Ning'), arxiv.Result.Author('Feng Xie'), arxiv.Result.Author('Seyed Ehsan Saffari'), arxiv.Result.Author('Victor Volovici'), arxiv.Result.Author('Bibhas Chakraborty'), arxiv.Result.Author('Nan Liu')]","Objective: The proper handling of missing values is critical to delivering
reliable estimates and decisions, especially in high-stakes fields such as
clinical research. The increasing diversity and complexity of data have led
many researchers to develop deep learning (DL)-based imputation techniques. We
conducted a systematic review to evaluate the use of these techniques, with a
particular focus on data types, aiming to assist healthcare researchers from
various disciplines in dealing with missing values.
  Methods: We searched five databases (MEDLINE, Web of Science, Embase, CINAHL,
and Scopus) for articles published prior to August 2021 that applied DL-based
models to imputation. We assessed selected publications from four perspectives:
health data types, model backbone (i.e., main architecture), imputation
strategies, and comparison with non-DL-based methods. Based on data types, we
created an evidence map to illustrate the adoption of DL models.
  Results: We included 64 articles, of which tabular static (26.6%, 17/64) and
temporal data (37.5%, 24/64) were the most frequently investigated. We found
that model backbone(s) differed among data types as well as the imputation
strategy. The ""integrated"" strategy, that is, the imputation task being solved
concurrently with downstream tasks, was popular for tabular temporal (50%,
12/24) and multi-modal data (71.4%, 5/7), but limited for other data types.
Moreover, DL-based imputation methods yielded better imputation accuracy in
most studies, compared with non-DL-based methods.
  Conclusion: DL-based imputation models can be customized based on data type,
addressing the corresponding missing patterns, and its associated ""integrated""
strategy can enhance the efficacy of imputation, especially in scenarios where
data is complex. Future research may focus on the portability and fairness of
DL-based models for healthcare data imputation."
12512,"Motivated by
this connection, in Section 3.3, we further study its feature collapse issue and point out that MAE
suffers from dimensional feature collapse.","Speciﬁcally,
in Section 3.2, we show how a small MAE loss implies a small alignment loss.","To address this issue, in Section 3.4, we propose a
Uniformity-enhanced (U-MAE) loss to further promote feature diversity.",2022-10-15 17:36:03+00:00,How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Yifei Wang'), arxiv.Result.Author('Yisen Wang')]","Masked Autoencoders (MAE) based on a reconstruction task have risen to be a
promising paradigm for self-supervised learning (SSL) and achieve
state-of-the-art performance across different benchmark datasets. However,
despite its impressive empirical success, there is still limited theoretical
understanding of it. In this paper, we propose a theoretical understanding of
how masking matters for MAE to learn meaningful features. We establish a close
connection between MAE and contrastive learning, which shows that MAE implicit
aligns the mask-induced positive pairs. Built upon this connection, we develop
the first downstream guarantees for MAE methods, and analyze the effect of mask
ratio. Besides, as a result of the implicit alignment, we also point out the
dimensional collapse issue of MAE, and propose a Uniformity-enhanced MAE
(U-MAE) loss that can effectively address this issue and bring significant
improvements on real-world datasets, including CIFAR-10, ImageNet-100, and
ImageNet-1K. Code is available at (https://github.com/zhangq327/U-MAE)."
12517,"5.3 challenges and further research direction

Albeit successful results in learning and analysis of the proposed framework, we believe that this
study deserves deeper mathematical investigation and analysis.","Figure 12b shows the robustness of the learned policies against uncertainty
in permeability.","In particular, we would like to study
and analyze the eﬀect of the approximation introduced in the MLMC estimation.",2022-10-15 23:52:48+00:00,A multilevel reinforcement learning framework for PDE based control,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Atish Dixit'), arxiv.Result.Author('Ahmed Elsheikh')]","Reinforcement learning (RL) is a promising method to solve control problems.
However, model-free RL algorithms are sample inefficient and require thousands
if not millions of samples to learn optimal control policies. A major source of
computational cost in RL corresponds to the transition function, which is
dictated by the model dynamics. This is especially problematic when model
dynamics is represented with coupled PDEs. In such cases, the transition
function often involves solving a large-scale discretization of the said PDEs.
We propose a multilevel RL framework in order to ease this cost by exploiting
sublevel models that correspond to coarser scale discretization (i.e.
multilevel models). This is done by formulating an approximate multilevel Monte
Carlo estimate of the objective function of the policy and / or value network
instead of Monte Carlo estimates, as done in the classical framework. As a
demonstration of this framework, we present a multilevel version of the
proximal policy optimization (PPO) algorithm. Here, the level refers to the
grid fidelity of the chosen simulation-based environment. We provide two
examples of simulation-based environments that employ stochastic PDEs that are
solved using finite-volume discretization. For the case studies presented, we
observed substantial computational savings using multilevel PPO compared to its
classical counterpart."
12518,"5.3 challenges and further research direction

Albeit successful results in learning and analysis of the proposed framework, we believe that this
study deserves deeper mathematical investigation and analysis.","Figure 12b shows the robustness of the learned policies against uncertainty
in permeability.","In particular, we would like to study
and analyze the eﬀect of the approximation introduced in the MLMC estimation.",2022-10-15 23:52:48+00:00,A Multilevel Reinforcement Learning Framework for PDE-based Control,cs.LG,"['cs.LG', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Atish Dixit'), arxiv.Result.Author('Ahmed Elsheikh')]","Reinforcement learning (RL) is a promising method to solve control problems.
However, model-free RL algorithms are sample inefficient and require thousands
if not millions of samples to learn optimal control policies. A major source of
computational cost in RL corresponds to the transition function, which is
dictated by the model dynamics. This is especially problematic when model
dynamics is represented with coupled PDEs. In such cases, the transition
function often involves solving a large-scale discretization of the said PDEs.
We propose a multilevel RL framework in order to ease this cost by exploiting
sublevel models that correspond to coarser scale discretization (i.e.
multilevel models). This is done by formulating an approximate multilevel Monte
Carlo estimate of the objective function of the policy and / or value network
instead of Monte Carlo estimates, as done in the classical framework. As a
demonstration of this framework, we present a multilevel version of the
proximal policy optimization (PPO) algorithm. Here, the level refers to the
grid fidelity of the chosen simulation-based environment. We provide two
examples of simulation-based environments that employ stochastic PDEs that are
solved using finite-volume discretization. For the case studies presented, we
observed substantial computational savings using multilevel PPO compared to its
classical counterpart."
12520,"To further study the effectiveness of our PU Extra Trees algorithm
we also compare against neural network based methods including a baseline method uPU [11, 10]
and two state of the art methods nnPU [16] and Self-PU [6].","Selected tree-
based methods include NaivePUET whereby ET is trained on the PU dataset by simply treating
U data as N data; PUBaggingET which uses PUBagging from [23] with ET base classiﬁer; and
SupervisedET where ET is trained on the original fully labeled dataset (expected to be an upper
bound for tree based PU methods).","We ran experiments on heterogeneous
devices.",2022-10-16 06:30:16+00:00,Positive-Unlabeled Learning using Random Forests via Recursive Greedy Risk Minimization,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Jonathan Wilton'), arxiv.Result.Author('Abigail M. Y. Koay'), arxiv.Result.Author('Ryan K. L. Ko'), arxiv.Result.Author('Miao Xu'), arxiv.Result.Author('Nan Ye')]","The need to learn from positive and unlabeled data, or PU learning, arises in
many applications and has attracted increasing interest. While random forests
are known to perform well on many tasks with positive and negative data, recent
PU algorithms are generally based on deep neural networks, and the potential of
tree-based PU learning is under-explored. In this paper, we propose new random
forest algorithms for PU-learning. Key to our approach is a new interpretation
of decision tree algorithms for positive and negative data as \emph{recursive
greedy risk minimization algorithms}. We extend this perspective to the PU
setting to develop new decision tree learning algorithms that directly
minimizes PU-data based estimators for the expected risk. This allows us to
develop an efficient PU random forest algorithm, PU extra trees. Our approach
features three desirable properties: it is robust to the choice of the loss
function in the sense that various loss functions lead to the same decision
trees; it requires little hyperparameter tuning as compared to neural network
based PU learning; it supports a feature importance that directly measures a
feature's contribution to risk minimization. Our algorithms demonstrate strong
performance on several datasets. Our code is available at
\url{https://github.com/puetpaper/PUExtraTrees}."
12521,"This is consistent with results from additional
experiments in Appendix K.

Comparison of PU ET with other tree-based PU learning methods We now compare our PU
ET algorithm with several tree-based baselines to further study the effectiveness of our novel PU tree

                                                           7
Table 2: Accuracy mean% (sd) on the test set for PU ET using various PU data based risk estimators.","The more aggressive splitting behavior of quadratic loss leads to deeper trees that
tend to overﬁt more than the logistic loss in general.","Dataset    Unbiased risk estimator       Nonnegative risk estimator

           Quadratic Loss Logistic Loss Quadratic Loss Logistic Loss

Epsilon    50.04 (0.00) 50.67 (0.08) 57.39 (0.76) 57.83 (0.70)

20News     43.64 (0.05) 73.44 (0.87) 83.34 (0.22) 83.34 (0.31)

Covtype    48.72 (0.00) 72.63 (0.87) 76.51 (0.52) 75.63 (0.52)

Mushroom   0.607 (0.48) 99.02 (0.32) 99.70 (0.24) 99.36 (0.48)

MNIST      50.74 (0.0) 89.05 (0.61) 93.60 (0.39) 94.01 (0.19)

CIFAR-10   60.0.",2022-10-16 06:30:16+00:00,Positive-Unlabeled Learning using Random Forests via Recursive Greedy Risk Minimization,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Jonathan Wilton'), arxiv.Result.Author('Abigail M. Y. Koay'), arxiv.Result.Author('Ryan K. L. Ko'), arxiv.Result.Author('Miao Xu'), arxiv.Result.Author('Nan Ye')]","The need to learn from positive and unlabeled data, or PU learning, arises in
many applications and has attracted increasing interest. While random forests
are known to perform well on many tasks with positive and negative data, recent
PU algorithms are generally based on deep neural networks, and the potential of
tree-based PU learning is under-explored. In this paper, we propose new random
forest algorithms for PU-learning. Key to our approach is a new interpretation
of decision tree algorithms for positive and negative data as \emph{recursive
greedy risk minimization algorithms}. We extend this perspective to the PU
setting to develop new decision tree learning algorithms that directly
minimizes PU-data based estimators for the expected risk. This allows us to
develop an efficient PU random forest algorithm, PU extra trees. Our approach
features three desirable properties: it is robust to the choice of the loss
function in the sense that various loss functions lead to the same decision
trees; it requires little hyperparameter tuning as compared to neural network
based PU learning; it supports a feature importance that directly measures a
feature's contribution to risk minimization. Our algorithms demonstrate strong
performance on several datasets. Our code is available at
\url{https://github.com/puetpaper/PUExtraTrees}."
12539,"We think this phenomenon worthy of further study to better understand
the nuances affecting privacy in networks trained with DP-SGD.","We believe this to be the intersection of a few factors, including weight
initialization (affecting initial variance of the network embeddings), the max grad norm clipping
value, and the learning rate.","17
D.2 Random Forest
Derivation of attack: The class label y(l) of a leaf l is determined as

P (y(l) = z) ∝ exp  · u(z, l)
                    2e−j

where u(z, l) is 1 if z is the majority label in l and 0 otherwise, and j is the number of excess points
for the majority.",2022-10-16 21:34:18+00:00,A General Framework for Auditing Differentially Private Machine Learning,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Fred Lu'), arxiv.Result.Author('Joseph Munoz'), arxiv.Result.Author('Maya Fuchs'), arxiv.Result.Author('Tyler LeBlond'), arxiv.Result.Author('Elliott Zaresky-Williams'), arxiv.Result.Author('Edward Raff'), arxiv.Result.Author('Francis Ferraro'), arxiv.Result.Author('Brian Testa')]","We present a framework to statistically audit the privacy guarantee conferred
by a differentially private machine learner in practice. While previous works
have taken steps toward evaluating privacy loss through poisoning attacks or
membership inference, they have been tailored to specific models or have
demonstrated low statistical power. Our work develops a general methodology to
empirically evaluate the privacy of differentially private machine learning
implementations, combining improved privacy search and verification methods
with a toolkit of influence-based poisoning attacks. We demonstrate
significantly improved auditing power over previous approaches on a variety of
models including logistic regression, Naive Bayes, and random forest. Our
method can be used to detect privacy violations due to implementation errors or
misuse. When violations are not present, it can aid in understanding the amount
of information that can be leaked from a given dataset, algorithm, and privacy
specification."
12584,"To facilitate further research development, we open-sourced our code for the simulator
and training agents with A3C, PPO, and IMPALA.","We have created several demo
tasks, i.e., navigation, supply gathering, and supply battle, to evaluate the performance of different
RL methods and enable researchers to develop more powerful algorithms through conﬁgurable
environments.","We also host the open-world FPS game AI
competition to attract global researchers to innovate on the algorithms.",2022-10-14 13:39:41+00:00,WILD-SCAV: Benchmarking FPS Gaming AI on Unity3D-based Environments,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Xi Chen'), arxiv.Result.Author('Tianyu Shi'), arxiv.Result.Author('Qingpeng Zhao'), arxiv.Result.Author('Yuchen Sun'), arxiv.Result.Author('Yunfei Gao'), arxiv.Result.Author('Xiangjun Wang')]","Recent advances in deep reinforcement learning (RL) have demonstrated complex
decision-making capabilities in simulation environments such as Arcade Learning
Environment, MuJoCo, and ViZDoom. However, they are hardly extensible to more
complicated problems, mainly due to the lack of complexity and variations in
the environments they are trained and tested on. Furthermore, they are not
extensible to an open-world environment to facilitate long-term exploration
research. To learn realistic task-solving capabilities, we need to develop an
environment with greater diversity and complexity. We developed WILD-SCAV, a
powerful and extensible environment based on a 3D open-world FPS (First-Person
Shooter) game to bridge the gap. It provides realistic 3D environments of
variable complexity, various tasks, and multiple modes of interaction, where
agents can learn to perceive 3D environments, navigate and plan, compete and
cooperate in a human-like manner. WILD-SCAV also supports different
complexities, such as configurable maps with different terrains, building
structures and distributions, and multi-agent settings with cooperative and
competitive tasks. The experimental results on configurable complexity,
multi-tasking, and multi-agent scenarios demonstrate the effectiveness of
WILD-SCAV in benchmarking various RL algorithms, as well as it is potential to
give rise to intelligent agents with generalized task-solving abilities. The
link to our open-sourced code can be found here
https://github.com/inspirai/wilderness-scavenger."
12588,"Different from the first two stations, Station C does not have                                                                                                                                                                                           To further study the temporal robustness of ST-former, we

obvious commuting characteristics (morning and evening peak                                                                                                                                                                                         focus on passenger flow prediction during rush hours.","Time    Metric                             ARIMA LSTM                                   CNN                                                                                                                T-GCN DCRNN ConvLSTM GWN ST-ResNet Transformer                                              MGT Informer ST-former

 10min      RMSE                        31.355                   19.051                       19.323                                                                                                            18.609      18.769              18.246       18.074    18.095       17.757+                 19.601        18.142  17.543
 15min       MAE                        16.325                   10.376                       10.597                                                                                                            10.225      10.309               9.962       10.013    9.920         9.789+                 10.701        10.172   9.614
 20min     WMAPE                        28.47%                   17.89%                      18.24%                                                                                                             17.63%      17.78%              17.15%       17.27%   17.10%+       16.83%+                 18.41%       17.52%   16.58%
 30min      RMSE                        46.279                   26.959                                                                                                                                         25.381                                                24.725+                                                     23.910
 60min       MAE                        23.881                   14.980                       25.391                                                                                                            14.029      26.589              24.902        26.44    14.009       25.777                  28.627        24.738  13.066
Average    WMAPE                        27.92%                   17.34%                       13.885                                                                                                            16.22%      14.279              13.534       14.158   16.08%        14.283                  15.351       13.520+  14.95%
            RMSE                        62.575                   33.461                      16.08%                                                                                                             32.304      16.45%              15.56%       16.30%    32.033       16.30%                  17.62%       15.46%+  30.265
             MAE                        32.903                   17.940                                                                                                                                         17.758                                                17.234+                                            31.872+  16.707
           WMAPE                        28.05%                   15.47%                       33.465                                                                                                            15.27%      35.354              33.051       35.731   14.82%+       35.271                  40.619        18.370  14.36%
            RMSE                        108.052                  45.269+                      17.998                                                                                                            48.047      19.361              17.977       19.403    46.856       19.553                  22.811       15.80%   42.040
             MAE                        56.888                   24.876                      15.47%                                                                                                             25.910      16.68%              15.50%       16.77%    25.068       16.85%                  19.77%                22.457
           WMAPE                        33.04%                   14.29%                                                                                                                                         14.88%                                                14.42%                                              45.752  12.90%
            RMSE                        187.077                  113.782                      45.474                                                                                                            98.932      55.141              49.534       52.259    88.424       67.961                  57.983        26.622  74.336
             MAE                        101.024                  61.716                      24.262+                                                                                                            53.705      30.017              26.684       29.098    50.462       39.854                  31.319       15.29%   40.767
           WMAPE                        29.19%                   17.75%                      13.94%+                                                                                                            15.45%      17.27%              15.36%       16.73%   14.52%        22.88%                  18.04%       79.128+  11.73%
            RMSE                        87.068                   47.704                                                                                                                                         44.655                                                 42.027                                            43.273+  37.319
             MAE                        46.204                   25.978                       91.921                                                                                                            24.325      90.747              91.453       111.806   23.339       138.327                 105.146      12.45%+  20.522
           WMAPE                        29.33%                   16.55%                       50.066                                                                                                            15.89%      49.675              51.021       64.656    15.39        84.783                  57.853       39.926+  14.10%
                                                                                             14.40%                                                                                                                         14.29%              14.82%       18.60%                 24.39%                  16.64%       22.391+
              105                     T-GCN                      RMSE                         43.115                                                                                                                    60  45.320              43.437       48.862         22                                           15.30%+
                                      DCRNN                                                   23.362                                                                                                                        24.728              23.836       27.466                 57.019                  50.395
                90                    ConvLSTM                                               15.63%                                                                                                                     50  16.49%              15.68%       17.13%         20      33.652                  27.607
                                      Graph WaveNet                                                                                                                                                                                                                                 19.45%                  18.09%
                75                    ST-ResNet                                                                                                                                                                         40                                                  18
                                      Informer                                                                                                                                                                                                  MAE                                          WMAPE(%)
                                      ST-former
                                                                                                                                                                                                                                T-GCN                                               T-GCN
                                                                                                                                                                                                                                DCRNN                                               DCRNN
                                                                                                                                                                                                                                ConvLSTM                                            ConvLSTM
                                                                                                                                                                                                                                Graph WaveNet                                       Graph WaveNet
                                                                                                                                                                                                                                ST-ResNet                                           ST-ResNet
                                                                                                                                                                                                                                Informer                                            Informer
                                                                                                                                                                                                                                ST-former                                           ST-former

             60
                                                                                               30
                                                                                                                                                                                 16

             45

                                                                                                                                                                                                                   20                                                 14

             30

                                                                                                                                                                                                                   10                                                 12

             15

                                  10            20           30       40                     50            60                                                                                                               10  20              30       40  50  60             10  20             30       40       50  60

                                                     Time Interval (min)                                                                                                                                                            Time Interval (min)                                 Time Interval (min)

                                                                 (a)                                                                                                                                                                                (b)                                                (c)

                                                                                                           Figure 13 The overall evaluate metrics of methods

   Figure13 (c) represents the passenger flow prediction                                                                                                                                                                                            5.4.3        Comparison of Rush hours
performance of Station C, which is a large transfer hub.","In this
characteristics), and its passenger flow is significantly higher                                                                                                                                                                                    section, the rush hours are set to 7:00-9:00 and 17:00-20:00, and
than that of the other two stations.",2022-10-14 01:51:33+00:00,ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Shuxin Zhang'), arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Chengcheng Wang'), arxiv.Result.Author('Ziyou Gao')]","Accurate passenger flow prediction of urban rail transit is essential for
improving the performance of intelligent transportation systems, especially
during the epidemic. How to dynamically model the complex spatiotemporal
dependencies of passenger flow is the main issue in achieving accurate
passenger flow prediction during the epidemic. To solve this issue, this paper
proposes a brand-new transformer-based architecture called STformer under the
encoder-decoder framework specifically for COVID-19. Concretely, we develop a
modified self-attention mechanism named Causal-Convolution ProbSparse
Self-Attention (CPSA) to model the multiple temporal dependencies of passenger
flow with low computational costs. To capture the complex and dynamic spatial
dependencies, we introduce a novel Adaptive Multi-Graph Convolution Network
(AMGCN) by leveraging multiple graphs in a self-adaptive manner. Additionally,
the Multi-source Data Fusion block fuses the passenger flow data, COVID-19
confirmed case data, and the relevant social media data to study the impact of
COVID-19 to passenger flow. Experiments on real-world passenger flow datasets
demonstrate the superiority of ST-former over the other eleven state-of-the-art
methods. Several ablation studies are carried out to verify the effectiveness
and reliability of our model structure. Results can provide critical insights
for the operation of URT systems."
12589,"Table Ⅵ Ablation study of the self-attention mechanism

         Model                       RMSE MAE WMAPE                 Table Ⅴ Ablation study of AMGCN
                                                                                     Model
ST-former with Conventional          54.78 30.638 16.08%                                                                    RMSE                             MAE                                                                          WMAPE
         Self-Attention                                                 ST-former with Single Graph                         45.823                           25.098                                                                       14.22%
                                                                       ST-former with General GCN                           46.169                           25.126                                                                       14.18%
ST-former with ProbSparse Self-      46.031 25.132      14.16%          ST-former without AMGCN                             46.599                           25.376                                                                       14.27%
              Attention                                                                                                     44.639                           23.337                                                                       13.82%
                                                                                  ST-former
         ST-former                   44.639 24.337 13.82%
                                                            Ablation study of AMGCN                                                                    Periodicity Modeling: To further research the influence of
                                                                                                                                                    periodicity, we develop one variant of ST-former without
                             RMSE                   45 MAE                                                            WMAPE(%)                      weekly periodicity and daily periodicity.","The prediction results fully demonstrate the

effectiveness of CPSA in conducting passenger flow prediction.","80                                                               ST-former_Single graph
                                                                                                           ST-former_Single graph                      The results of the control variable experiment are shown in
             ST-former_Single graph                                                                                                                 Table Ⅷ and Figure 18.",2022-10-14 01:51:33+00:00,ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Shuxin Zhang'), arxiv.Result.Author('Jinlei Zhang'), arxiv.Result.Author('Lixing Yang'), arxiv.Result.Author('Chengcheng Wang'), arxiv.Result.Author('Ziyou Gao')]","Accurate passenger flow prediction of urban rail transit is essential for
improving the performance of intelligent transportation systems, especially
during the epidemic. How to dynamically model the complex spatiotemporal
dependencies of passenger flow is the main issue in achieving accurate
passenger flow prediction during the epidemic. To solve this issue, this paper
proposes a brand-new transformer-based architecture called STformer under the
encoder-decoder framework specifically for COVID-19. Concretely, we develop a
modified self-attention mechanism named Causal-Convolution ProbSparse
Self-Attention (CPSA) to model the multiple temporal dependencies of passenger
flow with low computational costs. To capture the complex and dynamic spatial
dependencies, we introduce a novel Adaptive Multi-Graph Convolution Network
(AMGCN) by leveraging multiple graphs in a self-adaptive manner. Additionally,
the Multi-source Data Fusion block fuses the passenger flow data, COVID-19
confirmed case data, and the relevant social media data to study the impact of
COVID-19 to passenger flow. Experiments on real-world passenger flow datasets
demonstrate the superiority of ST-former over the other eleven state-of-the-art
methods. Several ablation studies are carried out to verify the effectiveness
and reliability of our model structure. Results can provide critical insights
for the operation of URT systems."
12617,"4.3 Further Applications of SGConv

We further study SGConv as a generic network architecture drop-in component targeting tasks
in language modeling and computer vision.","2.The results suggest

that for the SC 35-classiﬁcation, SGConv achieves SoTA on both full length task and 2X sampling

rate, zero-shot task.","In Section 4.3.1 we present an eﬃcient mixture
of attention and SGConv layers architecture that replaces half of the attention blocks in the
Transformer with the SGConv blocks.",2022-10-17 17:53:29+00:00,What Makes Convolutional Models Great on Long Sequence Modeling?,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Yuhong Li'), arxiv.Result.Author('Tianle Cai'), arxiv.Result.Author('Yi Zhang'), arxiv.Result.Author('Deming Chen'), arxiv.Result.Author('Debadeepta Dey')]","Convolutional models have been widely used in multiple domains. However, most
existing models only use local convolution, making the model unable to handle
long-range dependency efficiently. Attention overcomes this problem by
aggregating global information but also makes the computational complexity
quadratic to the sequence length. Recently, Gu et al. [2021] proposed a model
called S4 inspired by the state space model. S4 can be efficiently implemented
as a global convolutional model whose kernel size equals the input sequence
length. S4 can model much longer sequences than Transformers and achieve
significant gains over SoTA on several long-range tasks. Despite its empirical
success, S4 is involved. It requires sophisticated parameterization and
initialization schemes. As a result, S4 is less intuitive and hard to use. Here
we aim to demystify S4 and extract basic principles that contribute to the
success of S4 as a global convolutional model. We focus on the structure of the
convolution kernel and identify two critical but intuitive principles enjoyed
by S4 that are sufficient to make up an effective global convolutional model:
1) The parameterization of the convolutional kernel needs to be efficient in
the sense that the number of parameters should scale sub-linearly with sequence
length. 2) The kernel needs to satisfy a decaying structure that the weights
for convolving with closer neighbors are larger than the more distant ones.
Based on the two principles, we propose a simple yet effective convolutional
model called Structured Global Convolution (SGConv). SGConv exhibits strong
empirical performance over several tasks: 1) With faster speed, SGConv
surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging
SGConv into standard language and vision models, it shows the potential to
improve both efficiency and performance."
12631,"There are several avenues for improvement upon the AMPNet operator, which could be addressed in
further research.","We benchmark AMPNet on multiple datasets against standard Graph Neural Architectures, and
provide multiple methods for managing the computational complexity of the operator.","A better selection strategy for node features might outperform random sampling,
yielding better representations for nodes during forward passes through the architecture.",2022-10-17 23:44:30+00:00,AMPNet: Attention as Message Passing for Graph Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Syed Asad Rizvi'), arxiv.Result.Author('Nhi Nguyen'), arxiv.Result.Author('Haoran Lyu'), arxiv.Result.Author('Ben Christensen'), arxiv.Result.Author('Josue Ortega Caro'), arxiv.Result.Author('Emanuele Zappala'), arxiv.Result.Author('Maria Brbic'), arxiv.Result.Author('Rahul Madhav Dhodapkar'), arxiv.Result.Author('David van Dijk')]","Feature-level interactions between nodes can carry crucial information for
understanding complex interactions in graph-structured data. Current
interpretability techniques, however, are limited in their ability to capture
feature-level interactions between different nodes. In this work, we propose
AMPNet, a general Graph Neural Network (GNN) architecture for uncovering
feature-level interactions between different spatial locations within
graph-structured data. Our framework applies a multiheaded attention operation
during message-passing to contextualize messages based on the feature
interactions between different nodes. We evaluate AMPNet on several benchmark
and real-world datasets, and develop a synthetic benchmark based on cyclic
cellular automata to test the ability of our framework to recover cyclic
patterns in node states based on feature-interactions. We also propose several
methods for addressing the scalability of our architecture to large graphs,
including subgraph sampling during training and node feature downsampling."
12661,"Although our theoretical analysis relies on the
closeness to linear gradient descent which has shown to result in less powerful models in practice,
we hope that our surprising empirical success of noise disentanglement sparks further research into
using the lens of linear gradient descent to understand the mysteries of deep learning.","This allows us to extrapolate insights of the tractable linearly trained deep ensembles into
the non-linear regime which can lead to improved out-of-distribution detection of deep ensembles
by eliminating potentially unfavorable noise sources.","Acknowledgments and Disclosure of Funding

Seijin Kobayashi was supported by the Swiss National Science Foundation (SNF) grant CR-
SII5_173721.",2022-10-18 12:55:53+00:00,Disentangling the Predictive Variance of Deep Ensembles through the Neural Tangent Kernel,cs.LG,['cs.LG'],"[arxiv.Result.Author('Seijin Kobayashi'), arxiv.Result.Author('Pau Vilimelis Aceituno'), arxiv.Result.Author('Johannes von Oswald')]","Identifying unfamiliar inputs, also known as out-of-distribution (OOD)
detection, is a crucial property of any decision making process. A simple and
empirically validated technique is based on deep ensembles where the variance
of predictions over different neural networks acts as a substitute for input
uncertainty. Nevertheless, a theoretical understanding of the inductive biases
leading to the performance of deep ensemble's uncertainty estimation is
missing. To improve our description of their behavior, we study deep ensembles
with large layer widths operating in simplified linear training regimes, in
which the functions trained with gradient descent can be described by the
neural tangent kernel. We identify two sources of noise, each inducing a
distinct inductive bias in the predictive variance at initialization. We
further show theoretically and empirically that both noise sources affect the
predictive variance of non-linear deep ensembles in toy models and realistic
settings after training. Finally, we propose practical ways to eliminate part
of these noise sources leading to significant changes and improved OOD
detection in trained deep ensembles."
12724,We further study the performance of our model with graph sizes.,"Our analysis later shows that our model
gains an advantage by removing some irrelevant nodes from the target graph before predicting SED values.","0.9 CiteSeer-Prune4SED
By controlling the random sampling procedure, we get graph                     Cora-Prune4SED
pairs with diﬀerent sizes and form three sets (small, medium,        0.8       CiteSeer-NeuroSED
and large) of graph pairs.",2022-10-19 15:16:28+00:00,Towards Accurate Subgraph Similarity Computation via Neural Graph Pruning,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Linfeng Liu'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Dawei Zhou'), arxiv.Result.Author('Li-Ping Liu')]","Subgraph similarity search, one of the core problems in graph search,
concerns whether a target graph approximately contains a query graph. The
problem is recently touched by neural methods. However, current neural methods
do not consider pruning the target graph, though pruning is critically
important in traditional calculations of subgraph similarities. One obstacle to
applying pruning in neural methods is {the discrete property of pruning}. In
this work, we convert graph pruning to a problem of node relabeling and then
relax it to a differentiable problem. Based on this idea, we further design a
novel neural network to approximate a type of subgraph distance: the subgraph
edit distance (SED). {In particular, we construct the pruning component using a
neural structure, and the entire model can be optimized end-to-end.} In the
design of the model, we propose an attention mechanism to leverage the
information about the query graph and guide the pruning of the target graph.
Moreover, we develop a multi-head pruning strategy such that the model can
better explore multiple ways of pruning the target graph. The proposed model
establishes new state-of-the-art results across seven benchmark datasets.
Extensive analysis of the model indicates that the proposed model can
reasonably prune the target graph for SED computation. The implementation of
our algorithm is released at our Github repo:
https://github.com/tufts-ml/Prune4SED."
12725,"The further research shows this algorithm can optimize the continuous nonlinear functions(Kennedy and
Eberhart, 1995).","Through the iteration of
individual particles and particle groups, this algorithm is able to obtain the expected value from the continuous nonlinear
functions.","vn+1 = vn + c1r1(P best − xn) + c2r2(Gbest − xn)   (14)

          xn+1 = xn + vn+1                                   (15)

          vn+1 = wvn + c1r1(P best − xn) + c2r2(Gbest − xn)  (16)

          3
                         POGD

Equation 14 and Equation 15 is the functions to achieve TLC model.",2022-10-15 12:31:02+00:00,POGD: Gradient Descent with New Stochastic Rules,cs.LG,['cs.LG'],"[arxiv.Result.Author('Feihu Han'), arxiv.Result.Author('Sida Xing'), arxiv.Result.Author('Sui Yang Khoo')]","There introduce Particle Optimized Gradient Descent (POGD), an algorithm
based on the gradient descent but integrates the particle swarm optimization
(PSO) principle to achieve the iteration. From the experiments, this algorithm
has adaptive learning ability. The experiments in this paper mainly focus on
the training speed to reach the target value and the ability to prevent the
local minimum. The experiments in this paper are achieved by the convolutional
neural network (CNN) image classification on the MNIST and cifar-10 datasets."
12731,"We hope that further research can help
close this gap, and furnish us with a more intuitive grasp on the maximum likelihood as a framework
for assessing goodness-of-ﬁt in generative models.","While models trained under the NCML framework show greater invariance to imperceptible noise,
they are by no means robust, indicating that the underlying model still differs signiﬁcantly from the
theoretical human model qhuman proposed in Husza´r (2015).","9
REFERENCES

Brian DO Anderson.",2022-10-19 16:47:51+00:00,Autoregressive Generative Modeling with Noise Conditional Maximum Likelihood Estimation,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Henry Li'), arxiv.Result.Author('Yuval Kluger')]","We introduce a simple modification to the standard maximum likelihood
estimation (MLE) framework. Rather than maximizing a single unconditional
likelihood of the data under the model, we maximize a family of \textit{noise
conditional} likelihoods consisting of the data perturbed by a continuum of
noise levels. We find that models trained this way are more robust to noise,
obtain higher test likelihoods, and generate higher quality images. They can
also be sampled from via a novel score-based sampling scheme which combats the
classical \textit{covariate shift} problem that occurs during sample generation
in autoregressive models. Applying this augmentation to autoregressive image
models, we obtain 3.32 bits per dimension on the ImageNet 64x64 dataset, and
substantially improve the quality of generated samples in terms of the Frechet
Inception distance (FID) -- from 37.50 to 12.09 on the CIFAR-10 dataset."
12736,"Similarities would potentially indicate candidates
for further study in gaining a more fundamental understanding of overoptimization in general, and
differences opportunities for better optimization algorithms.","As such, we ask whether this difference in optimization
results in different overoptimization characteristics.","We note the following:

RL is far less KL-efﬁcient than BoN.",2022-10-19 17:56:10+00:00,Scaling Laws for Reward Model Overoptimization,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Leo Gao'), arxiv.Result.Author('John Schulman'), arxiv.Result.Author('Jacob Hilton')]","In reinforcement learning from human feedback, it is common to optimize
against a reward model trained to predict human preferences. Because the reward
model is an imperfect proxy, optimizing its value too much can hinder ground
truth performance, in accordance with Goodhart's law. This effect has been
frequently observed, but not carefully measured due to the expense of
collecting human preference data. In this work, we use a synthetic setup in
which a fixed ""gold-standard"" reward model plays the role of humans, providing
labels used to train a proxy reward model. We study how the gold reward model
score changes as we optimize against the proxy reward model using either
reinforcement learning or best-of-$n$ sampling. We find that this relationship
follows a different functional form depending on the method of optimization,
and that in both cases its coefficients scale smoothly with the number of
reward model parameters. We also study the effect on this relationship of the
size of the reward model dataset, the number of reward model and policy
parameters, and the coefficient of the KL penalty added to the reward in the
reinforcement learning setup. We explore the implications of these empirical
results for theoretical considerations in AI alignment."
12737,"As a feasibility study, we hope that our empirical analysis and ﬁndings on cross-task transfer
with model-based RL will inspire further research in this direction.","We ﬁnd that XTRA improves sample-efﬁciency substantially across
most tasks, improving mean and median performance of EfﬁcientZero by 23% and 25%, respectively,
overall.",Acknowledgements.,2022-10-19 17:57:06+00:00,On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning,cs.LG,"['cs.LG', 'cs.CV', 'cs.RO']","[arxiv.Result.Author('Yifan Xu'), arxiv.Result.Author('Nicklas Hansen'), arxiv.Result.Author('Zirui Wang'), arxiv.Result.Author('Yung-Chieh Chan'), arxiv.Result.Author('Hao Su'), arxiv.Result.Author('Zhuowen Tu')]","Reinforcement Learning (RL) algorithms can solve challenging control problems
directly from image observations, but they often require millions of
environment interactions to do so. Recently, model-based RL algorithms have
greatly improved sample-efficiency by concurrently learning an internal model
of the world, and supplementing real environment interactions with imagined
rollouts for policy improvement. However, learning an effective model of the
world from scratch is challenging, and in stark contrast to humans that rely
heavily on world understanding and visual cues for learning new skills. In this
work, we investigate whether internal models learned by modern model-based RL
algorithms can be leveraged to solve new, distinctly different tasks faster. We
propose Model-Based Cross-Task Transfer (XTRA), a framework for
sample-efficient online RL with scalable pretraining and finetuning of learned
world models. By offline multi-task pretraining and online cross-task
finetuning, we achieve substantial improvements on the Atari100k benchmark over
a baseline trained from scratch; we improve mean performance of model-based
algorithm EfficientZero by 23%, and by as much as 71% in some instances.
Project page: https://nicklashansen.github.io/xtra."
12750,"[Yes] The instructions
                will be provided in the supplemental material and we plan to release the code and
                 pre-trained models to facilitate further research.","If you ran experiments...
            (a) Did you include the code, data, and instructions needed to reproduce the main experi-
                 mental results (either in the supplemental material or as a URL)?","(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
                were chosen)?",2022-10-19 22:26:12+00:00,Palm up: Playing in the Latent Manifold for Unsupervised Pretraining,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Hao Liu'), arxiv.Result.Author('Tom Zahavy'), arxiv.Result.Author('Volodymyr Mnih'), arxiv.Result.Author('Satinder Singh')]","Large and diverse datasets have been the cornerstones of many impressive
advancements in artificial intelligence. Intelligent creatures, however, learn
by interacting with the environment, which changes the input sensory signals
and the state of the environment. In this work, we aim to bring the best of
both worlds and propose an algorithm that exhibits an exploratory behavior
whilst it utilizes large diverse datasets. Our key idea is to leverage deep
generative models that are pretrained on static datasets and introduce a
dynamic model in the latent space. The transition dynamics simply mixes an
action and a random sampled latent. It then applies an exponential moving
average for temporal persistency, the resulting latent is decoded to image
using pretrained generator. We then employ an unsupervised reinforcement
learning algorithm to explore in this environment and perform unsupervised
representation learning on the collected data. We further leverage the temporal
information of this data to pair data points as a natural supervision for
representation learning. Our experiments suggest that the learned
representations can be successfully transferred to downstream tasks in both
vision and reinforcement learning domains."
12766,"Nonetheless, the arising optimization procedure deserves further study in terms of conver-
gence and optimality guarantees, but our duality results provide an intuition on the method.","Moreover, the gradient descent is not iterated until convergence in
practice.","Future
work will focus on understanding how to implicitly describe transport plans, rather than transport
maps.",2022-10-20 10:04:35+00:00,Trust Region Policy Optimization with Optimal Transport Discrepancies: Duality and Algorithm for Continuous Actions,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY']","[arxiv.Result.Author('Antonio Terpin'), arxiv.Result.Author('Nicolas Lanzetti'), arxiv.Result.Author('Batuhan Yardim'), arxiv.Result.Author('Florian Dörfler'), arxiv.Result.Author('Giorgia Ramponi')]","Policy Optimization (PO) algorithms have been proven particularly suited to
handle the high-dimensionality of real-world continuous control tasks. In this
context, Trust Region Policy Optimization methods represent a popular approach
to stabilize the policy updates. These usually rely on the Kullback-Leibler
(KL) divergence to limit the change in the policy. The Wasserstein distance
represents a natural alternative, in place of the KL divergence, to define
trust regions or to regularize the objective function. However,
state-of-the-art works either resort to its approximations or do not provide an
algorithm for continuous state-action spaces, reducing the applicability of the
method. In this paper, we explore optimal transport discrepancies (which
include the Wasserstein distance) to define trust regions, and we propose a
novel algorithm - Optimal Transport Trust Region Policy Optimization (OT-TRPO)
- for continuous state-action spaces. We circumvent the infinite-dimensional
optimization problem for PO by providing a one-dimensional dual reformulation
for which strong duality holds. We then analytically derive the optimal policy
update given the solution of the dual problem. This way, we bypass the
computation of optimal transport costs and of optimal transport maps, which we
implicitly characterize by solving the dual formulation. Finally, we provide an
experimental evaluation of our approach across various control tasks. Our
results show that optimal transport discrepancies can offer an advantage over
state-of-the-art approaches."
12769,"In the future, we plan to further study the effect of the  A simple framework for contrastive learning of visual repre-
margin α on the Triplet Loss.",ing.,sentations.,2022-10-20 11:21:17+00:00,Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem,cs.LG,['cs.LG'],"[arxiv.Result.Author('Albert Xu'), arxiv.Result.Author('Jhih-Yi Hsieh'), arxiv.Result.Author('Bhaskar Vundurthy'), arxiv.Result.Author('Eliana Cohen'), arxiv.Result.Author('Howie Choset'), arxiv.Result.Author('Lu Li')]","In deep metric learning, the Triplet Loss has emerged as a popular method to
learn many computer vision and natural language processing tasks such as facial
recognition, object detection, and visual-semantic embeddings. One issue that
plagues the Triplet Loss is network collapse, an undesirable phenomenon where
the network projects the embeddings of all data onto a single point.
Researchers predominately solve this problem by using triplet mining
strategies. While hard negative mining is the most effective of these
strategies, existing formulations lack strong theoretical justification for
their empirical success. In this paper, we utilize the mathematical theory of
isometric approximation to show an equivalence between the Triplet Loss sampled
by hard negative mining and an optimization problem that minimizes a
Hausdorff-like distance between the neural network and its ideal counterpart
function. This provides the theoretical justifications for hard negative
mining's empirical efficacy. In addition, our novel application of the
isometric approximation theorem provides the groundwork for future forms of
hard negative mining that avoid network collapse. Our theory can also be
extended to analyze other Euclidean space-based metric learning methods like
Ladder Loss or Contrastive Learning."
12774,"We were able to improve on R2 scores obtained
using traditional ML without DA, which is an important ﬁrst step toward creating more sophisticated
transfer learning and domain adaptation based pipelines, suggesting that further research into using
these techniques is warranted.","of Datapoints

PA          577

WI          338

KY          38

MS          184

GA          84

Table 3: Datasets per state with number of datapoints

5 Conclusion

We demonstrated that the problem of training ML models on scarce crop yield datasets for one geo-
graphical region can be partially remedied by training those models on data aggregated from other
regions, even when they have disparate climates.","In the interest of improving machine learning’s utility in the crucial ﬁeld of agriculture, we expanded
on previous work that compared feature selection techniques to pick the best features for predicting
alfalfa crop yields.",2022-10-20 13:00:33+00:00,Comparing Machine Learning Techniques for Alfalfa Biomass Yield Prediction,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Jonathan Vance'), arxiv.Result.Author('Khaled Rasheed'), arxiv.Result.Author('Ali Missaoui'), arxiv.Result.Author('Frederick Maier'), arxiv.Result.Author('Christian Adkins'), arxiv.Result.Author('Chris Whitmire')]","The alfalfa crop is globally important as livestock feed, so highly efficient
planting and harvesting could benefit many industries, especially as the global
climate changes and traditional methods become less accurate. Recent work using
machine learning (ML) to predict yields for alfalfa and other crops has shown
promise. Previous efforts used remote sensing, weather, planting, and soil data
to train machine learning models for yield prediction. However, while remote
sensing works well, the models require large amounts of data and cannot make
predictions until the harvesting season begins. Using weather and planting data
from alfalfa variety trials in Kentucky and Georgia, our previous work compared
feature selection techniques to find the best technique and best feature set.
In this work, we trained a variety of machine learning models, using cross
validation for hyperparameter optimization, to predict biomass yields, and we
showed better accuracy than similar work that employed more complex techniques.
Our best individual model was a random forest with a mean absolute error of
0.081 tons/acre and R{$^2$} of 0.941. Next, we expanded this dataset to include
Wisconsin and Mississippi, and we repeated our experiments, obtaining a higher
best R{$^2$} of 0.982 with a regression tree. We then isolated our testing
datasets by state to explore this problem's eligibility for domain adaptation
(DA), as we trained on multiple source states and tested on one target state.
This Trivial DA (TDA) approach leaves plenty of room for improvement through
exploring more complex DA techniques in forthcoming work."
12776,"To further study how models perform on
each relation, we plot several models and their micro F1 scores for each relation in

                                               15
                                                   (a) HRERE

                                                   (b) KGPool

                                                   (c) RECON

                                                  (d) CoLAKE

                                                    (e) CGRE
Figure 4: Micro F1 (indicated by color intensity, lighter is better) of models on different relations
from each dataset.","Here we can also observe that the observation of NYT10m
is quite different from Wiki80 and Wiki20m, possibly due to the much fewer
relational facts we built for it in KG.","Columns represent the relations and rows represent the percentile of degrees
involved in each relation.",2022-10-19 13:23:10+00:00,Knowledge Graph Enhanced Relation Extraction Datasets,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yucong Lin'), arxiv.Result.Author('Hongming Xiao'), arxiv.Result.Author('Jiani Liu'), arxiv.Result.Author('Zichao Lin'), arxiv.Result.Author('Keming Lu'), arxiv.Result.Author('Feifei Wang'), arxiv.Result.Author('Wei Wei')]","Knowledge-enhanced methods that take advantage of auxiliary knowledge graphs
recently emerged in relation extraction, and they surpass traditional
text-based relation extraction methods. However, there are no unified public
benchmarks that currently involve evidence sentences and knowledge graphs for
knowledge-enhanced relation extraction. To combat these issues, we propose
KGRED, a knowledge graph enhanced relation extraction dataset with features as
follows: (1) the benchmarks are based on widely-used distantly supervised
relation extraction datasets; (2) we refine these existing datasets to improve
the data quality, and we also construct auxiliary knowledge graphs for these
existing datasets through entity linking to support knowledge-enhanced relation
extraction tasks; (3) with the new benchmarks we curated, we build baselines in
two popular relation extraction settings including sentence-level and bag-level
relation extraction, and we also make comparisons among the latest
knowledge-enhanced relation extraction methods. KGRED provides high-quality
relation extraction datasets with auxiliary knowledge graphs for evaluating the
performance of knowledge-enhanced relation extraction methods. Meanwhile, our
experiments on KGRED reveal the influence of knowledge graph information on
relation extraction tasks."
12777,"To further study how models perform on each relation,
we plot several models and their micro F1 scores for each relation in Figure B.1
in Appendix B.","Degrees are maximum when the percentile of degree is 95 and
minimum when it is 5.

different from Wiki80 and Wiki20m, possibly due to the much fewer relational
facts we built for it in KG.","We can see that the abilities of the models to correctly categorize
each relation vary a lot.",2022-10-19 13:23:10+00:00,Knowledge Graph Enhanced Relation Extraction Datasets,cs.LG,['cs.LG'],"[arxiv.Result.Author('Yucong Lin'), arxiv.Result.Author('Hongming Xiao'), arxiv.Result.Author('Jiani Liu'), arxiv.Result.Author('Zichao Lin'), arxiv.Result.Author('Keming Lu'), arxiv.Result.Author('Feifei Wang'), arxiv.Result.Author('Wei Wei')]","Knowledge-enhanced methods that take advantage of auxiliary knowledge graphs
recently emerged in relation extraction, and they surpass traditional
text-based relation extraction methods. However, there are no unified public
benchmarks that currently involve evidence sentences and knowledge graphs for
knowledge-enhanced relation extraction. To combat these issues, we propose
KGRED, a knowledge graph enhanced relation extraction dataset with features as
follows: (1) the benchmarks are based on widely-used distantly supervised
relation extraction datasets; (2) we refine these existing datasets to improve
the data quality, and we also construct auxiliary knowledge graphs for these
existing datasets through entity linking to support knowledge-enhanced relation
extraction tasks; (3) with the new benchmarks we curated, we build baselines in
two popular relation extraction settings including sentence-level and bag-level
relation extraction, and we also make comparisons among the latest
knowledge-enhanced relation extraction methods. KGRED provides high-quality
relation extraction datasets with auxiliary knowledge graphs for evaluating the
performance of knowledge-enhanced relation extraction methods. Meanwhile, our
experiments on KGRED reveal the influence of knowledge graph information on
relation extraction tasks."
12781,"There is extensive further research needed to fully realize the utility
of structural causal hypothesis testing in conjugation with deep learning function approximation.","CSHTEST offers a novel architecture, along with a deliberate data
split methodology that can empower practitioners and domain experts to improve causally informed
modeling and deep learning.","We hope to better differentiate leaky causal models, without constraints on losses, using minimum
entropy properties such as in [14] .",2022-10-20 13:46:15+00:00,Causal Structural Hypothesis Testing and Data Generation Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Jeffrey Jiang'), arxiv.Result.Author('Omead Pooladzandi'), arxiv.Result.Author('Sunay Bhat'), arxiv.Result.Author('Gregory Pottie')]","A vast amount of expert and domain knowledge is captured by causal structural
priors, yet there has been little research on testing such priors for
generalization and data synthesis purposes. We propose a novel model
architecture, Causal Structural Hypothesis Testing, that can use nonparametric,
structural causal knowledge and approximate a causal model's functional
relationships using deep neural networks. We use these architectures for
comparing structural priors, akin to hypothesis testing, using a deliberate
(non-random) split of training and testing data. Extensive simulations
demonstrate the effectiveness of out-of-distribution generalization error as a
proxy for causal structural prior hypothesis testing and offers a statistical
baseline for interpreting results. We show that the variational version of the
architecture, Causal Structural Variational Hypothesis Testing can improve
performance in low SNR regimes. Due to the simplicity and low parameter count
of the models, practitioners can test and compare structural prior hypotheses
on small dataset and use the priors with the best generalization capacity to
synthesize much larger, causally-informed datasets. Finally, we validate our
methods on a synthetic pendulum dataset, and show a use-case on a real-world
trauma surgery ground-level falls dataset."
12782,"The results are a promising start
to much further research integrating deep learning causal models with real-world priors and domain
knowledge.","We also hope to extend both CSHTEST and CSVHTEST to
more ﬂexible architecture which can combine recent progress with differential causal inference and
binary sampling to better automate full or partial causal discovery.","7
References

 [1] J. Pearl, “Causal inference in statistics: An overview,” Statistics Surveys, vol.",2022-10-20 13:46:15+00:00,Causal Structural Hypothesis Testing and Data Generation Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Jeffrey Jiang'), arxiv.Result.Author('Omead Pooladzandi'), arxiv.Result.Author('Sunay Bhat'), arxiv.Result.Author('Gregory Pottie')]","A vast amount of expert and domain knowledge is captured by causal structural
priors, yet there has been little research on testing such priors for
generalization and data synthesis purposes. We propose a novel model
architecture, Causal Structural Hypothesis Testing, that can use nonparametric,
structural causal knowledge and approximate a causal model's functional
relationships using deep neural networks. We use these architectures for
comparing structural priors, akin to hypothesis testing, using a deliberate
(non-random) split of training and testing data. Extensive simulations
demonstrate the effectiveness of out-of-distribution generalization error as a
proxy for causal structural prior hypothesis testing and offers a statistical
baseline for interpreting results. We show that the variational version of the
architecture, Causal Structural Variational Hypothesis Testing can improve
performance in low SNR regimes. Due to the simplicity and low parameter count
of the models, practitioners can test and compare structural prior hypotheses
on small dataset and use the priors with the best generalization capacity to
synthesize much larger, causally-informed datasets. Finally, we validate our
methods on a synthetic pendulum dataset, and show a use-case on a real-world
trauma surgery ground-level falls dataset."
12783,"We leave it to further research to investigate optimizer performance
and considerations for causally informed deep learning architecture that are constrained in unique
ways than traditional deep learning models.","Although PSGD routinely outperform AdaBelief both in mean ﬁnal loss and variance (across
3 iterations per test), there were select conditions and DAGs in which any single optimizer would
underperform or not converge.","PSGD had better loss in 171 cases, and lower variance
in 148 of the 176 test cases.",2022-10-20 13:46:15+00:00,Causal Structural Hypothesis Testing and Data Generation Models,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Jeffrey Jiang'), arxiv.Result.Author('Omead Pooladzandi'), arxiv.Result.Author('Sunay Bhat'), arxiv.Result.Author('Gregory Pottie')]","A vast amount of expert and domain knowledge is captured by causal structural
priors, yet there has been little research on testing such priors for
generalization and data synthesis purposes. We propose a novel model
architecture, Causal Structural Hypothesis Testing, that can use nonparametric,
structural causal knowledge and approximate a causal model's functional
relationships using deep neural networks. We use these architectures for
comparing structural priors, akin to hypothesis testing, using a deliberate
(non-random) split of training and testing data. Extensive simulations
demonstrate the effectiveness of out-of-distribution generalization error as a
proxy for causal structural prior hypothesis testing and offers a statistical
baseline for interpreting results. We show that the variational version of the
architecture, Causal Structural Variational Hypothesis Testing can improve
performance in low SNR regimes. Due to the simplicity and low parameter count
of the models, practitioners can test and compare structural prior hypotheses
on small dataset and use the priors with the best generalization capacity to
synthesize much larger, causally-informed datasets. Finally, we validate our
methods on a synthetic pendulum dataset, and show a use-case on a real-world
trauma surgery ground-level falls dataset."
12795,"We hope that our work can motivate further research in fairness, where techniques similar
to DFR can be considered to improve the fairness of ML models.","Research on spurious correlations is closely related to ML Fairness [16, 23, 41,
69, 1, 39].","A potential negative outcome
that can result from misinterpretation of our analysis is if the practitioners assume that spurious
correlations are not an important issue, as ERM learns high quality representation of the core features.",2022-10-20 16:10:28+00:00,On Feature Learning in the Presence of Spurious Correlations,cs.LG,"['cs.LG', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Pavel Izmailov'), arxiv.Result.Author('Polina Kirichenko'), arxiv.Result.Author('Nate Gruver'), arxiv.Result.Author('Andrew Gordon Wilson')]","Deep classifiers are known to rely on spurious features $\unicode{x2013}$
patterns which are correlated with the target on the training data but not
inherently relevant to the learning problem, such as the image backgrounds when
classifying the foregrounds. In this paper we evaluate the amount of
information about the core (non-spurious) features that can be decoded from the
representations learned by standard empirical risk minimization (ERM) and
specialized group robustness training. Following recent work on Deep Feature
Reweighting (DFR), we evaluate the feature representations by re-training the
last layer of the model on a held-out set where the spurious correlation is
broken. On multiple vision and NLP problems, we show that the features learned
by simple ERM are highly competitive with the features learned by specialized
group robustness methods targeted at reducing the effect of spurious
correlations. Moreover, we show that the quality of learned feature
representations is greatly affected by the design decisions beyond the training
method, such as the model architecture and pre-training strategy. On the other
hand, we find that strong regularization is not necessary for learning high
quality feature representations. Finally, using insights from our analysis, we
significantly improve upon the best results reported in the literature on the
popular Waterbirds, CelebA hair color prediction and WILDS-FMOW problems,
achieving 97%, 92% and 50% worst-group accuracies, respectively."
12815,"We expect that the release of bilingual and monolingual models trained on identical conditions will
motivate further research in this area by cognitive scientists doing computational research.","The code for the linear
experiments is written in JAX [41].","The main
motivation for this paper was a theory from Cognitive Science regarding increased Cognitive Reserve
in bilingual people.",2022-10-20 22:23:27+00:00,Multitasking Models are Robust to Structural Failure: A Neural Model for Bilingual Cognitive Reserve,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Giannis Daras'), arxiv.Result.Author('Negin Raoof'), arxiv.Result.Author('Zoi Gkalitsiou'), arxiv.Result.Author('Alexandros G. Dimakis')]","We find a surprising connection between multitask learning and robustness to
neuron failures. Our experiments show that bilingual language models retain
higher performance under various neuron perturbations, such as random
deletions, magnitude pruning and weight noise compared to equivalent
monolingual ones. We provide a theoretical justification for this robustness by
mathematically analyzing linear representation learning and showing that
multitasking creates more robust representations. Our analysis connects
robustness to spectral properties of the learned representation and proves that
multitasking leads to higher robustness for diverse task vectors. We
open-source our code and models:
https://github.com/giannisdaras/multilingual_robustness"
12825,"More controllable factors like the nature of entities (shape, size, color of objects),
    underlying physics (mass, friction, elasticity), or procedural background generation can be
    introduced to further study generalization capabilities of RL agents.","• In this work, BBS was explored with only 2 factors of variation: partial-observability and
    stochasticity.","• SVSG being a purely stochastic model can further be used to estimate state uncertainty by
    marginalizing over multiple samples paths to efﬁciently explore in an unknown environment.",2022-10-21 02:56:51+00:00,Learning Robust Dynamics through Variational Sparse Gating,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Arnav Kumar Jain'), arxiv.Result.Author('Shivakanth Sujit'), arxiv.Result.Author('Shruti Joshi'), arxiv.Result.Author('Vincent Michalski'), arxiv.Result.Author('Danijar Hafner'), arxiv.Result.Author('Samira Ebrahimi-Kahou')]","Learning world models from their sensory inputs enables agents to plan for
actions by imagining their future outcomes. World models have previously been
shown to improve sample-efficiency in simulated environments with few objects,
but have not yet been applied successfully to environments with many objects.
In environments with many objects, often only a small number of them are moving
or interacting at the same time. In this paper, we investigate integrating this
inductive bias of sparse interactions into the latent dynamics of world models
trained from pixels. First, we introduce Variational Sparse Gating (VSG), a
latent dynamics model that updates its feature dimensions sparsely through
stochastic binary gates. Moreover, we propose a simplified architecture Simple
Variational Sparse Gating (SVSG) that removes the deterministic pathway of
previous models, resulting in a fully stochastic transition function that
leverages the VSG mechanism. We evaluate the two model architectures in the
BringBackShapes (BBS) environment that features a large number of moving
objects and partial observability, demonstrating clear improvements over prior
models."
12853,"Our results strongly motivate further research utilizing the adversarial properties of linearized training
we observed as a defense mechanism for adversarial attacks.","We additionally demonstrated that
a frozen feature set is necessary for retaining the full robustness aspect of these kernels, as either
allowing the batchnorm parameters to vary or using SGD as opposed to linearized training in the
second stage results in a drop in robust accuracy.","Furthermore, we note that linearized
training costs approximately double the cost of benign training, however, adversarial training is
signiﬁcantly more expensive.",2022-10-21 15:21:15+00:00,Evolution of Neural Tangent Kernels under Benign and Adversarial Training,cs.LG,"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']","[arxiv.Result.Author('Noel Loo'), arxiv.Result.Author('Ramin Hasani'), arxiv.Result.Author('Alexander Amini'), arxiv.Result.Author('Daniela Rus')]","Two key challenges facing modern deep learning are mitigating deep networks'
vulnerability to adversarial attacks and understanding deep learning's
generalization capabilities. Towards the first issue, many defense strategies
have been developed, with the most common being Adversarial Training (AT).
Towards the second challenge, one of the dominant theories that has emerged is
the Neural Tangent Kernel (NTK) -- a characterization of neural network
behavior in the infinite-width limit. In this limit, the kernel is frozen, and
the underlying feature map is fixed. In finite widths, however, there is
evidence that feature learning happens at the earlier stages of the training
(kernel learning) before a second phase where the kernel remains fixed (lazy
training). While prior work has aimed at studying adversarial vulnerability
through the lens of the frozen infinite-width NTK, there is no work that
studies the adversarial robustness of the empirical/finite NTK during training.
In this work, we perform an empirical study of the evolution of the empirical
NTK under standard and adversarial training, aiming to disambiguate the effect
of adversarial training on kernel learning and lazy training. We find under
adversarial training, the empirical NTK rapidly converges to a different kernel
(and feature map) than standard training. This new kernel provides adversarial
robustness, even when non-robust training is performed on top of it.
Furthermore, we find that adversarial training on top of a fixed kernel can
yield a classifier with $76.1\%$ robust accuracy under PGD attacks with
$\varepsilon = 4/255$ on CIFAR-10."
12861,"Whether the deviation from physical intuition
can have similarly harmful implications, should be the subject of further research.","The ﬁndings show similarities to
the famous Clever-Hans effect in classiﬁcation [23].","In any case, we recommend including the analysis of model strategies in the model selection process
since often physically more plausible models were obtained with only minor performance losses.",2022-10-21 16:59:06+00:00,Towards transparent ANN wind turbine power curve models,cs.LG,"['cs.LG', 'eess.SP']",[arxiv.Result.Author('Simon Letzgus')],"Accurate wind turbine power curve models, which translate ambient conditions
into turbine power output, are crucial for wind energy to scale and fulfil its
proposed role in the global energy transition. Machine learning methods, in
particular deep neural networks (DNNs), have shown significant advantages over
parametric, physics-informed power curve modelling approaches. Nevertheless,
they are often criticised as opaque black boxes with no physical understanding
of the system they model, which hinders their application in practice. We apply
Shapley values, a popular explainable artificial intelligence (XAI) method, to,
for the first time, uncover and validate the strategies learned by DNNs from
operational wind turbine data. Our findings show that the trend towards ever
larger model architectures, driven by the focus on test-set performance, can
result in physically implausible model strategies, similar to the Clever Hans
effect observed in classification. We, therefore, call for a more prominent
role of XAI methods in model selection and additionally offer a practical
strategy to use model explanations for wind turbine condition monitoring."
12881,"These observations motivate further research into automatic selection of variant-expert policies
for transfer.",experts in Breakout.,"The rich off-diagonal structure also recommends the use of Atari game variations as systematic curricula
in a continual learning setting, if only for leveraging the potential for policy reuse highlighted by our evaluations.",2022-10-22 13:40:12+00:00,Probing Transfer in Deep Reinforcement Learning without Task Engineering,cs.LG,['cs.LG'],"[arxiv.Result.Author('Andrei A. Rusu'), arxiv.Result.Author('Sebastian Flennerhag'), arxiv.Result.Author('Dushyant Rao'), arxiv.Result.Author('Razvan Pascanu'), arxiv.Result.Author('Raia Hadsell')]","We evaluate the use of original game curricula supported by the Atari 2600
console as a heterogeneous transfer benchmark for deep reinforcement learning
agents. Game designers created curricula using combinations of several discrete
modifications to the basic versions of games such as Space Invaders, Breakout
and Freeway, making them progressively more challenging for human players. By
formally organising these modifications into several factors of variation, we
are able to show that Analyses of Variance (ANOVA) are a potent tool for
studying the effects of human-relevant domain changes on the learning and
transfer performance of a deep reinforcement learning agent. Since no manual
task engineering is needed on our part, leveraging the original multi-factorial
design avoids the pitfalls of unintentionally biasing the experimental setup.
We find that game design factors have a large and statistically significant
impact on an agent's ability to learn, and so do their combinatorial
interactions. Furthermore, we show that zero-shot transfer from the basic games
to their respective variations is possible, but the variance in performance is
also largely explained by interactions between factors. As such, we argue that
Atari game curricula offer a challenging benchmark for transfer learning in RL,
that can help the community better understand the generalisation capabilities
of RL agents along dimensions which meaningfully impact human generalisation
performance. As a start, we report that value-function finetuning of regularly
trained agents achieves positive transfer in a majority of cases, but
significant headroom for algorithmic innovation remains. We conclude with the
observation that selective transfer from multiple variants could further
improve performance."
12914,"4.2 Event Prediction

We further study the performance of CaseQ in Stack Overﬂow and ATM datasets, which have distinct
statistical features compared to sequential recommendation datasets.","Also, CaseQ is agnostic to the
sequential prediction backbones and is promising for combining with other advanced architectures.","We consider three commonly
adopted models for predicting discrete sequential events, i.e., GRU, LSTM and Transformer [48].",2022-10-24 07:54:13+00:00,Towards Out-of-Distribution Sequential Event Prediction: A Causal Treatment,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Chenxiao Yang'), arxiv.Result.Author('Qitian Wu'), arxiv.Result.Author('Qingsong Wen'), arxiv.Result.Author('Zhiqiang Zhou'), arxiv.Result.Author('Liang Sun'), arxiv.Result.Author('Junchi Yan')]","The goal of sequential event prediction is to estimate the next event based
on a sequence of historical events, with applications to sequential
recommendation, user behavior analysis and clinical treatment. In practice, the
next-event prediction models are trained with sequential data collected at one
time and need to generalize to newly arrived sequences in remote future, which
requires models to handle temporal distribution shift from training to testing.
In this paper, we first take a data-generating perspective to reveal a negative
result that existing approaches with maximum likelihood estimation would fail
for distribution shift due to the latent context confounder, i.e., the common
cause for the historical events and the next event. Then we devise a new
learning objective based on backdoor adjustment and further harness variational
inference to make it tractable for sequence learning problems. On top of that,
we propose a framework with hierarchical branching structures for learning
context-specific representations. Comprehensive experiments on diverse tasks
(e.g., sequential recommendation) demonstrate the effectiveness, applicability
and scalability of our method with various off-the-shelf models as backbones."
12922,"such as channel state information and transmission error,
                                                                           require further study.","One potential way to solve this issue              the convergence properties and communication complex-
      is to propose a meta learning algorithm that is robust to            ity of FedMeta considering factors of wireless networks,
      tasks with a non-stationary distribution.","• Most meta learning algorithms are trained and tested
      using a small number of benchmark datasets, which                                       IX.",2022-10-24 10:59:29+00:00,Federated and Meta learning over Non-Wireless and Wireless Networks: A Tutorial,cs.LG,"['cs.LG', 'cs.AI', 'cs.NI']","[arxiv.Result.Author('Xiaonan Liu'), arxiv.Result.Author('Yansha Deng'), arxiv.Result.Author('Arumugam Nallanathan'), arxiv.Result.Author('Mehdi Bennis')]","In recent years, various machine learning (ML) solutions have been developed
to solve resource management, interference management, autonomy, and
decision-making problems in non-wireless and wireless networks. Standard ML
approaches require collecting data at a central server for training, which
cannot preserve the data privacy of devices. To address this issue, federated
learning (FL) is an effective method to allow edge devices to collaboratively
train ML models without sharing local datasets for data privacy. Typically, FL
focuses on learning a global model for a given task and all devices and hence
cannot adapt the model to devices with different data distributions. In such
cases, meta learning can be employed to adapt learning models to different data
distributions using a few data samples. In this tutorial, we conduct a
comprehensive review on FL, meta learning, and federated meta learning
(FedMeta). Compared to other tutorial papers, our objective is to leverage how
FL/meta-learning/FedMeta can be designed, optimized, and evolved over
non-wireless and wireless networks. Furthermore, we analyze not only the
relationship among these learning algorithms but also their advantages and
disadvantages in real-world applications."
12945,"Addi-
tionally, we believe our ﬁndings and method oﬀer avenues of further research in
understanding the interaction between network compressibility and generalisa-
tion, particularly when viewing deep learning through the minimal description
length principle lens.","The motivation is that this combination of outcomes will oﬀer accelerator
designers more scope for weight re-use and the ability to keep most/all weights
close to computation to reduce the energy-hungry data movement costs.","Weight Fixing Networks  15

Acknowledgements.",2022-10-24 19:18:02+00:00,Weight Fixing Networks,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Christopher Subia-Waud'), arxiv.Result.Author('Srinandan Dasmahapatra')]","Modern iterations of deep learning models contain millions (billions) of
unique parameters, each represented by a b-bit number. Popular attempts at
compressing neural networks (such as pruning and quantisation) have shown that
many of the parameters are superfluous, which we can remove (pruning) or
express with less than b-bits (quantisation) without hindering performance.
Here we look to go much further in minimising the information content of
networks. Rather than a channel or layer-wise encoding, we look to lossless
whole-network quantisation to minimise the entropy and number of unique
parameters in a network. We propose a new method, which we call Weight Fixing
Networks (WFN) that we design to realise four model outcome objectives: i) very
few unique weights, ii) low-entropy weight encodings, iii) unique weight values
which are amenable to energy-saving versions of hardware multiplication, and
iv) lossless task-performance. Some of these goals are conflicting. To best
balance these conflicts, we combine a few novel (and some well-trodden) tricks;
a novel regularisation term, (i, ii) a view of clustering cost as relative
distance change (i, ii, iv), and a focus on whole-network re-use of weights (i,
iii). Our Imagenet experiments demonstrate lossless compression using 56x fewer
unique weights and a 1.9x lower weight-space entropy than SOTA quantisation
approaches."
12950,"5.3 Additional Experiments

To further study the generalization of our observations, we perform four additional experiments.","We do not
observe signiﬁcant differences between these two visualizations.","5.3.1 RL Algorithm

To test how our ﬁndings generalize to RL algorithms other than PPO, we repeat our experiments by
training deep RL policies with stochastic actor-critic (SAC) [Haarnoja et al., 2018] algorithm.",2022-10-24 21:22:12+00:00,Understanding the Evolution of Linear Regions in Deep Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Setareh Cohan'), arxiv.Result.Author('Nam Hee Kim'), arxiv.Result.Author('David Rolnick'), arxiv.Result.Author('Michiel van de Panne')]","Policies produced by deep reinforcement learning are typically characterised
by their learning curves, but they remain poorly understood in many other
respects. ReLU-based policies result in a partitioning of the input space into
piecewise linear regions. We seek to understand how observed region counts and
their densities evolve during deep reinforcement learning using empirical
results that span a range of continuous control tasks and policy network
dimensions. Intuitively, we may expect that during training, the region density
increases in the areas that are frequently visited by the policy, thereby
affording fine-grained control. We use recent theoretical and empirical results
for the linear regions induced by neural networks in supervised learning
settings for grounding and comparison of our results. Empirically, we find that
the region density increases only moderately throughout training, as measured
along fixed trajectories coming from the final policy. However, the
trajectories themselves also increase in length during training, and thus the
region densities decrease as seen from the perspective of the current
trajectory. Our findings suggest that the complexity of deep reinforcement
learning policies does not principally emerge from a significant growth in the
complexity of functions observed on-and-around trajectories of the policy."
12951,"5.3 Additional Experiments

To further study the generalization of our observations, we perform four additional experiments.","We do not
observe signiﬁcant differences between these two visualizations.","5.3.1 RL Algorithm

To test how our ﬁndings generalize to RL algorithms other than PPO, we repeat our experiments by
training deep RL policies with stochastic actor-critic (SAC) [Haarnoja et al., 2018] algorithm.",2022-10-24 21:22:12+00:00,Understanding the Evolution of Linear Regions in Deep Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Setareh Cohan'), arxiv.Result.Author('Nam Hee Kim'), arxiv.Result.Author('David Rolnick'), arxiv.Result.Author('Michiel van de Panne')]","Policies produced by deep reinforcement learning are typically characterised
by their learning curves, but they remain poorly understood in many other
respects. ReLU-based policies result in a partitioning of the input space into
piecewise linear regions. We seek to understand how observed region counts and
their densities evolve during deep reinforcement learning using empirical
results that span a range of continuous control tasks and policy network
dimensions. Intuitively, we may expect that during training, the region density
increases in the areas that are frequently visited by the policy, thereby
affording fine-grained control. We use recent theoretical and empirical results
for the linear regions induced by neural networks in supervised learning
settings for grounding and comparison of our results. Empirically, we find that
the region density increases only moderately throughout training, as measured
along fixed trajectories coming from the final policy. However, the
trajectories themselves also increase in length during training, and thus the
region densities decrease as seen from the perspective of the current
trajectory. Our findings suggest that the complexity of deep reinforcement
learning policies does not principally emerge from a significant growth in the
complexity of functions observed on-and-around trajectories of the policy."
12954,"Our study revealed preliminary results showing great potential for
                                        further research with larger experiments.","We developed a device as well as a procedure to evaluate
                                        the ambient environment of a room to perform a study that attempts to use sensor data to predict a person’s well-being score in that
                                        environment, therefore evaluating the primary aspect of Feng Shui.","CCS Concepts: • Human-centered computing → Ubiquitous computing; • Mathematics of computing → Exploratory data
                                        analysis; • Information systems → Clustering and classification; Content analysis and feature selection.",2022-10-25 00:13:05+00:00,Feng-Shui Compass: A Modern Exploration of Traditional Chinese Environmental Analysis,cs.LG,"['cs.LG', 'cs.HC', 'cs.RO']","[arxiv.Result.Author('Xuanyu Fang'), arxiv.Result.Author('Yunzhu Pan'), arxiv.Result.Author('Hongjun Wu')]","The technological advancement in data analysis and sensor technology has
contributed to a growth in knowledge of the surrounding environments. Feng
Shui, the Chinese philosophy of evaluating a certain environment and how it
influences human well-being, can only be determined by self-claimed specialists
for the past thousands of years. We developed a device as well as a procedure
to evaluate the ambient environment of a room to perform a study that attempts
to use sensor data to predict the well-being score of a person in that
environment, therefore evaluating the primary aspect of Feng Shui. Our study
revealed preliminary results showing great potential for further research with
larger experiments."
12976,"Section 4 concludes the paper and suggests directions for possible
improvements of the work and directions for further study.","Section 3 introduced the
performance of models with different weather variable inputs and provided possible
explanation.","3
2.",2022-10-24 13:01:47+00:00,Exploring the impact of weather on Metro demand forecasting using machine learning method,cs.LG,"['cs.LG', 'stat.AP']","[arxiv.Result.Author('Yiming Hu'), arxiv.Result.Author('Yangchuan Huang'), arxiv.Result.Author('Shuyin Liu'), arxiv.Result.Author('Yuanyang Qi'), arxiv.Result.Author('Danhui Bai')]","Urban rail transit provides significant comprehensive benefits such as large
traffic volume and high speed, serving as one of the most important components
of urban traffic construction management and congestion solution. Using real
passenger flow data of an Asian subway system from April to June of 2018, this
work analyzes the space-time distribution of the passenger flow using
short-term traffic flow prediction. Stations are divided into four types for
passenger flow forecasting, and meteorological records are collected for the
same period. Then, machine learning methods with different inputs are applied
and multivariate regression is performed to evaluate the improvement effect of
each weather element on passenger flow forecasting of representative metro
stations on hourly basis. Our results show that by inputting weather variables
the precision of prediction on weekends enhanced while the performance on
weekdays only improved marginally, while the contribution of different elements
of weather differ. Also, different categories of stations are affected
differently by weather. This study provides a possible method to further
improve other prediction models, and attests to the promise of data-driven
analytics for optimization of short-term scheduling in transit management."
12983,It hints that this approach is not far-fetched and requires further study.,"However, the ”reg-
ulated model” shows an advantage in all KPIs compared to the vanilla model.","11
Figure 8: Comparison of KPIs between vanilla model and regulation term
                                               12
5 Summary and Future Work

We described the approaches for evaluating the goodness of ﬁt of ML models and
discussed some of their inherent failures.",2022-10-25 13:21:19+00:00,Parametric PDF for Goodness of Fit,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Natan Katz'), arxiv.Result.Author('Uri Itai')]","The goodness of fit methods for classification problems relies traditionally
on confusion matrices. This paper aims to enrich these methods with a risk
evaluation and stability analysis tools. For this purpose, we present a
parametric PDF framework."
12987,"In order to further study the amount
of distortion in the resulting embedding space, we apply nearest-neighbour probing with respect to
the clean data.","As expected, deep neural networks indeed manage to achieve
zero training error across a variety of datasets while not generalizing at all, both with respect to a
random test labeling as well as the original, clean test labels.","More precisely, given the features hψ learnt from training on the random label task
{(xi, y˜i)}ni=1, we apply probing based on the clean training data {(xi, yi)}ni=1 and evaluate with re-
spect to clean test data.",2022-10-25 13:41:31+00:00,The Curious Case of Benign Memorization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Sotiris Anagnostidis'), arxiv.Result.Author('Gregor Bachmann'), arxiv.Result.Author('Lorenzo Noci'), arxiv.Result.Author('Thomas Hofmann')]","Despite the empirical advances of deep learning across a variety of learning
tasks, our theoretical understanding of its success is still very restricted.
One of the key challenges is the overparametrized nature of modern models,
enabling complete overfitting of the data even if the labels are randomized,
i.e. networks can completely memorize all given patterns. While such a
memorization capacity seems worrisome, in this work we show that under training
protocols that include data augmentation, neural networks learn to memorize
entirely random labels in a benign way, i.e. they learn embeddings that lead to
highly non-trivial performance under nearest neighbour probing. We demonstrate
that deep models have the surprising ability to separate noise from signal by
distributing the task of memorization and feature learning to different layers.
As a result, only the very last layers are used for memorization, while
preceding layers encode performant features which remain largely unaffected by
the label noise. We explore the intricate role of the augmentations used for
training and identify a memorization-generalization trade-off in terms of their
diversity, marking a clear distinction to all previous works. Finally, we give
a first explanation for the emergence of benign memorization by showing that
malign memorization under data augmentation is infeasible due to the
insufficient capacity of the model for the increased sample size. As a
consequence, the network is forced to leverage the correlated nature of the
augmentations and as a result learns meaningful features. To complete the
picture, a better theory of feature learning in deep neural networks is
required to fully understand the origins of this phenomenon."
12988,"In order to further study the amount
of distortion in the resulting embedding space, we apply nearest-neighbour probing with respect to
the clean data.","As expected, deep neural networks indeed manage to achieve
zero training error across a variety of datasets while not generalizing at all, both with respect to a
random test labeling as well as the original, clean test labels.","More precisely, given the features hψ learnt from training on the random label task
{(xi, y˜i)}ni=1, we apply probing based on the clean training data {(xi, yi)}ni=1 and evaluate with re-
spect to clean test data.",2022-10-25 13:41:31+00:00,The Curious Case of Benign Memorization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Sotiris Anagnostidis'), arxiv.Result.Author('Gregor Bachmann'), arxiv.Result.Author('Lorenzo Noci'), arxiv.Result.Author('Thomas Hofmann')]","Despite the empirical advances of deep learning across a variety of learning
tasks, our theoretical understanding of its success is still very restricted.
One of the key challenges is the overparametrized nature of modern models,
enabling complete overfitting of the data even if the labels are randomized,
i.e. networks can completely memorize all given patterns. While such a
memorization capacity seems worrisome, in this work we show that under training
protocols that include data augmentation, neural networks learn to memorize
entirely random labels in a benign way, i.e. they learn embeddings that lead to
highly non-trivial performance under nearest neighbour probing. We demonstrate
that deep models have the surprising ability to separate noise from signal by
distributing the task of memorization and feature learning to different layers.
As a result, only the very last layers are used for memorization, while
preceding layers encode performant features which remain largely unaffected by
the label noise. We explore the intricate role of the augmentations used for
training and identify a memorization-generalization trade-off in terms of their
diversity, marking a clear distinction to all previous works. Finally, we give
a first explanation for the emergence of benign memorization by showing that
malign memorization under data augmentation is infeasible due to the
insufficient capacity of the model for the increased sample size. As a
consequence, the network is forced to leverage the correlated nature of the
augmentations and as a result learns meaningful features. To complete the
picture, a better theory of feature learning in deep neural networks is
required to fully understand the origins of this phenomenon."
13018,"We further study other related notions of non-stationarity
                                                for which we also prove near-optimal dynamic regret guarantees under additional assumptions
                                                on the underlying preference model.","This yields the ﬁrst near-optimal dynamic
                                                regret algorithm for unknown SCW.","1 Introduction

                                        Multi-Armed Bandits (MAB) [42, 26, 23] are a well-studied online learning framework, which can
                                        be used to model online decision-making under uncertainty.",2022-10-25 20:26:02+00:00,ANACONDA: An Improved Dynamic Regret Algorithm for Adaptive Non-Stationary Dueling Bandits,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Thomas Kleine Buening'), arxiv.Result.Author('Aadirupa Saha')]","We study the problem of non-stationary dueling bandits and provide the first
adaptive dynamic regret algorithm for this problem. The only two existing
attempts in this line of work fall short across multiple dimensions, including
pessimistic measures of non-stationary complexity and non-adaptive parameter
tuning that requires knowledge of the number of preference changes. We develop
an elimination-based rescheduling algorithm to overcome these shortcomings and
show a near-optimal $\tilde{O}(\sqrt{S^{\texttt{CW}} T})$ dynamic regret bound,
where $S^{\texttt{CW}}$ is the number of times the Condorcet winner changes in
$T$ rounds. This yields the first near-optimal dynamic regret algorithm for
unknown $S^{\texttt{CW}}$. We further study other related notions of
non-stationarity for which we also prove near-optimal dynamic regret guarantees
under additional assumptions on the underlying preference model."
13024,We further study the impact of      context is represented by a model.,"Unlike the hybrid linear model, where the coefﬁcients of
some features are shared by all arms while the others are      We consider a contextual bandit (Li et al., 2010), where
not, we introduce arm-speciﬁc random variables to capture      the relationship between the mean reward of an arm and its
the model misspeciﬁcation.","In round t ∈ [n], an
this structure on regret and propose an especially efﬁcient    agent pulls one of K arms with feature vectors xi,t ∈ Rd
implementation for this setting.",2022-10-26 05:18:09+00:00,Robust Contextual Linear Bandits,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Rong Zhu'), arxiv.Result.Author('Branislav Kveton')]","Model misspecification is a major consideration in applications of
statistical methods and machine learning. However, it is often neglected in
contextual bandits. This paper studies a common form of misspecification, an
inter-arm heterogeneity that is not captured by context. To address this issue,
we assume that the heterogeneity arises due to arm-specific random variables,
which can be learned. We call this setting a robust contextual bandit. The
arm-specific variables explain the unknown inter-arm heterogeneity, and we
incorporate them in the robust contextual estimator of the mean reward and its
uncertainty. We develop two efficient bandit algorithms for our setting: a UCB
algorithm called RoLinUCB and a posterior-sampling algorithm called RoLinTS. We
analyze both algorithms and bound their $n$-round Bayes regret. Our experiments
show that RoLinTS is comparably statistically efficient to the classic methods
when the misspecification is low, more robust when the misspecification is
high, and significantly more computationally efficient than its naive
implementation."
13028,"The aforementioned studies test their approaches on small-scale environments for illustration
(e.g., one intersection or dozens of intersections) while leaving scalability issues and large-scale
applications for further research.","Each agent then chooses relevant features from the container to
make its own decision based on the Deep Deterministic Policy Gradient (DDPG) algorithm.","In practice, megalopolis are usually installed with thousands
of traﬃc light intersections, which need to be controlled simultaneously.",2022-10-26 07:34:51+00:00,A Bibliometric Analysis and Review on Reinforcement Learning for Transportation Applications,cs.LG,['cs.LG'],"[arxiv.Result.Author('Can Li'), arxiv.Result.Author('Lei Bai'), arxiv.Result.Author('Lina Yao'), arxiv.Result.Author('S. Travis Waller'), arxiv.Result.Author('Wei Liu')]","Transportation is the backbone of the economy and urban development.
Improving the efficiency, sustainability, resilience, and intelligence of
transportation systems is critical and also challenging. The constantly
changing traffic conditions, the uncertain influence of external factors (e.g.,
weather, accidents), and the interactions among multiple travel modes and
multi-type flows result in the dynamic and stochastic natures of transportation
systems. The planning, operation, and control of transportation systems require
flexible and adaptable strategies in order to deal with uncertainty,
non-linearity, variability, and high complexity. In this context, Reinforcement
Learning (RL) that enables autonomous decision-makers to interact with the
complex environment, learn from the experiences, and select optimal actions has
been rapidly emerging as one of the most useful approaches for smart
transportation. This paper conducts a bibliometric analysis to identify the
development of RL-based methods for transportation applications, typical
journals/conferences, and leading topics in the field of intelligent
transportation in recent ten years. Then, this paper presents a comprehensive
literature review on applications of RL in transportation by categorizing
different methods with respect to the specific application domains. The
potential future research directions of RL applications and developments are
also discussed."
13036,"The ﬂexibility of latent space priors makes extensions towards classes
of problems in which broader repertoires of skills are required, e.g., object manipulation, attractive subjects
of further study.","Our approach is aimed at settings that beneﬁt from the availability of proprioceptive motor skills, which is a
common case in robotics applications.",Acknowledgements: We thank Josh Merel and Alessandro Lazaric for insightful and helpful discussions.,2022-10-26 13:08:46+00:00,Leveraging Demonstrations with Latent Space Priors,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']","[arxiv.Result.Author('Jonas Gehring'), arxiv.Result.Author('Deepak Gopinath'), arxiv.Result.Author('Jungdam Won'), arxiv.Result.Author('Andreas Krause'), arxiv.Result.Author('Gabriel Synnaeve'), arxiv.Result.Author('Nicolas Usunier')]","Demonstrations provide insight into relevant state or action space regions,
bearing great potential to boost the efficiency and practicality of
reinforcement learning agents. In this work, we propose to leverage
demonstration datasets by combining skill learning and sequence modeling.
Starting with a learned joint latent space, we separately train a generative
model of demonstration sequences and an accompanying low-level policy. The
sequence model forms a latent space prior over plausible demonstration
behaviors to accelerate learning of high-level policies. We show how to acquire
such priors from state-only motion capture demonstrations and explore several
methods for integrating them into policy learning on transfer tasks. Our
experimental results confirm that latent space priors provide significant gains
in learning speed and final performance in a set of challenging sparse-reward
environments with a complex, simulated humanoid. Videos, source code and
pre-trained models are available at the corresponding project website at
https://facebookresearch.github.io/latent-space-priors ."
13038,"Surprisingly, the CATE performance seems to have little correlation to the causal discovery
performance and warrants further study in the future.","All Rhino-based method perform sim-
ilarly.","19
                     Dim = 5                  Dim = 10
      2
RMSE

      0                                       Dim = 40                Model Name
                    Dim = 20                                           Rhino (L=2)
RMSE                                                                   Rhino+g (L=2)
      2                                                                Rhino+s (L=2)

      0 E[ATE]                ML ATE  E[ATE]            ML ATE

Figure 2: Comparison of the RMSE of the average treatment effects (CATEs) of the different in-
stantiations of Rhino depending on the dimensionality.",2022-10-26 13:33:58+00:00,Rhino: Deep Causal Temporal Relationship Learning With History-dependent Noise,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Wenbo Gong'), arxiv.Result.Author('Joel Jennings'), arxiv.Result.Author('Cheng Zhang'), arxiv.Result.Author('Nick Pawlowski')]","Discovering causal relationships between different variables from time series
data has been a long-standing challenge for many domains such as climate
science, finance, and healthcare. Given the complexity of real-world
relationships and the nature of observations in discrete time, causal discovery
methods need to consider non-linear relations between variables, instantaneous
effects and history-dependent noise (the change of noise distribution due to
past actions). However, previous works do not offer a solution addressing all
these problems together. In this paper, we propose a novel causal relationship
learning framework for time-series data, called Rhino, which combines vector
auto-regression, deep learning and variational inference to model non-linear
relationships with instantaneous effects while allowing the noise distribution
to be modulated by historical observations. Theoretically, we prove the
structural identifiability of Rhino. Our empirical results from extensive
synthetic experiments and two real-world benchmarks demonstrate better
discovery performance compared to relevant baselines, with ablation studies
revealing its robustness under model misspecification."
13078,This is an interesting direction for further research.,"Nonetheless, there is a lack of work studying the impact of the aggregation mechanism on explainability.","Reliably measuring ﬁdelity can be tricky for node classiﬁcation
too.",2022-10-27 10:25:51+00:00,Explaining the Explainers in Graph Neural Networks: a Comparative Study,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Antonio Longa'), arxiv.Result.Author('Steve Azzolin'), arxiv.Result.Author('Gabriele Santin'), arxiv.Result.Author('Giulia Cencetti'), arxiv.Result.Author('Pietro Liò'), arxiv.Result.Author('Bruno Lepri'), arxiv.Result.Author('Andrea Passerini')]","Following a fast initial breakthrough in graph based learning, Graph Neural
Networks (GNNs) have reached a widespread application in many science and
engineering fields, prompting the need for methods to understand their decision
process.
  GNN explainers have started to emerge in recent years, with a multitude of
methods both novel or adapted from other domains. To sort out this plethora of
alternative approaches, several studies have benchmarked the performance of
different explainers in terms of various explainability metrics. However, these
earlier works make no attempts at providing insights into why different GNN
architectures are more or less explainable, or which explainer should be
preferred in a given setting.
  In this survey, we fill these gaps by devising a systematic experimental
study, which tests ten explainers on eight representative architectures trained
on six carefully designed graph and node classification datasets. With our
results we provide key insights on the choice and applicability of GNN
explainers, we isolate key components that make them usable and successful and
provide recommendations on how to avoid common interpretation pitfalls. We
conclude by highlighting open questions and directions of possible future
research."
13080,"It would be interesting to further study the effect of the mirror potential on the performance
of SMD-trained models for different learning problems via analytic and numerical study of this PDE.","We characterized the time evolution of the distribution of the parameters through a nonlinear PDE and gave a
Riemannian interpretation.","It would also be
interesting to generalize these results to other network models and learning algorithms.",2022-10-27 11:04:00+00:00,Stochastic Mirror Descent in Average Ensemble Models,cs.LG,"['cs.LG', 'math.AP', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Taylan Kargin'), arxiv.Result.Author('Fariborz Salehi'), arxiv.Result.Author('Babak Hassibi')]","The stochastic mirror descent (SMD) algorithm is a general class of training
algorithms, which includes the celebrated stochastic gradient descent (SGD), as
a special case. It utilizes a mirror potential to influence the implicit bias
of the training algorithm. In this paper we explore the performance of the SMD
iterates on mean-field ensemble models. Our results generalize earlier ones
obtained for SGD on such models. The evolution of the distribution of
parameters is mapped to a continuous time process in the space of probability
distributions. Our main result gives a nonlinear partial differential equation
to which the continuous time process converges in the asymptotic regime of
large networks. The impact of the mirror potential appears through a
multiplicative term that is equal to the inverse of its Hessian and which can
be interpreted as defining a gradient flow over an appropriately defined
Riemannian manifold. We provide numerical simulations which allow us to study
and characterize the effect of the mirror potential on the performance of
networks trained with SMD for some binary classification problems."
13134,"This not only
  provides valuable information about the extent to which each individual
  prediction can be trusted, but also allows for a further study on how traﬃc
  behaves in diﬀerent time intervals and seasons over the year.","Unlike customary regression
  scores, which are commonly obtained for a whole test dataset and ap-
  prise about the general (averaged) performance of the trained model, un-
  certainty metrics reach the individual prediction level, giving conﬁdence
  information for each data point predicted by the model.","• Model selection and comparison: as explained before, a part of the total
  uncertainty of the output of a forecasting scheme can be attributed to the
  way the information is modeled.",2022-10-28 10:49:55+00:00,"Measuring the Confidence of Traffic Forecasting Models: Techniques, Experimental Comparison and Guidelines towards Their Actionability",cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Ibai Laña'), arxiv.Result.Author('Ignacio'), arxiv.Result.Author('Olabarrieta'), arxiv.Result.Author('Javier Del Ser')]","The estimation of the amount of uncertainty featured by predictive machine
learning models has acquired a great momentum in recent years. Uncertainty
estimation provides the user with augmented information about the model's
confidence in its predicted outcome. Despite the inherent utility of this
information for the trustworthiness of the user, there is a thin consensus
around the different types of uncertainty that one can gauge in machine
learning models and the suitability of different techniques that can be used to
quantify the uncertainty of a specific model. This subject is mostly non
existent within the traffic modeling domain, even though the measurement of the
confidence associated to traffic forecasts can favor significantly their
actionability in practical traffic management systems. This work aims to cover
this lack of research by reviewing different techniques and metrics of
uncertainty available in the literature, and by critically discussing how
confidence levels computed for traffic forecasting models can be helpful for
researchers and practitioners working in this research area. To shed light with
empirical evidence, this critical discussion is further informed by
experimental results produced by different uncertainty estimation techniques
over real traffic data collected in Madrid (Spain), rendering a general
overview of the benefits and caveats of every technique, how they can be
compared to each other, and how the measured uncertainty decreases depending on
the amount, quality and diversity of data used to produce the forecasts."
13135,"the system and
                                                                    In order to keep our methods comparable to literature we
   • an outlook into open questions for further research.","a novel dataset of patient histories from the CheXpert
      dataset of chest X-ray images,                                   From the 14 observations, the CheXpert challenge proposes
                                                                    a subset of 5 pathologies as classiﬁcation target due to their
   • an overview of model architecture and argue for the            prevalence and clinical importance:
      inclusion of previous expert labels in the classiﬁcation
      process,                                                         1) Cardiomegaly,
                                                                       2) Edema,
   • an evaluation of the effectiveness of the proposed method         3) Consolidation,
      against baseline methods in which we show that, when             4) Atelectasis and
      available, patient history provides valuable information to      5) Pleural Effusion.","restrict our model training and evaluation on the 5 pathologies
                                                                    as well.",2022-10-28 11:47:15+00:00,Improving Chest X-Ray Classification by RNN-based Patient Monitoring,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('David Biesner'), arxiv.Result.Author('Helen Schneider'), arxiv.Result.Author('Benjamin Wulff'), arxiv.Result.Author('Ulrike Attenberger'), arxiv.Result.Author('Rafet Sifa')]","Chest X-Ray imaging is one of the most common radiological tools for
detection of various pathologies related to the chest area and lung function.
In a clinical setting, automated assessment of chest radiographs has the
potential of assisting physicians in their decision making process and optimize
clinical workflows, for example by prioritizing emergency patients.
  Most work analyzing the potential of machine learning models to classify
chest X-ray images focuses on vision methods processing and predicting
pathologies for one image at a time. However, many patients undergo such a
procedure multiple times during course of a treatment or during a single
hospital stay. The patient history, that is previous images and especially the
corresponding diagnosis contain useful information that can aid a
classification system in its prediction.
  In this study, we analyze how information about diagnosis can improve
CNN-based image classification models by constructing a novel dataset from the
well studied CheXpert dataset of chest X-rays. We show that a model trained on
additional patient history information outperforms a model trained without the
information by a significant margin.
  We provide code to replicate the dataset creation and model training."
13142,"In this section, we further study      VCA and nVCA do not require another data set, we
(ii) through experiments for anomaly detection.","Since
data in Section 6.1.","In          simply design an anomaly detector using VCA and
particular, we chose random aﬃne transformed data           nVCA as follows: First, following Livni et al.",2022-10-27 07:59:59+00:00,Vanishing Component Analysis with Contrastive Normalization,cs.LG,"['cs.LG', 'cs.SC']","[arxiv.Result.Author('Ryosuke Masuya'), arxiv.Result.Author('Yuichi Ike'), arxiv.Result.Author('Hiroshi Kera')]","Vanishing component analysis (VCA) computes approximate generators of
vanishing ideals of samples, which are further used for extracting nonlinear
features of the samples. Recent studies have shown that normalization of
approximate generators plays an important role and different normalization
leads to generators of different properties. In this paper, inspired by recent
self-supervised frameworks, we propose a contrastive normalization method for
VCA, where we impose the generators to vanish on the target samples and to be
normalized on the transformed samples. We theoretically show that a contrastive
normalization enhances the discriminative power of VCA, and provide the
algebraic interpretation of VCA under our normalization. Numerical experiments
demonstrate the effectiveness of our method. This is the first study to tailor
the normalization of approximate generators of vanishing ideals to obtain
discriminative features."
13158,"We further study the behavior of the set
                                                                   where fk,η+ε(z) = 1, and, by introducing the notion of
randomness of xi ∼ Unif ([−1, 1]), a candidate set with            ε-extensionm, we construct another sequence of indica-
size of the order n = Ω log η−1 is enough to guarantee             tor functions {fˆk}nk=1 that lower bound fk,η yet shows
that η∗ ≤ η for all z ∈ [−1/2, 1/2].","ever, this sequence is hard to control as it involves the
(Lueker, 1998) shows that, with high probability over the          fk,η+ε(z).",the advantage of large ε.,2022-10-29 12:22:17+00:00,Strong Lottery Ticket Hypothesis with $\varepsilon$--perturbation,cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'math.OC']","[arxiv.Result.Author('Zheyang Xiong'), arxiv.Result.Author('Fangshuo Liao'), arxiv.Result.Author('Anastasios Kyrillidis')]","The strong Lottery Ticket Hypothesis (LTH) claims the existence of a
subnetwork in a sufficiently large, randomly initialized neural network that
approximates some target neural network without the need of training. We extend
the theoretical guarantee of the strong LTH literature to a scenario more
similar to the original LTH, by generalizing the weight change in the
pre-training step to some perturbation around initialization. In particular, we
focus on the following open questions: By allowing an $\varepsilon$-scale
perturbation on the random initial weights, can we reduce the
over-parameterization requirement for the candidate network in the strong LTH?
Furthermore, does the weight change by SGD coincide with a good set of such
perturbation?
  We answer the first question by first extending the theoretical result on
subset sum to allow perturbation on the candidates. Applying this result to the
neural network setting, we show that such $\varepsilon$-perturbation reduces
the over-parameterization requirement of the strong LTH. To answer the second
question, we show via experiments that the perturbed weight achieved by the
projected SGD shows better performance under the strong LTH pruning."
13192,"[2020]) examined several methods on ﬁve regression datasets and concluded that
existing UQ methods are inadequate for all common-use cases and further research is needed.","Furthermore, (Hirschfeld et al.","A recently published
study by (Abdar et al.",2022-10-31 06:38:40+00:00,Confidence-Nets: A Step Towards better Prediction Intervals for regression Neural Networks on small datasets,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Mohamedelmujtaba Altayeb'), arxiv.Result.Author('Abdelrahman M. Elamin'), arxiv.Result.Author('Hozaifa Ahmed'), arxiv.Result.Author('Eithar Elfatih Elfadil Ibrahim'), arxiv.Result.Author('Omer Haydar'), arxiv.Result.Author('Saba Abdulaziz'), arxiv.Result.Author('Najlaa H. M. Mohamed')]","The recent decade has seen an enormous rise in the popularity of deep
learning and neural networks. These algorithms have broken many previous
records and achieved remarkable results. Their outstanding performance has
significantly sped up the progress of AI, and so far various milestones have
been achieved earlier than expected. However, in the case of relatively small
datasets, the performance of Deep Neural Networks (DNN) may suffer from reduced
accuracy compared to other Machine Learning models. Furthermore, it is
difficult to construct prediction intervals or evaluate the uncertainty of
predictions when dealing with regression tasks. In this paper, we propose an
ensemble method that attempts to estimate the uncertainty of predictions,
increase their accuracy and provide an interval for the expected variation.
Compared with traditional DNNs that only provide a prediction, our proposed
method can output a prediction interval by combining DNNs, extreme gradient
boosting (XGBoost) and dissimilarity computation techniques. Albeit the simple
design, this approach significantly increases accuracy on small datasets and
does not introduce much complexity to the architecture of the neural network.
The proposed method is tested on various datasets, and a significant
improvement in the performance of the neural network model is seen. The model's
prediction interval can include the ground truth value at an average rate of
71% and 78% across training sizes of 90% and 55%, respectively. Finally, we
highlight other aspects and applications of the approach in experimental error
estimation, and the application of transfer learning."
13202,"We hope that our work will stimulate further research in this area, and can be a further step towards making these
models available to a wider audience.","This high degree of compression may appear unsurprising, as these networks are
overparametrized; yet, as we discuss in our detailed analysis of results, compression induces non-trivial tradeoffs
between the accuracy of the language modeling (perplexity), bit-width, and the size of the original model.","In terms of limitations, our method currently does not provide speedups for the
actual multiplications, due to the lack of direct hardware support for mixed-precision operands (e.g.",2022-10-31 13:42:40+00:00,GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers,cs.LG,['cs.LG'],"[arxiv.Result.Author('Elias Frantar'), arxiv.Result.Author('Saleh Ashkboos'), arxiv.Result.Author('Torsten Hoefler'), arxiv.Result.Author('Dan Alistarh')]","Generative Pre-trained Transformer (GPT) models set themselves apart through
breakthrough performance across complex language modelling tasks, but also by
their extremely high computational and storage costs. Specifically, due to
their massive size, even inference for large, highly-accurate GPT models may
require multiple performant GPUs to execute, which limits the usability of such
models. While there is emerging work on relieving this pressure via model
compression, the applicability and performance of existing compression
techniques is limited by the scale and complexity of GPT models. In this paper,
we address this challenge, and propose GPTQ, a new one-shot weight quantization
method based on approximate second-order information, that is both
highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT
models with 175 billion parameters in approximately four GPU hours, reducing
the bitwidth down to 3 or 4 bits per weight, with negligible accuracy
degradation relative to the uncompressed baseline. Our method more than doubles
the compression gains relative to previously-proposed one-shot quantization
methods, preserving accuracy, allowing us for the first time to execute an 175
billion-parameter model inside a single GPU. We show experimentally that these
improvements can be leveraged for end-to-end inference speedups over FP16, of
around 2x when using high-end GPUs (NVIDIA A100) and 4x when using more
cost-effective ones (NVIDIA A6000). The implementation is available at
https://github.com/IST-DASLab/gptq."
13217,"At a higher level, the task
  are provided with the Open MatSci ML Toolkit, facilitating       speciﬁc S2EFLitModule is instantiated by passing an
  further research into data efﬁciency.","readout operation to reduce node and graph level features
  We also provide the mechanism for creating other splits          to a scalar value for the energy.","To use the devsets for     instance of EGNN, and implements the logic for training
  development, there is a convenient mechanism for retrieving      (i.e.",2022-10-31 17:11:36+00:00,The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in Materials Science,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Santiago Miret'), arxiv.Result.Author('Kin Long Kelvin Lee'), arxiv.Result.Author('Carmelo Gonzales'), arxiv.Result.Author('Marcel Nassar'), arxiv.Result.Author('Matthew Spellings')]","We present the Open MatSci ML Toolkit: a flexible, self-contained, and
scalable Python-based framework to apply deep learning models and methods on
scientific data with a specific focus on materials science and the OpenCatalyst
Dataset. Our toolkit provides: 1. A scalable machine learning workflow for
materials science leveraging PyTorch Lightning, which enables seamless scaling
across different computation capabilities (laptop, server, cluster) and
hardware platforms (CPU, GPU, XPU). 2. Deep Graph Library (DGL) support for
rapid graph neural network prototyping and development. By publishing and
sharing this toolkit with the research community via open-source release, we
hope to: 1. Lower the entry barrier for new machine learning researchers and
practitioners that want to get started with the OpenCatalyst dataset, which
presently comprises the largest computational materials science dataset. 2.
Enable the scientific community to apply advanced machine learning tools to
high-impact scientific challenges, such as modeling of materials behavior for
clean energy applications. We demonstrate the capabilities of our framework by
enabling three new equivariant neural network models for multiple OpenCatalyst
tasks and arrive at promising results for compute scaling and model
performance."
13238,"the distributed GNN and sampling-based GNN,
leads to an interesting training paradigm for further study.","This simple
combination of the two basic GNN acceleration techniques, i.e.",DistDGLv2 [143] is a continuous work of DistDGL by the DGL team from Amazon.,2022-11-01 01:57:00+00:00,Distributed Graph Neural Network Training: A Survey,cs.LG,"['cs.LG', 'cs.AI', 'cs.DB', 'cs.DC']","[arxiv.Result.Author('Yingxia Shao'), arxiv.Result.Author('Hongzheng Li'), arxiv.Result.Author('Xizhi Gu'), arxiv.Result.Author('Hongbo Yin'), arxiv.Result.Author('Yawen Li'), arxiv.Result.Author('Xupeng Miao'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Bin Cui'), arxiv.Result.Author('Lei Chen')]","Graph neural networks (GNNs) are a type of deep learning models that learning
over graphs, and have been successfully applied in many domains. Despite the
effectiveness of GNNs, it is still challenging for GNNs to efficiently scale to
large graphs. As a remedy, distributed computing becomes a promising solution
of training large-scale GNNs, since it is able to provide abundant computing
resources. However, the dependency of graph structure increases the difficulty
of achieving high-efficiency distributed GNN training, which suffers from the
massive communication and workload imbalance. In recent years, many efforts
have been made on distributed GNN training, and an array of training algorithms
and systems have been proposed. Yet, there is a lack of systematic review on
the optimization techniques from graph processing to distributed execution. In
this survey, we analyze three major challenges in distributed GNN training that
are massive feature communication, the loss of model accuracy and workload
imbalance. Then we introduce a new taxonomy for the optimization techniques in
distributed GNN training that address the above challenges. The new taxonomy
classifies existing techniques into four categories that are GNN data
partition, GNN batch generation, GNN execution model, and GNN communication
protocol.We carefully discuss the techniques in each category. In the end, we
summarize existing distributed GNN systems for multi-GPUs, GPU-clusters and
CPU-clusters, respectively, and give a discussion about the future direction on
scalable GNNs."
13246,"It is not possible to nominate
a clear winner among SADT variants, hinting that the SADT could beneﬁt from further study.","Hence, SADT can offer a better alternative to compared training schemes while
avoiding computational costs of repeated forward and backward passes.","Compared       Simple CNN   VGG Net      InceptionResNet  Compared       Simple CNN   VGG Net      InceptionResNet
methods        512 2048     512 2048     512 2048         methods        512 2048     512 2048     512 2048
Baseline       0.783 0.745  0.841 0.817  0.817 0.775      Baseline       0.381 0.344  0.517 0.485  0.521 0.466
GC [9]         0.784 0.738  0.843 0.804  0.801 0.763      GC [9]         0.370 0.310  0.533 0.475  0.515 0.462
AGC [10]       0.786 0.746  0.854 0.818  0.811 0.782      AGC [10]       0.382 0.352  0.524 0.495  0.526 0.471
SAM [1]        0.784 0.730  0.856 0.826  0.824 0.766      SAM [1]        0.385 0.363  0.542 0.498  0.525 0.479
SADT Variant1  0.801 0.774  0.870 0.851  0.847 0.822      SADT Variant1  0.406 0.394  0.552 0.564  0.560 0.527
SADT Variant2  0.809 0.775  0.876 0.847  0.847 0.811      SADT Variant2  0.425 0.394  0.565 0.565  0.553 0.513
SADT Variant3  0.812 0.777  0.852 0.854  0.852 0.822      SADT Variant3  0.413 0.395  0.560 0.572  0.566 0.529

                       (a) CIFAR10                                       (b) CIFAR100

Table 1: Test time accuracy for all methods [1, 9–11] for CIFAR10 and CIFAR100 datasets.",2022-11-01 07:30:53+00:00,SADT: Combining Sharpness-Aware Minimization with Self-Distillation for Improved Model Generalization,cs.LG,['cs.LG'],"[arxiv.Result.Author('Masud An-Nur Islam Fahim'), arxiv.Result.Author('Jani Boutellier')]","Methods for improving deep neural network training times and model
generalizability consist of various data augmentation, regularization, and
optimization approaches, which tend to be sensitive to hyperparameter settings
and make reproducibility more challenging. This work jointly considers two
recent training strategies that address model generalizability: sharpness-aware
minimization, and self-distillation, and proposes the novel training strategy
of Sharpness-Aware Distilled Teachers (SADT). The experimental section of this
work shows that SADT consistently outperforms previously published training
strategies in model convergence time, test-time performance, and model
generalizability over various neural architectures, datasets, and
hyperparameter settings."
13267,"Beyond further research in extreme classiﬁcation towards fully har-
nessing the representation capabilities of transformer encoders, we believe our approach can inspire
future multi-resolution architectures in other domains as well which leverage label hierarchy.","The proposed instantiation of
our framework in the form of CascadeXML not only yields state-of-the-art prediction performance,
but is also end-to-end trainable (without intermediate reclustering steps), simpler to implement, and
fast on training and inference.","10
9 Acknowledgments

The authors would like to thank Devaansh Gupta and Mohammadreza Qaraei for useful discussions.",2022-10-29 11:03:23+00:00,CascadeXML: Rethinking Transformers for End-to-end Multi-resolution Training in Extreme Multi-label Classification,cs.LG,"['cs.LG', 'cs.CL', 'stat.ML']","[arxiv.Result.Author('Siddhant Kharbanda'), arxiv.Result.Author('Atmadeep Banerjee'), arxiv.Result.Author('Erik Schultheis'), arxiv.Result.Author('Rohit Babbar')]","Extreme Multi-label Text Classification (XMC) involves learning a classifier
that can assign an input with a subset of most relevant labels from millions of
label choices. Recent approaches, such as XR-Transformer and LightXML, leverage
a transformer instance to achieve state-of-the-art performance. However, in
this process, these approaches need to make various trade-offs between
performance and computational requirements. A major shortcoming, as compared to
the Bi-LSTM based AttentionXML, is that they fail to keep separate feature
representations for each resolution in a label tree. We thus propose
CascadeXML, an end-to-end multi-resolution learning pipeline, which can harness
the multi-layered architecture of a transformer model for attending to
different label resolutions with separate feature representations. CascadeXML
significantly outperforms all existing approaches with non-trivial gains
obtained on benchmark datasets consisting of up to three million labels. Code
for CascadeXML will be made publicly available at
\url{https://github.com/xmc-aalto/cascadexml}."
13268,"Besides their useful uncertainty management capabil-     of uncertainty quantiﬁcation, we motivate further research
ities, BNNs overall training task, even if more computation-    efforts toward farm-wide sensor placement studies capable
ally demanding than for DNNs, is automatically regulated        of allocating monitoring installation and maintenance actions
by Bayesian inference principles, thus avoiding the risk of     by following optimal adaptive management policies, e.g.,
overﬁtting and eluding the need of a separate cross-validation  asset management strategies identiﬁed via Markov decision
dataset.","Beneﬁting from BNNs’ internal properties in terms
stage.",processes and/or deep reinforcement learning methods.,2022-10-31 13:02:50+00:00,Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY', 'stat.CO']","[arxiv.Result.Author('N. Hlaing'), arxiv.Result.Author('Pablo G. Morato'), arxiv.Result.Author('F. d. N. Santos'), arxiv.Result.Author('W. Weijtjens'), arxiv.Result.Author('C. Devriendt'), arxiv.Result.Author('P. Rigo')]","Offshore wind structures are subject to deterioration mechanisms throughout
their operational lifetime. Even if the deterioration evolution of structural
elements can be estimated through physics-based deterioration models, the
uncertainties involved in the process hurdle the selection of lifecycle
management decisions. In this scenario, the collection of relevant information
through an efficient monitoring system enables the reduction of uncertainties,
ultimately driving more optimal lifecycle decisions. However, a full monitoring
instrumentation implemented on all wind turbines in a farm might become
unfeasible due to practical and economical constraints. Besides, certain load
monitoring systems often become defective after a few years of marine
environment exposure. Addressing the aforementioned concerns, a farm-wide
virtual load monitoring scheme directed by a fleet-leader wind turbine offers
an attractive solution. Fetched with data retrieved from a fully-instrumented
wind turbine, a model can be trained and then deployed, thus yielding load
predictions of non-fully monitored wind turbines, from which only standard data
remains available. In this paper, we propose a virtual load monitoring
framework formulated via Bayesian neural networks (BNNs) and we provide
relevant implementation details needed for the construction, training, and
deployment of BNN data-based virtual monitoring models. As opposed to their
deterministic counterparts, BNNs intrinsically announce the uncertainties
associated with generated load predictions and allow to detect inaccurate load
estimations generated for non-fully monitored wind turbines. The proposed
virtual load monitoring is thoroughly tested through an experimental campaign
in an operational offshore wind farm and the results demonstrate the
effectiveness of BNN models for fleet-leader-based farm-wide virtual
monitoring."
13273,"To our knowledge, ﬁnite-sample properties of such
stochastic optimization problems have not been addressed (Shapiro et al., 2021) and our work may
open up avenues for further research in this area.","Additionally, in our analy-
sis, we solve a subset of stochastic optimization problems with possibly large or inﬁnite stochastic
constraints involving conditional expectations.","Other interesting future directions include con-
ducting empirical evaluations of ALM, examining the possibility of removing strong realizability
assumptions, and investigating practical and optimal oﬄine RL algorithms whose guarantees hold
under milder variants of single-policy concentrability.",2022-11-01 19:28:48+00:00,Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian,cs.LG,"['cs.LG', 'cs.AI', 'math.OC', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Paria Rashidinejad'), arxiv.Result.Author('Hanlin Zhu'), arxiv.Result.Author('Kunhe Yang'), arxiv.Result.Author('Stuart Russell'), arxiv.Result.Author('Jiantao Jiao')]","Offline reinforcement learning (RL), which refers to decision-making from a
previously-collected dataset of interactions, has received significant
attention over the past years. Much effort has focused on improving offline RL
practicality by addressing the prevalent issue of partial data coverage through
various forms of conservative policy learning. While the majority of algorithms
do not have finite-sample guarantees, several provable conservative offline RL
algorithms are designed and analyzed within the single-policy concentrability
framework that handles partial coverage. Yet, in the nonlinear function
approximation setting where confidence intervals are difficult to obtain,
existing provable algorithms suffer from computational intractability,
prohibitively strong assumptions, and suboptimal statistical rates. In this
paper, we leverage the marginalized importance sampling (MIS) formulation of RL
and present the first set of offline RL algorithms that are statistically
optimal and practical under general function approximation and single-policy
concentrability, bypassing the need for uncertainty quantification. We identify
that the key to successfully solving the sample-based approximation of the MIS
problem is ensuring that certain occupancy validity constraints are nearly
satisfied. We enforce these constraints by a novel application of the augmented
Lagrangian method and prove the following result: with the MIS formulation,
augmented Lagrangian is enough for statistically optimal offline RL. In stark
contrast to prior algorithms that induce additional conservatism through
methods such as behavior regularization, our approach provably eliminates this
need and reinterprets regularizers as ""enforcers of occupancy validity"" than
""promoters of conservatism."""
13275,"Another
avenue of further research to be explored is the development and consideration of formal and objective evaluation
methods for artistic tasks such as music generation and artistic painting.",[2022] can be applied to music.,"Being able to accurately and appropriately
evaluate models for such subjective tasks will greatly improve the quality of generated samples.",2022-11-01 20:23:49+00:00,Comparision Of Adversarial And Non-Adversarial LSTM Music Generative Models,cs.LG,"['cs.LG', 'cs.SC', 'cs.SD', 'eess.AS']","[arxiv.Result.Author(""Moseli Mots'oehli""), arxiv.Result.Author('Anna Sergeevna Bosman'), arxiv.Result.Author('Johan Pieter De Villiers')]","Algorithmic music composition is a way of composing musical pieces with
minimal to no human intervention. While recurrent neural networks are
traditionally applied to many sequence-to-sequence prediction tasks, including
successful implementations of music composition, their standard supervised
learning approach based on input-to-output mapping leads to a lack of note
variety. These models can therefore be seen as potentially unsuitable for tasks
such as music generation. Generative adversarial networks learn the generative
distribution of data and lead to varied samples. This work implements and
compares adversarial and non-adversarial training of recurrent neural network
music composers on MIDI data. The resulting music samples are evaluated by
human listeners, their preferences recorded. The evaluation indicates that
adversarial training produces more aesthetically pleasing music."
13276,"Lastly, the
simulations within the experiments were intended to show empirically the consequences
if the outcomes of interventions are not known, as to illustrate a need for further research
from experts who are working in the education domain.","Therefore, despite the potential
unintended consequences of these systems, they are significant and potentially
                                                                                                                 13

instrumental in recommending interventions to students within education.","5 Conclusion

It is essential to design smart systems within higher education institutions that are gen-
eralizable and can adapt to both similarities and differences between students.",2022-11-01 22:47:17+00:00,Reinforcement Learning in Education: A Multi-Armed Bandit Approach,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Herkulaas Combrink'), arxiv.Result.Author('Vukosi Marivate'), arxiv.Result.Author('Benjamin Rosman')]","Advances in reinforcement learning research have demonstrated the ways in
which different agent-based models can learn how to optimally perform a task
within a given environment. Reinforcement leaning solves unsupervised problems
where agents move through a state-action-reward loop to maximize the overall
reward for the agent, which in turn optimizes the solving of a specific problem
in a given environment. However, these algorithms are designed based on our
understanding of actions that should be taken in a real-world environment to
solve a specific problem. One such problem is the ability to identify,
recommend and execute an action within a system where the users are the
subject, such as in education. In recent years, the use of blended learning
approaches integrating face-to-face learning with online learning in the
education context, has in-creased. Additionally, online platforms used for
education require the automation of certain functions such as the
identification, recommendation or execution of actions that can benefit the
user, in this sense, the student or learner. As promising as these scientific
advances are, there is still a need to conduct research in a variety of
different areas to ensure the successful deployment of these agents within
education systems. Therefore, the aim of this study was to contextualise and
simulate the cumulative reward within an environment for an intervention
recommendation problem in the education context."
13280,"We hope that this work will serve as initial steps and
motivate further research in CL community on the important while less explored problem, i.e., how
to design algorithms that can provide targeted treatments to achieve backward knowledge transfer.","In this work, we go beyond this strategy and shed light
on the relationship between backward knowledge transfer and model modiﬁcation, by characterizing
the task correlations with gradient projection.","However, CUBER also comes with several limitations.",2022-11-01 23:55:51+00:00,Beyond Not-Forgetting: Continual Learning with Backward Knowledge Transfer,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Sen Lin'), arxiv.Result.Author('Li Yang'), arxiv.Result.Author('Deliang Fan'), arxiv.Result.Author('Junshan Zhang')]","By learning a sequence of tasks continually, an agent in continual learning
(CL) can improve the learning performance of both a new task and `old' tasks by
leveraging the forward knowledge transfer and the backward knowledge transfer,
respectively. However, most existing CL methods focus on addressing
catastrophic forgetting in neural networks by minimizing the modification of
the learnt model for old tasks. This inevitably limits the backward knowledge
transfer from the new task to the old tasks, because judicious model updates
could possibly improve the learning performance of the old tasks as well. To
tackle this problem, we first theoretically analyze the conditions under which
updating the learnt model of old tasks could be beneficial for CL and also lead
to backward knowledge transfer, based on the gradient projection onto the input
subspaces of old tasks. Building on the theoretical analysis, we next develop a
ContinUal learning method with Backward knowlEdge tRansfer (CUBER), for a fixed
capacity neural network without data replay. In particular, CUBER first
characterizes the task correlation to identify the positively correlated old
tasks in a layer-wise manner, and then selectively modifies the learnt model of
the old tasks when learning the new task. Experimental studies show that CUBER
can even achieve positive backward knowledge transfer on several existing CL
benchmarks for the first time without data replay, where the related baselines
still suffer from catastrophic forgetting (negative backward knowledge
transfer). The superior performance of CUBER on the backward knowledge transfer
also leads to higher accuracy accordingly."
13306,"We
hope our work spurs further study of aligning synthetic data generation with human perception,
and motivates the design of more human-aligned synthetic data to improve ML systems on
the web and beyond.","7.4 Extending to New Synthetic Data Paradigms

In this work, we focused on the synthetic data classically used in mixup, as the simplicity of the
data generating process – a single mixing coeﬃcient parameter – enables us to precisely compare
human versus traditional parameterizations of the synthetic data construction process.","We release the code of all interfaces included in our HILL MixE Suite,
which we hope will empower researchers with additional tools to investigate humans’ percepts
over synthetically-constructed data.",2022-11-02 15:27:31+00:00,Web-based Elicitation of Human Perception on mixup Data,cs.LG,"['cs.LG', 'cs.CV', 'cs.HC']","[arxiv.Result.Author('Katherine M. Collins'), arxiv.Result.Author('Umang Bhatt'), arxiv.Result.Author('Weiyang Liu'), arxiv.Result.Author('Vihari Piratla'), arxiv.Result.Author('Bradley Love'), arxiv.Result.Author('Adrian Weller')]","Synthetic data is proliferating on the web and powering many advances in
machine learning. However, it is not always clear if synthetic labels are
perceptually sensible to humans. The web provides us with a platform to take a
step towards addressing this question through online elicitation. We design a
series of elicitation interfaces, which we release as \texttt{HILL MixE Suite},
and recruit 159 participants, to provide perceptual judgments over the kinds of
synthetic data constructed during \textit{mixup} training: a powerful
regularizer shown to improve model robustness, generalization, and calibration.
We find that human perception does not consistently align with the labels
traditionally used for synthetic points and begin to demonstrate the
applicability of these findings to potentially increase the reliability of
downstream models. We release all elicited judgments in a new data hub we call
\texttt{H-Mix}."
13326,We plan further research in the future.,"The scope of the GRAIMATTER project did not allow for a detailed investigation of MQCs and therefore these
recommendations should be implemented with caution.","Responsibility: Researchers; Understanding: Data Governance and Ethics Committees, and TRE staff

10.2.17 TECH 17: MQC: Controls placed on a model to limit the number of queries should be tested by an
         external party

In addition to internal security processes, TREs are regularly penetration-tested by an external party to check for
security holes.",2022-11-03 09:00:57+00:00,GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs),cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Emily Jefferson'), arxiv.Result.Author('James Liley'), arxiv.Result.Author('Maeve Malone'), arxiv.Result.Author('Smarti Reel'), arxiv.Result.Author('Alba Crespi-Boixader'), arxiv.Result.Author('Xaroula Kerasidou'), arxiv.Result.Author('Francesco Tava'), arxiv.Result.Author('Andrew McCarthy'), arxiv.Result.Author('Richard Preen'), arxiv.Result.Author('Alberto Blanco-Justicia'), arxiv.Result.Author('Esma Mansouri-Benssassi'), arxiv.Result.Author('Josep Domingo-Ferrer'), arxiv.Result.Author('Jillian Beggs'), arxiv.Result.Author('Antony Chuter'), arxiv.Result.Author('Christian Cole'), arxiv.Result.Author('Felix Ritchie'), arxiv.Result.Author('Angela Daly'), arxiv.Result.Author('Simon Rogers'), arxiv.Result.Author('Jim Smith')]","TREs are widely, and increasingly used to support statistical analysis of
sensitive data across a range of sectors (e.g., health, police, tax and
education) as they enable secure and transparent research whilst protecting
data confidentiality. There is an increasing desire from academia and industry
to train AI models in TREs. The field of AI is developing quickly with
applications including spotting human errors, streamlining processes, task
automation and decision support. These complex AI models require more
information to describe and reproduce, increasing the possibility that
sensitive personal data can be inferred from such descriptions. TREs do not
have mature processes and controls against these risks. This is a complex
topic, and it is unreasonable to expect all TREs to be aware of all risks or
that TRE researchers have addressed these risks in AI-specific training.
GRAIMATTER has developed a draft set of usable recommendations for TREs to
guard against the additional risks when disclosing trained AI models from TREs.
The development of these recommendations has been funded by the GRAIMATTER UKRI
DARE UK sprint research project. This version of our recommendations was
published at the end of the project in September 2022. During the course of the
project, we have identified many areas for future investigations to expand and
test these recommendations in practice. Therefore, we expect that this document
will evolve over time."
13327,"It is important to note that these clauses require further research, input and final approval from a qualified and
insured legal team.","As part of the GRAIMATTER project, we have drafted template legal clauses which could be utilised within EULAs.",Such clauses would need to be approved by the relevant Data Controller.,2022-11-03 09:00:57+00:00,GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs),cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Emily Jefferson'), arxiv.Result.Author('James Liley'), arxiv.Result.Author('Maeve Malone'), arxiv.Result.Author('Smarti Reel'), arxiv.Result.Author('Alba Crespi-Boixader'), arxiv.Result.Author('Xaroula Kerasidou'), arxiv.Result.Author('Francesco Tava'), arxiv.Result.Author('Andrew McCarthy'), arxiv.Result.Author('Richard Preen'), arxiv.Result.Author('Alberto Blanco-Justicia'), arxiv.Result.Author('Esma Mansouri-Benssassi'), arxiv.Result.Author('Josep Domingo-Ferrer'), arxiv.Result.Author('Jillian Beggs'), arxiv.Result.Author('Antony Chuter'), arxiv.Result.Author('Christian Cole'), arxiv.Result.Author('Felix Ritchie'), arxiv.Result.Author('Angela Daly'), arxiv.Result.Author('Simon Rogers'), arxiv.Result.Author('Jim Smith')]","TREs are widely, and increasingly used to support statistical analysis of
sensitive data across a range of sectors (e.g., health, police, tax and
education) as they enable secure and transparent research whilst protecting
data confidentiality. There is an increasing desire from academia and industry
to train AI models in TREs. The field of AI is developing quickly with
applications including spotting human errors, streamlining processes, task
automation and decision support. These complex AI models require more
information to describe and reproduce, increasing the possibility that
sensitive personal data can be inferred from such descriptions. TREs do not
have mature processes and controls against these risks. This is a complex
topic, and it is unreasonable to expect all TREs to be aware of all risks or
that TRE researchers have addressed these risks in AI-specific training.
GRAIMATTER has developed a draft set of usable recommendations for TREs to
guard against the additional risks when disclosing trained AI models from TREs.
The development of these recommendations has been funded by the GRAIMATTER UKRI
DARE UK sprint research project. This version of our recommendations was
published at the end of the project in September 2022. During the course of the
project, we have identified many areas for future investigations to expand and
test these recommendations in practice. Therefore, we expect that this document
will evolve over time."
13328,"These clauses have been drafted to a stage where further research and input is
required, and final approval from a qualified and insured legal team.","As part of the GRAIMATTER project, we have drafted template legal clauses which could be utilised within
researcher declaration forms.","Data controllers would have to agree such
terms are acceptable.",2022-11-03 09:00:57+00:00,GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs),cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Emily Jefferson'), arxiv.Result.Author('James Liley'), arxiv.Result.Author('Maeve Malone'), arxiv.Result.Author('Smarti Reel'), arxiv.Result.Author('Alba Crespi-Boixader'), arxiv.Result.Author('Xaroula Kerasidou'), arxiv.Result.Author('Francesco Tava'), arxiv.Result.Author('Andrew McCarthy'), arxiv.Result.Author('Richard Preen'), arxiv.Result.Author('Alberto Blanco-Justicia'), arxiv.Result.Author('Esma Mansouri-Benssassi'), arxiv.Result.Author('Josep Domingo-Ferrer'), arxiv.Result.Author('Jillian Beggs'), arxiv.Result.Author('Antony Chuter'), arxiv.Result.Author('Christian Cole'), arxiv.Result.Author('Felix Ritchie'), arxiv.Result.Author('Angela Daly'), arxiv.Result.Author('Simon Rogers'), arxiv.Result.Author('Jim Smith')]","TREs are widely, and increasingly used to support statistical analysis of
sensitive data across a range of sectors (e.g., health, police, tax and
education) as they enable secure and transparent research whilst protecting
data confidentiality. There is an increasing desire from academia and industry
to train AI models in TREs. The field of AI is developing quickly with
applications including spotting human errors, streamlining processes, task
automation and decision support. These complex AI models require more
information to describe and reproduce, increasing the possibility that
sensitive personal data can be inferred from such descriptions. TREs do not
have mature processes and controls against these risks. This is a complex
topic, and it is unreasonable to expect all TREs to be aware of all risks or
that TRE researchers have addressed these risks in AI-specific training.
GRAIMATTER has developed a draft set of usable recommendations for TREs to
guard against the additional risks when disclosing trained AI models from TREs.
The development of these recommendations has been funded by the GRAIMATTER UKRI
DARE UK sprint research project. This version of our recommendations was
published at the end of the project in September 2022. During the course of the
project, we have identified many areas for future investigations to expand and
test these recommendations in practice. Therefore, we expect that this document
will evolve over time."
13349,"Unfortunately, this is still the case of many modern
Machine Learning methods, specially neural network, and a further study of
this identiﬁcation is a way to truly insert prior information into the learning
process.","If prior information
cannot be translated into the parameters representing the hypotheses or into
the optimization algorithm employed to learn, then the steps described above
cannot be performed.","If this understanding of neural networks was achieved, then inserting prior
information into the learning process could be as follows:

    (1) Translate domain knowledge about the problem at hand into prior in-
         formation about properties that f should satisfy and identify these
         properties with the parameters of a pool of neural networks architec-
         tures.",2022-10-31 20:39:53+00:00,The role of prior information and computational power in Machine Learning,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Diego Marcondes'), arxiv.Result.Author('Adilson Simonis'), arxiv.Result.Author('Junior Barrera')]","Science consists on conceiving hypotheses, confronting them with empirical
evidence, and keeping only hypotheses which have not yet been falsified. Under
deductive reasoning they are conceived in view of a theory and confronted with
empirical evidence in an attempt to falsify it, and under inductive reasoning
they are conceived based on observation, confronted with empirical evidence and
a theory is established based on the not falsified hypotheses. When the
hypotheses testing can be performed with quantitative data, the confrontation
can be achieved with Machine Learning methods, whose quality is highly
dependent on the hypotheses' complexity, hence on the proper insertion of prior
information into the set of hypotheses seeking to decrease its complexity
without loosing good hypotheses. However, Machine Learning tools have been
applied under the pragmatic view of instrumentalism, which is concerned only
with the performance of the methods and not with the understanding of their
behavior, leading to methods which are not fully understood. In this context,
we discuss how prior information and computational power can be employed to
solve a learning problem, but while prior information and a careful design of
the hypotheses space has as advantage the interpretability of the results,
employing high computational power has the advantage of a higher performance.
We discuss why learning methods which combine both should work better from an
understanding and performance perspective, arguing in favor of basic
theoretical research on Machine Learning, in special about how properties of
classifiers may be identified in parameters of modern learning models."
13352,"APPENDIX

   The software implementation for this algorithm along with
the system for synthetic benchmarks, search domain speciﬁ-
cation and simulator integration layers and samples will be
subsequently released, potentially on github, for the purpose
of further research and integration.","550–557, 2000.","It may include other design
evaluation simulators and optimization problems.",2022-11-03 16:08:40+00:00,Theta-Resonance: A Single-Step Reinforcement Learning Method for Design Space Exploration,cs.LG,"['cs.LG', 'cs.AI', 'A.1; C.3; C.4; G.3; H.1; I.2; I.6; J.6']","[arxiv.Result.Author('Masood S. Mortazavi'), arxiv.Result.Author('Tiancheng Qin'), arxiv.Result.Author('Ning Yan')]","Given an environment (e.g., a simulator) for evaluating samples in a
specified design space and a set of weighted evaluation metrics -- one can use
Theta-Resonance, a single-step Markov Decision Process (MDP), to train an
intelligent agent producing progressively more optimal samples. In
Theta-Resonance, a neural network consumes a constant input tensor and produces
a policy as a set of conditional probability density functions (PDFs) for
sampling each design dimension. We specialize existing policy gradient
algorithms in deep reinforcement learning (D-RL) in order to use evaluation
feedback (in terms of cost, penalty or reward) to update our policy network
with robust algorithmic stability and minimal design evaluations. We study
multiple neural architectures (for our policy network) within the context of a
simple SoC design space and propose a method of constructing synthetic space
exploration problems to compare and improve design space exploration (DSE)
algorithms. Although we only present categorical design spaces, we also outline
how to use Theta-Resonance in order to explore continuous and mixed
continuous-discrete design spaces."
13353,"APPENDIX

   The software implementation for this algorithm along with
the system for synthetic benchmarks, search domain speciﬁ-
cation and simulator integration layers and samples will be
subsequently released, potentially on github, for the purpose
of further research and integration.","550–557, 2000.","It may include other design
evaluation simulators and optimization problems.",2022-11-03 16:08:40+00:00,Theta-Resonance: A Single-Step Reinforcement Learning Method for Design Space Exploration,cs.LG,"['cs.LG', 'cs.AI', 'A.1; C.3; C.4; G.3; H.1; I.2; I.6; J.6']","[arxiv.Result.Author('Masood S. Mortazavi'), arxiv.Result.Author('Tiancheng Qin'), arxiv.Result.Author('Ning Yan')]","Given an environment (e.g., a simulator) for evaluating samples in a
specified design space and a set of weighted evaluation metrics -- one can use
Theta-Resonance, a single-step Markov Decision Process (MDP), to train an
intelligent agent producing progressively more optimal samples. In
Theta-Resonance, a neural network consumes a constant input tensor and produces
a policy as a set of conditional probability density functions (PDFs) for
sampling each design dimension. We specialize existing policy gradient
algorithms in deep reinforcement learning (D-RL) in order to use evaluation
feedback (in terms of cost, penalty or reward) to update our policy network
with robust algorithmic stability and minimal design evaluations. We study
multiple neural architectures (for our policy network) within the context of a
simple SoC design space and propose a method of constructing synthetic space
exploration problems to compare and improve design space exploration (DSE)
algorithms. Although we only present categorical design spaces, we also outline
how to use Theta-Resonance in order to explore continuous and mixed
continuous-discrete design spaces."
13358,"Beyond the DAMS with UCAR setting, there are several open avenues for further research in domain adaptation under
missingness shift.","Depending on quantity of data or amount of missingness in each domain, weightings
may vary.","While we explored the case of underreporting completely at random, this assumption is likely too
strong to hold in practice.",2022-11-03 18:49:38+00:00,Domain Adaptation under Missingness Shift,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Helen Zhou'), arxiv.Result.Author('Sivaraman Balakrishnan'), arxiv.Result.Author('Zachary C. Lipton')]","Rates of missing data often depend on record-keeping policies and thus may
change across times and locations, even when the underlying features are
comparatively stable. In this paper, we introduce the problem of Domain
Adaptation under Missingness Shift (DAMS). Here, (labeled) source data and
(unlabeled) target data would be exchangeable but for different missing data
mechanisms. We show that when missing data indicators are available, DAMS can
reduce to covariate shift. Focusing on the setting where missing data
indicators are absent, we establish the following theoretical results for
underreporting completely at random: (i) covariate shift is violated
(adaptation is required); (ii) the optimal source predictor can perform worse
on the target domain than a constant one; (iii) the optimal target predictor
can be identified, even when the missingness rates themselves are not; and (iv)
for linear models, a simple analytic adjustment yields consistent estimates of
the optimal target parameters. In experiments on synthetic and semi-synthetic
data, we demonstrate the promise of our methods when assumptions hold. Finally,
we discuss a rich family of future extensions."
13359,"REPRODUCIBILITY STATEMENT

We ensure reproducibility of our method via a) releasing the ofﬂine MetaWorld dataset that we
used for our main results to allow the community to conduct further research on this domain upon
publication, b) releasing the code used to obtain our results upon publication and c) detailing the
hyperparameters and design choices for the implementation of CVL and the computational resources
used for our experiments in Section 6.1.","9
Deep Reinforcement Learning Workshop, NeurIPS 2022.","REFERENCES

R. Agarwal, M. C. Machado, P. S. Castro, and M. G. Bellemare.",2022-11-03 19:10:05+00:00,Contrastive Value Learning: Implicit Models for Simple Offline RL,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bogdan Mazoure'), arxiv.Result.Author('Benjamin Eysenbach'), arxiv.Result.Author('Ofir Nachum'), arxiv.Result.Author('Jonathan Tompson')]","Model-based reinforcement learning (RL) methods are appealing in the offline
setting because they allow an agent to reason about the consequences of actions
without interacting with the environment. Prior methods learn a 1-step dynamics
model, which predicts the next state given the current state and action. These
models do not immediately tell the agent which actions to take, but must be
integrated into a larger RL framework. Can we model the environment dynamics in
a different way, such that the learned model does directly indicate the value
of each action? In this paper, we propose Contrastive Value Learning (CVL),
which learns an implicit, multi-step model of the environment dynamics. This
model can be learned without access to reward functions, but nonetheless can be
used to directly estimate the value of each action, without requiring any TD
learning. Because this model represents the multi-step transitions implicitly,
it avoids having to predict high-dimensional observations and thus scales to
high-dimensional tasks. Our experiments demonstrate that CVL outperforms prior
offline RL methods on complex continuous control benchmarks."
13366,"However,
the unbiased estimator does not exhibit signiﬁcant advantages compared to a na¨ıve biased
one, which will be explored in further research.","To estimate the trace norm of the bi-inﬁnite Hankel matrix in the spectral regularizer, we
construct an unbiased stochastic estimator to relax the loss minimization problem.","Acknowledgments

We would like to acknowledge the support of the 2021 Globalink Research Internship Mitacs
program (Project ID 24986).",2022-11-04 04:07:05+00:00,Spectral Regularization: an Inductive Bias for Sequence Modeling,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']","[arxiv.Result.Author('Kaiwen Hou'), arxiv.Result.Author('Guillaume Rabusseau')]","Various forms of regularization in learning tasks strive for different
notions of simplicity. This paper presents a spectral regularization technique,
which attaches a unique inductive bias to sequence modeling based on an
intuitive concept of simplicity defined in the Chomsky hierarchy. From
fundamental connections between Hankel matrices and regular grammars, we
propose to use the trace norm of the Hankel matrix, the tightest convex
relaxation of its rank, as the spectral regularizer. To cope with the fact that
the Hankel matrix is bi-infinite, we propose an unbiased stochastic estimator
for its trace norm. Ultimately, we demonstrate experimental results on Tomita
grammars, which exhibit the potential benefits of spectral regularization and
validate the proposed stochastic estimator."
13367,"10, we further study the impact of the maximum                                                  learning,” IEEE Trans.","[1] M. Setayesh, S. Bahrami, and V. W. Wong, “Resource slicing for eMBB
                                                                                                           and URLLC services in radio access network using hierarchical deep
   In Fig.","Wireless Commun., early access, 2022.
tolerable dropping ratio and average packet arrival rate on
the subframe allocation policy.",2022-11-04 07:39:21+00:00,Decentralized Federated Reinforcement Learning for User-Centric Dynamic TFDD Control,cs.LG,"['cs.LG', 'cs.NI']","[arxiv.Result.Author('Ziyan Yin'), arxiv.Result.Author('Zhe Wang'), arxiv.Result.Author('Jun Li'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Wen Chen'), arxiv.Result.Author('Shi Jin')]","The explosive growth of dynamic and heterogeneous data traffic brings great
challenges for 5G and beyond mobile networks. To enhance the network capacity
and reliability, we propose a learning-based dynamic time-frequency division
duplexing (D-TFDD) scheme that adaptively allocates the uplink and downlink
time-frequency resources of base stations (BSs) to meet the asymmetric and
heterogeneous traffic demands while alleviating the inter-cell interference. We
formulate the problem as a decentralized partially observable Markov decision
process (Dec-POMDP) that maximizes the long-term expected sum rate under the
users' packet dropping ratio constraints. In order to jointly optimize the
global resources in a decentralized manner, we propose a federated
reinforcement learning (RL) algorithm named federated Wolpertinger deep
deterministic policy gradient (FWDDPG) algorithm. The BSs decide their local
time-frequency configurations through RL algorithms and achieve global training
via exchanging local RL models with their neighbors under a decentralized
federated learning framework. Specifically, to deal with the large-scale
discrete action space of each BS, we adopt a DDPG-based algorithm to generate
actions in a continuous space, and then utilize Wolpertinger policy to reduce
the mapping errors from continuous action space back to discrete action space.
Simulation results demonstrate the superiority of our proposed algorithm to
benchmark algorithms with respect to system sum rate."
13370,"without additional
task-speciﬁc training), we provide a roadmap with 5 milestone steps to guide further research.","In order for us to obtain uniﬁed AI models that can perform the
entire spectrum of meaningful remote sensing tasks in a zero-shot manner (i.e.",Step 1.,2022-11-04 09:58:57+00:00,A General Purpose Neural Architecture for Geospatial Systems,cs.LG,"['cs.LG', 'cs.AI', 'cs.CY']","[arxiv.Result.Author('Nasim Rahaman'), arxiv.Result.Author('Martin Weiss'), arxiv.Result.Author('Frederik Träuble'), arxiv.Result.Author('Francesco Locatello'), arxiv.Result.Author('Alexandre Lacoste'), arxiv.Result.Author('Yoshua Bengio'), arxiv.Result.Author('Chris Pal'), arxiv.Result.Author('Li Erran Li'), arxiv.Result.Author('Bernhard Schölkopf')]","Geospatial Information Systems are used by researchers and Humanitarian
Assistance and Disaster Response (HADR) practitioners to support a wide variety
of important applications. However, collaboration between these actors is
difficult due to the heterogeneous nature of geospatial data modalities (e.g.,
multi-spectral images of various resolutions, timeseries, weather data) and
diversity of tasks (e.g., regression of human activity indicators or detecting
forest fires). In this work, we present a roadmap towards the construction of a
general-purpose neural architecture (GPNA) with a geospatial inductive bias,
pre-trained on large amounts of unlabelled earth observation data in a
self-supervised manner. We envision how such a model may facilitate cooperation
between members of the community. We show preliminary results on the first step
of the roadmap, where we instantiate an architecture that can process a wide
variety of geospatial data modalities and demonstrate that it can achieve
competitive performance with domain-specific architectures on tasks relating to
the U.N.'s Sustainable Development Goals."
13390,"Our results indicate that both prior depression
                                                  detection algorithms and domain generalization techniques show potential but need
                                                  further research to achieve adequate cross-dataset generalizability.","As a starting point, we provide the benchmark results of 18 algorithms
                                                  on the task of depression detection.","We envision our
                                                  multi-year datasets can support the ML community in developing generalizable
                                                  longitudinal behavior modeling algorithms.",2022-11-04 20:16:59+00:00,GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC', '68T09', 'I.2.1; E.m']","[arxiv.Result.Author('Xuhai Xu'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Yasaman Sefidgar'), arxiv.Result.Author('Yiyi Ren'), arxiv.Result.Author('Xin Liu'), arxiv.Result.Author('Woosuk Seo'), arxiv.Result.Author('Jennifer Brown'), arxiv.Result.Author('Kevin Kuehn'), arxiv.Result.Author('Mike Merrill'), arxiv.Result.Author('Paula Nurius'), arxiv.Result.Author('Shwetak Patel'), arxiv.Result.Author('Tim Althoff'), arxiv.Result.Author('Margaret E. Morris'), arxiv.Result.Author('Eve Riskin'), arxiv.Result.Author('Jennifer Mankoff'), arxiv.Result.Author('Anind K. Dey')]","Recent research has demonstrated the capability of behavior signals captured
by smartphones and wearables for longitudinal behavior modeling. However, there
is a lack of a comprehensive public dataset that serves as an open testbed for
fair comparison among algorithms. Moreover, prior studies mainly evaluate
algorithms using data from a single population within a short period, without
measuring the cross-dataset generalizability of these algorithms. We present
the first multi-year passive sensing datasets, containing over 700 user-years
and 497 unique users' data collected from mobile and wearable sensors, together
with a wide range of well-being metrics. Our datasets can support multiple
cross-dataset evaluations of behavior modeling algorithms' generalizability
across different users and years. As a starting point, we provide the benchmark
results of 18 algorithms on the task of depression detection. Our results
indicate that both prior depression detection algorithms and domain
generalization techniques show potential but need further research to achieve
adequate cross-dataset generalizability. We envision our multi-year datasets
can support the ML community in developing generalizable longitudinal behavior
modeling algorithms."
13404,"The results
                                                  obtained are expected to usher in further research in this ﬁeld of study.","Further, the proposed method signiﬁcantly outperformed state-of-
                                                  the-art methods in terms of log-likelihood or evidence lower bound.","1 Introduction

                                       Many machine learning algorithms aim to automatically learn and extract latent factors that explain
                                       a speciﬁc dataset.",2022-11-05 13:13:32+00:00,Grassmann Manifold Flow,cs.LG,"['cs.LG', 'math.DG', 'stat.ML']","[arxiv.Result.Author('Ryoma Yataka'), arxiv.Result.Author('Masashi Shiraishi')]","Recently, studies on machine learning have focused on methods that use
symmetry implicit in a specific manifold as an inductive bias. In particular,
approaches using Grassmann manifolds have been found to exhibit effective
performance in fields such as point cloud and image set analysis. However,
there is a lack of research on the construction of general learning models to
learn distributions on the Grassmann manifold. In this paper, we lay the
theoretical foundations for learning distributions on the Grassmann manifold
via continuous normalizing flows. Experimental results show that the proposed
method can generate high-quality samples by capturing the data structure.
Further, the proposed method significantly outperformed state-of-the-art
methods in terms of log-likelihood or evidence lower bound. The results
obtained are expected to usher in further research in this field of study."
13447,"[2] Jakub Konecˇny`, H Brendan McMahan, Felix X Yu, Peter Richta´rik,
We here list further research items.","arXiv preprint arXiv:2205.11779, 2022.
experienced models can achieve higher accuracy because the
disturbance can contribute to ﬁnding a better optimal model.","Ananda Theertha Suresh, and Dave Bacon.",2022-11-07 12:19:38+00:00,Resilience of Wireless Ad Hoc Federated Learning against Model Poisoning Attacks,cs.LG,"['cs.LG', 'cs.CR', 'cs.NI']","[arxiv.Result.Author('Naoya Tezuka'), arxiv.Result.Author('Hideya Ochiai'), arxiv.Result.Author('Yuwei Sun'), arxiv.Result.Author('Hiroshi Esaki')]","Wireless ad hoc federated learning (WAFL) is a fully decentralized
collaborative machine learning framework organized by opportunistically
encountered mobile nodes. Compared to conventional federated learning, WAFL
performs model training by weakly synchronizing the model parameters with
others, and this shows great resilience to a poisoned model injected by an
attacker. In this paper, we provide our theoretical analysis of the WAFL's
resilience against model poisoning attacks, by formulating the force balance
between the poisoned model and the legitimate model. According to our
experiments, we confirmed that the nodes directly encountered the attacker has
been somehow compromised to the poisoned model but other nodes have shown great
resilience. More importantly, after the attacker has left the network, all the
nodes have finally found stronger model parameters combined with the poisoned
model. Most of the attack-experienced cases achieved higher accuracy than the
no-attack-experienced cases."
13471,"Positive constant c can be determined using binary
                                       prompted further research to (1) develop new attack tech-            search.",This         effective.,"A box constraint on x + δ is satisﬁed by clipping or
                                       niques, (2) defend against instances created with such meth-         change of variables.",2022-11-07 17:40:08+00:00,Deviations in Representations Induced by Adversarial Attacks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Daniel Steinberg'), arxiv.Result.Author('Paul Munro')]","Deep learning has been a popular topic and has achieved success in many
areas. It has drawn the attention of researchers and machine learning
practitioners alike, with developed models deployed to a variety of settings.
Along with its achievements, research has shown that deep learning models are
vulnerable to adversarial attacks. This finding brought about a new direction
in research, whereby algorithms were developed to attack and defend vulnerable
networks. Our interest is in understanding how these attacks effect change on
the intermediate representations of deep learning models. We present a method
for measuring and analyzing the deviations in representations induced by
adversarial attacks, progressively across a selected set of layers. Experiments
are conducted using an assortment of attack algorithms, on the CIFAR-10
dataset, with plots created to visualize the impact of adversarial attacks
across different layers in a network."
13517,"To further study and address
                                                  the task of instruction following, we equip RL agents with an internal structured
                                                  representation of natural language instructions in the form of Linear Temporal
                                                  Logic (LTL), a formal language that is increasingly used for temporally extended
                                                  reward speciﬁcation in RL.","We conduct experiments
                                                  that show that the performance of state-of-the-art text-based game agents is largely
                                                  unaffected by the presence or absence of such instructions, and that these agents
                                                  are typically unable to execute tasks to completion.","Our framework both supports and highlights the beneﬁt
                                                  of understanding the temporal semantics of instructions and in measuring progress
                                                  towards achievement of such a temporally extended behaviour.",2022-11-08 22:20:17+00:00,Learning to Follow Instructions in Text-Based Games,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Mathieu Tuli'), arxiv.Result.Author('Andrew C. Li'), arxiv.Result.Author('Pashootan Vaezipoor'), arxiv.Result.Author('Toryn Q. Klassen'), arxiv.Result.Author('Scott Sanner'), arxiv.Result.Author('Sheila A. McIlraith')]","Text-based games present a unique class of sequential decision making problem
in which agents interact with a partially observable, simulated environment via
actions and observations conveyed through natural language. Such observations
typically include instructions that, in a reinforcement learning (RL) setting,
can directly or indirectly guide a player towards completing reward-worthy
tasks. In this work, we study the ability of RL agents to follow such
instructions. We conduct experiments that show that the performance of
state-of-the-art text-based game agents is largely unaffected by the presence
or absence of such instructions, and that these agents are typically unable to
execute tasks to completion. To further study and address the task of
instruction following, we equip RL agents with an internal structured
representation of natural language instructions in the form of Linear Temporal
Logic (LTL), a formal language that is increasingly used for temporally
extended reward specification in RL. Our framework both supports and highlights
the benefit of understanding the temporal semantics of instructions and in
measuring progress towards achievement of such a temporally extended behaviour.
Experiments with 500+ games in TextWorld demonstrate the superior performance
of our approach."
13518,"To further study and address the task of instruction following, we equip GATA with an internal
structured representation of natural language instructions speciﬁed in Linear Temporal Logic (LTL)
(Pnueli, 1977), a formal language that is increasingly used for temporally extended goals in planning
and reward speciﬁcation and other purposes in RL (e.g., Bacchus & Kabanza, 2000; Baier & McIlraith,
2006; Patrizi et al., 2011; Camacho & McIlraith, 2019; Littman et al., 2017; Toro Icarte et al.,
2018a,b; Camacho et al., 2019; Leon et al., 2020; Kuo et al., 2020; Vaezipoor et al., 2021).","Low success rate (i.e., task completion) rate is also seen in level 2.

they are not typically successful in completing tasks – an important vulnerability to the deployment
of such techniques in environments where partial completion of tasks can be unsafe.","LTL
also provides a mechanism to monitor progress towards completion of instructions.",2022-11-08 22:20:17+00:00,Learning to Follow Instructions in Text-Based Games,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Mathieu Tuli'), arxiv.Result.Author('Andrew C. Li'), arxiv.Result.Author('Pashootan Vaezipoor'), arxiv.Result.Author('Toryn Q. Klassen'), arxiv.Result.Author('Scott Sanner'), arxiv.Result.Author('Sheila A. McIlraith')]","Text-based games present a unique class of sequential decision making problem
in which agents interact with a partially observable, simulated environment via
actions and observations conveyed through natural language. Such observations
typically include instructions that, in a reinforcement learning (RL) setting,
can directly or indirectly guide a player towards completing reward-worthy
tasks. In this work, we study the ability of RL agents to follow such
instructions. We conduct experiments that show that the performance of
state-of-the-art text-based game agents is largely unaffected by the presence
or absence of such instructions, and that these agents are typically unable to
execute tasks to completion. To further study and address the task of
instruction following, we equip RL agents with an internal structured
representation of natural language instructions in the form of Linear Temporal
Logic (LTL), a formal language that is increasingly used for temporally
extended reward specification in RL. Our framework both supports and highlights
the benefit of understanding the temporal semantics of instructions and in
measuring progress towards achievement of such a temporally extended behaviour.
Experiments with 500+ games in TextWorld demonstrate the superior performance
of our approach."
13519,"Given these insights, we wish to further study and address instruction following in TBGs.",(2020)).,"In the next
section, we propose using LTL and demonstrate how existing work can be easily augmented.",2022-11-08 22:20:17+00:00,Learning to Follow Instructions in Text-Based Games,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Mathieu Tuli'), arxiv.Result.Author('Andrew C. Li'), arxiv.Result.Author('Pashootan Vaezipoor'), arxiv.Result.Author('Toryn Q. Klassen'), arxiv.Result.Author('Scott Sanner'), arxiv.Result.Author('Sheila A. McIlraith')]","Text-based games present a unique class of sequential decision making problem
in which agents interact with a partially observable, simulated environment via
actions and observations conveyed through natural language. Such observations
typically include instructions that, in a reinforcement learning (RL) setting,
can directly or indirectly guide a player towards completing reward-worthy
tasks. In this work, we study the ability of RL agents to follow such
instructions. We conduct experiments that show that the performance of
state-of-the-art text-based game agents is largely unaffected by the presence
or absence of such instructions, and that these agents are typically unable to
execute tasks to completion. To further study and address the task of
instruction following, we equip RL agents with an internal structured
representation of natural language instructions in the form of Linear Temporal
Logic (LTL), a formal language that is increasingly used for temporally
extended reward specification in RL. Our framework both supports and highlights
the benefit of understanding the temporal semantics of instructions and in
measuring progress towards achievement of such a temporally extended behaviour.
Experiments with 500+ games in TextWorld demonstrate the superior performance
of our approach."
13527,"Therefore, this serves as an ideal moment to take

note of the field as a whole, understand its current status and development, and use that knowledge to provide collated information

to increase overall awareness and motivate further research.","This domain is currently neither too embryonic in development,

like some other properties, nor is it too widely researched and too far developed.","In the following sections, we shall look at a few generic methods to

evaluate robustness.",2022-11-09 10:14:21+00:00,On the Robustness of Explanations of Deep Neural Network Models: A Survey,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Amlan Jyoti'), arxiv.Result.Author('Karthik Balaji Ganesh'), arxiv.Result.Author('Manoj Gayala'), arxiv.Result.Author('Nandita Lakshmi Tunuguntla'), arxiv.Result.Author('Sandesh Kamath'), arxiv.Result.Author('Vineeth N Balasubramanian')]","Explainability has been widely stated as a cornerstone of the responsible and
trustworthy use of machine learning models. With the ubiquitous use of Deep
Neural Network (DNN) models expanding to risk-sensitive and safety-critical
domains, many methods have been proposed to explain the decisions of these
models. Recent years have also seen concerted efforts that have shown how such
explanations can be distorted (attacked) by minor input perturbations. While
there have been many surveys that review explainability methods themselves,
there has been no effort hitherto to assimilate the different methods and
metrics proposed to study the robustness of explanations of DNN models. In this
work, we present a comprehensive survey of methods that study, understand,
attack, and defend explanations of DNN models. We also present a detailed
review of different metrics used to evaluate explanation methods, as well as
describe attributional attack and defense methods. We conclude with lessons and
take-aways for the community towards ensuring robust explanations of DNN model
predictions."
13528,"However, it is not extensively
researched either, making this an opportune moment for such a survey to take note of the field as a whole, understand its current
status and development, and use that knowledge to provide collated information to increase overall awareness and motivate
further research.","Many methods have been proposed to measure, attack, and defend the robustness/stability of
explanations in recent years, making this a field that is not nascent or embryonic in its development.","This survey presented a detailed listing of evaluation metrics for explanation methods, with a focus on metrics used to study
their robustness/stability.",2022-11-09 10:14:21+00:00,On the Robustness of Explanations of Deep Neural Network Models: A Survey,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Amlan Jyoti'), arxiv.Result.Author('Karthik Balaji Ganesh'), arxiv.Result.Author('Manoj Gayala'), arxiv.Result.Author('Nandita Lakshmi Tunuguntla'), arxiv.Result.Author('Sandesh Kamath'), arxiv.Result.Author('Vineeth N Balasubramanian')]","Explainability has been widely stated as a cornerstone of the responsible and
trustworthy use of machine learning models. With the ubiquitous use of Deep
Neural Network (DNN) models expanding to risk-sensitive and safety-critical
domains, many methods have been proposed to explain the decisions of these
models. Recent years have also seen concerted efforts that have shown how such
explanations can be distorted (attacked) by minor input perturbations. While
there have been many surveys that review explainability methods themselves,
there has been no effort hitherto to assimilate the different methods and
metrics proposed to study the robustness of explanations of DNN models. In this
work, we present a comprehensive survey of methods that study, understand,
attack, and defend explanations of DNN models. We also present a detailed
review of different metrics used to evaluate explanation methods, as well as
describe attributional attack and defense methods. We conclude with lessons and
take-aways for the community towards ensuring robust explanations of DNN model
predictions."
13546,"Hence, we further study the performance of models on incorrect evidence.","However, in realistic situations, we do
not know whether the evidence is correct, and FC models would still provide a veracity
for a claim.","For each instance in the original test splits, we retrieve incorrect evidence by selecting
the closest evidence of another claim in the dataset by word overlap between the
claim and the evidence candidates.",2022-11-09 15:14:52+00:00,Accountable and Explainable Methods for Complex Reasoning over Text,cs.LG,"['cs.LG', 'cs.CL', 'cs.CY', '68T50', 'I.2.7']",[arxiv.Result.Author('Pepa Atanasova')],"A major concern of Machine Learning (ML) models is their opacity. They are
deployed in an increasing number of applications where they often operate as
black boxes that do not provide explanations for their predictions. Among
others, the potential harms associated with the lack of understanding of the
models' rationales include privacy violations, adversarial manipulations, and
unfair discrimination. As a result, the accountability and transparency of ML
models have been posed as critical desiderata by works in policy and law,
philosophy, and computer science.
  In computer science, the decision-making process of ML models has been
studied by developing accountability and transparency methods. Accountability
methods, such as adversarial attacks and diagnostic datasets, expose
vulnerabilities of ML models that could lead to malicious manipulations or
systematic faults in their predictions. Transparency methods explain the
rationales behind models' predictions gaining the trust of relevant
stakeholders and potentially uncovering mistakes and unfairness in models'
decisions. To this end, transparency methods have to meet accountability
requirements as well, e.g., being robust and faithful to the underlying
rationales of a model.
  This thesis presents my research that expands our collective knowledge in the
areas of accountability and transparency of ML models developed for complex
reasoning tasks over text."
13547,"We further study whether multi-hop
reasoning learned with Transformer-XH can be transferred to PolitiHop.","Transformer-XH allows for the sharing
of information between sentences located anywhere in the document by eXtra Hop
attention and achieves the best performance.","We ﬁnd
that the model cannot leverage any reasoning skills from training on FEVER, while
training on LIAR-PLUS improves the performance on PolitiHop.",2022-11-09 15:14:52+00:00,Accountable and Explainable Methods for Complex Reasoning over Text,cs.LG,"['cs.LG', 'cs.CL', 'cs.CY', '68T50', 'I.2.7']",[arxiv.Result.Author('Pepa Atanasova')],"A major concern of Machine Learning (ML) models is their opacity. They are
deployed in an increasing number of applications where they often operate as
black boxes that do not provide explanations for their predictions. Among
others, the potential harms associated with the lack of understanding of the
models' rationales include privacy violations, adversarial manipulations, and
unfair discrimination. As a result, the accountability and transparency of ML
models have been posed as critical desiderata by works in policy and law,
philosophy, and computer science.
  In computer science, the decision-making process of ML models has been
studied by developing accountability and transparency methods. Accountability
methods, such as adversarial attacks and diagnostic datasets, expose
vulnerabilities of ML models that could lead to malicious manipulations or
systematic faults in their predictions. Transparency methods explain the
rationales behind models' predictions gaining the trust of relevant
stakeholders and potentially uncovering mistakes and unfairness in models'
decisions. To this end, transparency methods have to meet accountability
requirements as well, e.g., being robust and faithful to the underlying
rationales of a model.
  This thesis presents my research that expands our collective knowledge in the
areas of accountability and transparency of ML models developed for complex
reasoning tasks over text."
13548,"We also consider the latter and further study the agreement
across model architectures, downstream tasks, and explainability methods.","Another direction of explainability evaluation is to compare the agreement of salient
words annotated by humans to the saliency scores assigned by explanation techniques
(DeYoung et al., 2020a).","While we
consider human annotations at the word level (Camburu et al., 2018; Lei et al., 2016),
there are also datasets (Clark et al., 2019; Khashabi et al., 2018) with annotations at
the sentence-level, which would require other model architectures, so we leave this
for future work.",2022-11-09 15:14:52+00:00,Accountable and Explainable Methods for Complex Reasoning over Text,cs.LG,"['cs.LG', 'cs.CL', 'cs.CY', '68T50', 'I.2.7']",[arxiv.Result.Author('Pepa Atanasova')],"A major concern of Machine Learning (ML) models is their opacity. They are
deployed in an increasing number of applications where they often operate as
black boxes that do not provide explanations for their predictions. Among
others, the potential harms associated with the lack of understanding of the
models' rationales include privacy violations, adversarial manipulations, and
unfair discrimination. As a result, the accountability and transparency of ML
models have been posed as critical desiderata by works in policy and law,
philosophy, and computer science.
  In computer science, the decision-making process of ML models has been
studied by developing accountability and transparency methods. Accountability
methods, such as adversarial attacks and diagnostic datasets, expose
vulnerabilities of ML models that could lead to malicious manipulations or
systematic faults in their predictions. Transparency methods explain the
rationales behind models' predictions gaining the trust of relevant
stakeholders and potentially uncovering mistakes and unfairness in models'
decisions. To this end, transparency methods have to meet accountability
requirements as well, e.g., being robust and faithful to the underlying
rationales of a model.
  This thesis presents my research that expands our collective knowledge in the
areas of accountability and transparency of ML models developed for complex
reasoning tasks over text."
13549,"We
further study this in Sec.","We
conjecture that this might also be due to the model learning spurious correlations.",8.6.1.,2022-11-09 15:14:52+00:00,Accountable and Explainable Methods for Complex Reasoning over Text,cs.LG,"['cs.LG', 'cs.CL', 'cs.CY', '68T50', 'I.2.7']",[arxiv.Result.Author('Pepa Atanasova')],"A major concern of Machine Learning (ML) models is their opacity. They are
deployed in an increasing number of applications where they often operate as
black boxes that do not provide explanations for their predictions. Among
others, the potential harms associated with the lack of understanding of the
models' rationales include privacy violations, adversarial manipulations, and
unfair discrimination. As a result, the accountability and transparency of ML
models have been posed as critical desiderata by works in policy and law,
philosophy, and computer science.
  In computer science, the decision-making process of ML models has been
studied by developing accountability and transparency methods. Accountability
methods, such as adversarial attacks and diagnostic datasets, expose
vulnerabilities of ML models that could lead to malicious manipulations or
systematic faults in their predictions. Transparency methods explain the
rationales behind models' predictions gaining the trust of relevant
stakeholders and potentially uncovering mistakes and unfairness in models'
decisions. To this end, transparency methods have to meet accountability
requirements as well, e.g., being robust and faithful to the underlying
rationales of a model.
  This thesis presents my research that expands our collective knowledge in the
areas of accountability and transparency of ML models developed for complex
reasoning tasks over text."
13550,"An avenue that might be promising
for further study is pre-training readout networks, such that they can be quickly deployed on related
tasks and ﬁne-tuned.","However, graph neural
networks with that readout function tend to occasionally experience divergence on very large datasets
or deep architectures (6+ layers), which can most likely be fully resolved with parameter tuning,
especially the latent/hidden dimension of the attention mechanism.","One starting point could be pre-training on large molecular databases, such as
subsets of GDB-17 [21] with inexpensive to compute molecular tasks (generated with RDKit, for
example) as prediction targets, or unsupervised variations.",2022-11-09 15:21:09+00:00,Graph Neural Networks with Adaptive Readouts,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('David Buterez'), arxiv.Result.Author('Jon Paul Janet'), arxiv.Result.Author('Steven J. Kiddle'), arxiv.Result.Author('Dino Oglic'), arxiv.Result.Author('Pietro Liò')]","An effective aggregation of node features into a graph-level representation
via readout functions is an essential step in numerous learning tasks involving
graph neural networks. Typically, readouts are simple and non-adaptive
functions designed such that the resulting hypothesis space is permutation
invariant. Prior work on deep sets indicates that such readouts might require
complex node embeddings that can be difficult to learn via standard
neighborhood aggregation schemes. Motivated by this, we investigate the
potential of adaptive readouts given by neural networks that do not necessarily
give rise to permutation invariant hypothesis spaces. We argue that in some
problems such as binding affinity prediction where molecules are typically
presented in a canonical form it might be possible to relax the constraints on
permutation invariance of the hypothesis space and learn a more effective model
of the affinity by employing an adaptive readout function. Our empirical
results demonstrate the effectiveness of neural readouts on more than 40
datasets spanning different domains and graph characteristics. Moreover, we
observe a consistent improvement over standard readouts (i.e., sum, max, and
mean) relative to the number of neighborhood aggregation iterations and
different convolutional operators."
13571,"While some of these issues are alleviated
automatically by training a model against ground sensor pollution levels, understanding and correct-
ing for these data biases requires further study.","Another limitation relates to the biases introduced by relying on search data, which may not reﬂect
the underlying population demographics or experiences.","In the future, we plan to investigate other sources of
crowd-based surveillance data such as self-reports in social media, to augment traditional physical
sensor methods, thus providing a more direct, human-centered measure of how people experience
elevated air pollution levels.",2022-11-09 23:56:35+00:00,Detecting Elevated Air Pollution Levels by Monitoring Web Search Queries: Deep Learning-Based Time Series Forecasting,cs.LG,"['cs.LG', 'cs.IR']","[arxiv.Result.Author('Chen Lin'), arxiv.Result.Author('Safoora Yousefi'), arxiv.Result.Author('Elvis Kahoro'), arxiv.Result.Author('Payam Karisani'), arxiv.Result.Author('Donghai Liang'), arxiv.Result.Author('Jeremy Sarnat'), arxiv.Result.Author('Eugene Agichtein')]","Real-time air pollution monitoring is a valuable tool for public health and
environmental surveillance. In recent years, there has been a dramatic increase
in air pollution forecasting and monitoring research using artificial neural
networks (ANNs). Most of the prior work relied on modeling pollutant
concentrations collected from ground-based monitors and meteorological data for
long-term forecasting of outdoor ozone, oxides of nitrogen, and PM2.5. Given
that traditional, highly sophisticated air quality monitors are expensive and
are not universally available, these models cannot adequately serve those not
living near pollutant monitoring sites. Furthermore, because prior models were
built on physical measurement data collected from sensors, they may not be
suitable for predicting public health effects experienced from pollution
exposure. This study aims to develop and validate models to nowcast the
observed pollution levels using Web search data, which is publicly available in
near real-time from major search engines. We developed novel machine
learning-based models using both traditional supervised classification methods
and state-of-the-art deep learning methods to detect elevated air pollution
levels at the US city level, by using generally available meteorological data
and aggregate Web-based search volume data derived from Google Trends. We
validated the performance of these methods by predicting three critical air
pollutants (ozone (O3), nitrogen dioxide (NO2), and fine particulate matter
(PM2.5)), across ten major U.S. metropolitan statistical areas (MSAs) in 2017
and 2018."
13580,"The
                          Springer Nature 2021 LATEX template

A classiﬁcation performance evaluation measure considering data separability 27

existing separability measures still lack the above generalization properties,
and this is a point that deserves further study.","The values are still comparable when
comparing datasets with diﬀerent sample sizes and feature dimensions.","Declarations

• Conﬂict of interests The authors declare that they have no conﬂict of
   interest.",2022-11-10 09:18:26+00:00,A classification performance evaluation measure considering data separability,cs.LG,"['cs.LG', 'cs.IT', 'math.IT']","[arxiv.Result.Author('Lingyan Xue'), arxiv.Result.Author('Xinyu Zhang'), arxiv.Result.Author('Weidong Jiang'), arxiv.Result.Author('Kai Huo')]","Machine learning and deep learning classification models are data-driven, and
the model and the data jointly determine their classification performance. It
is biased to evaluate the model's performance only based on the classifier
accuracy while ignoring the data separability. Sometimes, the model exhibits
excellent accuracy, which might be attributed to its testing on highly
separable data. Most of the current studies on data separability measures are
defined based on the distance between sample points, but this has been
demonstrated to fail in several circumstances. In this paper, we propose a new
separability measure--the rate of separability (RS), which is based on the data
coding rate. We validate its effectiveness as a supplement to the separability
measure by comparing it to four other distance-based measures on synthetic
datasets. Then, we demonstrate the positive correlation between the proposed
measure and recognition accuracy in a multi-task scenario constructed from a
real dataset. Finally, we discuss the methods for evaluating the classification
performance of machine learning and deep learning models considering data
separability."
13636,"To further study the
Performance vs. Incentive Acceptance Probability.","adopted in related literature [5], [8].","This set                                             impact of different learning rates on the proposed ac-PPO
of experiments investigates the effectiveness of the proposed                                          algorithm, in this set of experiments we vary the learning rate
ac-PPO algorithm for ﬂeet rebalancing under different user                                             during training and evaluate the performance of obtained mod-
behaviour.",2022-11-11 11:25:30+00:00,Fleet Rebalancing for Expanding Shared e-Mobility Systems: A Multi-agent Deep Reinforcement Learning Approach,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY']","[arxiv.Result.Author('Man Luo'), arxiv.Result.Author('Bowen Du'), arxiv.Result.Author('Wenzhe Zhang'), arxiv.Result.Author('Tianyou Song'), arxiv.Result.Author('Kun Li'), arxiv.Result.Author('Hongming Zhu'), arxiv.Result.Author('Mark Birkin'), arxiv.Result.Author('Hongkai Wen')]","The electrification of shared mobility has become popular across the globe.
Many cities have their new shared e-mobility systems deployed, with
continuously expanding coverage from central areas to the city edges. A key
challenge in the operation of these systems is fleet rebalancing, i.e., how EVs
should be repositioned to better satisfy future demand. This is particularly
challenging in the context of expanding systems, because i) the range of the
EVs is limited while charging time is typically long, which constrain the
viable rebalancing operations; and ii) the EV stations in the system are
dynamically changing, i.e., the legitimate targets for rebalancing operations
can vary over time. We tackle these challenges by first investigating rich sets
of data collected from a real-world shared e-mobility system for one year,
analyzing the operation model, usage patterns and expansion dynamics of this
new mobility mode. With the learned knowledge we design a high-fidelity
simulator, which is able to abstract key operation details of EV sharing at
fine granularity. Then we model the rebalancing task for shared e-mobility
systems under continuous expansion as a Multi-Agent Reinforcement Learning
(MARL) problem, which directly takes the range and charging properties of the
EVs into account. We further propose a novel policy optimization approach with
action cascading, which is able to cope with the expansion dynamics and solve
the formulated MARL. We evaluate the proposed approach extensively, and
experimental results show that our approach outperforms the state-of-the-art,
offering significant performance gain in both satisfied demand and net revenue."
13663,"In Section 3 the conclusions and important further research
developments are discussed.",In Section 2 the formalism and notation are discussed.,"2 Formalism

In this section the case for feed forward neural networks (FFNN) is discussed and the deﬁnition of equivalent minima
and their exact number depending on the network architecture is respectively given and calculated.",2022-11-12 07:51:14+00:00,On the High Symmetry of Neural Network Functions,cs.LG,['cs.LG'],[arxiv.Result.Author('Umberto Michelucci')],"Training neural networks means solving a high-dimensional optimization
problem. Normally the goal is to minimize a loss function that depends on what
is called the network function, or in other words the function that gives the
network output given a certain input. This function depends on a large number
of parameters, also known as weights, that depends on the network architecture.
In general the goal of this optimization problem is to find the global minimum
of the network function. In this paper it is discussed how due to how neural
networks are designed, the neural network function present a very large
symmetry in the parameter space. This work shows how the neural network
function has a number of equivalent minima, in other words minima that give the
same value for the loss function and the same exact output, that grows
factorially with the number of neurons in each layer for feed forward neural
network or with the number of filters in a convolutional neural networks. When
the number of neurons and layers is large, the number of equivalent minima
grows extremely fast. This will have of course consequences for the study of
how neural networks converges to minima during training. This results is known,
but in this paper for the first time a proper mathematical discussion is
presented and an estimate of the number of equivalent minima is derived."
13665,"Although we have built an evaluation framework for XRL approaches, there is still
a lack of specific measurement methods (especially quantitative methods) for different XRL frameworks which need
much further research focus on it.","These two types are complementary and can be implemented as a combination to get a more precise
and comprehensive measurement.","4 EXPLAINABILITY IN RL

                         Agent                                                 High-level Agent

   Explainable  Env                            Env    Agent               Env                                      Env
       Agent                                        Attention

                                    Reward                                                       High-level Agent
                                Reconstructor

   (a) Model-explaining  (b) Reward-explaining      (c) State-explaining       (d) Task-explaining

Fig.",2022-11-12 13:52:06+00:00,"A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yunpeng Qing'), arxiv.Result.Author('Shunyu Liu'), arxiv.Result.Author('Jie Song'), arxiv.Result.Author('Mingli Song')]","Reinforcement Learning (RL) is a popular machine learning paradigm where
intelligent agents interact with the environment to fulfill a long-term goal.
Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great
success over a wide spectrum of complex control tasks. Despite the encouraging
results achieved, the deep neural network-based backbone is widely deemed as a
black box that impedes practitioners to trust and employ trained agents in
realistic scenarios where high security and reliability are essential. To
alleviate this issue, a large volume of literature devoted to shedding light on
the inner workings of the intelligent agents has been proposed, by constructing
intrinsic interpretability or post-hoc explainability. In this survey, we
provide a comprehensive review of existing works on eXplainable RL (XRL) and
introduce a new taxonomy where prior works are clearly categorized into
model-explaining, reward-explaining, state-explaining, and task-explaining
methods. We also review and highlight RL methods that conversely leverage human
knowledge to promote learning efficiency and final performance of agents while
this kind of method is often ignored in XRL field. Some open challenges and
opportunities in XRL are discussed. This survey intends to provide a high-level
summarization and better understanding of XRL and to motivate future research
on more effective XRL solutions. Corresponding open source codes are collected
and categorized at
https://github.com/Plankson/awesome-explainable-reinforcement-learning."
13666,"More further research should be conducted to decide how to
maintain the balance of explainability and performance.","[127], Rudin and Carlson [172] point out that sometimes we rely too much on deep learning methods, for some
specific tasks simple models can be considered and can also receive an excellent performance without the intransparency
of deep learning models that may lose the generalizability.","6.3 Evaluation methods

Although we have talked about the current evaluation method for XRL in Section 3, there is still no evaluation approach
that can be accepted by most of the experts in DRL community.",2022-11-12 13:52:06+00:00,"A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yunpeng Qing'), arxiv.Result.Author('Shunyu Liu'), arxiv.Result.Author('Jie Song'), arxiv.Result.Author('Mingli Song')]","Reinforcement Learning (RL) is a popular machine learning paradigm where
intelligent agents interact with the environment to fulfill a long-term goal.
Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great
success over a wide spectrum of complex control tasks. Despite the encouraging
results achieved, the deep neural network-based backbone is widely deemed as a
black box that impedes practitioners to trust and employ trained agents in
realistic scenarios where high security and reliability are essential. To
alleviate this issue, a large volume of literature devoted to shedding light on
the inner workings of the intelligent agents has been proposed, by constructing
intrinsic interpretability or post-hoc explainability. In this survey, we
provide a comprehensive review of existing works on eXplainable RL (XRL) and
introduce a new taxonomy where prior works are clearly categorized into
model-explaining, reward-explaining, state-explaining, and task-explaining
methods. We also review and highlight RL methods that conversely leverage human
knowledge to promote learning efficiency and final performance of agents while
this kind of method is often ignored in XRL field. Some open challenges and
opportunities in XRL are discussed. This survey intends to provide a high-level
summarization and better understanding of XRL and to motivate future research
on more effective XRL solutions. Corresponding open source codes are collected
and categorized at
https://github.com/Plankson/awesome-explainable-reinforcement-learning."
13667,"More further research
should be conducted to decide how to maintain the balance of explainability and performance.","[127], Rudin and Carlson
[172] point out that sometimes we rely too much on deep learning methods, for some specific
tasks simple models can be considered and can also receive an excellent performance without the
intransparency of deep learning models that may lose the generalizability.","6.3 Evaluation methods

Although we have talked about the current evaluation method for XRL in Section 3, there is still no
evaluation approach that can be accepted by most of the experts in DRL community.",2022-11-12 13:52:06+00:00,"A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yunpeng Qing'), arxiv.Result.Author('Shunyu Liu'), arxiv.Result.Author('Jie Song'), arxiv.Result.Author('Mingli Song')]","Reinforcement Learning (RL) is a popular machine learning paradigm where
intelligent agents interact with the environment to fulfill a long-term goal.
Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great
success over a wide spectrum of complex control tasks. Despite the encouraging
results achieved, the deep neural network-based backbone is widely deemed as a
black box that impedes practitioners to trust and employ trained agents in
realistic scenarios where high security and reliability are essential. To
alleviate this issue, a large volume of literature devoted to shedding light on
the inner workings of the intelligent agents has been proposed, by constructing
intrinsic interpretability or post-hoc explainability. In this survey, we
provide a comprehensive review of existing works on eXplainable RL (XRL) and
introduce a new taxonomy where prior works are clearly categorized into
model-explaining, reward-explaining, state-explaining, and task-explaining
methods. We also review and highlight RL methods that conversely leverage human
knowledge to promote learning efficiency and final performance of agents while
this kind of method is often ignored in XRL field. Some open challenges and
opportunities in XRL are discussed. This survey intends to provide a high-level
summarization and better understanding of XRL and to motivate future research
on more effective XRL solutions. Corresponding open source codes are collected
and categorized at
https://github.com/Plankson/awesome-explainable-reinforcement-learning."
13668,"More further research should be conducted to decide how to
maintain the balance of explainability and performance.","[129], Rudin and Carlson [174] point out that sometimes we
rely too much on deep learning methods, for some specific tasks simple models can be considered
and can also receive an excellent performance without the intransparency of deep learning models
that may lose the generalizability.","7 CONCLUSION

Explainability has attracted increasing attention in the RL community due to practical, safe, and
trustworthy concerns.",2022-11-12 13:52:06+00:00,"A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Yunpeng Qing'), arxiv.Result.Author('Shunyu Liu'), arxiv.Result.Author('Jie Song'), arxiv.Result.Author('Huiqiong Wang'), arxiv.Result.Author('Mingli Song')]","Reinforcement Learning (RL) is a popular machine learning paradigm where
intelligent agents interact with the environment to fulfill a long-term goal.
Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great
success over a wide spectrum of complex control tasks. Despite the encouraging
results achieved, the deep neural network-based backbone is widely deemed as a
black box that impedes practitioners to trust and employ trained agents in
realistic scenarios where high security and reliability are essential. To
alleviate this issue, a large volume of literature devoted to shedding light on
the inner workings of the intelligent agents has been proposed, by constructing
intrinsic interpretability or post-hoc explainability. In this survey, we
provide a comprehensive review of existing works on eXplainable RL (XRL) and
introduce a new taxonomy where prior works are clearly categorized into
model-explaining, reward-explaining, state-explaining, and task-explaining
methods. We also review and highlight RL methods that conversely leverage human
knowledge to promote learning efficiency and performance of agents while this
kind of method is often ignored in XRL field. Some challenges and opportunities
in XRL are discussed. This survey intends to provide a high-level summarization
of XRL and to motivate future research on more effective XRL solutions.
Corresponding open source codes are collected and categorized at
https://github.com/Plankson/awesome-explainable-reinforcement-learning."
13721,"They also come with
                                        unique drawbacks and suﬀer from unresolved limitations (e.g., stability, convergence, and generalization) that
                                        call for further research.","These approaches oﬀer unique
                                        advantages to accelerate the modeling of complex multiscale multi-physics phenomena.","This study aims to present an in-depth review of the three neural network frame-
                                        works (i.e., PgNN, PiNN, and PeNN) used in scientiﬁc computing research.",2022-11-14 15:44:07+00:00,"Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing",cs.LG,['cs.LG'],"[arxiv.Result.Author('Salah A Faroughi'), arxiv.Result.Author('Nikhil Pawar'), arxiv.Result.Author('Celio Fernandes'), arxiv.Result.Author('Subasish Das'), arxiv.Result.Author('Nima K. Kalantari'), arxiv.Result.Author('Seyed Kourosh Mahjour')]","Recent breakthroughs in computing power have made it feasible to use machine
learning and deep learning to advance scientific computing in many fields, such
as fluid mechanics, solid mechanics, materials science, etc. Neural networks,
in particular, play a central role in this hybridization. Due to their
intrinsic architecture, conventional neural networks cannot be successfully
trained and scoped when data is sparse; a scenario that is true in many
scientific fields. Nonetheless, neural networks offer a strong foundation to
digest physical-driven or knowledge-based constraints during training.
Generally speaking, there are three distinct neural network frameworks to
enforce underlying physics: (i) physics-guided neural networks (PgNN), (ii)
physics-informed neural networks (PiNN) and (iii) physics-encoded neural
networks (PeNN). These approaches offer unique advantages to accelerate the
modeling of complex multiscale multi-physics phenomena. They also come with
unique drawbacks and suffer from unresolved limitations (e.g., stability,
convergence, and generalization) that call for further research. This study
aims to present an in-depth review of the three neural network frameworks
(i.e., PgNN, PiNN, and PeNN) used in scientific computing research. The
state-of-the-art architectures and their applications are reviewed; limitations
are discussed; and future research opportunities in terms of improving
algorithms, considering causalities, expanding applications, and coupling
scientific and deep learning solvers are presented. This critical review
provides a solid starting point for researchers and engineers to comprehend how
to integrate different layers of physics into neural networks."
13752,"and that further study of contrastive explanations is needed to understand how best to use them to
interpret RL policies.","These results suggest that participants generally prefer complete explanations over
                                        contrastive ones, at least in cases when complete explanations are small enough to be understandable,

                                        36th Conference on Neural Information Processing Systems (NeurIPS 2022).","(b) An example of a complete explana-

(a) Our interface showing the task and a small, contrastive explanation.tion which does not reference any con-

The box on the left shows the maze domain where black squares aretrast policy.",2022-11-14 19:43:03+00:00,(When) Are Contrastive Explanations of Reinforcement Learning Helpful?,cs.LG,"['cs.LG', 'cs.HC']","[arxiv.Result.Author('Sanjana Narayanan'), arxiv.Result.Author('Isaac Lage'), arxiv.Result.Author('Finale Doshi-Velez')]","Global explanations of a reinforcement learning (RL) agent's expected
behavior can make it safer to deploy. However, such explanations are often
difficult to understand because of the complicated nature of many RL policies.
Effective human explanations are often contrastive, referencing a known
contrast (policy) to reduce redundancy. At the same time, these explanations
also require the additional effort of referencing that contrast when evaluating
an explanation. We conduct a user study to understand whether and when
contrastive explanations might be preferable to complete explanations that do
not require referencing a contrast. We find that complete explanations are
generally more effective when they are the same size or smaller than a
contrastive explanation of the same policy, and no worse when they are larger.
This suggests that contrastive explanations are not sufficient to solve the
problem of effectively explaining reinforcement learning policies, and require
additional careful study for use in this context."
13790,"In addition, to further study the FFNN algorithm, we used a SIM dataset with
continuous responses.","Datasets
         Our analysis is based on three real datasets: Home Lending Data, Personal Lines and Loans Data,

and the Bike Share Data.","Table 1: Description of datasets

        Name        Abbreviation   Type of    Number of                Dataset Size –
                                  Response     variables            Training: Validation
  Home Lending            HL                                         333,431: 333,706
  Personal Line of       PLL        Binary         62
                                    Binary        143                 35,583: 26,687
        Loans
     Bike Share     BS            Continuous  11                        9,384: 3,997
  Simulation (see                                                     30,000: 10,000
model form below)   SIM           Continuous  14

a) Home Lending Data
The Home Lending dataset deals with residential mortgages, and the response is an indicator (= 1) for
“troubled loans”.",2022-11-15 22:14:52+00:00,Behavior of Hyper-Parameters for Selected Machine Learning Algorithms: An Empirical Investigation,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anwesha Bhattacharyya'), arxiv.Result.Author('Joel Vaughan'), arxiv.Result.Author('Vijayan N. Nair')]","Hyper-parameters (HPs) are an important part of machine learning (ML) model
development and can greatly influence performance. This paper studies their
behavior for three algorithms: Extreme Gradient Boosting (XGB), Random Forest
(RF), and Feedforward Neural Network (FFNN) with structured data. Our empirical
investigation examines the qualitative behavior of model performance as the HPs
vary, quantifies the importance of each HP for different ML algorithms, and
stability of the performance near the optimal region. Based on the findings, we
propose a set of guidelines for efficient HP tuning by reducing the search
space."
13791,"So it appears
that these two additional HPs can be used as alternate forms of regularization, but any broad conclusion
needs further study.","However, there
was indeed a large change for the over-fit model, and the gap statistic lowered considerably.",6.,2022-11-15 22:14:52+00:00,Behavior of Hyper-Parameters for Selected Machine Learning Algorithms: An Empirical Investigation,cs.LG,['cs.LG'],"[arxiv.Result.Author('Anwesha Bhattacharyya'), arxiv.Result.Author('Joel Vaughan'), arxiv.Result.Author('Vijayan N. Nair')]","Hyper-parameters (HPs) are an important part of machine learning (ML) model
development and can greatly influence performance. This paper studies their
behavior for three algorithms: Extreme Gradient Boosting (XGB), Random Forest
(RF), and Feedforward Neural Network (FFNN) with structured data. Our empirical
investigation examines the qualitative behavior of model performance as the HPs
vary, quantifies the importance of each HP for different ML algorithms, and
stability of the performance near the optimal region. Based on the findings, we
propose a set of guidelines for efficient HP tuning by reducing the search
space."
13803,"[38], we use visual coherence           further research in the ﬁeld.","sults and hypotheses about the positive effect of adversar-
                                                                    ial training on model interpretability and sets the stage for
   Following Smilkov et al.","Moreover, we quantitatively
to indicate that the salient areas highlight mainly the ob-         demonstrated the superiority of our proposed method using
ject of interest, rather than the background.",2022-11-16 05:40:29+00:00,Improving Interpretability via Regularization of Neural Activation Sensitivity,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Ofir Moshe'), arxiv.Result.Author('Gil Fidel'), arxiv.Result.Author('Ron Bitton'), arxiv.Result.Author('Asaf Shabtai')]","State-of-the-art deep neural networks (DNNs) are highly effective at tackling
many real-world tasks. However, their wide adoption in mission-critical
contexts is hampered by two major weaknesses - their susceptibility to
adversarial attacks and their opaqueness. The former raises concerns about the
security and generalization of DNNs in real-world conditions, whereas the
latter impedes users' trust in their output. In this research, we (1) examine
the effect of adversarial robustness on interpretability and (2) present a
novel approach for improving the interpretability of DNNs that is based on
regularization of neural activation sensitivity. We evaluate the
interpretability of models trained using our method to that of standard models
and models trained using state-of-the-art adversarial robustness techniques.
Our results show that adversarially robust models are superior to standard
models and that models trained using our proposed method are even better than
adversarially robust models in terms of interpretability."
13805,To further study          mented in health literature.,2012).,"Two clusters exhibited a large
                                                                       local bias towards patients without diabetes, clusters 1
    4A patient that has passed within 48 hours of their ICU stay is    and 4.",2022-11-16 08:04:12+00:00,Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Anaelia Ovalle'), arxiv.Result.Author('Sunipa Dev'), arxiv.Result.Author('Jieyu Zhao'), arxiv.Result.Author('Majid Sarrafzadeh'), arxiv.Result.Author('Kai-Wei Chang')]","Auditing machine learning-based (ML) healthcare tools for bias is critical to
preventing patient harm, especially in communities that disproportionately face
health inequities. General frameworks are becoming increasingly available to
measure ML fairness gaps between groups. However, ML for health (ML4H) auditing
principles call for a contextual, patient-centered approach to model
assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H
auditing principles and (2) able to illuminate and characterize communities
vulnerable to the most harm. To address this gap, we propose supplementing ML4H
auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs
detectioN), an automatic tool for capturing local biases in a clinical
prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs
detectioN), by contextualizing group bias detection in patient illness severity
and past medical history. We investigate and compare SLOGAN's bias detection
capabilities to LOGAN and other clustering techniques across patient subgroups
in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness
disparities in over 75% of patient groups than LOGAN while maintaining
clustering quality. Furthermore, in a diabetes case study, health disparity
literature corroborates the characterizations of the most biased clusters
identified by SLOGAN. Our results contribute to the broader discussion of how
machine learning biases may perpetuate existing healthcare disparities."
13806,"As discussed in Section 2, while existing
meta-RL benchmarks tend to be either readily accessible or impactful and realistic, automatically
providing feedback simultaneously provides both, and we release code for a meta-RL wrapper of the
Bounce programming assignment to spur further research in this direction.","Additionally, this connection also offers a
new testbed for meta-exploration and meta-RL research.","5 Experiments

In our experiments, we aim to answer ﬁve main questions: (1) How does automated feedback grading
accuracy compare to human grading accuracy?",2022-11-16 10:00:23+00:00,Giving Feedback on Interactive Student Programs with Meta-Exploration,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Evan Zheran Liu'), arxiv.Result.Author('Moritz Stephan'), arxiv.Result.Author('Allen Nie'), arxiv.Result.Author('Chris Piech'), arxiv.Result.Author('Emma Brunskill'), arxiv.Result.Author('Chelsea Finn')]","Developing interactive software, such as websites or games, is a particularly
engaging way to learn computer science. However, teaching and giving feedback
on such software is time-consuming -- standard approaches require instructors
to manually grade student-implemented interactive programs. As a result, online
platforms that serve millions, like Code.org, are unable to provide any
feedback on assignments for implementing interactive programs, which critically
hinders students' ability to learn. One approach toward automatic grading is to
learn an agent that interacts with a student's program and explores states
indicative of errors via reinforcement learning. However, existing work on this
approach only provides binary feedback of whether a program is correct or not,
while students require finer-grained feedback on the specific errors in their
programs to understand their mistakes. In this work, we show that exploring to
discover errors can be cast as a meta-exploration problem. This enables us to
construct a principled objective for discovering errors and an algorithm for
optimizing this objective, which provides fine-grained feedback. We evaluate
our approach on a set of over 700K real anonymized student programs from a
Code.org interactive assignment. Our approach provides feedback with 94.3%
accuracy, improving over existing approaches by 17.7% and coming within 1.5% of
human-level accuracy. Project web page: https://ezliu.github.io/dreamgrader."
13812,"While Figure 5 presents the result of a single attack magnitude, we further study the inﬂuence of hy-
perparameters under diﬀerent attack magnitudes.","If we trade some robustness for the natural accuracy, we
can achieve the same level of robustness (80.20%) at 91.27% accuracy, thus closing the gap between the
natural accuracy of DP and non-DP models without sacriﬁcing the robustness.","We illustrate on CIFAR10 the l∞ attack performance
in Table 2 and the l2 one in Table 3.",2022-11-16 14:44:27+00:00,Differentially Private Optimizers Can Learn Adversarially Robust Models,cs.LG,"['cs.LG', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Yuan Zhang'), arxiv.Result.Author('Zhiqi Bu')]","Machine learning models have shone in a variety of domains and attracted
increasing attention from both the security and the privacy communities. One
important yet worrying question is: will training models under the differential
privacy (DP) constraint unfavorably impact on the adversarial robustness? While
previous works have postulated that privacy comes at the cost of worse
robustness, we give the first theoretical analysis to show that DP models can
indeed be robust and accurate, even sometimes more robust than their
naturally-trained non-private counterparts. We observe three key factors that
influence the privacy-robustness-accuracy tradeoff: (1) hyperparameters for DP
optimizers are critical; (2) pre-training on public data significantly
mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a
difference. With these factors set properly, we achieve 90\% natural accuracy,
72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$
attack, and 69\% robust accuracy ($+16\%$ than the non-private model) with
pre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with
$\epsilon=2$. In fact, we show both theoretically and empirically that DP
models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the
robustness of DP models is consistently observed on MNIST, Fashion MNIST and
CelebA datasets, with ResNet and Vision Transformer. We believe our encouraging
results are a significant step towards training models that are private as well
as robust."
13820,"To further study the effects of teaching addition alongside subtraction, we evaluate two subtraction-only prompts.","Thus, scaling to larger number of algorithms may call for more efﬁcient strategies.",The ﬁrst one removes the addition-only prompt examples from the combined addition-subtraction prompt.,2022-11-15 06:12:28+00:00,Teaching Algorithmic Reasoning via In-context Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Hattie Zhou'), arxiv.Result.Author('Azade Nova'), arxiv.Result.Author('Hugo Larochelle'), arxiv.Result.Author('Aaron Courville'), arxiv.Result.Author('Behnam Neyshabur'), arxiv.Result.Author('Hanie Sedghi')]","Large language models (LLMs) have shown increasing in-context learning
capabilities through scaling up model and data size. Despite this progress,
LLMs are still unable to solve algorithmic reasoning problems. While providing
a rationale with the final answer has led to further improvements in multi-step
reasoning problems, Anil et al. 2022 showed that even simple algorithmic
reasoning tasks such as parity are far from solved. In this work, we identify
and study four key stages for successfully teaching algorithmic reasoning to
LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills
simultaneously (skill accumulation), (3) teaching how to combine skills (skill
composition) and (4) teaching how to use skills as tools. We show that it is
possible to teach algorithmic reasoning to LLMs via in-context learning, which
we refer to as algorithmic prompting. We evaluate our approach on a variety of
arithmetic and quantitative reasoning tasks, and demonstrate significant boosts
in performance over existing prompting techniques. In particular, for long
parity, addition, multiplication and subtraction, we achieve an error reduction
of approximately 10x, 9x, 5x and 2x respectively compared to the best available
baselines."
13822,We leave further study of these settings for future work.,"the deterministic-label setting where y ∈ [−1, 1] and Pr(x,y)∼µ[s(x) = y] = 1 for
some s ∈ S).","In this section, we prove the following sample complexity upper bound for comparative regres-
sion in terms of the mutual fat-shattering dimension:

Theorem 7.1.",2022-11-16 18:38:24+00:00,Comparative Learning: A Sample Complexity Theory for Two Hypothesis Classes,cs.LG,"['cs.LG', 'cs.CC', 'cs.DS', 'stat.ML']","[arxiv.Result.Author('Lunjia Hu'), arxiv.Result.Author('Charlotte Peale')]","In many learning theory problems, a central role is played by a hypothesis
class: we might assume that the data is labeled according to a hypothesis in
the class (usually referred to as the realizable setting), or we might evaluate
the learned model by comparing it with the best hypothesis in the class (the
agnostic setting).
  Taking a step beyond these classic setups that involve only a single
hypothesis class, we introduce comparative learning as a combination of the
realizable and agnostic settings in PAC learning: given two binary hypothesis
classes $S$ and $B$, we assume that the data is labeled according to a
hypothesis in the source class $S$ and require the learned model to achieve an
accuracy comparable to the best hypothesis in the benchmark class $B$. Even
when both $S$ and $B$ have infinite VC dimensions, comparative learning can
still have a small sample complexity. We show that the sample complexity of
comparative learning is characterized by the mutual VC dimension
$\mathsf{VC}(S,B)$ which we define to be the maximum size of a subset shattered
by both $S$ and $B$. We also show a similar result in the online setting, where
we give a regret characterization in terms of the mutual Littlestone dimension
$\mathsf{Ldim}(S,B)$. These results also hold for partial hypotheses.
  We additionally show that the insights necessary to characterize the sample
complexity of comparative learning can be applied to characterize the sample
complexity of realizable multiaccuracy and multicalibration using the mutual
fat-shattering dimension, an analogue of the mutual VC dimension for
real-valued hypotheses. This not only solves an open problem proposed by Hu,
Peale, Reingold (2022), but also leads to independently interesting results
extending classic ones about regression, boosting, and covering number to our
two-hypothesis-class setting."
13832,"We remark that the proper examination of active
learning in conditions of shift demands further study.","We performed experiments with datasets under conditions of data shift, which have not been thor-
oughly examined in the active learning literature.","In particular, there are a variety of questions
related to designing and adapting active learning methods to the data shift setting that take advantage
of the information afforded by the validation sets.",2022-11-17 01:02:12+00:00,Active Learning with Expected Error Reduction,cs.LG,['cs.LG'],"[arxiv.Result.Author('Stephen Mussmann'), arxiv.Result.Author('Julia Reisler'), arxiv.Result.Author('Daniel Tsai'), arxiv.Result.Author('Ehsan Mousavi'), arxiv.Result.Author(""Shayne O'Brien""), arxiv.Result.Author('Moises Goldszmidt')]","Active learning has been studied extensively as a method for efficient data
collection. Among the many approaches in literature, Expected Error Reduction
(EER) (Roy and McCallum) has been shown to be an effective method for active
learning: select the candidate sample that, in expectation, maximally decreases
the error on an unlabeled set. However, EER requires the model to be retrained
for every candidate sample and thus has not been widely used for modern deep
neural networks due to this large computational cost. In this paper we
reformulate EER under the lens of Bayesian active learning and derive a
computationally efficient version that can use any Bayesian parameter sampling
method (such as arXiv:1506.02142). We then compare the empirical performance of
our method using Monte Carlo dropout for parameter sampling against state of
the art methods in the deep active learning literature. Experiments are
performed on four standard benchmark datasets and three WILDS datasets
(arXiv:2012.07421). The results indicate that our method outperforms all other
methods except one in the data shift scenario: a model dependent,
non-information theoretic method that requires an order of magnitude higher
computational cost (arXiv:1906.03671)."
13873,"We see this as a promising initial result, but further study is needed.","We ﬁnd our learned optimizer is capable of optimizing this model and continually
making improvements.","I Open Source Details

In this section we give a practical overview of how to use our learned optimizer.",2022-11-17 18:39:07+00:00,VeLO: Training Versatile Learned Optimizers by Scaling Up,cs.LG,"['cs.LG', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Luke Metz'), arxiv.Result.Author('James Harrison'), arxiv.Result.Author('C. Daniel Freeman'), arxiv.Result.Author('Amil Merchant'), arxiv.Result.Author('Lucas Beyer'), arxiv.Result.Author('James Bradbury'), arxiv.Result.Author('Naman Agrawal'), arxiv.Result.Author('Ben Poole'), arxiv.Result.Author('Igor Mordatch'), arxiv.Result.Author('Adam Roberts'), arxiv.Result.Author('Jascha Sohl-Dickstein')]","While deep learning models have replaced hand-designed features across many
domains, these models are still trained with hand-designed optimizers. In this
work, we leverage the same scaling approach behind the success of deep learning
to learn versatile optimizers. We train an optimizer for deep learning which is
itself a small neural network that ingests gradients and outputs parameter
updates. Meta-trained with approximately four thousand TPU-months of compute on
a wide variety of optimization tasks, our optimizer not only exhibits
compelling performance, but optimizes in interesting and unexpected ways. It
requires no hyperparameter tuning, instead automatically adapting to the
specifics of the problem being optimized. We open source our learned optimizer,
meta-training code, the associated train and test data, and an extensive
optimizer benchmark suite with baselines at velo-code.github.io."
13874,"For further research, we plan to provide experimental results to validate
the effectiveness of our models based on the attention-ambiguity mechanism.","Our work is the ﬁrst to explore the combination of attention mechanism and ambiguity by
deep neural networks.","In addition, we plan
to extend the GAMMT framework to vision, audio, reinforcement learning and other domains in
the future works.",2022-11-16 06:24:26+00:00,GAMMT: Generative Ambiguity Modeling Using Multiple Transformers,cs.LG,"['cs.LG', 'cs.AI', 'math.PR']",[arxiv.Result.Author('Xingcheng Xu')],"We introduce a new model based on sets of probabilities for sequential data.
We name the model GAMMT, which stands for Generative Ambiguity Models using
Multiple Transformers. We suppose that data generating process of a sequence is
ambiguous and determined by a set of probabilities rather than one as in the
conventional model. We use multiple parallel transformers connected by a
selection mechanism to approximate ambiguous probabilities. The GAMMT allows
for ambiguity modeling in a generative way and multiple representations of the
input tokens and the input sequence. This work explores the combination of
attention mechanism and ambiguity by deep neural networks. We expect that this
framework will facilitate new research into machine learning, improving our
understanding of the attention-ambiguity mechanism."
13880,"In order to further study the sparsity and the stability of the FCCA proce-
dure, in Figs.","In all datasets, we can see that the
FCCA procedure uses fewer features w.r.t B.","7-12 we analyze the frequency of each feature of each dataset both
in the baseline model and in the discretized model.",2022-11-17 21:16:14+00:00,Supervised Feature Compression based on Counterfactual Analysis,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Veronica Piccialli'), arxiv.Result.Author('Dolores Romero Morales'), arxiv.Result.Author('Cecilia Salvatore')]","Counterfactual Explanations are becoming a de-facto standard in post-hoc
interpretable machine learning. For a given classifier and an instance
classified in an undesired class, its counterfactual explanation corresponds to
small perturbations of that instance that allows changing the classification
outcome. This work aims to leverage Counterfactual Explanations to detect the
important decision boundaries of a pre-trained black-box model. This
information is used to build a supervised discretization of the features in the
dataset with a tunable granularity. Using the discretized dataset, a smaller,
therefore more interpretable Decision Tree can be trained, which, in addition,
enhances the stability and robustness of the baseline Decision Tree. Numerical
results on real-world datasets show the effectiveness of the approach in terms
of accuracy and sparsity compared to the baseline Decision Tree."
13912,"Overall, further study is warranted into using
Walsh ensembles to model a decision boundary formed from             [15] F. Lin, "" Learning in Neural Networks: Feedback-Network-Free
DNNs for adversarial robustness and for understanding DNN                 Implementation and Biological Plausibility,"" in IEEE Transactions on
transferability and learning.",(ECOC) [24].,"Neural Networks and Learning Systems, doi:
                                                                          10.1109/TNNLS.2021.3089134.",2022-11-18 13:26:57+00:00,Adversarial Detection by Approximation of Ensemble Boundary,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']",[arxiv.Result.Author('T. Windeatt')],"A spectral approximation of a Boolean function is proposed for approximating
the decision boundary of an ensemble of Deep Neural Networks (DNNs) solving
two-class pattern recognition problems. The Walsh combination of relatively
weak DNN classifiers is shown experimentally to be capable of detecting
adversarial attacks. By observing the difference in Walsh coefficient
approximation between clean and adversarial images, it appears that
transferability of attack may be used for detection. Approximating the decision
boundary may also aid in understanding the learning and transferability
properties of DNNs. While the experiments here use images, the proposed
approach of modelling two-class ensemble decision boundaries could in principle
be applied to any application area."
13913,"Overall,                     Information Processing Systems (NeurIPS), Dec 5-10, Barcelona, Spain,
further study is warranted into using Walsh ensembles to model                      2016.
a decision boundary formed from DNNs for adversarial
robustness and for understanding DNN transferability and                       [20] C. Zhang, P. Benz, C. Lin, A. Karjauv, J. Wu, and I. S. Kweon, “A Survey
learning.","on Neural
[24], which reduces multi-class problems to two class.","On Universal Adversarial Attack”, arXiv:2103.01498, 2021.",2022-11-18 13:26:57+00:00,Adversarial Detection by Approximation of Ensemble Boundary,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']",[arxiv.Result.Author('T. Windeatt')],"A spectral approximation of a Boolean function is proposed for approximating
the decision boundary of an ensemble of Deep Neural Networks (DNNs) solving
two-class pattern recognition problems. The Walsh combination of relatively
weak DNN classifiers is shown experimentally to be capable of detecting
adversarial attacks. By observing the difference in Walsh coefficient
approximation between clean and adversarial images, it appears that
transferability of attack may be used for detection. Approximating the decision
boundary may also aid in understanding the learning and transferability
properties of DNNs. While the experiments here use images, the proposed
approach of modelling two-class ensemble decision boundaries could in principle
be applied to any application area."
13914,"Overall, further study is warranted into using Walsh                          Spain, Dec. 2016.
ensembles to model a decision boundary formed from DNNs
for adversarial robustness and for understanding DNN                            [20]C. Zhang, P. Benz, C. Lin, A. Karjauv, J. Wu and I. S. Kweon, “A Survey
transferability and learning.","NeurIPS, Barcelona,
class.","On Universal Adversarial Attack”, arXiv:2103.01498v2 Jan. 2022.",2022-11-18 13:26:57+00:00,Adversarial Detection by Approximation of Ensemble Boundary,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']",[arxiv.Result.Author('T. Windeatt')],"A spectral approximation of a Boolean function is proposed for approximating
the decision boundary of an ensemble of Deep Neural Networks (DNNs) solving
two-class pattern recognition problems. The Walsh combination of relatively
weak DNN classifiers is shown experimentally to be capable of detecting
adversarial attacks. By observing the difference in Walsh coefficient
approximation between clean and adversarial images, it appears that
transferability of attack may be used for detection. Approximating the decision
boundary may also aid in understanding the learning and transferability
properties of DNNs. While the experiments here use images, the proposed
approach of modelling two-class ensemble decision boundaries could in principle
be applied to any application area. Code for this paper implementing Walsh
Coefficient Examples of approximating artificial Boolean functions can be found
at https://doi.org/10.24433/CO.3695905.v1"
13943,"The continuous neural dy-
and improve the ability of the long-term prediction, instead      namics of Neural ODE determines that it can handle or incor-
of directly using ODE systems to model the entire network         porate time-series data, especially with non-uniform intervals,
dynamics process like NDCN, we model the network dynam-           which inspires further research greatly [21–23].","To mitigate or    ODE system as a fundamental component, and the output is
overcome the effect of the sparsity of observations on NDCN       calculated through the ODESolver.","Rubanova
ics in segments using the ODE equations in an autoregressive      et al.",2022-11-19 05:43:10+00:00,Autoregressive GNN-ODE GRU Model for Network Dynamics,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Bo Liang'), arxiv.Result.Author('Lin Wang'), arxiv.Result.Author('Xiaofan Wang')]","Revealing the continuous dynamics on the networks is essential for
understanding, predicting, and even controlling complex systems, but it is hard
to learn and model the continuous network dynamics because of complex and
unknown governing equations, high dimensions of complex systems, and
unsatisfactory observations. Moreover, in real cases, observed time-series data
are usually non-uniform and sparse, which also causes serious challenges. In
this paper, we propose an Autoregressive GNN-ODE GRU Model (AGOG) to learn and
capture the continuous network dynamics and realize predictions of node states
at an arbitrary time in a data-driven manner. The GNN module is used to model
complicated and nonlinear network dynamics. The hidden state of node states is
specified by the ODE system, and the augmented ODE system is utilized to map
the GNN into the continuous time domain. The hidden state is updated through
GRUCell by observations. As prior knowledge, the true observations at the same
timestamp are combined with the hidden states for the next prediction. We use
the autoregressive model to make a one-step ahead prediction based on
observation history. The prediction is achieved by solving an initial-value
problem for ODE. To verify the performance of our model, we visualize the
learned dynamics and test them in three tasks: interpolation reconstruction,
extrapolation prediction, and regular sequences prediction. The results
demonstrate that our model can capture the continuous dynamic process of
complex systems accurately and make precise predictions of node states with
minimal error. Our model can consistently outperform other baselines or achieve
comparable performance."
13950,"5.2 Recommendations for Future Research

The works introduced in the previous chapters naturally lead to various avenues for
further research.",Extensive results verify the effectiveness of CIFS on robustiﬁcation.,We mention several possible extensions in this section.,2022-11-19 11:46:49+00:00,Towards Adversarial Robustness of Deep Vision Algorithms,cs.LG,['cs.LG'],[arxiv.Result.Author('Hanshu Yan')],"Deep learning methods have achieved great success in solving computer vision
tasks, and they have been widely utilized in artificially intelligent systems
for image processing, analysis, and understanding. However, deep neural
networks have been shown to be vulnerable to adversarial perturbations in input
data. The security issues of deep neural networks have thus come to the fore.
It is imperative to study the adversarial robustness of deep vision algorithms
comprehensively. This talk focuses on the adversarial robustness of image
classification models and image denoisers. We will discuss the robustness of
deep vision algorithms from three perspectives: 1) robustness evaluation (we
propose the ObsAtk to evaluate the robustness of denoisers), 2) robustness
improvement (HAT, TisODE, and CIFS are developed to robustify vision models),
and 3) the connection between adversarial robustness and generalization
capability to new domains (we find that adversarially robust denoisers can deal
with unseen types of real-world noise)."
13951,"5.2 Recommendations for Future Research

The works introduced in the previous chapters naturally lead to various avenues for
further research.",Extensive results verify the effectiveness of CIFS on robustiﬁcation.,We mention several possible extensions in this section.,2022-11-19 11:46:49+00:00,Towards Adversarial Robustness of Deep Vision Algorithms,cs.LG,['cs.LG'],[arxiv.Result.Author('Hanshu Yan')],"Deep learning methods have achieved great success in solving computer vision
tasks, and they have been widely utilized in artificially intelligent systems
for image processing, analysis, and understanding. However, deep neural
networks have been shown to be vulnerable to adversarial perturbations in input
data. The security issues of deep neural networks have thus come to the fore.
It is imperative to study the adversarial robustness of deep vision algorithms
comprehensively. This talk focuses on the adversarial robustness of image
classification models and image denoisers. We will discuss the robustness of
deep vision algorithms from three perspectives: 1) robustness evaluation (we
propose the ObsAtk to evaluate the robustness of denoisers), 2) robustness
improvement (HAT, TisODE, and CIFS are developed to robustify vision models),
and 3) the connection between adversarial robustness and generalization
capability to new domains (we find that adversarially robust denoisers can deal
with unseen types of real-world noise)."
13959,"A method which could take into account a possible diﬀerence
between the feature vector domains for controls and treatments can be regarded as a direction for
further research.","It does not mean that they have to totally coincide, but the corresponding diﬀerence
of domains should not be very large.","An idea behind the method is to combine the domain adaptation models and
BENK.",2022-11-19 20:36:52+00:00,BENK: The Beran Estimator with Neural Kernels for Estimating the Heterogeneous Treatment Effect,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Stanislav R. Kirpichenko'), arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","A method for estimating the conditional average treatment effect under
condition of censored time-to-event data called BENK (the Beran Estimator with
Neural Kernels) is proposed. The main idea behind the method is to apply the
Beran estimator for estimating the survival functions of controls and
treatments. Instead of typical kernel functions in the Beran estimator, it is
proposed to implement kernels in the form of neural networks of a specific form
called the neural kernels. The conditional average treatment effect is
estimated by using the survival functions as outcomes of the control and
treatment neural networks which consists of a set of neural kernels with shared
parameters. The neural kernels are more flexible and can accurately model a
complex location structure of feature vectors. Various numerical simulation
experiments illustrate BENK and compare it with the well-known T-learner,
S-learner and X-learner for several types of the control and treatment outcome
functions based on the Cox models, the random survival forest and the
Nadaraya-Watson regression with Gaussian kernels. The code of proposed
algorithms implementing BENK is available in https://github.com/Stasychbr/BENK."
13960,"Another direction for further research is to study robust versions of BENK when there are
anomalous observations which may impact on training the neural network.","An idea behind the method is to combine the domain adaptation models and
BENK.","An idea behind the robust
version is to use attention weights for feature vectors, but also to introduce additional attention
weights for predictions.",2022-11-19 20:36:52+00:00,BENK: The Beran Estimator with Neural Kernels for Estimating the Heterogeneous Treatment Effect,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Stanislav R. Kirpichenko'), arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","A method for estimating the conditional average treatment effect under
condition of censored time-to-event data called BENK (the Beran Estimator with
Neural Kernels) is proposed. The main idea behind the method is to apply the
Beran estimator for estimating the survival functions of controls and
treatments. Instead of typical kernel functions in the Beran estimator, it is
proposed to implement kernels in the form of neural networks of a specific form
called the neural kernels. The conditional average treatment effect is
estimated by using the survival functions as outcomes of the control and
treatment neural networks which consists of a set of neural kernels with shared
parameters. The neural kernels are more flexible and can accurately model a
complex location structure of feature vectors. Various numerical simulation
experiments illustrate BENK and compare it with the well-known T-learner,
S-learner and X-learner for several types of the control and treatment outcome
functions based on the Cox models, the random survival forest and the
Nadaraya-Watson regression with Gaussian kernels. The code of proposed
algorithms implementing BENK is available in https://github.com/Stasychbr/BENK."
13961,"Therefore, the BENK implementations
and studies by using other estimators and deﬁnitions of CATE can be also considered as directions
for further research.","There are other deﬁnitions, for instance, the diﬀerence in SFs and the
hazard ratio, which may lead to more interesting models.","References

   [1] D. Hosmer, S. Lemeshow, and S. May.",2022-11-19 20:36:52+00:00,BENK: The Beran Estimator with Neural Kernels for Estimating the Heterogeneous Treatment Effect,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Author('Stanislav R. Kirpichenko'), arxiv.Result.Author('Lev V. Utkin'), arxiv.Result.Author('Andrei V. Konstantinov')]","A method for estimating the conditional average treatment effect under
condition of censored time-to-event data called BENK (the Beran Estimator with
Neural Kernels) is proposed. The main idea behind the method is to apply the
Beran estimator for estimating the survival functions of controls and
treatments. Instead of typical kernel functions in the Beran estimator, it is
proposed to implement kernels in the form of neural networks of a specific form
called the neural kernels. The conditional average treatment effect is
estimated by using the survival functions as outcomes of the control and
treatment neural networks which consists of a set of neural kernels with shared
parameters. The neural kernels are more flexible and can accurately model a
complex location structure of feature vectors. Various numerical simulation
experiments illustrate BENK and compare it with the well-known T-learner,
S-learner and X-learner for several types of the control and treatment outcome
functions based on the Cox models, the random survival forest and the
Nadaraya-Watson regression with Gaussian kernels. The code of proposed
algorithms implementing BENK is available in https://github.com/Stasychbr/BENK."
13970,"To further study the effect of the proposed method, we
provide the true positive ratio (TPR) of the selected pairs at the end of training with 20 runs.",Selected positive pairs (End of Training).,"As
shown in Table 5, the relatively large TPR and low variance suggests that the quality of the selected
positive pairs is good and the learning process is stable.",2022-11-20 07:18:56+00:00,Can Single-Pass Contrastive Learning Work for Both Homophilic and Heterophilic Graph?,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Haonan Wang'), arxiv.Result.Author('Jieyu Zhang'), arxiv.Result.Author('Qi Zhu'), arxiv.Result.Author('Wei Huang')]","Existing graph contrastive learning (GCL) typically requires two forward pass
for a single instance to construct the contrastive loss. Despite its remarkable
success, it is unclear whether such a dual-pass design is (theoretically)
necessary. Besides, the empirical results are hitherto limited to the
homophilic graph benchmarks. Then a natural question arises: Can we design a
method that works for both homophilic and heterophilic graphs with a
performance guarantee? To answer this, we analyze the concentration property of
features obtained by neighborhood aggregation on both homophilic and
heterophilic graphs, introduce the single-pass graph contrastive learning loss
based on the property, and provide performance guarantees of the minimizer of
the loss on downstream tasks. As a direct consequence of our analysis, we
implement the Single-Pass Graph Contrastive Learning method (SP-GCL).
Empirically, on 14 benchmark datasets with varying degrees of heterophily, the
features learned by the SP-GCL can match or outperform existing strong
baselines with significantly less computational overhead, which verifies the
usefulness of our findings in real-world cases."
13971,"SimPGCN-SAT          5.3±0.9   87.1±0.5  72.4±0.6  3.4±0.3
                     69.5±1.3                      82.0±1.4              Studying the robustness of GNNs is an important problem,
GCN                  66.5±1.3  NETTACK             77.0±1.4          and this work provides essential insights for further research.","Furthermore, the SAT approach is
GCN-SAT              86.9±0.7  86.2±0.2  73.2±0.4  86.4±0.5          generic to be employed on most GNN models without sacriﬁcing
GCN-SAT (α = β = 0)            77.4±0.9  67.9±1.5                    classiﬁcation power and training efﬁciency.","In
GCN-SAT              68.9±1.1                      79.9±1.2          the future, we plan to study tasks beyond node classiﬁcation and
GCN-SAT (α = β = 0)            6.8±0.8   6.3±1.2                     extend our research to other graph structures such as multiple
SimPGCN-SAT          7.0±0.8   70.3±1.5  67.1±1.5  4.6±0.5           and heterogeneous information graphs.",2022-11-20 07:56:55+00:00,Spectral Adversarial Training for Robust Graph Neural Network,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Jintang Li'), arxiv.Result.Author('Jiaying Peng'), arxiv.Result.Author('Liang Chen'), arxiv.Result.Author('Zibin Zheng'), arxiv.Result.Author('Tingting Liang'), arxiv.Result.Author('Qing Ling')]","Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency."
14010,"28
    Other order and further combination of these techniques and also their adaptive tuning were also
partly analysed, however, their comprehensive testing is outside of the scope of the current paper, more-
over, there is a signiﬁcant, further research potential in this direction as well.","The last graph at the bottom mirrors the algorithm accelerated by ’SuperSAB’ method with the
same initial τ1 as at the last ’Momentum’ (1).","The main aim here was
to make the described tests easier, more stable and quicker, so, the combined algorithm was applied, as
introduced in pseudo code 1.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
14011,"Diabetes), however, for higher τ values its
ranges seems to be similar, so it may have some advantages as well, but, this ﬁeld could be a challenge of
further research analysis.","As it is shown in ﬁgure 28, the ﬁnal MSE values of the dynamic τ algorithm did not reach better
results than the ﬁxed one in case of a regression datasets (e.g.","In this research ﬁeld Heravi & Hodtani (2018a,c,b, 2019) presented excellent research results analysing
noise and other features inherited in the training datasets to predict various properties of the resulted train-
ing models.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
14012,"Probably this is because the introduced EExpAbs measure
combines CE with MSE and CE has advances typically in classiﬁcation tasks, but this is only an assump-
tion that needs further research.","The proposed, novel dynamic training algorithm serves with various superiority behaviour but it is
important to note that these results are valid only for the classiﬁcation, but not for the regression assign-
ments, according to the various assessments.","54
12.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
14013,"To integrate more measures into a novel one could be a
       promising direction for further research.","Re´nyi Entropy) as it was
       introduced at the beginning of the paper.","Ideal would be having a ”super-measure” that integrates
       (almost) all of the beneﬁcial measures into one error ”calculus”.",2022-11-21 14:26:46+00:00,"Self-Adaptive, Dynamic, Integrated Statistical and Information Theory Learning",cs.LG,"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'stat.ML', '68T07, 62B10 (Primary) 62-08, 94-08 (Secondary)', 'I.5.1; I.2.6']","[arxiv.Result.Author('Zsolt János Viharos'), arxiv.Result.Author('Ágnes Szűcs')]","The paper analyses and serves with a positioning of various error measures
applied in neural network training and identifies that there is no best of
measure, although there is a set of measures with changing superiorities in
different learning situations. An outstanding, remarkable measure called
$E_{Exp}$ published by Silva and his research partners represents a research
direction to combine more measures successfully with fixed importance weighting
during learning. The main idea of the paper is to go far beyond and to
integrate this relative importance into the neural network training
algorithm(s) realized through a novel error measure called $E_{ExpAbs}$. This
approach is included into the Levenberg-Marquardt training algorithm, so, a
novel version of it is also introduced, resulting a self-adaptive, dynamic
learning algorithm. This dynamism does not has positive effects on the resulted
model accuracy only, but also on the training process itself. The described
comprehensive algorithm tests proved that the proposed, novel algorithm
integrates dynamically the two big worlds of statistics and information theory
that is the key novelty of the paper."
14021,"Moreover, further research is needed to understand how
performance depends on the amount of data collected — and how best to interleave training and
interaction.","Preliminary experiments on a question-answering task
(‘Name objects’), which may relate to our generally stronger improves from RL on instruction-following
than question-answering discussed in 6.6.",7.,2022-11-21 16:00:31+00:00,Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback,cs.LG,"['cs.LG', 'cs.HC', 'cs.MA']","[arxiv.Result.Author('Josh Abramson'), arxiv.Result.Author('Arun Ahuja'), arxiv.Result.Author('Federico Carnevale'), arxiv.Result.Author('Petko Georgiev'), arxiv.Result.Author('Alex Goldin'), arxiv.Result.Author('Alden Hung'), arxiv.Result.Author('Jessica Landon'), arxiv.Result.Author('Jirka Lhotka'), arxiv.Result.Author('Timothy Lillicrap'), arxiv.Result.Author('Alistair Muldal'), arxiv.Result.Author('George Powell'), arxiv.Result.Author('Adam Santoro'), arxiv.Result.Author('Guy Scully'), arxiv.Result.Author('Sanjana Srivastava'), arxiv.Result.Author('Tamara von Glehn'), arxiv.Result.Author('Greg Wayne'), arxiv.Result.Author('Nathaniel Wong'), arxiv.Result.Author('Chen Yan'), arxiv.Result.Author('Rui Zhu')]","An important goal in artificial intelligence is to create agents that can
both interact naturally with humans and learn from their feedback. Here we
demonstrate how to use reinforcement learning from human feedback (RLHF) to
improve upon simulated, embodied agents trained to a base level of competency
with imitation learning. First, we collected data of humans interacting with
agents in a simulated 3D world. We then asked annotators to record moments
where they believed that agents either progressed toward or regressed from
their human-instructed goal. Using this annotation data we leveraged a novel
method - which we call ""Inter-temporal Bradley-Terry"" (IBT) modelling - to
build a reward model that captures human judgments. Agents trained to optimise
rewards delivered from IBT reward models improved with respect to all of our
metrics, including subsequent human judgment during live interactions with
agents. Altogether our results demonstrate how one can successfully leverage
human judgments to improve agent behaviour, allowing us to use reinforcement
learning in complex, embodied domains without programmatic reward functions.
Videos of agent behaviour may be found at https://youtu.be/v_Z9F2_eKk4."
14044,"We hope our research can motivate further study and applications or even research in biological
reasoning.",This could further help us study and understand the performance of generative synthesis.,"Acknowledgments

We would like to thank DarwinAI Corporation, Vision and Image Processing Lab, and Saeejith
Muralidharan Nair (University of Waterloo) for preparing the computing resources and platforms.",2022-11-22 01:41:48+00:00,COVID-Net Assistant: A Deep Learning-Driven Virtual Assistant for COVID-19 Symptom Prediction and Recommendation,cs.LG,"['cs.LG', 'cs.SD', 'eess.AS']","[arxiv.Result.Author('Pengyuan Shi'), arxiv.Result.Author('Yuetong Wang'), arxiv.Result.Author('Saad Abbasi'), arxiv.Result.Author('Alexander Wong')]","As the COVID-19 pandemic continues to put a significant burden on healthcare
systems worldwide, there has been growing interest in finding inexpensive
symptom pre-screening and recommendation methods to assist in efficiently using
available medical resources such as PCR tests. In this study, we introduce the
design of COVID-Net Assistant, an efficient virtual assistant designed to
provide symptom prediction and recommendations for COVID-19 by analyzing users'
cough recordings through deep convolutional neural networks. We explore a
variety of highly customized, lightweight convolutional neural network
architectures generated via machine-driven design exploration (which we refer
to as COVID-Net Assistant neural networks) on the Covid19-Cough benchmark
dataset. The Covid19-Cough dataset comprises 682 cough recordings from a
COVID-19 positive cohort and 642 from a COVID-19 negative cohort. Among the 682
cough recordings labeled positive, 382 recordings were verified by PCR test.
Our experimental results show promising, with the COVID-Net Assistant neural
networks demonstrating robust predictive performance, achieving AUC scores of
over 0.93, with the best score over 0.95 while being fast and efficient in
inference. The COVID-Net Assistant models are made available in an open source
manner through the COVID-Net open initiative and, while not a production-ready
solution, we hope their availability acts as a good resource for clinical
scientists, machine learning researchers, as well as citizen scientists to
develop innovative solutions."
14069,"in the neural dependency, i.e., we require b = 0 if

   We further study the potential utilities of neural depen-       f (x)i ≈        k     θij f (x)ij  +  b.","dependency system (where y = Ax + b), we omit bias
It implies that a category involved in neural dependencies
only relates to several instead of numerous other categories.","Thus  the  existence  of  neural
dencies, as a support to our theoretical contributions.",2022-11-21 09:42:15+00:00,Neural Dependencies Emerging from Learning Massive Categories,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ruili Feng'), arxiv.Result.Author('Kecheng Zheng'), arxiv.Result.Author('Kai Zhu'), arxiv.Result.Author('Yujun Shen'), arxiv.Result.Author('Jian Zhao'), arxiv.Result.Author('Yukun Huang'), arxiv.Result.Author('Deli Zhao'), arxiv.Result.Author('Jingren Zhou'), arxiv.Result.Author('Michael Jordan'), arxiv.Result.Author('Zheng-Jun Zha')]","This work presents two astonishing findings on neural networks learned for
large-scale image classification. 1) Given a well-trained model, the logits
predicted for some category can be directly obtained by linearly combining the
predictions of a few other categories, which we call \textbf{neural
dependency}. 2) Neural dependencies exist not only within a single model, but
even between two independently learned models, regardless of their
architectures. Towards a theoretical analysis of such phenomena, we demonstrate
that identifying neural dependencies is equivalent to solving the Covariance
Lasso (CovLasso) regression problem proposed in this paper. Through
investigating the properties of the problem solution, we confirm that neural
dependency is guaranteed by a redundant logit covariance matrix, which
condition is easily met given massive categories, and that neural dependency is
highly sparse, implying that one category correlates to only a few others. We
further empirically show the potential of neural dependencies in understanding
internal data correlations, generalizing models to unseen categories, and
improving model robustness with a dependency-derived regularizer. Code for this
work will be made publicly available."
14087,"We thus note that these results provide preliminary insights on an ex-ante
impact assessment for the agricultural practices studied in the context of climate change and
enable the formulation of scientific hypotheses for further study.","In Figure 5.5 for example, a quadratic trend between the CR CATE and maximum tempera-
ture is seen, allowing us to hypothesize that in slightly warmer conditions the benefit of CR
would increase.","5.2 Causal ML: Land suitability                                225

 Figure 5.5 CR impact as a function of Max.",2022-11-22 20:58:54+00:00,Big Earth Data and Machine Learning for Sustainable and Resilient Agriculture,cs.LG,"['cs.LG', 'cs.AI']",[arxiv.Result.Author('Vasileios Sitokonstantinou')],"Big streams of Earth images from satellites or other platforms (e.g., drones
and mobile phones) are becoming increasingly available at low or no cost and
with enhanced spatial and temporal resolution. This thesis recognizes the
unprecedented opportunities offered by the high quality and open access Earth
observation data of our times and introduces novel machine learning and big
data methods to properly exploit them towards developing applications for
sustainable and resilient agriculture. The thesis addresses three distinct
thematic areas, i.e., the monitoring of the Common Agricultural Policy (CAP),
the monitoring of food security and applications for smart and resilient
agriculture. The methodological innovations of the developments related to the
three thematic areas address the following issues: i) the processing of big
Earth Observation (EO) data, ii) the scarcity of annotated data for machine
learning model training and iii) the gap between machine learning outputs and
actionable advice.
  This thesis demonstrated how big data technologies such as data cubes,
distributed learning, linked open data and semantic enrichment can be used to
exploit the data deluge and extract knowledge to address real user needs.
Furthermore, this thesis argues for the importance of semi-supervised and
unsupervised machine learning models that circumvent the ever-present challenge
of scarce annotations and thus allow for model generalization in space and
time. Specifically, it is shown how merely few ground truth data are needed to
generate high quality crop type maps and crop phenology estimations. Finally,
this thesis argues there is considerable distance in value between model
inferences and decision making in real-world scenarios and thereby showcases
the power of causal and interpretable machine learning in bridging this gap."
14092,"It demands
further study on the conﬁdence functions.","Our
experiments suggest that well calibrated conﬁdence scores are essential for the success of auto-labeling.","References

     [Air22] Airbus.",2022-11-22 22:53:17+00:00,Good Data from Bad Models : Foundations of Threshold-based Auto-labeling,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Harit Vishwakarma'), arxiv.Result.Author('Heguang Lin'), arxiv.Result.Author('Frederic Sala'), arxiv.Result.Author('Ramya Korlakai Vinayak')]","Creating large-scale high-quality labeled datasets is a major bottleneck in
supervised machine learning workflows. Auto-labeling systems are a promising
way to reduce reliance on manual labeling for dataset construction.
Threshold-based auto-labeling, where validation data obtained from humans is
used to find a threshold for confidence above which the data is
machine-labeled, is emerging as a popular solution used widely in practice.
Given the long shelf-life and diverse usage of the resulting datasets,
understanding when the data obtained by such auto-labeling systems can be
relied on is crucial. In this work, we analyze threshold-based auto-labeling
systems and derive sample complexity bounds on the amount of human-labeled
validation data required for guaranteeing the quality of machine-labeled data.
Our results provide two insights. First, reasonable chunks of the unlabeled
data can be automatically and accurately labeled by seemingly bad models.
Second, a hidden downside of threshold-based auto-labeling systems is
potentially prohibitive validation data usage. Together, these insights
describe the promise and pitfalls of using such systems. We validate our
theoretical guarantees with simulations and study the efficacy of
threshold-based auto-labeling on real datasets."
14109,"In Appendix F, we further study the effectiveness of EurNet on modeling an important
kind of multi-relational data without spatial information, i.e., knowledge graphs.","4
4 INSTANTIATIONS OF EURNET

In the main paper, we focus on two application domains, i.e., computer vision and protein science,
where modeling spatial multi-relational data (i.e., images and protein structures) can solve important
problems.","4.1 EURNET FOR IMAGE MODELING

4.1.1 RELATIONAL EDGES FOR SHORT-, MEDIUM- AND LONG-RANGE INTERACTIONS

Following previous practices (Dosovitskiy et al., 2020; Liu et al., 2021b), we split an image into
local patches and regard these patches as the node set V of our multi-relational graph.",2022-11-23 13:24:36+00:00,EurNet: Efficient Multi-Range Relational Modeling of Spatial Multi-Relational Data,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Minghao Xu'), arxiv.Result.Author('Yuanfan Guo'), arxiv.Result.Author('Yi Xu'), arxiv.Result.Author('Jian Tang'), arxiv.Result.Author('Xinlei Chen'), arxiv.Result.Author('Yuandong Tian')]","Modeling spatial relationship in the data remains critical across many
different tasks, such as image classification, semantic segmentation and
protein structure understanding. Previous works often use a unified solution
like relative positional encoding. However, there exists different kinds of
spatial relations, including short-range, medium-range and long-range
relations, and modeling them separately can better capture the focus of
different tasks on the multi-range relations (e.g., short-range relations can
be important in instance segmentation, while long-range relations should be
upweighted for semantic segmentation). In this work, we introduce the EurNet
for Efficient multi-range relational modeling. EurNet constructs the
multi-relational graph, where each type of edge corresponds to short-, medium-
or long-range spatial interactions. In the constructed graph, EurNet adopts a
novel modeling layer, called gated relational message passing (GRMP), to
propagate multi-relational information across the data. GRMP captures multiple
relations within the data with little extra computational cost. We study
EurNets in two important domains for image and protein structure modeling.
Extensive experiments on ImageNet classification, COCO object detection and
ADE20K semantic segmentation verify the gains of EurNet over the previous SoTA
FocalNet. On the EC and GO protein function prediction benchmarks, EurNet
consistently surpasses the previous SoTA GearNet. Our results demonstrate the
strength of EurNets on modeling spatial multi-relational data from various
domains. The implementations of EurNet for image modeling are available at
https://github.com/hirl-team/EurNet-Image . The implementations for other
applied domains/tasks will be released soon."
14117,"We
leave further research on it in the future work.","While gradient clipping is triggered only at the beginning of the
training, it is interesting that it has a great effect on the performance throughout the training.","4
7  256 experts                                                    10  256 experts

6  256 experts w/ SparseClip                                      8   256 experts w/ SparseClip

Gating Entropy5                                                   6
                                             # Gradient Clipping
                                                                  4

4                                                                 2

3  5K 10K 15K 20K                                                 0 0 5K 10K 15K 20K
                                                                                   Steps
   Steps

   (a) Training Entropy of Routing     (b) Number of Gradient Clipping Triggered

Figure 4: Visualization of SparseClip and vanilla gradient clipping on neural machine translation.",2022-11-23 17:58:51+00:00,TorchScale: Transformers at Scale,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Shuming Ma'), arxiv.Result.Author('Hongyu Wang'), arxiv.Result.Author('Shaohan Huang'), arxiv.Result.Author('Wenhui Wang'), arxiv.Result.Author('Zewen Chi'), arxiv.Result.Author('Li Dong'), arxiv.Result.Author('Alon Benhaim'), arxiv.Result.Author('Barun Patra'), arxiv.Result.Author('Vishrav Chaudhary'), arxiv.Result.Author('Xia Song'), arxiv.Result.Author('Furu Wei')]","Large Transformers have achieved state-of-the-art performance across many
tasks. Most open-source libraries on scaling Transformers focus on improving
training or inference with better parallelization. In this work, we present
TorchScale, an open-source toolkit that allows researchers and developers to
scale up Transformers efficiently and effectively. TorchScale has the
implementation of several modeling techniques, which can improve modeling
generality and capability, as well as training stability and efficiency.
Experimental results on language modeling and neural machine translation
demonstrate that TorchScale can successfully scale Transformers to different
sizes without tears. The library is available at https://aka.ms/torchscale."
14131,"SMOEs show that learning location-dependence is a powerful inductive bias for certain types of data,
and there are many avenues for further study.","However, tasks such as facial
recognition and surveillance have also historically shown beneﬁt from such improvements [91] and
SMOEs should therefore be used with care.","Two key areas of particular interest are to develop
improved implementations for ﬁne-grained, sparse routing; and to generalize SMOEs from operating
on grids to general graphs, which would enable them to be applied to many additional tasks.",2022-11-24 09:31:02+00:00,Spatial Mixture-of-Experts,cs.LG,['cs.LG'],"[arxiv.Result.Author('Nikoli Dryden'), arxiv.Result.Author('Torsten Hoefler')]","Many data have an underlying dependence on spatial location; it may be
weather on the Earth, a simulation on a mesh, or a registered image. Yet this
feature is rarely taken advantage of, and violates common assumptions made by
many neural network layers, such as translation equivariance. Further, many
works that do incorporate locality fail to capture fine-grained structure. To
address this, we introduce the Spatial Mixture-of-Experts (SMoE) layer, a
sparsely-gated layer that learns spatial structure in the input domain and
routes experts at a fine-grained level to utilize it. We also develop new
techniques to train SMoEs, including a self-supervised routing loss and damping
expert errors. Finally, we show strong results for SMoEs on numerous tasks, and
set new state-of-the-art results for medium-range weather prediction and
post-processing ensemble weather forecasts."
14152,"Rapid training of
scientiﬁc ML models is essential to accelerate scientiﬁc
discovery, and we hope that our work will stimulate
further research in this direction.","it learns the representation of each atom by modeling pair-      • As a demonstration of extreme acceleration, we train

wise interactions between atoms based on a number of spec-       the SchNet model on the Hydronet dataset (Choudhury

                                                              2
           Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry

et al., 2020a) in 92 minutes using 64 IPUs, as compared
to previously reported 2.7 days obtained with a single-
GPU setting (Bilbrey et al., 2020).","2 BACKGROUND: GNN-BASED MODEL
    FOR QUANTUM CHEMISTRY

A molecule can be represented mathematically as a graph,
denoted by G = (V, E), with vertex set V and edge set E
(Figure 2).",2022-11-25 01:30:18+00:00,Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry,cs.LG,"['cs.LG', 'cs.AR', 'physics.chem-ph']","[arxiv.Result.Author('Hatem Helal'), arxiv.Result.Author('Jesun Firoz'), arxiv.Result.Author('Jenna Bilbrey'), arxiv.Result.Author('Mario Michael Krell'), arxiv.Result.Author('Tom Murray'), arxiv.Result.Author('Ang Li'), arxiv.Result.Author('Sotiris Xantheas'), arxiv.Result.Author('Sutanay Choudhury')]","Molecular property calculations are the bedrock of chemical physics.
High-fidelity \textit{ab initio} modeling techniques for computing the
molecular properties can be prohibitively expensive, and motivate the
development of machine-learning models that make the same predictions more
efficiently. Training graph neural networks over large molecular databases
introduces unique computational challenges such as the need to process millions
of small graphs with variable size and support communication patterns that are
distinct from learning over large graphs such as social networks. This paper
demonstrates a novel hardware-software co-design approach to scale up the
training of graph neural networks for molecular property prediction. We
introduce an algorithm to coalesce the batches of molecular graphs into fixed
size packs to eliminate redundant computation and memory associated with
alternative padding techniques and improve throughput via minimizing
communication. We demonstrate the effectiveness of our co-design approach by
providing an implementation of a well-established molecular property prediction
model on the Graphcore Intelligence Processing Units (IPU). We evaluate the
training performance on multiple molecular graph databases with varying degrees
of graph counts, sizes and sparsity. We demonstrate that such a co-design
approach can reduce the training time of such molecular property prediction
models from days to less than two hours, opening new possibilities for
AI-driven scientific discovery."
14161,"• Appendix E: Description of the environments (E.1), and additional experiments including
           the study of convergence rate of OS-VI (E.2), effect of model error (E.3), and further study
           of OS-Dyna (E.4).","• Appendix D: Basic properties of the Varga operator (D.1) and the proofs of the theoretical
           results for OS-VI (PE) (D.2) and OS-VI (Control) (D.2).","• Appendix F: Extended related work, including detailed comparison of the convergence rate
           of OS-VI with VI, PI, and MPI (F.1) and other examples of matrix splitting in dynamic
           programming and RL (F.2).",2022-11-25 07:34:26+00:00,Operator Splitting Value Iteration,cs.LG,"['cs.LG', 'cs.AI', 'cs.SY', 'eess.SY', 'math.OC', 'stat.ML']","[arxiv.Result.Author('Amin Rakhsha'), arxiv.Result.Author('Andrew Wang'), arxiv.Result.Author('Mohammad Ghavamzadeh'), arxiv.Result.Author('Amir-massoud Farahmand')]","We introduce new planning and reinforcement learning algorithms for
discounted MDPs that utilize an approximate model of the environment to
accelerate the convergence of the value function. Inspired by the splitting
approach in numerical linear algebra, we introduce Operator Splitting Value
Iteration (OS-VI) for both Policy Evaluation and Control problems. OS-VI
achieves a much faster convergence rate when the model is accurate enough. We
also introduce a sample-based version of the algorithm called OS-Dyna. Unlike
the traditional Dyna architecture, OS-Dyna still converges to the correct value
function in presence of model approximation error."
14167,"(line 5, 9, 20, 22), and more complex calculating operator
                                                               can be considered in further research.","For simplicity, we
hevw are considered in DMPNN, and a communicative              still use an addition operator as the communicative kernel
function is further added in CMPNN.","Considering that the
3.2 Bidirectional Message Communication GNN                    molecular graphs are seen as directed graphs, the hidden
                                                               state of an edge should not rely on its reverse message which
In MPNN, one key step is to leverage the aggregated mes-       should be subtracted in each updating stage (line 12).",2022-11-25 11:53:23+00:00,Molecular Joint Representation Learning via Multi-modal Information,cs.LG,"['cs.LG', 'cs.AI', 'q-bio.BM']","[arxiv.Result.Author('Tianyu Wu'), arxiv.Result.Author('Yang Tang'), arxiv.Result.Author('Qiyu Sun'), arxiv.Result.Author('Luolin Xiong')]","In recent years, artificial intelligence has played an important role on
accelerating the whole process of drug discovery. Various of molecular
representation schemes of different modals (e.g. textual sequence or graph) are
developed. By digitally encoding them, different chemical information can be
learned through corresponding network structures. Molecular graphs and
Simplified Molecular Input Line Entry System (SMILES) are popular means for
molecular representation learning in current. Previous works have done attempts
by combining both of them to solve the problem of specific information loss in
single-modal representation on various tasks. To further fusing such
multi-modal imformation, the correspondence between learned chemical feature
from different representation should be considered. To realize this, we propose
a novel framework of molecular joint representation learning via Multi-Modal
information of SMILES and molecular Graphs, called MMSG. We improve the
self-attention mechanism by introducing bond level graph representation as
attention bias in Transformer to reinforce feature correspondence between
multi-modal information. We further propose a Bidirectional Message
Communication Graph Neural Network (BMC GNN) to strengthen the information flow
aggregated from graphs for further combination. Numerous experiments on public
property prediction datasets have demonstrated the effectiveness of our model."
14170,"Next to providing
an in-depth review of the literature related to this particular subject, it also discusses our current
understanding of this behavior and provides intriguing open questions and interesting directions
for further research.","We refer to these learners as badly behaved or ill-behaved
and, by association, we refer to their learning curves in a likewise manner.","To keep the survey self-contained, we spend some paragraphs on properly
deﬁning (idealized) learning curves.",2022-11-25 12:36:52+00:00,A Survey of Learning Curves with Bad Behavior: or How More Data Need Not Lead to Better Performance,cs.LG,"['cs.LG', 'stat.ME', 'stat.ML', 'A.1; I.5.0; I.2.6']","[arxiv.Result.Author('Marco Loog'), arxiv.Result.Author('Tom Viering')]","Plotting a learner's generalization performance against the training set size
results in a so-called learning curve. This tool, providing insight in the
behavior of the learner, is also practically valuable for model selection,
predicting the effect of more training data, and reducing the computational
complexity of training. We set out to make the (ideal) learning curve concept
precise and briefly discuss the aforementioned usages of such curves. The
larger part of this survey's focus, however, is on learning curves that show
that more data does not necessarily leads to better generalization performance.
A result that seems surprising to many researchers in the field of artificial
intelligence. We point out the significance of these findings and conclude our
survey with an overview and discussion of open problems in this area that
warrant further theoretical and empirical investigation."
14173,"In particular, one covers the use of successive events in accelerometer
data for the purpose of shot counting [4] and has likely hindered further research
into exploiting this signal.","FN Herstal owns several international patents on the topic
since 2006.","Loeﬄer [7] and Reese [11] limit themselves to low scale
and sampling frequency, are restricted to a few shots and do not make use of
machine learning.",2022-11-25 12:51:19+00:00,EDGAR: Embedded Detection of Gunshots by AI in Real-time,cs.LG,"['cs.LG', 'eess.SP']",[arxiv.Result.Author('Nathan Morsa')],"Electronic shot counters allow armourers to perform preventive and predictive
maintenance based on quantitative measurements, improving reliability, reducing
the frequency of accidents, and reducing maintenance costs. To answer a market
pressure for both low lead time to market and increased customisation, we aim
to solve the shot detection and shot counting problem in a generic way through
machine learning.
  In this study, we describe a method allowing one to construct a dataset with
minimal labelling effort by only requiring the total number of shots fired in a
time series. To our knowledge, this is the first study to propose a technique,
based on learning from label proportions, that is able to exploit these weak
labels to derive an instance-level classifier able to solve the counting
problem and the more general discrimination problem. We also show that this
technique can be deployed in heavily constrained microcontrollers while still
providing hard real-time (<100ms) inference. We evaluate our technique against
a state-of-the-art unsupervised algorithm and show a sizeable improvement,
suggesting that the information from the weak labels is successfully leveraged.
Finally, we evaluate our technique against human-generated state-of-the-art
algorithms and show that it provides comparable performance and significantly
outperforms them in some offline and real-world benchmarks."
14174,"We hope to bring to light these results
and the associate technique improvements in a further study.","It is already being deployed for
that purpose in commercial applications.","Acknowledgements I would like to thank my colleague Louis Huggenberger for
his continuous help in data acquisition.",2022-11-25 12:51:19+00:00,EDGAR: Embedded Detection of Gunshots by AI in Real-time,cs.LG,"['cs.LG', 'eess.SP']",[arxiv.Result.Author('Nathan Morsa')],"Electronic shot counters allow armourers to perform preventive and predictive
maintenance based on quantitative measurements, improving reliability, reducing
the frequency of accidents, and reducing maintenance costs. To answer a market
pressure for both low lead time to market and increased customisation, we aim
to solve the shot detection and shot counting problem in a generic way through
machine learning.
  In this study, we describe a method allowing one to construct a dataset with
minimal labelling effort by only requiring the total number of shots fired in a
time series. To our knowledge, this is the first study to propose a technique,
based on learning from label proportions, that is able to exploit these weak
labels to derive an instance-level classifier able to solve the counting
problem and the more general discrimination problem. We also show that this
technique can be deployed in heavily constrained microcontrollers while still
providing hard real-time (<100ms) inference. We evaluate our technique against
a state-of-the-art unsupervised algorithm and show a sizeable improvement,
suggesting that the information from the weak labels is successfully leveraged.
Finally, we evaluate our technique against human-generated state-of-the-art
algorithms and show that it provides comparable performance and significantly
outperforms them in some offline and real-world benchmarks."
14175,We conclude and discuss possible further research directions in Section 5.,Then we describe the TN model in Section 3 and present the evaluation of our approach in Section 4.,"2 Related work

                                        We can divide most PUL approaches into four categories [1]: two-step techniques, biased learning, class prior
                                        incorporation techniques, and generative adversarial network techniques.",2022-11-25 13:14:33+00:00,Positive unlabeled learning with tensor networks,cs.LG,"['cs.LG', 'cs.CV']",[arxiv.Result.Author('Bojan Žunkovič')],"Positive unlabeled learning is a binary classification problem with positive
and unlabeled data. It is common in domains where negative labels are costly or
impossible to obtain, e.g., medicine and personalized advertising. We apply the
locally purified state tensor network to the positive unlabeled learning
problem and test our model on the MNIST image and 15 categorical/mixed
datasets. On the MNIST dataset, we achieve state-of-the-art results even with
very few labeled positive samples. Similarly, we significantly improve the
state-of-the-art on categorical datasets. Further, we show that the agreement
fraction between outputs of different models on unlabeled samples is a good
indicator of the model's performance. Finally, our method can generate new
positive and negative instances, which we demonstrate on simple synthetic
datasets."
14178,"We hope that
the MIMIC task can help lay the groundwork for further research in this direction.","Finally, we emphasize the importance
of robust and interpretable AI, especially in healthcare, where human safety is at stake.","We refer readers
to [46] for an in-depth discussion of the ethical issues surrounding AI in healthcare.",2022-11-25 17:07:53+00:00,Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time,cs.LG,['cs.LG'],"[arxiv.Result.Author('Huaxiu Yao'), arxiv.Result.Author('Caroline Choi'), arxiv.Result.Author('Bochuan Cao'), arxiv.Result.Author('Yoonho Lee'), arxiv.Result.Author('Pang Wei Koh'), arxiv.Result.Author('Chelsea Finn')]","Distribution shift occurs when the test distribution differs from the
training distribution, and it can considerably degrade performance of machine
learning models deployed in the real world. Temporal shifts -- distribution
shifts arising from the passage of time -- often occur gradually and have the
additional structure of timestamp metadata. By leveraging timestamp metadata,
models can potentially learn from trends in past distribution shifts and
extrapolate into the future. While recent works have studied distribution
shifts, temporal shifts remain underexplored. To address this gap, we curate
Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts
arising in a variety of real-world applications, including patient prognosis
and news classification. On these datasets, we systematically benchmark 13
prior approaches, including methods in domain generalization, continual
learning, self-supervised learning, and ensemble learning. We use two
evaluation strategies: evaluation with a fixed time split (Eval-Fix) and
evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation
strategy, aims to provide a simple evaluation protocol, while Eval-Stream is
more realistic for certain real-world applications. Under both evaluation
strategies, we observe an average performance drop of 20% from in-distribution
to out-of-distribution data. Existing methods are unable to close this gap.
Code is available at https://wild-time.github.io/."
14200,"5.4 Effect of the Sparsity Level

We further study the effect of the sparsity level of the auto-encoder model on the performance of
WAST.","The
test accuracy is reported using K of 50 except on Madelon, where K = 20.","We evaluated 5 different sparsity levels {20%, 40%, 60%, 80%, 90%}.",2022-11-26 17:49:32+00:00,Where to Pay Attention in Sparse Training for Feature Selection?,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Ghada Sokar'), arxiv.Result.Author('Zahra Atashgahi'), arxiv.Result.Author('Mykola Pechenizkiy'), arxiv.Result.Author('Decebal Constantin Mocanu')]","A new line of research for feature selection based on neural networks has
recently emerged. Despite its superiority to classical methods, it requires
many training iterations to converge and detect informative features. The
computational time becomes prohibitively long for datasets with a large number
of samples or a very high dimensional feature space. In this paper, we present
a new efficient unsupervised method for feature selection based on sparse
autoencoders. In particular, we propose a new sparse training algorithm that
optimizes a model's sparse topology during training to pay attention to
informative features quickly. The attention-based adaptation of the sparse
topology enables fast detection of informative features after a few training
iterations. We performed extensive experiments on 10 datasets of different
types, including image, speech, text, artificial, and biological. They cover a
wide range of characteristics, such as low and high-dimensional feature spaces,
and few and large training samples. Our proposed approach outperforms the
state-of-the-art methods in terms of selecting informative features while
reducing training iterations and computational costs substantially. Moreover,
the experiments show the robustness of our method in extremely noisy
environments."
14223,A further study may conduct         ActiveLink or develop novel methods for vision tasks.,"While effective, ALBench is now only             tion in graphs, future research can be conducted to adapt
for object detection tasks.","to incorporate more computer vision tasks and the latest
DeepAL algorithms.",2022-11-27 13:07:14+00:00,Deep Active Learning for Computer Vision: Past and Future,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Rinyoichi Takezoe'), arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Shunan Mao'), arxiv.Result.Author('Marco Tianyu Chen'), arxiv.Result.Author('Zhanpeng Feng'), arxiv.Result.Author('Shiliang Zhang'), arxiv.Result.Author('Xiaoyu Wang')]","As an important data selection schema, active learning emerges as the
essential component when iterating an Artificial Intelligence (AI) model. It
becomes even more critical given the dominance of deep neural network based
models, which are composed of a large number of parameters and data hungry, in
application. Despite its indispensable role for developing AI models, research
on active learning is not as intensive as other research directions. In this
paper, we present a review of active learning through deep active learning
approaches from the following perspectives: 1) technical advancements in active
learning, 2) applications of active learning in computer vision, 3) industrial
systems leveraging or with potential to leverage active learning for data
iteration, 4) current limitations and future research directions. We expect
this paper to clarify the significance of active learning in a modern AI model
manufacturing process and to bring additional research attention to active
learning. By addressing data automation challenges and coping with automated
machine learning systems, active learning will facilitate democratization of AI
technologies by boosting model production at scale."
14224,A further study may conduct         ActiveLink or develop novel methods for vision tasks.,"While effective, ALBench is now only             tion in graphs, future research can be conducted to adapt
for object detection tasks.","to incorporate more computer vision tasks and the latest
DeepAL algorithms.",2022-11-27 13:07:14+00:00,Deep Active Learning for Computer Vision: Past and Future,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Rinyoichi Takezoe'), arxiv.Result.Author('Xu Liu'), arxiv.Result.Author('Shunan Mao'), arxiv.Result.Author('Marco Tianyu Chen'), arxiv.Result.Author('Zhanpeng Feng'), arxiv.Result.Author('Shiliang Zhang'), arxiv.Result.Author('Xiaoyu Wang')]","As an important data selection schema, active learning emerges as the
essential component when iterating an Artificial Intelligence (AI) model. It
becomes even more critical given the dominance of deep neural network based
models, which are composed of a large number of parameters and data hungry, in
application. Despite its indispensable role for developing AI models, research
on active learning is not as intensive as other research directions. In this
paper, we present a review of active learning through deep active learning
approaches from the following perspectives: 1) technical advancements in active
learning, 2) applications of active learning in computer vision, 3) industrial
systems leveraging or with potential to leverage active learning for data
iteration, 4) current limitations and future research directions. We expect
this paper to clarify the significance of active learning in a modern AI model
manufacturing process and to bring additional research attention to active
learning. By addressing data automation challenges and coping with automated
machine learning systems, active learning will facilitate democratization of AI
technologies by boosting model production at scale."
14241,"We then further split the parame-    dominant task, so further research is needed to ensure the
ters into four layers in the synthetic dataset.","Very few parameters in AdaTask are dominated by          of dominated task, it could damage the performance of the
task B (yellow area in (e)).",The results are   performance of the dominant task.,2022-11-28 04:24:38+00:00,AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning,cs.LG,"['cs.LG', 'cs.CV', 'cs.IR']","[arxiv.Result.Author('Enneng Yang'), arxiv.Result.Author('Junwei Pan'), arxiv.Result.Author('Ximei Wang'), arxiv.Result.Author('Haibin Yu'), arxiv.Result.Author('Li Shen'), arxiv.Result.Author('Xihua Chen'), arxiv.Result.Author('Lei Xiao'), arxiv.Result.Author('Jie Jiang'), arxiv.Result.Author('Guibing Guo')]","Multi-task learning (MTL) models have demonstrated impressive results in
computer vision, natural language processing, and recommender systems. Even
though many approaches have been proposed, how well these approaches balance
different tasks on each parameter still remains unclear. In this paper, we
propose to measure the task dominance degree of a parameter by the total
updates of each task on this parameter. Specifically, we compute the total
updates by the exponentially decaying Average of the squared Updates (AU) on a
parameter from the corresponding task.Based on this novel metric, we observe
that many parameters in existing MTL methods, especially those in the higher
shared layers, are still dominated by one or several tasks. The dominance of AU
is mainly due to the dominance of accumulative gradients from one or several
tasks. Motivated by this, we propose a Task-wise Adaptive learning rate
approach, AdaTask in short, to separate the \emph{accumulative gradients} and
hence the learning rate of each task for each parameter in adaptive learning
rate approaches (e.g., AdaGrad, RMSProp, and Adam). Comprehensive experiments
on computer vision and recommender system MTL datasets demonstrate that AdaTask
significantly improves the performance of dominated tasks, resulting SOTA
average task-wise performance. Analysis on both synthetic and real-world
datasets shows AdaTask balance parameters in every shared layer well."
14249,"ANEMONE [32], SL-GAD [33], and Sub-                             heterogeneous graph with encoders maximizing the mutual
CR [25] do further research on the basis of CoLA.","After
tween the node and contrastive subgraph attributes in positive                      two graph views are generated, HeCO [37] is trained on the
and negative pairs.",Multi-scale                       information of the same nodes in the views.,2022-11-28 12:17:40+00:00,GADMSL: Graph Anomaly Detection on Attributed Networks via Multi-scale Substructure Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Duan Jingcan'), arxiv.Result.Author('Wang Siwei'), arxiv.Result.Author('Liu Xinwang'), arxiv.Result.Author('Zhou Haifang'), arxiv.Result.Author('Hu Jingtao'), arxiv.Result.Author('Jin Hu')]","Recently, graph anomaly detection has attracted increasing attention in data
mining and machine learning communities. Apart from existing attribute
anomalies, graph anomaly detection also captures suspicious
topological-abnormal nodes that differ from the major counterparts. Although
massive graph-based detection approaches have been proposed, most of them focus
on node-level comparison while pay insufficient attention on the surrounding
topology structures. Nodes with more dissimilar neighborhood substructures have
more suspicious to be abnormal. To enhance the local substructure detection
ability, we propose a novel Graph Anomaly Detection framework via Multi-scale
Substructure Learning (GADMSL for abbreviation). Unlike previous algorithms, we
manage to capture anomalous substructures where the inner similarities are
relatively low in dense-connected regions. Specifically, we adopt a region
proposal module to find high-density substructures in the network as suspicious
regions. Their inner-node embedding similarities indicate the anomaly degree of
the detected substructures. Generally, a lower degree of embedding similarities
means a higher probability that the substructure contains topology anomalies.
To distill better embeddings of node attributes, we further introduce a graph
contrastive learning scheme, which observes attribute anomalies in the
meantime. In this way, GADMSL can detect both topology and attribute anomalies.
Ultimately, extensive experiments on benchmark datasets show that GADMSL
greatly improves detection performance (up to 7.30% AUC and 17.46% AUPRC gains)
compared to state-of-the-art attributed networks anomaly detection algorithms."
14250,"ANEMONE [32], SL-GAD [33], and Sub-                             heterogeneous graph with encoders maximizing the mutual
CR [25] do further research on the basis of CoLA.","After
tween the node and contrastive subgraph attributes in positive                      two graph views are generated, HeCO [37] is trained on the
and negative pairs.",Multi-scale                       information of the same nodes in the views.,2022-11-28 12:17:40+00:00,GADMSL: Graph Anomaly Detection on Attributed Networks via Multi-scale Substructure Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jingcan Duan'), arxiv.Result.Author('Siwei Wang'), arxiv.Result.Author('Xinwang Liu'), arxiv.Result.Author('Haifang Zhou'), arxiv.Result.Author('Jingtao Hu'), arxiv.Result.Author('Hu Jin')]","Recently, graph anomaly detection has attracted increasing attention in data
mining and machine learning communities. Apart from existing attribute
anomalies, graph anomaly detection also captures suspicious
topological-abnormal nodes that differ from the major counterparts. Although
massive graph-based detection approaches have been proposed, most of them focus
on node-level comparison while pay insufficient attention on the surrounding
topology structures. Nodes with more dissimilar neighborhood substructures have
more suspicious to be abnormal. To enhance the local substructure detection
ability, we propose a novel Graph Anomaly Detection framework via Multi-scale
Substructure Learning (GADMSL for abbreviation). Unlike previous algorithms, we
manage to capture anomalous substructures where the inner similarities are
relatively low in dense-connected regions. Specifically, we adopt a region
proposal module to find high-density substructures in the network as suspicious
regions. Their inner-node embedding similarities indicate the anomaly degree of
the detected substructures. Generally, a lower degree of embedding similarities
means a higher probability that the substructure contains topology anomalies.
To distill better embeddings of node attributes, we further introduce a graph
contrastive learning scheme, which observes attribute anomalies in the
meantime. In this way, GADMSL can detect both topology and attribute anomalies.
Ultimately, extensive experiments on benchmark datasets show that GADMSL
greatly improves detection performance (up to 7.30% AUC and 17.46% AUPRC gains)
compared to state-of-the-art attributed networks anomaly detection algorithms."
14260,These considerations bring us to two natural direction for further research.,"chaos,
the majority of ﬁne details are irrelevant and can be safely dropped leading to much lower
dimensions for the trained networks.","First, to learn
the Navier-Stokes PDE and generate ﬂuid ﬂows from new unseen initial conditions [26].",2022-11-24 13:21:36+00:00,Neural Network Complexity of Chaos and Turbulence,cs.LG,"['cs.LG', 'hep-th', 'nlin.CD', 'physics.flu-dyn']","[arxiv.Result.Author('Tim Whittaker'), arxiv.Result.Author('Romuald A. Janik'), arxiv.Result.Author('Yaron Oz')]","We study the complexity of chaos and turbulence as viewed by deep neural
networks by considering network classification tasks of distinguishing
turbulent from chaotic fluid flows, noise and real world images of cats or
dogs. We analyze the relative difficulty of these classification tasks and
quantify the complexity of the computation at the intermediate and final
stages. We analyze incompressible as well as weakly compressible fluid flows
and provide evidence for the feature identified by the neural network to
distinguish turbulence from chaos."
14274,"For this reason, we plan to conduct further research on        Jonnavittula, A.; and Losey, D. P. 2021.",IEEE.,"I know what
how the agent can use these methods to self-assess its per-       you meant: learning human objectives by (under) estimat-
formance in more complex environments, including but not          ing their choice set.",2022-11-28 16:48:24+00:00,Autonomous Assessment of Demonstration Sufficiency via Bayesian Inverse Reinforcement Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tu Trinh'), arxiv.Result.Author('Daniel S. Brown')]","In this paper we examine the problem of determining demonstration sufficiency
for AI agents that learn from demonstrations: how can an AI agent self-assess
whether it has received enough demonstrations from an expert to ensure a
desired level of performance? To address this problem we propose a novel
self-assessment approach based on Bayesian inverse reinforcement learning and
value-at-risk to enable agents that learn from demonstrations to compute
high-confidence bounds on their performance and use these bounds to determine
when they have a sufficient number of demonstrations. We propose and evaluate
two definitions of sufficiency: (1) normalized expected value difference, which
measures regret with respect to the expert's unobserved reward function, and
(2) improvement over a baseline policy. We demonstrate how to formulate
high-confidence bounds on both of these metrics. We evaluate our approach in
simulation and demonstrate the feasibility of developing an AI system that can
accurately evaluate whether it has received sufficient training data to
guarantee, with high confidence, that it can match an expert's performance or
surpass the performance of a baseline policy within some desired safety
threshold."
14275,"As such, further research can help identify
                                                                                                                    the network topology for which the proposed reconstruction
                                                                                                                    method is best suited.","In the last case study
                                                                                                                    we demonstrate that the performance of reconstructing the
                                                                                                                    network topology based on the cascading failure data varies
                                                                                                                    from network to network and heavily relies on the topology
                                                                                                                    of the networks.","0.775                                                                        30000                                  REFERENCES

0.750                                                                              Reconstructing Time (s)          [1] B. D. Fath, U. M. Scharler, R. E. Ulanowicz, and B. Hannon,
                                                                      25000                                               “Ecological network analysis: Network construction,” Ecological
                                                                                                                          Modelling, vol.",2022-11-28 17:45:41+00:00,A Bayesian Approach to Reconstructing Interdependent Infrastructure Networks from Cascading Failures,cs.LG,"['cs.LG', 'cs.SI']","[arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Jin-Zhu Yu'), arxiv.Result.Author('Hiba Baroud')]","Analyzing the behavior of complex interdependent networks requires complete
information about the network topology and the interdependent links across
networks. For many applications such as critical infrastructure systems,
understanding network interdependencies is crucial to anticipate cascading
failures and plan for disruptions. However, data on the topology of individual
networks are often publicly unavailable due to privacy and security concerns.
Additionally, interdependent links are often only revealed in the aftermath of
a disruption as a result of cascading failures. We propose a scalable
nonparametric Bayesian approach to reconstruct the topology of interdependent
infrastructure networks from observations of cascading failures.
Metropolis-Hastings algorithm coupled with the infrastructure-dependent
proposal are employed to increase the efficiency of sampling possible graphs.
Results of reconstructing a synthetic system of interdependent infrastructure
networks demonstrate that the proposed approach outperforms existing methods in
both accuracy and computational time. We further apply this approach to
reconstruct the topology of one synthetic and two real-world systems of
interdependent infrastructure networks, including gas-power-water networks in
Shelby County, TN, USA, and an interdependent system of power-water networks in
Italy, to demonstrate the general applicability of the approach."
14288,"(2022) proposed FasterMoE, a system for distributed
interesting problem for further study, the overall impact          training of MoEs based on efﬁcient communication strate-
on model performance is minimal because of the limited             gies and changes to the MoE routing algorithm to avoid
opportunity for improvement (<10%) combined with the               network congestion.",While this is an          et al.,"Our implementation could additionally
relatively small amount of end-to-end runtime that these           beneﬁt from these techniques, particularly for large-scale
two operations represent.",2022-11-29 00:27:08+00:00,MegaBlocks: Efficient Sparse Training with Mixture-of-Experts,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Trevor Gale'), arxiv.Result.Author('Deepak Narayanan'), arxiv.Result.Author('Cliff Young'), arxiv.Result.Author('Matei Zaharia')]","We present MegaBlocks, a system for efficient Mixture-of-Experts (MoE)
training on GPUs. Our system is motivated by the limitations of current
frameworks, which restrict the dynamic routing in MoE layers to satisfy the
constraints of existing software and hardware. These formulations force a
tradeoff between model quality and hardware efficiency, as users must choose
between dropping tokens from the computation or wasting computation and memory
on padding. To address these limitations, we reformulate MoE computation in
terms of block-sparse operations and develop new block-sparse GPU kernels that
efficiently handle the dynamism present in MoEs. Our approach never drops
tokens and maps efficiently to modern hardware, enabling end-to-end training
speedups of up to 40% over MoEs trained with the state-of-the-art Tutel library
and 2.4x over DNNs trained with the highly-optimized Megatron-LM framework."
14308,"The SE-state sta1:i is
                                           We further study extensive research problems regarding ACE,       deﬁned as the state st in the original MMDP along with the
                                           including extension, generalization and practicability.","states sta1 , ..., sta1:n , named SE-state.",Code is   decisions a1:i made by the preceding agents.,2022-11-29 10:22:55+00:00,ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency,cs.LG,"['cs.LG', 'cs.MA']","[arxiv.Result.Author('Chuming Li'), arxiv.Result.Author('Jie Liu'), arxiv.Result.Author('Yinmin Zhang'), arxiv.Result.Author('Yuhong Wei'), arxiv.Result.Author('Yazhe Niu'), arxiv.Result.Author('Yaodong Yang'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Wanli Ouyang')]","Multi-agent reinforcement learning (MARL) suffers from the non-stationarity
problem, which is the ever-changing targets at every iteration when multiple
agents update their policies at the same time. Starting from first principle,
in this paper, we manage to solve the non-stationarity problem by proposing
bidirectional action-dependent Q-learning (ACE). Central to the development of
ACE is the sequential decision-making process wherein only one agent is allowed
to take action at one time. Within this process, each agent maximizes its value
function given the actions taken by the preceding agents at the inference
stage. In the learning phase, each agent minimizes the TD error that is
dependent on how the subsequent agents have reacted to their chosen action.
Given the design of bidirectional dependency, ACE effectively turns a
multiagent MDP into a single-agent MDP. We implement the ACE framework by
identifying the proper network representation to formulate the action
dependency, so that the sequential decision process is computed implicitly in
one forward pass. To validate ACE, we compare it with strong baselines on two
MARL benchmarks. Empirical experiments demonstrate that ACE outperforms the
state-of-the-art algorithms on Google Research Football and StarCraft
Multi-Agent Challenge by a large margin. In particular, on SMAC tasks, ACE
achieves 100% success rate on almost all the hard and super-hard maps. We
further study extensive research problems regarding ACE, including extension,
generalization, and practicability. Code is made available to facilitate
further research."
14309,"Only one agent
                                           made available to facilitate further research.",Code is   decisions a1:i made by the preceding agents.,makes a decision at each SE-state.,2022-11-29 10:22:55+00:00,ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency,cs.LG,"['cs.LG', 'cs.MA']","[arxiv.Result.Author('Chuming Li'), arxiv.Result.Author('Jie Liu'), arxiv.Result.Author('Yinmin Zhang'), arxiv.Result.Author('Yuhong Wei'), arxiv.Result.Author('Yazhe Niu'), arxiv.Result.Author('Yaodong Yang'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Wanli Ouyang')]","Multi-agent reinforcement learning (MARL) suffers from the non-stationarity
problem, which is the ever-changing targets at every iteration when multiple
agents update their policies at the same time. Starting from first principle,
in this paper, we manage to solve the non-stationarity problem by proposing
bidirectional action-dependent Q-learning (ACE). Central to the development of
ACE is the sequential decision-making process wherein only one agent is allowed
to take action at one time. Within this process, each agent maximizes its value
function given the actions taken by the preceding agents at the inference
stage. In the learning phase, each agent minimizes the TD error that is
dependent on how the subsequent agents have reacted to their chosen action.
Given the design of bidirectional dependency, ACE effectively turns a
multiagent MDP into a single-agent MDP. We implement the ACE framework by
identifying the proper network representation to formulate the action
dependency, so that the sequential decision process is computed implicitly in
one forward pass. To validate ACE, we compare it with strong baselines on two
MARL benchmarks. Empirical experiments demonstrate that ACE outperforms the
state-of-the-art algorithms on Google Research Football and StarCraft
Multi-Agent Challenge by a large margin. In particular, on SMAC tasks, ACE
achieves 100% success rate on almost all the hard and super-hard maps. We
further study extensive research problems regarding ACE, including extension,
generalization, and practicability. Code is made available to facilitate
further research."
14310,"The SE-state sta1:i is
                                          We further study extensive research problems regarding ACE,       deﬁned as the state st in the original MMDP along with the
                                          including extension, generalization and practicability.","states sta1 , ..., sta1:n , named SE-state.",Code is   decisions a1:i made by the preceding agents.,2022-11-29 10:22:55+00:00,ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency,cs.LG,"['cs.LG', 'cs.MA']","[arxiv.Result.Author('Chuming Li'), arxiv.Result.Author('Jie Liu'), arxiv.Result.Author('Yinmin Zhang'), arxiv.Result.Author('Yuhong Wei'), arxiv.Result.Author('Yazhe Niu'), arxiv.Result.Author('Yaodong Yang'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Wanli Ouyang')]","Multi-agent reinforcement learning (MARL) suffers from the non-stationarity
problem, which is the ever-changing targets at every iteration when multiple
agents update their policies at the same time. Starting from first principle,
in this paper, we manage to solve the non-stationarity problem by proposing
bidirectional action-dependent Q-learning (ACE). Central to the development of
ACE is the sequential decision-making process wherein only one agent is allowed
to take action at one time. Within this process, each agent maximizes its value
function given the actions taken by the preceding agents at the inference
stage. In the learning phase, each agent minimizes the TD error that is
dependent on how the subsequent agents have reacted to their chosen action.
Given the design of bidirectional dependency, ACE effectively turns a
multiagent MDP into a single-agent MDP. We implement the ACE framework by
identifying the proper network representation to formulate the action
dependency, so that the sequential decision process is computed implicitly in
one forward pass. To validate ACE, we compare it with strong baselines on two
MARL benchmarks. Empirical experiments demonstrate that ACE outperforms the
state-of-the-art algorithms on Google Research Football and StarCraft
Multi-Agent Challenge by a large margin. In particular, on SMAC tasks, ACE
achieves 100% success rate on almost all the hard and super-hard maps. We
further study extensive research problems regarding ACE, including extension,
generalization, and practicability. Code is made available to facilitate
further research."
14311,"Only one agent
                                          made available to facilitate further research.",Code is   decisions a1:i made by the preceding agents.,makes a decision at each SE-state.,2022-11-29 10:22:55+00:00,ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency,cs.LG,"['cs.LG', 'cs.MA']","[arxiv.Result.Author('Chuming Li'), arxiv.Result.Author('Jie Liu'), arxiv.Result.Author('Yinmin Zhang'), arxiv.Result.Author('Yuhong Wei'), arxiv.Result.Author('Yazhe Niu'), arxiv.Result.Author('Yaodong Yang'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Wanli Ouyang')]","Multi-agent reinforcement learning (MARL) suffers from the non-stationarity
problem, which is the ever-changing targets at every iteration when multiple
agents update their policies at the same time. Starting from first principle,
in this paper, we manage to solve the non-stationarity problem by proposing
bidirectional action-dependent Q-learning (ACE). Central to the development of
ACE is the sequential decision-making process wherein only one agent is allowed
to take action at one time. Within this process, each agent maximizes its value
function given the actions taken by the preceding agents at the inference
stage. In the learning phase, each agent minimizes the TD error that is
dependent on how the subsequent agents have reacted to their chosen action.
Given the design of bidirectional dependency, ACE effectively turns a
multiagent MDP into a single-agent MDP. We implement the ACE framework by
identifying the proper network representation to formulate the action
dependency, so that the sequential decision process is computed implicitly in
one forward pass. To validate ACE, we compare it with strong baselines on two
MARL benchmarks. Empirical experiments demonstrate that ACE outperforms the
state-of-the-art algorithms on Google Research Football and StarCraft
Multi-Agent Challenge by a large margin. In particular, on SMAC tasks, ACE
achieves 100% success rate on almost all the hard and super-hard maps. We
further study extensive research problems regarding ACE, including extension,
generalization, and practicability. Code is made available to facilitate
further research."
14315,"Reversal patterns state
     • Presenting a dataset of technical analysis reports in the cryp-     a change in the direction of the market and continuation patterns
        tocurrencies market that enables further research in this          state that the trend remains the same.","Patterns can be categorized into two
                                                                           groups of reversal and continuation patterns.","In this work, most popular
        field.",2022-11-29 11:26:08+00:00,Text Representation Enrichment Utilizing Graph based Approaches: Stock Market Technical Analysis Case Study,cs.LG,"['cs.LG', 'q-fin.CP']","[arxiv.Result.Author('Sara Salamat'), arxiv.Result.Author('Nima Tavassoli'), arxiv.Result.Author('Behnam Sabeti'), arxiv.Result.Author('Reza Fahmi')]","Graph neural networks (GNNs) have been utilized for various natural language
processing (NLP) tasks lately. The ability to encode corpus-wide features in
graph representation made GNN models popular in various tasks such as document
classification. One major shortcoming of such models is that they mainly work
on homogeneous graphs, while representing text datasets as graphs requires
several node types which leads to a heterogeneous schema. In this paper, we
propose a transductive hybrid approach composed of an unsupervised node
representation learning model followed by a node classification/edge prediction
model. The proposed model is capable of processing heterogeneous graphs to
produce unified node embeddings which are then utilized for node classification
or link prediction as the downstream task. The proposed model is developed to
classify stock market technical analysis reports, which to our knowledge is the
first work in this domain. Experiments, which are carried away using a
constructed dataset, demonstrate the ability of the model in embedding
extraction and the downstream tasks."
14325,"We further study the
utility of user models by repeating the task with a misspeci-
ﬁed Menu Search model that models ∼ 35% of the saccades
incorrectly.","Our approach enables
modeling in < 10ms, corresponding to over eight orders of
magnitude smaller computation time.","We repeat the meta-training at different per-
centages of true user data; the results are shown in Figure 2.",2022-11-29 15:09:18+00:00,Differentiable User Models,cs.LG,"['cs.LG', 'cs.AI', 'cs.HC']","[arxiv.Result.Author('Alex Hämäläinen'), arxiv.Result.Author('Mustafa Mert Çelikok'), arxiv.Result.Author('Samuel Kaski')]","Probabilistic user modeling is essential for building collaborative AI
systems within probabilistic frameworks. However, modern advanced user models,
often designed as cognitive behavior simulators, are computationally
prohibitive for interactive use in cooperative AI assistants. In this extended
abstract, we address this problem by introducing widely-applicable
differentiable surrogates for bypassing this computational bottleneck; the
surrogates enable using modern behavioral models with online computational cost
which is independent of their original computational cost. We show
experimentally that modeling capabilities comparable to likelihood-free
inference methods are achievable, with over eight orders of magnitude reduction
in computational time. Finally, we demonstrate how AI-assistants can
computationally feasibly use cognitive models in a previously studied
menu-search task."
14369,"Furthermore, we                   predictions of the base models as the features to the meta
                                        openly share the source code of the proposed method to facilitate           machine learning model, however, the diversity of the base
                                        further research and comparison.","When using the
                                        used in the well-known data competitions.","predictions should be satisﬁed as otherwise the correlated
                                                                                                                    features might hinder (as always hinder as we show in our
                                           Index Terms—ensemble learning, regression, time series, mul-             simulations) the learning process of the meta learner [14].",2022-11-30 10:36:13+00:00,Context-Aware Ensemble Learning for Time Series,cs.LG,['cs.LG'],"[arxiv.Result.Author('Arda Fazla'), arxiv.Result.Author('Mustafa Enes Aydin'), arxiv.Result.Author('Orhun Tamyigit'), arxiv.Result.Author('Suleyman Serdar Kozat')]","We investigate ensemble methods for prediction in an online setting. Unlike
all the literature in ensembling, for the first time, we introduce a new
approach using a meta learner that effectively combines the base model
predictions via using a superset of the features that is the union of the base
models' feature vectors instead of the predictions themselves. Here, our model
does not use the predictions of the base models as inputs to a machine learning
algorithm, but choose the best possible combination at each time step based on
the state of the problem. We explore three different constraint spaces for the
ensembling of the base learners that linearly combines the base predictions,
which are convex combinations where the components of the ensembling vector are
all nonnegative and sum up to 1; affine combinations where the weight vector
components are required to sum up to 1; and the unconstrained combinations
where the components are free to take any real value. The constraints are both
theoretically analyzed under known statistics and integrated into the learning
procedure of the meta learner as a part of the optimization in an automated
manner. To show the practical efficiency of the proposed method, we employ a
gradient-boosted decision tree and a multi-layer perceptron separately as the
meta learners. Our framework is generic so that one can use other machine
learning architectures as the ensembler as long as they allow for a custom
differentiable loss for minimization. We demonstrate the learning behavior of
our algorithm on synthetic data and the significant performance improvements
over the conventional methods over various real life datasets, extensively used
in the well-known data competitions. Furthermore, we openly share the source
code of the proposed method to facilitate further research and comparison."
14387,"This
raises interesting questions for further research.","Our results suggest that ML models ground their predictions
on diﬀerent information than identiﬁed in the literature and through statistical analysis.","Funding

    This work was partly funded by the Austrian Research Promotion Agency (FFG) and the
BMDW within the COIN-program (#866855 and #866880), the Lower Austrian Research and
Education Company (NFB), the Provincial Government of Lower Austria (#FTI17-014).",2022-10-16 13:57:09+00:00,Explaining automated gender classification of human gait,cs.LG,['cs.LG'],"[arxiv.Result.Author('Fabian Horst'), arxiv.Result.Author('Djordje Slijepcevic'), arxiv.Result.Author('Matthias Zeppelzauer'), arxiv.Result.Author('Anna-Maria Raberger'), arxiv.Result.Author('Sebastian Lapuschkin'), arxiv.Result.Author('Wojciech Samek'), arxiv.Result.Author('Wolfgang I. Schöllhorn'), arxiv.Result.Author('Christian Breiteneder'), arxiv.Result.Author('Brian Horsak')]","State-of-the-art machine learning (ML) models are highly effective in
classifying gait analysis data, however, they lack in providing explanations
for their predictions. This ""black-box"" characteristic makes it impossible to
understand on which input patterns, ML models base their predictions. The
present study investigates whether Explainable Artificial Intelligence methods,
i.e., Layer-wise Relevance Propagation (LRP), can be useful to enhance the
explainability of ML predictions in gait classification. The research question
was: Which input patterns are most relevant for an automated gender
classification model and do they correspond to characteristics identified in
the literature? We utilized a subset of the GAITREC dataset containing five
bilateral ground reaction force (GRF) recordings per person during barefoot
walking of 62 healthy participants: 34 females and 28 males. Each input signal
(right and left side) was min-max normalized before concatenation and fed into
a multi-layer Convolutional Neural Network (CNN). The classification accuracy
was obtained over a stratified ten-fold cross-validation. To identify
gender-specific patterns, the input relevance scores were derived using LRP.
The mean classification accuracy of the CNN with 83.3% showed a clear
superiority over the zero-rule baseline of 54.8%."
14388,"MoCvated by this, the work presented here provides precursory informaCon that
will inform planned further research aiming to improve ramp event forecasCng
accuracy.","Cost reducCons will play an essen-
Cal role in improving the compeCCveness of wind energy against non-renewable
energy sources, enabling a global transiCon to cleaner fuels, and facilitaCng cli-
mate change miCgaCon (Figure 4).",6.,2022-08-31 10:40:35+00:00,Integrating wind variability to modelling wind-ramp events using a non-binary ramp function and deep learning models,cs.LG,['cs.LG'],"[arxiv.Result.Author('Russell Sharp'), arxiv.Result.Author('Hisham Ihshaish'), arxiv.Result.Author('J. Ignacio Deza')]","The forecasting of large ramps in wind power output known as ramp events is
crucial for the incorporation of large volumes of wind energy into national
electricity grids. Large variations in wind power supply must be compensated by
ancillary energy sources which can include the use of fossil fuels. Improved
prediction of wind power will help to reduce dependency on supplemental energy
sources along with their associated costs and emissions. In this paper, we
discuss limitations of current predictive practices and explore the use of
Machine Learning methods to enhance wind ramp event classification and
prediction. We additionally outline a design for a novel approach to wind ramp
prediction, in which high-resolution wind fields are incorporated to the
modelling of wind power."
14389,"Section 5 is devoted to a brief discussion of our outcome
and Section 6 summarizes the results and outlines further research directions.","Therefore we modify our
approach in Section 4.","2 Previous Work

Obviously it is not acceptable and is disastrous for the domain of cluster-
ing algorithms if an axiomatic system consisting of ”natural axioms” is self-
contradictory.",2022-11-30 14:31:04+00:00,High-Dimensional Wide Gap $k$-Means Versus Clustering Axioms,cs.LG,"['cs.LG', 'stat.ML']",[arxiv.Result.Author('Mieczysław A. Kłopotek')],"Kleinberg's axioms for distance based clustering proved to be contradictory.
  Various efforts have been made to overcome this problem.
  Here we make an attempt to handle the issue by embedding in high-dimensional
space and granting wide gaps between clusters."
14391,"Fi-
NB-201     0.8230   0.8590   0.6870   0.7180   0.0002            nally, we open-source1 these new benchmarks to facilitate
NB-301     -0.2100  -0.1900  -0.3050  -0.3360  0.5778            further research on generalizable neural predictors.","We show that GEN-
                                                                 NAPE can obtain over 0.78 NDCG on all three families.","OFA-PN     0.0857   -0.0219  0.6765   0.7309   0.6886
OFA-MBv3   0.6477   0.0352   0.6018   0.6488   0.6141                                 Related Work
OFA-RN     0.7243   -0.0935  0.6507   0.7524   0.7850
                                                                 Neural predictors rely on benchmark datasets, namely NAS-
HiAML      0.1540   -0.0100  -0.1020  -0.1680  0.2767            Bench-101 (Ying et al.",2022-11-30 18:27:41+00:00,GENNAPE: Towards Generalized Neural Architecture Performance Estimators,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Keith G. Mills'), arxiv.Result.Author('Fred X. Han'), arxiv.Result.Author('Jialin Zhang'), arxiv.Result.Author('Fabian Chudak'), arxiv.Result.Author('Ali Safari Mamaghani'), arxiv.Result.Author('Mohammad Salameh'), arxiv.Result.Author('Wei Lu'), arxiv.Result.Author('Shangling Jui'), arxiv.Result.Author('Di Niu')]","Predicting neural architecture performance is a challenging task and is
crucial to neural architecture design and search. Existing approaches either
rely on neural performance predictors which are limited to modeling
architectures in a predefined design space involving specific sets of operators
and connection rules, and cannot generalize to unseen architectures, or resort
to zero-cost proxies which are not always accurate. In this paper, we propose
GENNAPE, a Generalized Neural Architecture Performance Estimator, which is
pretrained on open neural architecture benchmarks, and aims to generalize to
completely unseen architectures through combined innovations in network
representation, contrastive pretraining, and fuzzy clustering-based predictor
ensemble. Specifically, GENNAPE represents a given neural network as a
Computation Graph (CG) of atomic operations which can model an arbitrary
architecture. It first learns a graph encoder via Contrastive Learning to
encourage network separation by topological features, and then trains multiple
predictor heads, which are soft-aggregated according to the fuzzy membership of
a neural network. Experiments show that GENNAPE pretrained on NAS-Bench-101 can
achieve superior transferability to 5 different public neural network
benchmarks, including NAS-Bench-201, NAS-Bench-301, MobileNet and ResNet
families under no or minimum fine-tuning. We further introduce 3 challenging
newly labelled neural network benchmarks: HiAML, Inception and Two-Path, which
can concentrate in narrow accuracy ranges. Extensive experiments show that
GENNAPE can correctly discern high-performance architectures in these families.
Finally, when paired with a search algorithm, GENNAPE can find architectures
that improve accuracy while reducing FLOPs on three families."
14396,"Finally, the chapter will conclude with a
future works section, in which the possibilities for further research will be discussed.","After the discussion of the
results, the conclusions of this project will be given.","7.1 Discussion of Results

For all experiments, the reward as compared to the baseline was plotted.",2022-11-29 11:15:50+00:00,Autotuning PID control using Actor-Critic Deep Reinforcement Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.RO']",[arxiv.Result.Author('Vivien van Veldhuizen')],"This work is an exploratory research concerned with determining in what way
reinforcement learning can be used to predict optimal PID parameters for a
robot designed for apple harvest. To study this, an algorithm called Advantage
Actor Critic (A2C) is implemented on a simulated robot arm. The simulation
primarily relies on the ROS framework. Experiments for tuning one actuator at a
time and two actuators a a time are run, which both show that the model is able
to predict PID gains that perform better than the set baseline. In addition, it
is studied if the model is able to predict PID parameters based on where an
apple is located. Initial tests show that the model is indeed able to adapt its
predictions to apple locations, making it an adaptive controller."
14416,"In order to figure out the reason
the other participant holds the remaining 90% of features, the ASR      why our attacks’ performance drops on CIFAR10, we further study
of nearly 50% is still a severe threat to real-world business.","Moreover, for the binary classification task, where  is over 50%, our attacks all fail.",the role of dropout.,2022-12-01 07:12:38+00:00,Hijack Vertical Federated Learning Models with Adversarial Embedding,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR']","[arxiv.Result.Author('Pengyu Qiu'), arxiv.Result.Author('Xuhong Zhang'), arxiv.Result.Author('Shouling Ji'), arxiv.Result.Author('Changjiang Li'), arxiv.Result.Author('Yuwen Pu'), arxiv.Result.Author('Xing Yang'), arxiv.Result.Author('Ting Wang')]","Vertical federated learning (VFL) is an emerging paradigm that enables
collaborators to build machine learning models together in a distributed
fashion. In general, these parties have a group of users in common but own
different features. Existing VFL frameworks use cryptographic techniques to
provide data privacy and security guarantees, leading to a line of works
studying computing efficiency and fast implementation. However, the security of
VFL's model remains underexplored."
14459,"Interestingly, on context dependent tasks such as SORT, SELECT, MIPS, CONTEXTSHIFT and
SOLVE even the deeper DLR model fails, demonstrating that ATTENTION is not subsumed by a
deeper DLR stack and that further research is required to alleviate the shortcomings of SSMs on
context dependent tasks.","After repeating the experiments with a
6-layer DLR model we indeed ﬁnd almost perfect results on tasks such as REVERSE which require a
large number of (context independent) kernels.","This is inline with [MGCN22] who reported a signiﬁcant reduction in the
perplexity of their SSM after sparingly interleaving in chunked attention layers, therefore suggesting
that SSMs and attention offer complementary beneﬁts.",2022-12-01 18:53:06+00:00,Simplifying and Understanding State Space Models with Diagonal Linear RNNs,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Ankit Gupta'), arxiv.Result.Author('Harsh Mehta'), arxiv.Result.Author('Jonathan Berant')]","Sequence models based on linear state spaces (SSMs) have recently emerged as
a promising choice of architecture for modeling long range dependencies across
various modalities. However, they invariably rely on discretization of a
continuous state space, which complicates their presentation and understanding.
In this work, we dispose of the discretization step, and propose a model based
on vanilla Diagonal Linear RNNs ($\mathrm{DLR}$). We empirically show that
$\mathrm{DLR}$ is as performant as previously-proposed SSMs in the presence of
strong supervision, despite being conceptually much simpler. Moreover, we
characterize the expressivity of SSMs (including $\mathrm{DLR}$) and
attention-based models via a suite of $13$ synthetic sequence-to-sequence tasks
involving interactions over tens of thousands of tokens, ranging from simple
operations, such as shifting an input sequence, to detecting co-dependent
visual features over long spatial ranges in flattened images. We find that
while SSMs report near-perfect performance on tasks that can be modeled via
$\textit{few}$ convolutional kernels, they struggle on tasks requiring
$\textit{many}$ such kernels and especially when the desired sequence
manipulation is $\textit{context-dependent}$. For example, $\mathrm{DLR}$
learns to perfectly shift a $0.5M$-long input by an arbitrary number of
positions but fails when the shift size depends on context. Despite these
limitations, $\mathrm{DLR}$ reaches high performance on two higher-order
reasoning tasks $\mathrm{ListOpsSubTrees}$ and
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{256}$ with input lengths $8K$
and $65K$ respectively, and gives encouraging performance on
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{512}$ with input length $262K$
for which attention is not a viable choice."
14460,"Interestingly, on context dependent tasks such as SORT, SELECT, MIPS, CONTEXTSHIFT and
SOLVE even the deeper DLR model fails, demonstrating that ATTENTION is not subsumed by a
deeper DLR stack and that further research is required to alleviate the shortcomings of SSMs on
context dependent tasks.","After repeating the experiments with a
6-layer DLR model we indeed ﬁnd almost perfect results on tasks such as REVERSE which require a
large number of (context independent) kernels.","This is inline with [MGCN22] who reported a signiﬁcant reduction in the
perplexity of their SSM after sparingly interleaving in chunked attention layers, therefore suggesting
that SSMs and attention offer complementary beneﬁts.",2022-12-01 18:53:06+00:00,Simplifying and Understanding State Space Models with Diagonal Linear RNNs,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Ankit Gupta'), arxiv.Result.Author('Harsh Mehta'), arxiv.Result.Author('Jonathan Berant')]","Sequence models based on linear state spaces (SSMs) have recently emerged as
a promising choice of architecture for modeling long range dependencies across
various modalities. However, they invariably rely on discretization of a
continuous state space, which complicates their presentation and understanding.
In this work, we dispose of the discretization step, and propose a model based
on vanilla Diagonal Linear RNNs ($\mathrm{DLR}$). We empirically show that
$\mathrm{DLR}$ is as performant as previously-proposed SSMs in the presence of
strong supervision, despite being conceptually much simpler. Moreover, we
characterize the expressivity of SSMs (including $\mathrm{DLR}$) and
attention-based models via a suite of $13$ synthetic sequence-to-sequence tasks
involving interactions over tens of thousands of tokens, ranging from simple
operations, such as shifting an input sequence, to detecting co-dependent
visual features over long spatial ranges in flattened images. We find that
while SSMs report near-perfect performance on tasks that can be modeled via
$\textit{few}$ convolutional kernels, they struggle on tasks requiring
$\textit{many}$ such kernels and especially when the desired sequence
manipulation is $\textit{context-dependent}$. For example, $\mathrm{DLR}$
learns to perfectly shift a $0.5M$-long input by an arbitrary number of
positions but fails when the shift size depends on context. Despite these
limitations, $\mathrm{DLR}$ reaches high performance on two higher-order
reasoning tasks $\mathrm{ListOpsSubTrees}$ and
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{256}$ with input lengths $8K$
and $65K$ respectively, and gives encouraging performance on
$\mathrm{PathfinderSegmentation}\text{-}\mathrm{512}$ with input length $262K$
for which attention is not a viable choice."
14466,"ing to share data over the cloud, thus enabling privacy preserving    Still, these works are in their early stages, and we believe that a
                                       computation by design; (2) models can be refined on devices to        comprehensive view of what is done and what is left, can catalyze
                                       provide personalized services and cope with model drift in order      further research ahead.","data locally to preserve privacy, researchers have recently started
                                       On-device training, however, is attracting more and more interest     to propose schemes to train ML models on-device, despite their
                                       because: (1) it enables training models on local data without need-   computational resource limitations [2, 10, 28, 32, 34, 36, 43, 45].","Unlike previous work that target to analyze
                                       to adapt to the changes of the real-world environment; and (3) it     the research space on an algorithmic perspective [7], we take a
                                       enables the deployment of models in remote, hardly accessible loca-   systematic view, and present a survey on systems and frameworks
                                       tions or places without stable internet connectivity.",2022-12-01 19:22:29+00:00,On-device Training: A First Overview on Existing Systems,cs.LG,['cs.LG'],"[arxiv.Result.Author('Shuai Zhu'), arxiv.Result.Author('Thiemo Voigt'), arxiv.Result.Author('JeongGil Ko'), arxiv.Result.Author('Fatemeh Rahimian')]","The recent breakthroughs in machine learning (ML) and deep learning (DL) have
enabled many new capabilities across plenty of application domains. While most
existing machine learning models require large memory and computing power,
efforts have been made to deploy some models on resource-constrained devices as
well. There are several systems that perform inference on the device, while
direct training on the device still remains a challenge. On-device training,
however, is attracting more and more interest because: (1) it enables training
models on local data without needing to share data over the cloud, thus
enabling privacy preserving computation by design; (2) models can be refined on
devices to provide personalized services and cope with model drift in order to
adapt to the changes of the real-world environment; and (3) it enables the
deployment of models in remote, hardly accessible locations or places without
stable internet connectivity. We summarize and analyze the-state-of-art systems
research to provide the first survey of on-device training from a systems
perspective."
14479,"Subsequent work will further study the combination of
sequence analysis and DCSA.","A stable sequence analysis method
is an important prerequisite for the application of the DCSA.","In fact, if we don’t have prior knowledge, we can identify patterns through the periodic
upward trend in the DCSA sequence chart.",2022-12-02 06:20:04+00:00,Clustering through Feature Space Sequence Discovery and Analysis,cs.LG,"['cs.LG', '62H30', 'I.5.3']",[arxiv.Result.Author('Shi Guobin')],"Identifying high-dimensional data patterns without a priori knowledge is an
important task of data science. This paper proposes a simple and efficient
noparametric algorithm: Data Convert to Sequence Analysis, DCSA, which
dynamically explore each point in the feature space without repetition, and a
Directed Hamilton Path will be found. Based on the change point analysis
theory, The sequence corresponding to the path is cut into several fragments to
achieve clustering. The experiments on real-world datasets from different
fields with dimensions ranging from 4 to 20531 confirm that the method in this
work is robust and has visual interpretability in result analysis."
14481,"2.4Speedup ratio
                        2.0                                                                                                                  B. Micro Benchmark
                        1.6              5.5
                        1.2                     3.3                                                                                             In this subsection, we further study where the performance
                        0.8                            2.8                                                                                   gain of AGO comes from, to evaluate our intensive fusion and
                        0.4                                                                                                                  reformer layer.","Moreover, it is much shorter than
                                                                                                                                             weeks or even months of hand-tuning.","Then, we will evaluate our graph partitioning
                        0.0                                            130.8                                                                 algorithm, by respectively inspecting the generated subgraphs
                                                                              121.3                                                          partitioned by our algorithm and Relay [5].",2022-12-02 07:16:49+00:00,AGO: Boosting Mobile AI Inference Performance by Removing Constraints on Graph Optimization,cs.LG,"['cs.LG', 'cs.CL', 'cs.DC']","[arxiv.Result.Author('Zhiying Xu'), arxiv.Result.Author('Hongding Peng'), arxiv.Result.Author('Wei Wang')]","Traditional deep learning compilers rely on heuristics for subgraph
generation, which impose extra constraints on graph optimization, e.g., each
subgraph can only contain at most one complex operator. In this paper, we
propose AGO, a framework for graph optimization with arbitrary structures to
boost the inference performance of deep models by removing such constraints. To
create new optimization opportunities for complicated subgraphs, we propose
intensive operator fusion, which can effectively stitch multiple complex
operators together for better performance. Further, we design a graph
partitioning scheme that allows an arbitrary structure for each subgraph while
guaranteeing the acyclic property among all generated subgraphs. Additionally,
to enable efficient performance tuning on complicated subgraphs, we devise a
novel divide-and-conquer tuning mechanism to orchestrate different system
components. Through extensive experiments on various neural networks and mobile
devices, we show that our system can improve the inference performance by up to
3.3x when compared with state-of-the-art deep compilers."
14489,"If the algorithm fails to    In this work, we have not tuned α, an important parameter
update the best model√for 10 consecutive epochs, we multiply    of sampling procedure and hypernetwork, which need to be
the learning rate by 2 and terminate the program early if       further study to good incorporate with Partitioning algorithm.","0.2 CE 0L.o4ss task L0e.6ft

   Text Classiﬁcation and Regression: We train all methods      Figure 9: Impact of partition algorithm in different datasets
using an Adam optimizer with learning rate 1e−3 for maxi-
mal 400 epochs and batch size 256.",35 consecutive epochs.,2022-12-02 12:19:12+00:00,Improving Pareto Front Learning via Multi-Sample Hypernetworks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Long Phi Hoang'), arxiv.Result.Author('Dung Duy Le'), arxiv.Result.Author('Tuan Anh Tran'), arxiv.Result.Author('Thang Tran Ngoc')]","Pareto Front Learning (PFL) was recently introduced as an effective approach
to obtain a mapping function from a given trade-off vector to a solution on the
Pareto front, which solves the multi-objective optimization (MOO) problem. Due
to the inherent trade-off between conflicting objectives, PFL offers a flexible
approach in many scenarios in which the decision makers can not specify the
preference of one Pareto solution over another, and must switch between them
depending on the situation. However, existing PFL methods ignore the
relationship between the solutions during the optimization process, which
hinders the quality of the obtained front. To overcome this issue, we propose a
novel PFL framework namely \ourmodel, which employs a hypernetwork to generate
multiple solutions from a set of diverse trade-off preferences and enhance the
quality of the Pareto front by maximizing the Hypervolume indicator defined by
these solutions. The experimental results on several MOO machine learning tasks
show that the proposed framework significantly outperforms the baselines in
producing the trade-off Pareto front."
14502,"We hope that this surprising result will lead into further research
aiming to understand the interplay between the manifold hypothesis and DGMs.","In spite of being strongly motivated, both of our proposed procedures do not obtain consistent
improvements over simply using full-dimensional models, unlike some of the aforementioned more
involved manifold-aware models.","2 Background

2.1 Likelihood-based DGMs and Tweedie’s formula

Throughout this work we will assume that we have access to samples from a distribution p(x) in RD.",2022-11-30 19:00:00+00:00,Denoising Deep Generative Models,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Gabriel Loaiza-Ganem'), arxiv.Result.Author('Brendan Leigh Ross'), arxiv.Result.Author('Luhuan Wu'), arxiv.Result.Author('John P. Cunningham'), arxiv.Result.Author('Jesse C. Cresswell'), arxiv.Result.Author('Anthony L. Caterini')]","Likelihood-based deep generative models have recently been shown to exhibit
pathological behaviour under the manifold hypothesis as a consequence of using
high-dimensional densities to model data with low-dimensional structure. In
this paper we propose two methodologies aimed at addressing this problem. Both
are based on adding Gaussian noise to the data to remove the dimensionality
mismatch during training, and both provide a denoising mechanism whose goal is
to sample from the model as though no noise had been added to the data. Our
first approach is based on Tweedie's formula, and the second on models which
take the variance of added noise as a conditional input. We show that
surprisingly, while well motivated, these approaches only sporadically improve
performance over not adding noise, and that other methods of addressing the
dimensionality mismatch are more empirically adequate."
14503,"We hope that this surprising result will lead into further research
aiming to understand the interplay between the manifold hypothesis and DGMs.","In spite of being strongly motivated, both of our proposed procedures do not obtain consistent
improvements over simply using full-dimensional models, unlike some of the aforementioned more
involved manifold-aware models.","2 Background

2.1 Likelihood-based DGMs and Tweedie’s formula

Throughout this work we will assume that we have access to samples from a distribution p(x) in RD.",2022-11-30 19:00:00+00:00,Denoising Deep Generative Models,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Gabriel Loaiza-Ganem'), arxiv.Result.Author('Brendan Leigh Ross'), arxiv.Result.Author('Luhuan Wu'), arxiv.Result.Author('John P. Cunningham'), arxiv.Result.Author('Jesse C. Cresswell'), arxiv.Result.Author('Anthony L. Caterini')]","Likelihood-based deep generative models have recently been shown to exhibit
pathological behaviour under the manifold hypothesis as a consequence of using
high-dimensional densities to model data with low-dimensional structure. In
this paper we propose two methodologies aimed at addressing this problem. Both
are based on adding Gaussian noise to the data to remove the dimensionality
mismatch during training, and both provide a denoising mechanism whose goal is
to sample from the model as though no noise had been added to the data. Our
first approach is based on Tweedie's formula, and the second on models which
take the variance of added noise as a conditional input. We show that
surprisingly, while well motivated, these approaches only sporadically improve
performance over not adding noise, and that other methods of addressing the
dimensionality mismatch are more empirically adequate."
14505,"We also assume that Θ is             derstanding of algorithm design and spur further research on
compact and m1 + m2 ≥ nz, where nz is the dimension                this problem.","Given
orientation condition (Luo, Pang, and Ralph 1996), such that       the importance of this class of functions in the practical and
                                                                   theoretical arena, we expect our results to advance the un-
both κLP and κQP are bounded.","of the variables in (4); this assumption is easy to be relaxed

with a slightly more complicated (but not necessarily more

insightful) bound, thus we make the restriction to streamline

the presentation.",2022-12-02 17:16:04+00:00,On Solution Functions of Optimization: Universal Approximation and Covering Number Bounds,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Ming Jin'), arxiv.Result.Author('Vanshaj Khattar'), arxiv.Result.Author('Harshal Kaushik'), arxiv.Result.Author('Bilgehan Sel'), arxiv.Result.Author('Ruoxi Jia')]","We study the expressibility and learnability of convex optimization solution
functions and their multi-layer architectural extension. The main results are:
\emph{(1)} the class of solution functions of linear programming (LP) and
quadratic programming (QP) is a universal approximant for the $C^k$ smooth
model class or some restricted Sobolev space, and we characterize the
rate-distortion, \emph{(2)} the approximation power is investigated through a
viewpoint of regression error, where information about the target function is
provided in terms of data observations, \emph{(3)} compositionality in the form
of a deep architecture with optimization as a layer is shown to reconstruct
some basic functions used in numerical analysis without error, which implies
that \emph{(4)} a substantial reduction in rate-distortion can be achieved with
a universal network architecture, and \emph{(5)} we discuss the statistical
bounds of empirical covering numbers for LP/QP, as well as a generic
optimization problem (possibly nonconvex) by exploiting tame geometry. Our
results provide the \emph{first rigorous analysis of the approximation and
learning-theoretic properties of solution functions} with implications for
algorithmic design and performance guarantees."
14518,"While our empirical analysis raises the possibility that this guides reasonable allocations
of large compute budgets, further research is required to determine whether this is really the case.","Our analysis is based on an error upper
bound.","Furthermore, our analysis restricts attention to single-hidden-layer feedforward neural networks.",2022-12-02 18:46:41+00:00,An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws,cs.LG,['cs.LG'],"[arxiv.Result.Author('Hong Jun Jeon'), arxiv.Result.Author('Benjamin Van Roy')]","We study the compute-optimal trade-off between model and training data set
sizes for large neural networks. Our result suggests a linear relation similar
to that supported by the empirical analysis of Chinchilla. While that work
studies transformer-based large language models trained on the MassiveText
corpus (gopher), as a starting point for development of a mathematical theory,
we focus on a simpler learning model and data generating process, each based on
a neural network with a sigmoidal output unit and single hidden layer of ReLU
activation units. We establish an upper bound on the minimal
information-theoretically achievable expected error as a function of model and
data set sizes. We then derive allocations of computation that minimize this
bound. We present empirical results which suggest that this approximation
correctly identifies an asymptotic linear compute-optimal scaling. This
approximation can also generate new insights. Among other things, it suggests
that, as the input space dimension or latent space complexity grows, as might
be the case for example if a longer history of tokens is taken as input to a
language model, a larger fraction of the compute budget should be allocated to
growing the learning model rather than training data set."
14558,"• We theoretically and empirically demonstrate the effect of different design parameters on
           the accuracy, how it varies across different tasks, datasets, and network architectures, and
           provide preliminary insights and motivation for further study.","BEL outperforms direct regression for all
           the problems and specialized approaches for several tasks.","2 RELATED WORK

Binary classiﬁcation for regression: Prior works have proposed binary classiﬁcation-based ap-
proaches for ordinal regression (Crammer & Singer, 2001; Chu & Keerthi, 2005; Li & Lin, 2006).",2022-12-04 21:23:36+00:00,Label Encoding for Regression Networks,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Deval Shah'), arxiv.Result.Author('Zi Yu Xue'), arxiv.Result.Author('Tor M. Aamodt')]","Deep neural networks are used for a wide range of regression problems.
However, there exists a significant gap in accuracy between specialized
approaches and generic direct regression in which a network is trained by
minimizing the squared or absolute error of output labels. Prior work has shown
that solving a regression problem with a set of binary classifiers can improve
accuracy by utilizing well-studied binary classification algorithms. We
introduce binary-encoded labels (BEL), which generalizes the application of
binary classification to regression by providing a framework for considering
arbitrary multi-bit values when encoding target values. We identify desirable
properties of suitable encoding and decoding functions used for the conversion
between real-valued and binary-encoded labels based on theoretical and
empirical study. These properties highlight a tradeoff between classification
error probability and error-correction capabilities of label encodings. BEL can
be combined with off-the-shelf task-specific feature extractors and trained
end-to-end. We propose a series of sample encoding, decoding, and training loss
functions for BEL and demonstrate they result in lower error than direct
regression and specialized approaches while being suitable for a diverse set of
regression problems, network architectures, and evaluation metrics. BEL
achieves state-of-the-art accuracies for several regression benchmarks. Code is
available at https://github.com/ubc-aamodt-group/BEL_regression."
14577,"In 9th International Conference on Learning Represen-
our further research, we plan to pursue more optimal meth-               tations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
ods for aligning and guiding the local learning algorithms,              OpenReview.net, 2021.","Federated learning based on dynamic regulariza-
modiﬁcation to the established FedAvg method; however, in                tion.","1, 2
e.g.",2022-12-05 11:56:35+00:00,Partial Variance Reduction improves Non-Convex Federated learning on heterogeneous data,cs.LG,"['cs.LG', 'cs.DC']","[arxiv.Result.Author('Bo Li'), arxiv.Result.Author('Mikkel N. Schmidt'), arxiv.Result.Author('Tommy S. Alstrøm'), arxiv.Result.Author('Sebastian U. Stich')]","Data heterogeneity across clients is a key challenge in federated learning.
Prior works address this by either aligning client and server models or using
control variates to correct client model drift. Although these methods achieve
fast convergence in convex or simple non-convex problems, the performance in
over-parameterized models such as deep neural networks is lacking. In this
paper, we first revisit the widely used FedAvg algorithm in a deep neural
network to understand how data heterogeneity influences the gradient updates
across the neural network layers. We observe that while the feature extraction
layers are learned efficiently by FedAvg, the substantial diversity of the
final classification layers across clients impedes the performance. Motivated
by this, we propose to correct model drift by variance reduction only on the
final layers. We demonstrate that this significantly outperforms existing
benchmarks at a similar or lower communication cost. We furthermore provide
proof for the convergence rate of our algorithm."
14589,"In
Appendix A.6 a further study about the time parameter is included.","However, when t is larger than
zero, the best accuracy will be obtained with a speciﬁc value of time t > 0 on each benchmark.","Cora                  Citeseer                                     Pubmed

Figure 4: Comparison of accuracy of three benchmarks by changing the time parameter.",2022-12-05 18:42:55+00:00,TIDE: Time Derivative Diffusion for Deep Learning on Graphs,cs.LG,"['cs.LG', 'cs.CV', 'cs.SI']","[arxiv.Result.Author('Maximilian Krahn'), arxiv.Result.Author('Maysam Behmanesh'), arxiv.Result.Author('Maks Ovsjanikov')]","A prominent paradigm for graph neural networks is based on the message
passing framework. In this framework, information communication is realized
only between neighboring nodes. The challenge of approaches that use this
paradigm is to ensure efficient and accurate \textit{long distance
communication} between nodes, as deep convolutional networks are prone to
over-smoothing. In this paper, we present a novel method based on time
derivative graph diffusion (TIDE), with a learnable time parameter. Our
approach allows to adapt the spatial extent of diffusion across different tasks
and network channels, thus enabling medium and long-distance communication
efficiently. Furthermore, we show that our architecture directly enables local
message passing and thus inherits from the expressive power of local message
passing approaches. We show that on widely used graph benchmarks we achieve
comparable performance and on a synthetic mesh dataset we outperform
state-of-the-art methods like GCN or GRAND by a significant margin."
14592,"Furthermore, we identify typical application scenarios for synthetic data to assess the current
                                                  state of adoption and thereby unveil missed opportunities to pave the way for further research.","Therefore, we
                                                  present a taxonomy highlighting the various facets of deploying synthetic data for advanced analytics
                                                  systems.","Keywords Synthetic Data · Taxonomy · Advanced Analytics · Deep Learning · Cluster Analysis

                                       1 Introduction

                                       In the last decade, advanced approaches to the analysis and exploitation of large amounts of heterogeneous data (“big
                                       data”) have gained tremendous attention, particularly on the part of corporate decision-makers but also from academic
                                       researchers [1, 2, 3].",2022-12-05 22:13:58+00:00,Towards a Taxonomy for the Use of Synthetic Data in Advanced Analytics,cs.LG,"['cs.LG', 'cs.AI', 'A.1; H.0; H.4; I.2.1']","[arxiv.Result.Author('Peter Kowalczyk'), arxiv.Result.Author('Giacomo Welsch'), arxiv.Result.Author('Frédéric Thiesse')]","The proliferation of deep learning techniques led to a wide range of advanced
analytics applications in important business areas such as predictive
maintenance or product recommendation. However, as the effectiveness of
advanced analytics naturally depends on the availability of sufficient data, an
organization's ability to exploit the benefits might be restricted by limited
data or likewise data access. These challenges could force organizations to
spend substantial amounts of money on data, accept constrained analytics
capacities, or even turn into a showstopper for analytics projects. Against
this backdrop, recent advances in deep learning to generate synthetic data may
help to overcome these barriers. Despite its great potential, however,
synthetic data are rarely employed. Therefore, we present a taxonomy
highlighting the various facets of deploying synthetic data for advanced
analytics systems. Furthermore, we identify typical application scenarios for
synthetic data to assess the current state of adoption and thereby unveil
missed opportunities to pave the way for further research."
14593,"Nevertheless, the present paper may open avenues for further research regarding the use of synthetic data in advanced
analytics.","To this end, we consistently scrutinized our results qualitatively to ensure coherence and
applicability.","First, it outlines the rather unexplored ﬁelds of application for synthetic data which might be very appealing to

                                                                     12
Symthetic Data in Advanced Analytics

discover (e.g., public service, security, agriculture, commerce, as well as media and graphical).",2022-12-05 22:13:58+00:00,Towards a Taxonomy for the Use of Synthetic Data in Advanced Analytics,cs.LG,"['cs.LG', 'cs.AI', 'A.1; H.0; H.4; I.2.1']","[arxiv.Result.Author('Peter Kowalczyk'), arxiv.Result.Author('Giacomo Welsch'), arxiv.Result.Author('Frédéric Thiesse')]","The proliferation of deep learning techniques led to a wide range of advanced
analytics applications in important business areas such as predictive
maintenance or product recommendation. However, as the effectiveness of
advanced analytics naturally depends on the availability of sufficient data, an
organization's ability to exploit the benefits might be restricted by limited
data or likewise data access. These challenges could force organizations to
spend substantial amounts of money on data, accept constrained analytics
capacities, or even turn into a showstopper for analytics projects. Against
this backdrop, recent advances in deep learning to generate synthetic data may
help to overcome these barriers. Despite its great potential, however,
synthetic data are rarely employed. Therefore, we present a taxonomy
highlighting the various facets of deploying synthetic data for advanced
analytics systems. Furthermore, we identify typical application scenarios for
synthetic data to assess the current state of adoption and thereby unveil
missed opportunities to pave the way for further research."
14604,"We further study the performance of
RBF-MGN with diﬀerent numbers of collection points n and nearest neighbor nodes
m. All numerical experiments are mainly based on Pytorch.","We also consider diﬀerent time steps τ, PDE parameters and several types of RBFs on

                                                   8
the learning performance of the proposed method.","The MLPs with two
hidden layers, each with 64 neurons in the encoder, processor, and decoder of the
neural network are employed in all experiments.",2022-12-06 10:08:02+00:00,RBF-MGN:Solving spatiotemporal PDEs with Physics-informed Graph Neural Network,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zixue Xiang'), arxiv.Result.Author('Wei Peng'), arxiv.Result.Author('Wen Yao')]","Physics-informed neural networks (PINNs) have lately received significant
attention as a representative deep learning-based technique for solving partial
differential equations (PDEs). Most fully connected network-based PINNs use
automatic differentiation to construct loss functions that suffer from slow
convergence and difficult boundary enforcement. In addition, although
convolutional neural network (CNN)-based PINNs can significantly improve
training efficiency, CNNs have difficulty in dealing with irregular geometries
with unstructured meshes. Therefore, we propose a novel framework based on
graph neural networks (GNNs) and radial basis function finite difference
(RBF-FD). We introduce GNNs into physics-informed learning to better handle
irregular domains with unstructured meshes. RBF-FD is used to construct a
high-precision difference format of the differential equations to guide model
training. Finally, we perform numerical experiments on Poisson and wave
equations on irregular domains. We illustrate the generalizability, accuracy,
and efficiency of the proposed algorithms on different PDE parameters, numbers
of collection points, and several types of RBFs."
14639,We further study the                                                                                                                                                                              averagely have only 2 images per class.,"In the 10% data case, we
Different downstream data sizes.","(b) Average accuracy on
effectiveness of VQT under various training data sizes.",2022-12-06 18:39:45+00:00,Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV']","[arxiv.Result.Author('Cheng-Hao Tu'), arxiv.Result.Author('Zheda Mai'), arxiv.Result.Author('Wei-Lun Chao')]","Intermediate features of a pre-trained model have been shown informative for
making accurate predictions on downstream tasks, even if the model backbone is
kept frozen. The key challenge is how to utilize these intermediate features
given their gigantic amount. We propose visual query tuning (VQT), a simple yet
effective approach to aggregate intermediate features of Vision Transformers.
Through introducing a handful of learnable ``query'' tokens to each layer, VQT
leverages the inner workings of Transformers to ``summarize'' rich intermediate
features of each layer, which can then be used to train the prediction heads of
downstream tasks. As VQT keeps the intermediate features intact and only learns
to combine them, it enjoys memory efficiency in training, compared to many
other parameter-efficient fine-tuning approaches that learn to adapt features
and need back-propagation through the entire backbone. This also suggests the
complementary role between VQT and those approaches in transfer learning.
Empirically, VQT consistently surpasses the state-of-the-art approach that
utilizes intermediate features for transfer learning and outperforms full
fine-tuning in many cases. Compared to parameter-efficient approaches that
adapt features, VQT achieves much higher accuracy under memory constraints.
Most importantly, VQT is compatible with these approaches to attain even higher
accuracy, making it a simple add-on to further boost transfer learning."
14646,"This result
5 depicts the relative performance improvement of different                  suggests that further research is needed to seek an optimal
                                                                                                                                                                                                                8

Teacher                             SCCNet-22                                EEGNet-22                                                                                    ShallowConvNet-22
                                      80.25                                    67.82                                                                                             74.61

Student                  SCCNet-4b  EEGNet-4b  ShallowConvNet-4b  SCCNet-4b  EEGNet-4b                                                      ShallowConvNet-4b  SCCNet-4b  EEGNet-4b          ShallowConvNet-4b
                           47.38      44.15           43.81         47.38      44.15                                                               43.81         47.38      44.15                   43.81

HKD                      48.91      45.69*     46.78*             48.93      45.15                                                          46.76*             49.37*     45.37              48.05**
FitNet                   48.95      48.37**    45.49              47.35      45.35                                                          44.93              45.35*     42.97              47.55*
                         47.42      44.10      43.64              47.58*     44.07                                                          44.17              47.38      44.02              44.10
  AT                     48.09*     44.05      44.75              48.02*     43.82                                                          45.11              48.03*     43.88              45.52
 VID                     48.66*     43.67      42.91              47.64      44.28                                                          45.74              46.26      41.82*             46.55*
 AB                      49.09**    45.22      47.09**            49.02      47.00*                                                         47.47*             48.35      45.11              45.54*
 PKT                     49.51*     44.32      47.97*             49.11*     44.19                                                          47.89*             49.23*     44.54              47.90*
 CC                      46.05      44.03      44.68              45.85      44.75                                                          44.53              46.03      45.10              44.46
RKD                      50.45*     43.82      49.08*             48.01      47.10*                                                         46.41              49.49      43.31              47.93
  SP                     45.54      41.39**    43.81              45.64      43.57                                                          43.85              44.99*     41.90**            42.79
CRD

FitNet (w/ logits loss)  49.30*     49.67**    46.84*             48.65      45.97                                                          45.08              47.74      45.97              47.06
  AT (w/ logits loss)    48.91      45.74*     46.79*             49.02      45.03                                                          46.72*             49.38*     45.36              48.03**
 VID (w/ logits loss)    49.35*     45.86*     47.42**            48.88      45.17                                                          47.26*             49.79*     45.17              48.81**
 AB (w/ logits loss)     49.83**    46.66**    45.51              47.64      44.28                                                          47.32*             49.37**    43.78              49.00**
 PKT (w/ logits loss)    49.85**    47.09**    47.40*             49.17      46.81                                                          46.92*             49.78*     46.55**            48.54**
 CC (w/ logits loss)     50.04**    45.86*     48.71**            48.98      45.27                                                          47.59*             49.87*     45.72*             48.60**
RKD (w/ logits loss)     49.97*     46.27*     48.74*             48.47      45.10                                                          47.92*             49.52      44.39              48.10*
  SP (w/ logits loss)    50.74**    46.12      49.42*             49.15      47.15*                                                         46.72*             50.00*     45.76              48.97**
CRD (w/ logits loss)     48.36      44.88      46.84*             47.43      44.98                                                          46.59*             48.96      45.03              47.33*

         SK              50.84**    48.44**    48.56*             49.61*     46.47                                                          47.69*             49.23*     48.40**            47.43*
SK (w/ logits loss)      51.26**    49.35**    49.34**            49.07      45.86                                                          47.26*             50.34*     48.17**            48.36*

Bold: Highest accuracy.",always outperform other teacher network models.,Red: Signiﬁcantly outperform baseline student networks.,2022-12-06 21:22:02+00:00,Enhancing Low-Density EEG-Based Brain-Computer Interfaces with Similarity-Keeping Knowledge Distillation,cs.LG,"['cs.LG', 'eess.SP', 'q-bio.NC']","[arxiv.Result.Author('Xin-Yao Huang'), arxiv.Result.Author('Sung-Yu Chen'), arxiv.Result.Author('Chun-Shu Wei')]","Electroencephalogram (EEG) has been one of the common neuromonitoring
modalities for real-world brain-computer interfaces (BCIs) because of its
non-invasiveness, low cost, and high temporal resolution. Recently,
light-weight and portable EEG wearable devices based on low-density montages
have increased the convenience and usability of BCI applications. However, loss
of EEG decoding performance is often inevitable due to reduced number of
electrodes and coverage of scalp regions of a low-density EEG montage. To
address this issue, we introduce knowledge distillation (KD), a learning
mechanism developed for transferring knowledge/information between neural
network models, to enhance the performance of low-density EEG decoding. Our
framework includes a newly proposed similarity-keeping (SK) teacher-student KD
scheme that encourages a low-density EEG student model to acquire the
inter-sample similarity as in a pre-trained teacher model trained on
high-density EEG data. The experimental results validate that our SK-KD
framework consistently improves motor-imagery EEG decoding accuracy when number
of electrodes deceases for the input EEG data. For both common low-density
headphone-like and headband-like montages, our method outperforms
state-of-the-art KD methods across various EEG decoding model architectures. As
the first KD scheme developed for enhancing EEG decoding, we foresee the
proposed SK-KD framework to facilitate the practicality of low-density
EEG-based BCI in real-world applications."
14648,"Finally, we will summarize
our work and discuss some possible further research in Section 5.","Next, we will present our method in Section
3, then show our experimental results in Section 4.","2 Background

2.1 Gaussian-Bernoulli Restricted Boltzmann Machine

A Gaussian-Bernoulli restricted Boltzmann Machine (GB-RBM) [18] is deﬁned
on a complete bipartite graph as shown in Figure 1.",2022-12-07 00:01:20+00:00,Learning State Transition Rules from Hidden Layers of Restricted Boltzmann Machines,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Koji Watanabe'), arxiv.Result.Author('Katsumi Inoue')]","Understanding the dynamics of a system is important in many scientific and
engineering domains. This problem can be approached by learning state
transition rules from observations using machine learning techniques. Such
observed time-series data often consist of sequences of many continuous
variables with noise and ambiguity, but we often need rules of dynamics that
can be modeled with a few essential variables. In this work, we propose a
method for extracting a small number of essential hidden variables from
high-dimensional time-series data and for learning state transition rules
between these hidden variables. The proposed method is based on the Restricted
Boltzmann Machine (RBM), which treats observable data in the visible layer and
latent features in the hidden layer. However, real-world data, such as video
and audio, include both discrete and continuous variables, and these variables
have temporal relationships. Therefore, we propose Recurrent Temporal
GaussianBernoulli Restricted Boltzmann Machine (RTGB-RBM), which combines
Gaussian-Bernoulli Restricted Boltzmann Machine (GB-RBM) to handle continuous
visible variables, and Recurrent Temporal Restricted Boltzmann Machine (RT-RBM)
to capture time dependence between discrete hidden variables. We also propose a
rule-based method that extracts essential information as hidden variables and
represents state transition rules in interpretable form. We conduct experiments
on Bouncing Ball and Moving MNIST datasets to evaluate our proposed method.
Experimental results show that our method can learn the dynamics of those
physical systems as state transition rules between hidden variables and can
predict unobserved future states from observed state transitions."
14654,"(3) GNN-2: We inject faults only to weights in the second               Layer Sensitivity We further study the sensitivity of dif-
layer; same as GNN-1, the name of this layer is also network-           ferent layers in the four evaluated GNN networks as shown
speciﬁc, e.g., GCN-2, GAT-2, etc.","Note that the layer name is GNN-speciﬁc; so if the          C. Sensitivity Analysis
GNN model is GCN/GAT, then the layer name is GCN-1/GAT-
1.",Also note that unlike many            in Fig.,2022-12-07 06:14:14+00:00,Assessing and Analyzing the Resilience of Graph Neural Networks Against Hardware Faults,cs.LG,['cs.LG'],"[arxiv.Result.Author('Xun Jiao'), arxiv.Result.Author('Ruixuan Wang'), arxiv.Result.Author('Fred Lin'), arxiv.Result.Author('Daniel Moore'), arxiv.Result.Author('Sriram Sankar')]","Graph neural networks (GNNs) have recently emerged as a promising learning
paradigm in learning graph-structured data and have demonstrated wide success
across various domains such as recommendation systems, social networks, and
electronic design automation (EDA). Like other deep learning (DL) methods, GNNs
are being deployed in sophisticated modern hardware systems, as well as
dedicated accelerators. However, despite the popularity of GNNs and the recent
efforts of bringing GNNs to hardware, the fault tolerance and resilience of
GNNs has generally been overlooked. Inspired by the inherent algorithmic
resilience of DL methods, this paper conducts, for the first time, a
large-scale and empirical study of GNN resilience, aiming to understand the
relationship between hardware faults and GNN accuracy. By developing a
customized fault injection tool on top of PyTorch, we perform extensive fault
injection experiments to various GNN models and application datasets. We
observe that the error resilience of GNN models varies by orders of magnitude
with respect to different models and application datasets. Further, we explore
a low-cost error mitigation mechanism for GNN to enhance its resilience. This
GNN resilience study aims to open up new directions and opportunities for
future GNN accelerator design and architectural optimization."
14739,"Finally, our benchmark can be           fairness in ensembles that only utilize AND, OR operators to
                                       leveraged for further research on fair ensembles.","introduced a framework to understand the composition of
                                       to guide fair ensemble design.","To the best of       make a decision, e.g., two credit bureaus’ (AND) report a score
                                       our knowledge, this is one of the ﬁrst and largest studies on          to determine loan eligibility [4].",2022-12-08 22:51:13+00:00,Towards Understanding Fairness and its Composition in Ensemble Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Usman Gohar'), arxiv.Result.Author('Sumon Biswas'), arxiv.Result.Author('Hridesh Rajan')]","Machine Learning (ML) software has been widely adopted in modern society,
with reported fairness implications for minority groups based on race, sex,
age, etc. Many recent works have proposed methods to measure and mitigate
algorithmic bias in ML models. The existing approaches focus on single
classifier-based ML models. However, real-world ML models are often composed of
multiple independent or dependent learners in an ensemble (e.g., Random
Forest), where the fairness composes in a non-trivial way. How does fairness
compose in ensembles? What are the fairness impacts of the learners on the
ultimate fairness of the ensemble? Can fair learners result in an unfair
ensemble? Furthermore, studies have shown that hyperparameters influence the
fairness of ML models. Ensemble hyperparameters are more complex since they
affect how learners are combined in different categories of ensembles.
Understanding the impact of ensemble hyperparameters on fairness will help
programmers design fair ensembles. Today, we do not understand these fully for
different ensemble algorithms. In this paper, we comprehensively study popular
real-world ensembles: bagging, boosting, stacking and voting. We have developed
a benchmark of 168 ensemble models collected from Kaggle on four popular
fairness datasets. We use existing fairness metrics to understand the
composition of fairness. Our results show that ensembles can be designed to be
fairer without using mitigation techniques. We also identify the interplay
between fairness composition and data characteristics to guide fair ensemble
design. Finally, our benchmark can be leveraged for further research on fair
ensembles. To the best of our knowledge, this is one of the first and largest
studies on fairness composition in ensembles yet presented in the literature."
14740,"How does the number of learners impact the
  that leveraged for further research on building fairness-aware      fairness of the ensemble?","Do these learners introduce unfairness in
• A comprehensive fairness benchmark of popular ensembles             the predictions?","More importantly, we observed that
  ensembles.",2022-12-08 22:51:13+00:00,Towards Understanding Fairness and its Composition in Ensemble Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Usman Gohar'), arxiv.Result.Author('Sumon Biswas'), arxiv.Result.Author('Hridesh Rajan')]","Machine Learning (ML) software has been widely adopted in modern society,
with reported fairness implications for minority groups based on race, sex,
age, etc. Many recent works have proposed methods to measure and mitigate
algorithmic bias in ML models. The existing approaches focus on single
classifier-based ML models. However, real-world ML models are often composed of
multiple independent or dependent learners in an ensemble (e.g., Random
Forest), where the fairness composes in a non-trivial way. How does fairness
compose in ensembles? What are the fairness impacts of the learners on the
ultimate fairness of the ensemble? Can fair learners result in an unfair
ensemble? Furthermore, studies have shown that hyperparameters influence the
fairness of ML models. Ensemble hyperparameters are more complex since they
affect how learners are combined in different categories of ensembles.
Understanding the impact of ensemble hyperparameters on fairness will help
programmers design fair ensembles. Today, we do not understand these fully for
different ensemble algorithms. In this paper, we comprehensively study popular
real-world ensembles: bagging, boosting, stacking and voting. We have developed
a benchmark of 168 ensemble models collected from Kaggle on four popular
fairness datasets. We use existing fairness metrics to understand the
composition of fairness. Our results show that ensembles can be designed to be
fairer without using mitigation techniques. We also identify the interplay
between fairness composition and data characteristics to guide fair ensemble
design. Finally, our benchmark can be leveraged for further research on fair
ensembles. To the best of our knowledge, this is one of the first and largest
studies on fairness composition in ensembles yet presented in the literature."
14741,"Our
                                                                                              analysis should help guide further research into designing fair
                                                                                              learning techniques for boosting ensembles.","Consequently, we can see that adaptive
                                                                                              learning propagates less bias in highly biased datasets.","V. FAIR ENSEMBLES DESIGN

                                                                                                 In §IV, we found that base learners of ensembles propagate
                                                                                              bias.",2022-12-08 22:51:13+00:00,Towards Understanding Fairness and its Composition in Ensemble Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Usman Gohar'), arxiv.Result.Author('Sumon Biswas'), arxiv.Result.Author('Hridesh Rajan')]","Machine Learning (ML) software has been widely adopted in modern society,
with reported fairness implications for minority groups based on race, sex,
age, etc. Many recent works have proposed methods to measure and mitigate
algorithmic bias in ML models. The existing approaches focus on single
classifier-based ML models. However, real-world ML models are often composed of
multiple independent or dependent learners in an ensemble (e.g., Random
Forest), where the fairness composes in a non-trivial way. How does fairness
compose in ensembles? What are the fairness impacts of the learners on the
ultimate fairness of the ensemble? Can fair learners result in an unfair
ensemble? Furthermore, studies have shown that hyperparameters influence the
fairness of ML models. Ensemble hyperparameters are more complex since they
affect how learners are combined in different categories of ensembles.
Understanding the impact of ensemble hyperparameters on fairness will help
programmers design fair ensembles. Today, we do not understand these fully for
different ensemble algorithms. In this paper, we comprehensively study popular
real-world ensembles: bagging, boosting, stacking and voting. We have developed
a benchmark of 168 ensemble models collected from Kaggle on four popular
fairness datasets. We use existing fairness metrics to understand the
composition of fairness. Our results show that ensembles can be designed to be
fairer without using mitigation techniques. We also identify the interplay
between fairness composition and data characteristics to guide fair ensemble
design. Finally, our benchmark can be leveraged for further research on fair
ensembles. To the best of our knowledge, this is one of the first and largest
studies on fairness composition in ensembles yet presented in the literature."
14742,"Our analysis should also encourage                           n estimators =40 , random state =42) ) ,( ’knn 1 ’ ,
further research in fairness-aware weighting techniques to                          KNeighborsClassifier ( n neighbors =6) ) ]
handle fairness issues arising from model uncertainties.","The model construct is shown below:
develop frameworks to measure model uncertainties and their
fairness at a component level to aid developers in designing          1 layer one estimators = [( ’rf 1 ’ , RandomForestClassifier (
fair voting ensembles.","2 layer two estimators = [( ’rf 2 ’ , RandomForestClassifier (
   Finding 9: Two-layer stacking can signiﬁcantly reduce                            n estimators =40 , random state =42) ) ,( ’xg 2 ’ , XGBClassifier (
   unfairness.",2022-12-08 22:51:13+00:00,Towards Understanding Fairness and its Composition in Ensemble Machine Learning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Usman Gohar'), arxiv.Result.Author('Sumon Biswas'), arxiv.Result.Author('Hridesh Rajan')]","Machine Learning (ML) software has been widely adopted in modern society,
with reported fairness implications for minority groups based on race, sex,
age, etc. Many recent works have proposed methods to measure and mitigate
algorithmic bias in ML models. The existing approaches focus on single
classifier-based ML models. However, real-world ML models are often composed of
multiple independent or dependent learners in an ensemble (e.g., Random
Forest), where the fairness composes in a non-trivial way. How does fairness
compose in ensembles? What are the fairness impacts of the learners on the
ultimate fairness of the ensemble? Can fair learners result in an unfair
ensemble? Furthermore, studies have shown that hyperparameters influence the
fairness of ML models. Ensemble hyperparameters are more complex since they
affect how learners are combined in different categories of ensembles.
Understanding the impact of ensemble hyperparameters on fairness will help
programmers design fair ensembles. Today, we do not understand these fully for
different ensemble algorithms. In this paper, we comprehensively study popular
real-world ensembles: bagging, boosting, stacking and voting. We have developed
a benchmark of 168 ensemble models collected from Kaggle on four popular
fairness datasets. We use existing fairness metrics to understand the
composition of fairness. Our results show that ensembles can be designed to be
fairer without using mitigation techniques. We also identify the interplay
between fairness composition and data characteristics to guide fair ensemble
design. Finally, our benchmark can be leveraged for further research on fair
ensembles. To the best of our knowledge, this is one of the first and largest
studies on fairness composition in ensembles yet presented in the literature."
14747,"This implies that a good ini-
research is to remove the backdoor from the model, either            tialization may help reduce the ASR of a certain backdoor,
using recovered backdoor triggers [19] or through blind              which is an open problem for further research.","Essentially, the forgetting step could
                                                                     be viewed as an attempt to ﬁnd a good initialization point
    Compared with the detection approaches, another line of          for learning the primary task.",unlearning [38].,2022-12-09 06:29:43+00:00,"Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CV']","[arxiv.Result.Author('Rui Zhu'), arxiv.Result.Author('Di Tang'), arxiv.Result.Author('Siyuan Tang'), arxiv.Result.Author('XiaoFeng Wang'), arxiv.Result.Author('Haixu Tang')]","In this paper, we present a simple yet surprisingly effective technique to
induce ""selective amnesia"" on a backdoored model. Our approach, called SEAM,
has been inspired by the problem of catastrophic forgetting (CF), a long
standing issue in continual learning. Our idea is to retrain a given DNN model
on randomly labeled clean data, to induce a CF on the model, leading to a
sudden forget on both primary and backdoor tasks; then we recover the primary
task by retraining the randomized model on correctly labeled clean data. We
analyzed SEAM by modeling the unlearning process as continual learning and
further approximating a DNN using Neural Tangent Kernel for measuring CF. Our
analysis shows that our random-labeling approach actually maximizes the CF on
an unknown backdoor in the absence of triggered inputs, and also preserves some
feature extraction in the network to enable a fast revival of the primary task.
We further evaluated SEAM on both image processing and Natural Language
Processing tasks, under both data contamination and training manipulation
attacks, over thousands of models either trained on popular image datasets or
provided by the TrojAI competition. Our experiments show that SEAM vastly
outperforms the state-of-the-art unlearning techniques, achieving a high
Fidelity (measuring the gap between the accuracy of the primary task and that
of the backdoor) within a few minutes (about 30 times faster than training a
model from scratch using the MNIST dataset), with only a small amount of clean
data (0.1% of training data for TrojAI models)."
14748,"A natural direction for further study is to take account of other choices of La-
grangians.","The Boltzmann
machine of Model C still lacks an efﬁcient computation for training with the maxi-
mum likelihood, and further studies are desired.","As the generalized Hopﬁeld network can produce new energy-based
associative memory models by replacing the Lagrangians, the potential exists to
generate new Boltzmann machine models that possess preferable properties for
practical purposes.",2022-12-09 06:52:36+00:00,Attention in a family of Boltzmann machines emerging from modern Hopfield networks,cs.LG,"['cs.LG', 'cs.NE', 'stat.ML']","[arxiv.Result.Author('Toshihiro Ota'), arxiv.Result.Author('Ryo Karakida')]","Hopfield networks and Boltzmann machines (BMs) are fundamental energy-based
neural network models. Recent studies on modern Hopfield networks have broaden
the class of energy functions and led to a unified perspective on general
Hopfield networks including an attention module. In this letter, we consider
the BM counterparts of modern Hopfield networks using the associated energy
functions, and study their salient properties from a trainability perspective.
In particular, the energy function corresponding to the attention module
naturally introduces a novel BM, which we refer to as attentional BM (AttnBM).
We verify that AttnBM has a tractable likelihood function and gradient for a
special case and is easy to train. Moreover, we reveal the hidden connections
between AttnBM and some single-layer models, namely the Gaussian--Bernoulli
restricted BM and denoising autoencoder with softmax units. We also investigate
BMs introduced by other energy functions, and in particular, observe that the
energy function of dense associative memory models gives BMs belonging to
Exponential Family Harmoniums."
14750,"Overall, our results convey the optimistic message
that reward learning improves as we obtain better human models, and motivate further research into
improved models.","Experiments with multiple biases in
different environments, as well as an analysis of the true human policy (which potentially suffers
from unknown, yet to be characterized suboptimalities), reassuringly show remarkably consistent
results: over and over again, we see that as the human model and the true human behavior are more
and more aligned, the reward error decreases.",Limitations and future work.,2022-12-09 08:16:20+00:00,On the Sensitivity of Reward Inference to Misspecified Human Models,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Joey Hong'), arxiv.Result.Author('Kush Bhatia'), arxiv.Result.Author('Anca Dragan')]","Inferring reward functions from human behavior is at the center of value
alignment - aligning AI objectives with what we, humans, actually want. But
doing so relies on models of how humans behave given their objectives. After
decades of research in cognitive science, neuroscience, and behavioral
economics, obtaining accurate human models remains an open research topic. This
begs the question: how accurate do these models need to be in order for the
reward inference to be accurate? On the one hand, if small errors in the model
can lead to catastrophic error in inference, the entire framework of reward
learning seems ill-fated, as we will never have perfect models of human
behavior. On the other hand, if as our models improve, we can have a guarantee
that reward accuracy also improves, this would show the benefit of more work on
the modeling side. We study this question both theoretically and empirically.
We do show that it is unfortunately possible to construct small adversarial
biases in behavior that lead to arbitrarily large errors in the inferred
reward. However, and arguably more importantly, we are also able to identify
reasonable assumptions under which the reward inference error can be bounded
linearly in the error in the human model. Finally, we verify our theoretical
insights in discrete and continuous control tasks with simulated and human
data."
14756,We further note that a delayed relation could also            for further research.,"Meanwhile, high gas prices do not display a clear
correlation with overall electricity prices in the time se-             There remain many open questions and starting points
ries.","It would be interesting to investigate
be possible, if power plants purchase fuel well before the          how XAI price models diﬀer between electricity markets
usage.",2022-12-09 12:18:17+00:00,Understanding electricity prices beyond the merit order principle using explainable AI,cs.LG,['cs.LG'],"[arxiv.Result.Author('Julius Trebbien'), arxiv.Result.Author('Leonardo Rydin Gorjão'), arxiv.Result.Author('Aaron Praktiknjo'), arxiv.Result.Author('Benjamin Schäfer'), arxiv.Result.Author('Dirk Witthaut')]","Electricity prices in liberalized markets are determined by the supply and
demand for electric power, which are in turn driven by various external
influences that vary strongly in time. In perfect competition, the merit order
principle describes that dispatchable power plants enter the market in the
order of their marginal costs to meet the residual load, i.e. the difference of
load and renewable generation. Many market models implement this principle to
predict electricity prices but typically require certain assumptions and
simplifications. In this article, we present an explainable machine learning
model for the prices on the German day-ahead market, which substantially
outperforms a benchmark model based on the merit order principle. Our model is
designed for the ex-post analysis of prices and thus builds on various external
features. Using Shapley Additive exPlanation (SHAP) values, we can disentangle
the role of the different features and quantify their importance from empiric
data. Load, wind and solar generation are most important, as expected, but wind
power appears to affect prices stronger than solar power does. Fuel prices also
rank highly and show nontrivial dependencies, including strong interactions
with other features revealed by a SHAP interaction analysis. Large generation
ramps are correlated with high prices, again with strong feature interactions,
due to the limited flexibility of nuclear and lignite plants. Our results
further contribute to model development by providing quantitative insights
directly from data."
14769,"We further study how efﬁcient this method is at
identifying individuals with early-stage schizophrenia by measuring the Area Under the ROC curve
(AUROC) of the log-likelihood estimation with the diagnosis as the target variable.","We measure this through a correlation analysis between
the participants diagnosis and its log-likelihood.","As baseline, we
evaluated three normative models that have previously been considered for psychiatry data: Gaussian
process regression [17], Bayesian linear regression [13] and a state-of-the-art brain age prediction
model [19].",2022-12-08 18:22:36+00:00,Transformer-based normative modelling for anomaly detection of early schizophrenia,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Pedro F Da Costa'), arxiv.Result.Author('Jessica Dafflon'), arxiv.Result.Author('Sergio Leonardo Mendes'), arxiv.Result.Author('João Ricardo Sato'), arxiv.Result.Author('M. Jorge Cardoso'), arxiv.Result.Author('Robert Leech'), arxiv.Result.Author('Emily JH Jones'), arxiv.Result.Author('Walter H. L. Pinaya')]","Despite the impact of psychiatric disorders on clinical health, early-stage
diagnosis remains a challenge. Machine learning studies have shown that
classifiers tend to be overly narrow in the diagnosis prediction task. The
overlap between conditions leads to high heterogeneity among participants that
is not adequately captured by classification models. To address this issue,
normative approaches have surged as an alternative method. By using a
generative model to learn the distribution of healthy brain data patterns, we
can identify the presence of pathologies as deviations or outliers from the
distribution learned by the model. In particular, deep generative models showed
great results as normative models to identify neurological lesions in the
brain. However, unlike most neurological lesions, psychiatric disorders present
subtle changes widespread in several brain regions, making these alterations
challenging to identify. In this work, we evaluate the performance of
transformer-based normative models to detect subtle brain changes expressed in
adolescents and young adults. We trained our model on 3D MRI scans of
neurotypical individuals (N=1,765). Then, we obtained the likelihood of
neurotypical controls and psychiatric patients with early-stage schizophrenia
from an independent dataset (N=93) from the Human Connectome Project. Using the
predicted likelihood of the scans as a proxy for a normative score, we obtained
an AUROC of 0.82 when assessing the difference between controls and individuals
with early-stage schizophrenia. Our approach surpassed recent normative methods
based on brain age and Gaussian Process, showing the promising use of deep
generative models to help in individualised analyses."
14772,"The use of
     HP       24.46 25.00 29.73        1.2 billion                     2D and 3D partitioning schemes is another promising avenue for
     RP       34.70 42.88 65.14         13 billion                     further research.","Additionally,
   model      𝑑=1 𝑑=2 𝑑=5                volume                        there is scope for exploring several optimizations in the mini-batch
                                                                       sampling strategy and in the GPU communications.",Comparison against SOTA.,2022-12-09 17:51:13+00:00,Scalable Graph Convolutional Network Training on Distributed-Memory Systems,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Gunduz Vehbi Demirci'), arxiv.Result.Author('Aparajita Haldar'), arxiv.Result.Author('Hakan Ferhatosmanoglu')]","Graph Convolutional Networks (GCNs) are extensively utilized for deep
learning on graphs. The large data sizes of graphs and their vertex features
make scalable training algorithms and distributed memory systems necessary.
Since the convolution operation on graphs induces irregular memory access
patterns, designing a memory- and communication-efficient parallel algorithm
for GCN training poses unique challenges. We propose a highly parallel training
algorithm that scales to large processor counts. In our solution, the large
adjacency and vertex-feature matrices are partitioned among processors. We
exploit the vertex-partitioning of the graph to use non-blocking point-to-point
communication operations between processors for better scalability. To further
minimize the parallelization overheads, we introduce a sparse matrix
partitioning scheme based on a hypergraph partitioning model for full-batch
training. We also propose a novel stochastic hypergraph model to encode the
expected communication volume in mini-batch training. We show the merits of the
hypergraph model, previously unexplored for GCN training, over the standard
graph partitioning model which does not accurately encode the communication
costs. Experiments performed on real-world graph datasets demonstrate that the
proposed algorithms achieve considerable speedups over alternative solutions.
The optimizations achieved on communication costs become even more pronounced
at high scalability with many processors. The performance benefits are
preserved in deeper GCNs having more layers as well as on billion-scale graphs."
14773,On the other    is another promising avenue for further research.,"The use of 2D and 3D partitioning schemes
nificantly with increasing dimensionality of features.","hand, the communication benefit of HP, reducing communication
volume approximately by a factor of 10x, allows it to scale better.",2022-12-09 17:51:13+00:00,Scalable Graph Convolutional Network Training on Distributed-Memory Systems,cs.LG,"['cs.LG', 'cs.AI', 'cs.DC']","[arxiv.Result.Author('Gunduz Vehbi Demirci'), arxiv.Result.Author('Aparajita Haldar'), arxiv.Result.Author('Hakan Ferhatosmanoglu')]","Graph Convolutional Networks (GCNs) are extensively utilized for deep
learning on graphs. The large data sizes of graphs and their vertex features
make scalable training algorithms and distributed memory systems necessary.
Since the convolution operation on graphs induces irregular memory access
patterns, designing a memory- and communication-efficient parallel algorithm
for GCN training poses unique challenges. We propose a highly parallel training
algorithm that scales to large processor counts. In our solution, the large
adjacency and vertex-feature matrices are partitioned among processors. We
exploit the vertex-partitioning of the graph to use non-blocking point-to-point
communication operations between processors for better scalability. To further
minimize the parallelization overheads, we introduce a sparse matrix
partitioning scheme based on a hypergraph partitioning model for full-batch
training. We also propose a novel stochastic hypergraph model to encode the
expected communication volume in mini-batch training. We show the merits of the
hypergraph model, previously unexplored for GCN training, over the standard
graph partitioning model which does not accurately encode the communication
costs. Experiments performed on real-world graph datasets demonstrate that the
proposed algorithms achieve considerable speedups over alternative solutions.
The optimizations achieved on communication costs become even more pronounced
at high scalability with many processors. The performance benefits are
preserved in deeper GCNs having more layers as well as on billion-scale graphs."
14788,"However, a traditional GCN-based method only enables single-view data, meaning that further study on frameworks
addressing multi-graph-structural data should be conducted.","Attributed to these
factors, frameworks based on GCN have achieved noticeable performance for extracting robust numeric representations.","Some existing methods have endeavored to tackle this
deficiency with a linear weighted sum of heterogeneous graphs [15, 45], but it may not be ideal because such a strategy
can not extract deeper representation and results may be interfered by new noises encountered.",2022-12-09 21:48:36+00:00,Multi-view Graph Convolutional Networks with Differentiable Node Selection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhaoliang Chen'), arxiv.Result.Author('Lele Fu'), arxiv.Result.Author('Shunxin Xiao'), arxiv.Result.Author('Shiping Wang'), arxiv.Result.Author('Claudia Plant'), arxiv.Result.Author('Wenzhong Guo')]","Multi-view data containing complementary and consensus information can
facilitate representation learning by exploiting the intact integration of
multi-view features. Because most objects in real world often have underlying
connections, organizing multi-view data as heterogeneous graphs is beneficial
to extracting latent information among different objects. Due to the powerful
capability to gather information of neighborhood nodes, in this paper, we apply
Graph Convolutional Network (GCN) to cope with heterogeneous-graph data
originating from multi-view data, which is still under-explored in the field of
GCN. In order to improve the quality of network topology and alleviate the
interference of noises yielded by graph fusion, some methods undertake sorting
operations before the graph convolution procedure. These GCN-based methods
generally sort and select the most confident neighborhood nodes for each
vertex, such as picking the top-k nodes according to pre-defined confidence
values. Nonetheless, this is problematic due to the non-differentiable sorting
operators and inflexible graph embedding learning, which may result in blocked
gradient computations and undesired performance. To cope with these issues, we
propose a joint framework dubbed Multi-view Graph Convolutional Network with
Differentiable Node Selection (MGCN-DNS), which is constituted of an adaptive
graph fusion layer, a graph learning module and a differentiable node selection
schema. MGCN-DNS accepts multi-channel graph-structural data as inputs and aims
to learn more robust graph fusion through a differentiable neural network. The
effectiveness of the proposed method is verified by rigorous comparisons with
considerable state-of-the-art approaches in terms of multi-view semi-supervised
classification tasks."
14789,"In the future, we will devote ourselves to further study on multi-channel GCN-based models and exploit insightful
models to conduct sustained explorations of graph embeddings with better robustness and explainable semantic
information.","Comprehensive experimental results validated the effectiveness of MGCN-DNS and revealed that it gained encouraging
performance improvement.",A more dexterous strategy to fuse graphs from heterogeneous sources is also our next direction.,2022-12-09 21:48:36+00:00,Multi-view Graph Convolutional Networks with Differentiable Node Selection,cs.LG,['cs.LG'],"[arxiv.Result.Author('Zhaoliang Chen'), arxiv.Result.Author('Lele Fu'), arxiv.Result.Author('Shunxin Xiao'), arxiv.Result.Author('Shiping Wang'), arxiv.Result.Author('Claudia Plant'), arxiv.Result.Author('Wenzhong Guo')]","Multi-view data containing complementary and consensus information can
facilitate representation learning by exploiting the intact integration of
multi-view features. Because most objects in real world often have underlying
connections, organizing multi-view data as heterogeneous graphs is beneficial
to extracting latent information among different objects. Due to the powerful
capability to gather information of neighborhood nodes, in this paper, we apply
Graph Convolutional Network (GCN) to cope with heterogeneous-graph data
originating from multi-view data, which is still under-explored in the field of
GCN. In order to improve the quality of network topology and alleviate the
interference of noises yielded by graph fusion, some methods undertake sorting
operations before the graph convolution procedure. These GCN-based methods
generally sort and select the most confident neighborhood nodes for each
vertex, such as picking the top-k nodes according to pre-defined confidence
values. Nonetheless, this is problematic due to the non-differentiable sorting
operators and inflexible graph embedding learning, which may result in blocked
gradient computations and undesired performance. To cope with these issues, we
propose a joint framework dubbed Multi-view Graph Convolutional Network with
Differentiable Node Selection (MGCN-DNS), which is constituted of an adaptive
graph fusion layer, a graph learning module and a differentiable node selection
schema. MGCN-DNS accepts multi-channel graph-structural data as inputs and aims
to learn more robust graph fusion through a differentiable neural network. The
effectiveness of the proposed method is verified by rigorous comparisons with
considerable state-of-the-art approaches in terms of multi-view semi-supervised
classification tasks."
14809,"Therefore, further research is needed to investigate
pared with other two-stream CNNs.","are already many deep learning models that have demonstrated
                                                                   the performance in single-mode MI and SSVEP decoding [4],
   Another advantage of TSCNN is its interpretability, com-        [22], [34].","We analyzed the connec-          fusion frameworks that can achieve higher performance by
tion weights in Section III-C. Table IV and Fig.",2022-12-10 12:34:36+00:00,A Hybrid Brain-Computer Interface Using Motor Imagery and SSVEP Based on Convolutional Neural Network,cs.LG,['cs.LG'],"[arxiv.Result.Author('Wenwei Luo'), arxiv.Result.Author('Wanguang Yin'), arxiv.Result.Author('Quanying Liu'), arxiv.Result.Author('Youzhi Qu')]","The key to electroencephalography (EEG)-based brain-computer interface (BCI)
lies in neural decoding, and its accuracy can be improved by using hybrid BCI
paradigms, that is, fusing multiple paradigms. However, hybrid BCIs usually
require separate processing processes for EEG signals in each paradigm, which
greatly reduces the efficiency of EEG feature extraction and the
generalizability of the model. Here, we propose a two-stream convolutional
neural network (TSCNN) based hybrid brain-computer interface. It combines
steady-state visual evoked potential (SSVEP) and motor imagery (MI) paradigms.
TSCNN automatically learns to extract EEG features in the two paradigms in the
training process, and improves the decoding accuracy by 25.4% compared with the
MI mode, and 2.6% compared with SSVEP mode in the test data. Moreover, the
versatility of TSCNN is verified as it provides considerable performance in
both single-mode (70.2% for MI, 93.0% for SSVEP) and hybrid-mode scenarios
(95.6% for MI-SSVEP hybrid). Our work will facilitate the real-world
applications of EEG-based BCI systems."
14816,"Our vertical-
                                                                            layered models share these parameters among quantized networks
We further study the effectiveness of the self-KD paradigm de-              of different precision for convenient deployments.","Model    Replaced layers  # of additional                                 Top-1 Acc (%) @ W / A  Top-5 Acc (%) @ W / A
ResNet-18          ×           parameters
ResNet-50          √                 -           2/2                        3/3   4/4              2/2   3/3   4/4
                   ×
                   √        1.04 M (9.4%)        66.7                       69.9  70.6             87.3  89.1  89.5
                                     -
                                                 66.9                       70.0  70.5             87.3  89.3  89.4
                            4.12 M (17.6%)
                                                 72.4                       75.9  76.8             90.9  92.7  93.0

                                                 71.0                       74.8  75.8             90.3  92.3  92.7

6.3.4 Effect of Self-Knowledge Distillation                                 layer in full precision to preserve accuracy [23].","One might
scribed in Sec.",2022-12-10 15:57:38+00:00,Vertical Layering of Quantized Neural Networks for Heterogeneous Inference,cs.LG,['cs.LG'],"[arxiv.Result.Author('Hai Wu'), arxiv.Result.Author('Ruifei He'), arxiv.Result.Author('Haoru Tan'), arxiv.Result.Author('Xiaojuan Qi'), arxiv.Result.Author('Kaibin Huang')]","Although considerable progress has been obtained in neural network
quantization for efficient inference, existing methods are not scalable to
heterogeneous devices as one dedicated model needs to be trained, transmitted,
and stored for one specific hardware setting, incurring considerable costs in
model training and maintenance. In this paper, we study a new vertical-layered
representation of neural network weights for encapsulating all quantized models
into a single one. With this representation, we can theoretically achieve any
precision network for on-demand service while only needing to train and
maintain one model. To this end, we propose a simple once quantization-aware
training (QAT) scheme for obtaining high-performance vertical-layered models.
Our design incorporates a cascade downsampling mechanism which allows us to
obtain multiple quantized networks from one full precision source model by
progressively mapping the higher precision weights to their adjacent lower
precision counterparts. Then, with networks of different bit-widths from one
source model, multi-objective optimization is employed to train the shared
source model weights such that they can be updated simultaneously, considering
the performance of all networks. By doing this, the shared weights will be
optimized to balance the performance of different quantized models, thus making
the weights transferable among different bit widths. Experiments show that the
proposed vertical-layered representation and developed once QAT scheme are
effective in embodying multiple quantized networks into a single one and allow
one-time training, and it delivers comparable performance as that of quantized
models tailored to any specific bit-width. Code will be available."
14819,"We conclude that the dynamics induced by TD updates may be
particularly beneﬁcial to transfer between policies in the value-improvement path,
and further study of this phenomenon is a promising avenue for future work.",in Figure 5.4.,"5.5.2 Auxiliary tasks for large-scale environments with sparse
          rewards

We now turn our attention to deep reinforcement learning, particularly in the
context of environments with sparse reward structure.",2022-12-11 00:07:24+00:00,Generalization Through the Lens of Learning Dynamics,cs.LG,['cs.LG'],[arxiv.Result.Author('Clare Lyle')],"A machine learning (ML) system must learn not only to match the output of a
target function on a training set, but also to generalize to novel situations
in order to yield accurate predictions at deployment. In most practical
applications, the user cannot exhaustively enumerate every possible input to
the model; strong generalization performance is therefore crucial to the
development of ML systems which are performant and reliable enough to be
deployed in the real world. While generalization is well-understood
theoretically in a number of hypothesis classes, the impressive generalization
performance of deep neural networks has stymied theoreticians. In deep
reinforcement learning (RL), our understanding of generalization is further
complicated by the conflict between generalization and stability in widely-used
RL algorithms. This thesis will provide insight into generalization by studying
the learning dynamics of deep neural networks in both supervised and
reinforcement learning tasks."
14824,"OUTLOOK: INITIALIZATION
on a
        theoretical results also suggest further research
         practical problem in deep learning: weight

                                                                        6
          Sparse Deep Learning—Johannes Lederer

Approach  Mathematical techniques       References
FatShat   Fat-shattering dimension      Bartlett [1998]
RadCon    Rademacher complexity         Bartlett and Mendelson [2002], Golowich et al.",Our         IV.,"[2018]
                                        Neyshabur et al.",2022-12-11 06:45:45+00:00,Statistical guarantees for sparse deep learning,cs.LG,"['cs.LG', 'cs.AI', 'math.ST', 'stat.ML', 'stat.TH']",[arxiv.Result.Author('Johannes Lederer')],"Neural networks are becoming increasingly popular in applications, but our
mathematical understanding of their potential and limitations is still limited.
In this paper, we further this understanding by developing statistical
guarantees for sparse deep learning. In contrast to previous work, we consider
different types of sparsity, such as few active connections, few active nodes,
and other norm-based types of sparsity. Moreover, our theories cover important
aspects that previous theories have neglected, such as multiple outputs,
regularization, and l2-loss. The guarantees have a mild dependence on network
widths and depths, which means that they support the application of sparse but
wide and deep networks from a statistical perspective. Some of the concepts and
tools that we use in our derivations are uncommon in deep learning and, hence,
might be of additional interest."
14834,"The approximate solver in stage 1 may still output infea-           SafeRL-Kit: A Systematic Implementation
sible actions for the following reasons: (a) The supremum
of Lagrangian multipliers in Proposition 2 is hard to obtain,     To facilitate further research in this area, we release SafeRL-
and we only settle κ as a ﬁxed, large but sub-optimal hyper-      Kit1, a reproducible and open-source safe RL toolkit as
parameter.",Stage 2: Gradient-based Projection                                   More algorithmic details are summarized in Appendix A.,"(b) The inherent issues of safe RL, such as the        shown in Figure 2.",2022-12-12 06:30:17+00:00,Evaluating Model-free Reinforcement Learning toward Safety-critical Tasks,cs.LG,"['cs.LG', 'cs.RO']","[arxiv.Result.Author('Linrui Zhang'), arxiv.Result.Author('Qin Zhang'), arxiv.Result.Author('Li Shen'), arxiv.Result.Author('Bo Yuan'), arxiv.Result.Author('Xueqian Wang'), arxiv.Result.Author('Dacheng Tao')]","Safety comes first in many real-world applications involving autonomous
agents. Despite a large number of reinforcement learning (RL) methods focusing
on safety-critical tasks, there is still a lack of high-quality evaluation of
those algorithms that adheres to safety constraints at each decision step under
complex and unknown dynamics. In this paper, we revisit prior work in this
scope from the perspective of state-wise safe RL and categorize them as
projection-based, recovery-based, and optimization-based approaches,
respectively. Furthermore, we propose Unrolling Safety Layer (USL), a joint
method that combines safety optimization and safety projection. This novel
technique explicitly enforces hard constraints via the deep unrolling
architecture and enjoys structural advantages in navigating the trade-off
between reward improvement and constraint satisfaction. To facilitate further
research in this area, we reproduce related algorithms in a unified pipeline
and incorporate them into SafeRL-Kit, a toolkit that provides off-the-shelf
interfaces and evaluation utilities for safety-critical tasks. We then perform
a comparative study of the involved algorithms on six benchmarks ranging from
robotic control to autonomous driving. The empirical results provide an insight
into their applicability and robustness in learning zero-cost-return policies
without task-dependent handcrafting. The project page is available at
https://sites.google.com/view/saferlkit."
14836,"We encourage further research in
these directions, and hope that our LfS baselines will help accurately benchmark progress in this area.","However, ﬁnetuning large visual backbones present optimization challenges
(e.g., instability and catastrophical forgetting), and can be costly.","To conclude, we reiterate the main takeaways of our study:

 • A well-designed LfS is competitive with frozen pre-trained representations across a variety of
    algorithms and task domains.",2022-12-12 07:59:31+00:00,On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline,cs.LG,"['cs.LG', 'cs.CV', 'cs.RO']","[arxiv.Result.Author('Nicklas Hansen'), arxiv.Result.Author('Zhecheng Yuan'), arxiv.Result.Author('Yanjie Ze'), arxiv.Result.Author('Tongzhou Mu'), arxiv.Result.Author('Aravind Rajeswaran'), arxiv.Result.Author('Hao Su'), arxiv.Result.Author('Huazhe Xu'), arxiv.Result.Author('Xiaolong Wang')]","We revisit a simple Learning-from-Scratch baseline for visuo-motor control
that uses data augmentation and a shallow ConvNet. We find that this baseline
has competitive performance with recent methods that leverage frozen visual
representations trained on large-scale vision datasets."
14839,"In this section,
restriction (CMR) and propose a IV-aided Value Iteration        we point several lines for further research.","Using IVs, [194] derived a conditional moment          causal relationships in real-world problems.","(IVVI) algorithm based on a primal-dual reformulation of
CMR.",2022-12-12 08:59:04+00:00,Instrumental Variables in Causal Inference and Machine Learning: A Survey,cs.LG,"['cs.LG', 'cs.AI', 'stat.ME']","[arxiv.Result.Author('Anpeng Wu'), arxiv.Result.Author('Kun Kuang'), arxiv.Result.Author('Ruoxuan Xiong'), arxiv.Result.Author('Fei Wu')]","Causal inference is the process of using assumptions, study designs, and
estimation strategies to draw conclusions about the causal relationships
between variables based on data. This allows researchers to better understand
the underlying mechanisms at work in complex systems and make more informed
decisions. In many settings, we may not fully observe all the confounders that
affect both the treatment and outcome variables, complicating the estimation of
causal effects. To address this problem, a growing literature in both causal
inference and machine learning proposes to use Instrumental Variables (IV).
This paper serves as the first effort to systematically and comprehensively
introduce and discuss the IV methods and their applications in both causal
inference and machine learning. First, we provide the formal definition of IVs
and discuss the identification problem of IV regression methods under different
assumptions. Second, we categorize the existing work on IV methods into three
streams according to the focus on the proposed methods, including two-stage
least squares with IVs, control function with IVs, and evaluation of IVs. For
each stream, we present both the classical causal inference methods, and recent
developments in the machine learning literature. Then, we introduce a variety
of applications of IV methods in real-world scenarios and provide a summary of
the available datasets and algorithms. Finally, we summarize the literature,
discuss the open problems and suggest promising future research directions for
IV methods and their applications. We also develop a toolkit of IVs methods
reviewed in this survey at https://github.com/causal-machine-learning-lab/mliv."
14872,"We believe that due to its simplicity and practical appeal, the setting of learning with feature-
oblivious label DP merits further study, from both a theoretical and an empirical standpoint.","(2022) for computer vision tasks, and the ALIBI method (Malek Esmaeili
et al., 2021) for classiﬁcation tasks, which is based on Bayesian inference.","Finally, we leave the question of obtaining better “feature-aware” label DP regression algorithms for
a future investigation.",2022-12-12 17:41:32+00:00,Regression with Label Differential Privacy,cs.LG,"['cs.LG', 'cs.CR']","[arxiv.Result.Author('Badih Ghazi'), arxiv.Result.Author('Pritish Kamath'), arxiv.Result.Author('Ravi Kumar'), arxiv.Result.Author('Ethan Leeman'), arxiv.Result.Author('Pasin Manurangsi'), arxiv.Result.Author('Avinash Varadarajan'), arxiv.Result.Author('Chiyuan Zhang')]","We study the task of training regression models with the guarantee of label
differential privacy (DP). Based on a global prior distribution on label
values, which could be obtained privately, we derive a label DP randomization
mechanism that is optimal under a given regression loss function. We prove that
the optimal mechanism takes the form of a ``randomized response on bins'', and
propose an efficient algorithm for finding the optimal bin values. We carry out
a thorough experimental evaluation on several datasets demonstrating the
efficacy of our algorithm."
14899,"3

this observation to event log and process model characteristics, we can bet-
ter understand and position issues, which in turn allows us to propose several
ideas for further research.","By relating
                          Springer Nature 2021 LATEX template

             Can recurrent neural networks learn process model structure?","In summary, the main question addressed in this paper can be stated as follows:
to which degree can LSTM neural networks, trained for next event predic-
tion, learn process model structure?",2022-12-13 08:40:01+00:00,Can recurrent neural networks learn process model structure?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jari Peeperkorn'), arxiv.Result.Author('Seppe vanden Broucke'), arxiv.Result.Author('Jochen De Weerdt')]","Various methods using machine and deep learning have been proposed to tackle
different tasks in predictive process monitoring, forecasting for an ongoing
case e.g. the most likely next event or suffix, its remaining time, or an
outcome-related variable. Recurrent neural networks (RNNs), and more
specifically long short-term memory nets (LSTMs), stand out in terms of
popularity. In this work, we investigate the capabilities of such an LSTM to
actually learn the underlying process model structure of an event log. We
introduce an evaluation framework that combines variant-based resampling and
custom metrics for fitness, precision and generalization. We evaluate 4
hypotheses concerning the learning capabilities of LSTMs, the effect of
overfitting countermeasures, the level of incompleteness in the training set
and the level of parallelism in the underlying process model. We confirm that
LSTMs can struggle to learn process model structure, even with simplistic
process data and in a very lenient setup. Taking the correct anti-overfitting
measures can alleviate the problem. However, these measures did not present
themselves to be optimal when selecting hyperparameters purely on predicting
accuracy. We also found that decreasing the amount of information seen by the
LSTM during training, causes a sharp drop in generalization and precision
scores. In our experiments, we could not identify a relationship between the
extent of parallelism in the model and the generalization capability, but they
do indicate that the process' complexity might have impact."
14900,"Given that we kept the number of variants contained, this
should be conﬁrmed with further research.","in terms of the num-
ber of variants).",This work brings about a wide array of future research opportunities.,2022-12-13 08:40:01+00:00,Can recurrent neural networks learn process model structure?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jari Peeperkorn'), arxiv.Result.Author('Seppe vanden Broucke'), arxiv.Result.Author('Jochen De Weerdt')]","Various methods using machine and deep learning have been proposed to tackle
different tasks in predictive process monitoring, forecasting for an ongoing
case e.g. the most likely next event or suffix, its remaining time, or an
outcome-related variable. Recurrent neural networks (RNNs), and more
specifically long short-term memory nets (LSTMs), stand out in terms of
popularity. In this work, we investigate the capabilities of such an LSTM to
actually learn the underlying process model structure of an event log. We
introduce an evaluation framework that combines variant-based resampling and
custom metrics for fitness, precision and generalization. We evaluate 4
hypotheses concerning the learning capabilities of LSTMs, the effect of
overfitting countermeasures, the level of incompleteness in the training set
and the level of parallelism in the underlying process model. We confirm that
LSTMs can struggle to learn process model structure, even with simplistic
process data and in a very lenient setup. Taking the correct anti-overfitting
measures can alleviate the problem. However, these measures did not present
themselves to be optimal when selecting hyperparameters purely on predicting
accuracy. We also found that decreasing the amount of information seen by the
LSTM during training, causes a sharp drop in generalization and precision
scores. In our experiments, we could not identify a relationship between the
extent of parallelism in the model and the generalization capability, but they
do indicate that the process' complexity might have impact."
14901,"Theoretical underpinnings of
the generalization capacity of RNNs, in line with [32], can also be a subject
for further study.","With
a distinct focus on next event prediction and a pure control-ﬂow perspec-
tive, there is a clear opportunity for expansion towards other prediction tasks
(outcome, remaining time, suﬃx), as well as towards the inclusion of addi-
tional variables such as resources or timestamps.","Furthermore, broadening the array of techniques included
in the current assessment to convolutional neural networks (CNNs), Genera-
tive Adverserial Networks (GANs), and expanding the models to use diﬀerent
encoding techniques as in [33] seems worthwhile as well.",2022-12-13 08:40:01+00:00,Can recurrent neural networks learn process model structure?,cs.LG,['cs.LG'],"[arxiv.Result.Author('Jari Peeperkorn'), arxiv.Result.Author('Seppe vanden Broucke'), arxiv.Result.Author('Jochen De Weerdt')]","Various methods using machine and deep learning have been proposed to tackle
different tasks in predictive process monitoring, forecasting for an ongoing
case e.g. the most likely next event or suffix, its remaining time, or an
outcome-related variable. Recurrent neural networks (RNNs), and more
specifically long short-term memory nets (LSTMs), stand out in terms of
popularity. In this work, we investigate the capabilities of such an LSTM to
actually learn the underlying process model structure of an event log. We
introduce an evaluation framework that combines variant-based resampling and
custom metrics for fitness, precision and generalization. We evaluate 4
hypotheses concerning the learning capabilities of LSTMs, the effect of
overfitting countermeasures, the level of incompleteness in the training set
and the level of parallelism in the underlying process model. We confirm that
LSTMs can struggle to learn process model structure, even with simplistic
process data and in a very lenient setup. Taking the correct anti-overfitting
measures can alleviate the problem. However, these measures did not present
themselves to be optimal when selecting hyperparameters purely on predicting
accuracy. We also found that decreasing the amount of information seen by the
LSTM during training, causes a sharp drop in generalization and precision
scores. In our experiments, we could not identify a relationship between the
extent of parallelism in the model and the generalization capability, but they
do indicate that the process' complexity might have impact."
14902,"improve upon existing methods, we think that it opens up
This explains the better performance on in-domain datasets    interesting new directions for further research.","Although we
tent space leads to a better estimation of generalization.","compared to cross-domain, as well as the results of Fig-
ure 6.",2022-12-13 10:21:15+00:00,A Statistical Model for Predicting Generalization in Few-Shot Classification,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Author('Yassir Bendou'), arxiv.Result.Author('Vincent Gripon'), arxiv.Result.Author('Bastien Pasdeloup'), arxiv.Result.Author('Lukas Mauch'), arxiv.Result.Author('Stefan Uhlich'), arxiv.Result.Author('Fabien Cardinaux'), arxiv.Result.Author('Ghouthi Boukli Hacene'), arxiv.Result.Author('Javier Alonso Garcia')]","The estimation of the generalization error of classifiers often relies on a
validation set. Such a set is hardly available in few-shot learning scenarios,
a highly disregarded shortcoming in the field. In these scenarios, it is common
to rely on features extracted from pre-trained neural networks combined with
distance-based classifiers such as nearest class mean. In this work, we
introduce a Gaussian model of the feature distribution. By estimating the
parameters of this model, we are able to predict the generalization error on
new classification tasks with few samples. We observe that accurate distance
estimates between class-conditional densities are the key to accurate estimates
of the generalization performance. Therefore, we propose an unbiased estimator
for these distances and integrate it in our numerical analysis. We show that
our approach outperforms alternatives such as the leave-one-out
cross-validation strategy in few-shot settings."
14903,"We encourage further research on these important
       topics.","), model lifetimes, etc.","Finally, while the overall tone of our article is critical, we recognize and highlight that many recent works
employing public data have played an important role in showing that diﬀerential privacy can be preserved
for certain complex machine learning problems, without suﬀering devastating impacts on utility.",2022-12-13 10:41:12+00:00,Considerations for Differentially Private Learning with Large-Scale Public Pretraining,cs.LG,"['cs.LG', 'cs.CR', 'stat.ML']","[arxiv.Result.Author('Florian Tramèr'), arxiv.Result.Author('Gautam Kamath'), arxiv.Result.Author('Nicholas Carlini')]","The performance of differentially private machine learning can be boosted
significantly by leveraging the transfer learning capabilities of non-private
models pretrained on large public datasets. We critically review this approach.
  We primarily question whether the use of large Web-scraped datasets should be
viewed as differential-privacy-preserving. We caution that publicizing these
models pretrained on Web data as ""private"" could lead to harm and erode the
public's trust in differential privacy as a meaningful definition of privacy.
  Beyond the privacy considerations of using public data, we further question
the utility of this paradigm. We scrutinize whether existing machine learning
benchmarks are appropriate for measuring the ability of pretrained models to
generalize to sensitive domains, which may be poorly represented in public Web
data. Finally, we notice that pretraining has been especially impactful for the
largest available models -- models sufficiently large to prohibit end users
running them on their own devices. Thus, deploying such models today could be a
net loss for privacy, as it would require (private) data to be outsourced to a
more compute-powerful third party.
  We conclude by discussing potential paths forward for the field of private
learning, as public pretraining becomes more popular and powerful."
14915,"• Finally, we discuss further research opportunities to ap-      2017).",temperature from satellite-derived observations (Yang et al.,(ElSaadani et al.,2022-12-12 14:36:39+00:00,Forecasting Soil Moisture Using Domain Inspired Temporal Graph Convolution Neural Networks To Guide Sustainable Crop Management,cs.LG,"['cs.LG', 'stat.AP']","[arxiv.Result.Author('Muneeza Azmat'), arxiv.Result.Author('Malvern Madondo'), arxiv.Result.Author('Kelsey Dipietro'), arxiv.Result.Author('Raya Horesh'), arxiv.Result.Author('Arun Bawa'), arxiv.Result.Author('Michael Jacobs'), arxiv.Result.Author('Raghavan Srinivasan'), arxiv.Result.Author(""Fearghal O'Donncha"")]","Climate change, population growth, and water scarcity present unprecedented
challenges for agriculture. This project aims to forecast soil moisture using
domain knowledge and machine learning for crop management decisions that enable
sustainable farming. Traditional methods for predicting hydrological response
features require significant computational time and expertise. Recent work has
implemented machine learning models as a tool for forecasting hydrological
response features, but these models neglect a crucial component of traditional
hydrological modeling that spatially close units can have vastly different
hydrological responses. In traditional hydrological modeling, units with
similar hydrological properties are grouped together and share model parameters
regardless of their spatial proximity. Inspired by this domain knowledge, we
have constructed a novel domain-inspired temporal graph convolution neural
network. Our approach involves clustering units based on time-varying
hydrological properties, constructing graph topologies for each cluster, and
forecasting soil moisture using graph convolutions and a gated recurrent neural
network. We have trained, validated, and tested our method on field-scale time
series data consisting of approximately 99,000 hydrological response units
spanning 40 years in a case study in northeastern United States. Comparison
with existing models illustrates the effectiveness of using domain-inspired
clustering with time series graph neural networks. The framework is being
deployed as part of a pro bono social impact program. The trained models are
being deployed on small-holding farms in central Texas."
14920,"While
the purpose of our work is to introduce the notion of difﬁculty disparity and difﬁculty ampliﬁca-
tion, further research is needed to conﬁrm the role of weight decay across various settings, and its
interaction with other implicit regularization schemes.",(2019).,Gradient penalty.,2022-12-13 15:24:41+00:00,Simplicity Bias Leads to Amplified Performance Disparities,cs.LG,"['cs.LG', 'cs.CY']","[arxiv.Result.Author('Samuel J. Bell'), arxiv.Result.Author('Levent Sagun')]","The simple idea that not all things are equally difficult has surprising
implications when applied in a fairness context. In this work we explore how
""difficulty"" is model-specific, such that different models find different parts
of a dataset challenging. When difficulty correlates with group information, we
term this difficulty disparity. Drawing a connection with recent work exploring
the inductive bias towards simplicity of SGD-trained models, we show that when
such a disparity exists, it is further amplified by commonly-used models. We
quantify this amplification factor across a range of settings aiming towards a
fuller understanding of the role of model bias. We also present a challenge to
the simplifying assumption that ""fixing"" a dataset is sufficient to ensure
unbiased performance."
14934,"Therefore, towards addressing these issues and enabling further research on
AI systems, in this paper, we propose an exploratory study of AI system risk
assessment.","However, collecting and labeling new data for an existing system
   with pre-trained ML/DL models could be both hard and time-consuming.","As an early attempt, this paper performs analysis and risk assess-
ment of an AI system through characterizing data distribution and uncertainty
inside the system.",2022-12-13 03:34:25+00:00,An Exploratory Study of AI System Risk Assessment from the Lens of Data Distribution and Uncertainty,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Zhijie Wang'), arxiv.Result.Author('Yuheng Huang'), arxiv.Result.Author('Lei Ma'), arxiv.Result.Author('Haruki Yokoyama'), arxiv.Result.Author('Susumu Tokumoto'), arxiv.Result.Author('Kazuki Munakata')]","Deep learning (DL) has become a driving force and has been widely adopted in
many domains and applications with competitive performance. In practice, to
solve the nontrivial and complicated tasks in real-world applications, DL is
often not used standalone, but instead contributes as a piece of gadget of a
larger complex AI system. Although there comes a fast increasing trend to study
the quality issues of deep neural networks (DNNs) at the model level, few
studies have been performed to investigate the quality of DNNs at both the unit
level and the potential impacts on the system level. More importantly, it also
lacks systematic investigation on how to perform the risk assessment for AI
systems from unit level to system level. To bridge this gap, this paper
initiates an early exploratory study of AI system risk assessment from both the
data distribution and uncertainty angles to address these issues. We propose a
general framework with an exploratory study for analyzing AI systems. After
large-scale (700+ experimental configurations and 5000+ GPU hours) experiments
and in-depth investigations, we reached a few key interesting findings that
highlight the practical need and opportunities for more in-depth investigations
into AI systems."
14935,"In addition to OOD
detection, we further study the eﬀectiveness of two proposed general uncertainty
estimation metrics on diﬀerent AI systems by comparing clean data’s uncertainty
scores and corrupted counterparts."," How does the uncertainty estima-
tion perform on diﬀerent modules in AI systems?","We found that the proposed metrics could well
reﬂect the AI system’s prediction uncertainty on the corrupted datasets through
large-scale experiments.",2022-12-13 03:34:25+00:00,An Exploratory Study of AI System Risk Assessment from the Lens of Data Distribution and Uncertainty,cs.LG,"['cs.LG', 'cs.AI', 'cs.SE']","[arxiv.Result.Author('Zhijie Wang'), arxiv.Result.Author('Yuheng Huang'), arxiv.Result.Author('Lei Ma'), arxiv.Result.Author('Haruki Yokoyama'), arxiv.Result.Author('Susumu Tokumoto'), arxiv.Result.Author('Kazuki Munakata')]","Deep learning (DL) has become a driving force and has been widely adopted in
many domains and applications with competitive performance. In practice, to
solve the nontrivial and complicated tasks in real-world applications, DL is
often not used standalone, but instead contributes as a piece of gadget of a
larger complex AI system. Although there comes a fast increasing trend to study
the quality issues of deep neural networks (DNNs) at the model level, few
studies have been performed to investigate the quality of DNNs at both the unit
level and the potential impacts on the system level. More importantly, it also
lacks systematic investigation on how to perform the risk assessment for AI
systems from unit level to system level. To bridge this gap, this paper
initiates an early exploratory study of AI system risk assessment from both the
data distribution and uncertainty angles to address these issues. We propose a
general framework with an exploratory study for analyzing AI systems. After
large-scale (700+ experimental configurations and 5000+ GPU hours) experiments
and in-depth investigations, we reached a few key interesting findings that
highlight the practical need and opportunities for more in-depth investigations
into AI systems."
14993,"We further study Transformers with diﬀerent depths
for recurrent as well as non-recurrent architectures, and ﬁnd qualitatively equivalent results, see Figure 7.","We leave a
search for better weight corrections to future work.","Additionally, in Appendix A.6, we provide results obtained when using softmax self-attention layers as well
as when employing LayerNorm, thus essentially retrieving the standard Transformer architecture.",2022-12-15 09:21:21+00:00,Transformers learn in-context by gradient descent,cs.LG,"['cs.LG', 'cs.AI', 'cs.CL']","[arxiv.Result.Author('Johannes von Oswald'), arxiv.Result.Author('Eyvind Niklasson'), arxiv.Result.Author('Ettore Randazzo'), arxiv.Result.Author('João Sacramento'), arxiv.Result.Author('Alexander Mordvintsev'), arxiv.Result.Author('Andrey Zhmoginov'), arxiv.Result.Author('Max Vladymyrov')]","Transformers have become the state-of-the-art neural network architecture
across numerous domains of machine learning. This is partly due to their
celebrated ability to transfer and to learn in-context based on few examples.
Nevertheless, the mechanisms by which Transformers become in-context learners
are not well understood and remain mostly an intuition. Here, we argue that
training Transformers on auto-regressive tasks can be closely related to
well-known gradient-based meta-learning formulations. We start by providing a
simple weight construction that shows the equivalence of data transformations
induced by 1) a single linear self-attention layer and by 2) gradient-descent
(GD) on a regression loss. Motivated by that construction, we show empirically
that when training self-attention-only Transformers on simple regression tasks
either the models learned by GD and Transformers show great similarity or,
remarkably, the weights found by optimization match the construction. Thus we
show how trained Transformers implement gradient descent in their forward pass.
This allows us, at least in the domain of regression problems, to
mechanistically understand the inner workings of optimized Transformers that
learn in-context. Furthermore, we identify how Transformers surpass plain
gradient descent by an iterative curvature correction and learn linear models
on deep data representations to solve non-linear regression tasks. Finally, we
discuss intriguing parallels to a mechanism identified to be crucial for
in-context learning termed induction-head (Olsson et al., 2022) and show how it
could be understood as a specific case of in-context learning by gradient
descent learning within Transformers."
14994,"Finally, we conclude our investigations and point out possible directions
of further research in Section 6.","Subsequently, in Section 5, the enhanced PINN is applied to
clean as well as artiﬁcially noisy, synthetic displacement data of a plate with a
hole.","4
2.",2022-12-15 11:01:32+00:00,Physics-Informed Neural Networks for Material Model Calibration from Full-Field Displacement Data,cs.LG,['cs.LG'],"[arxiv.Result.Author('David Anton'), arxiv.Result.Author('Henning Wessels')]","The identification of material parameters occurring in constitutive models
has a wide range of applications in practice. One of these applications is the
monitoring and assessment of the actual condition of infrastructure buildings,
as the material parameters directly reflect the resistance of the structures to
external impacts. Physics-informed neural networks (PINNs) have recently
emerged as a suitable method for solving inverse problems. The advantages of
this method are a straightforward inclusion of observation data. Unlike
grid-based methods, such as the finite element method updating (FEMU) approach,
no computational grid and no interpolation of the data is required. In the
current work, we aim to further develop PINNs towards the calibration of the
linear-elastic constitutive model from full-field displacement and global force
data in a realistic regime. We show that normalization and conditioning of the
optimization problem play a crucial role in this process. Therefore, among
others, we identify the material parameters for initial estimates and balance
the individual terms in the loss function. In order to reduce the dependence of
the identified material parameters on local errors in the displacement
approximation, we base the identification not on the stress boundary conditions
but instead on the global balance of internal and external work. In addition,
we found that we get a better posed inverse problem if we reformulate it in
terms of bulk and shear modulus instead of Young's modulus and Poisson's ratio.
We demonstrate that the enhanced PINNs are capable of identifying material
parameters from both experimental one-dimensional data and synthetic full-field
displacement data in a realistic regime. Since displacement data measured by,
e.g., a digital image correlation (DIC) system is noisy, we additionally
investigate the robustness of the method to different levels of noise."
15002,"In general, the solution x⋆ is not a function of θ because there can be
                                        several solutions for a given θ and, even under the assumption that x⋆ is unique, it is not guaranteed that x⋆
                                        will be diﬀerentiable with respect to θ, therefore motivating further study.","Assuming that the solution x⋆(θ) is unique for each θ, our main objective in this paper is to investigate the

                                        regularity and diﬀerentiability of x⋆ with respect to θ as well as to develop calculus rules for computing a
                                        generalized derivative associated to it.","Understanding the regularity of

                                            ∗Toulouse School of Economics, University of Toulouse
                                            †IRIT, CNRS, Université Toulouse III Paul Sabatier.",2022-12-15 14:05:32+00:00,Differentiating Nonsmooth Solutions to Parametric Monotone Inclusion Problems,cs.LG,"['cs.LG', 'math.OC']","[arxiv.Result.Author('Jérôme Bolte'), arxiv.Result.Author('Edouard Pauwels'), arxiv.Result.Author('Antonio José Silveti-Falls')]","We leverage path differentiability and a recent result on nonsmooth implicit
differentiation calculus to give sufficient conditions ensuring that the
solution to a monotone inclusion problem will be path differentiable, with
formulas for computing its generalized gradient. A direct consequence of our
result is that these solutions happen to be differentiable almost everywhere.
Our approach is fully compatible with automatic differentiation and comes with
assumptions which are easy to check, roughly speaking: semialgebraicity and
strong monotonicity. We illustrate the scope of our results by considering
three fundamental composite problem settings: strongly convex problems, dual
solutions to convex minimization problems and primal-dual solutions to min-max
problems."
15009,"We hope that the our ﬁndings          compute-bound computation performs enough computation
will spur further research in improving and exploiting GPU         to keep all processing elements on a processor busy at all
utilization.","of a GPU
efﬁciency, but also for improving the throughput and/or            is that the computation in question be compute bound: a
accuracy of CNNs themselves.",times.,2022-12-15 16:11:40+00:00,A Study on the Intersection of GPU Utilization and CNN Inference,cs.LG,"['cs.LG', 'cs.PF']","[arxiv.Result.Author('Jack Kosaian'), arxiv.Result.Author('Amar Phanishayee')]","There has been significant progress in developing neural network
architectures that both achieve high predictive performance and that also
achieve high application-level inference throughput (e.g., frames per second).
Another metric of increasing importance is GPU utilization during inference:
the measurement of how well a deployed neural network uses the computational
capabilities of the GPU on which it runs. Achieving high GPU utilization is
critical to increasing application-level throughput and ensuring a good return
on investment for deploying GPUs.
  This paper analyzes the GPU utilization of convolutional neural network (CNN)
inference. We first survey the GPU utilization of CNNs to show that there is
room to improve the GPU utilization of many of these CNNs. We then investigate
the GPU utilization of networks within a neural architecture search (NAS)
search space, and explore how using GPU utilization as a metric could
potentially be used to accelerate NAS itself. Our study makes the case that
there is room to improve the inference-time GPU utilization of CNNs and that
knowledge of GPU utilization has the potential to benefit even applications
that do not target utilization itself. We hope that the results of this study
will spur future innovation in designing GPU-efficient neural networks."
15048,"We note that further study into hyper-parameter tuning
could signiﬁcantly improve this result.","In Figure 10 we ﬁnd
that we are able to signiﬁcantly improve both the sparsity of the explanations as well as the well
as the robustness of the network; however, it comes at the largest test set accuracy penalty of any
dataset tested at 20% test set accuracy loss.","F HYPER-PARAMETER VALUES

In this section we report the hyperparameters for the networks trained in

the main text.",2022-12-16 14:40:25+00:00,Robust Explanation Constraints for Neural Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Matthew Wicker'), arxiv.Result.Author('Juyeon Heo'), arxiv.Result.Author('Luca Costabello'), arxiv.Result.Author('Adrian Weller')]","Post-hoc explanation methods are used with the intent of providing insights
about neural networks and are sometimes said to help engender trust in their
outputs. However, popular explanations methods have been found to be fragile to
minor perturbations of input features or model parameters. Relying on
constraint relaxation techniques from non-convex optimization, we develop a
method that upper-bounds the largest change an adversary can make to a
gradient-based explanation via bounded manipulation of either the input
features or model parameters. By propagating a compact input or parameter set
as symbolic intervals through the forwards and backwards computations of the
neural network we can formally certify the robustness of gradient-based
explanations. Our bounds are differentiable, hence we can incorporate provable
explanation robustness into neural network training. Empirically, our method
surpasses the robustness provided by previous heuristic approaches. We find
that our training method is the only method able to learn neural networks with
certificates of explanation robustness across all six datasets tested."
15077,"Our study provides insights into best practices for federated HP tuning and
suggests several directions for further study in this broad area.","In this work, we systematically study the use of
noisy evaluation in federated HP tuning.","Our results also lead us to propose simple baselines
that can help to mitigate the eﬀect of noisy evaluation in practical FL applications.",2022-12-17 18:51:00+00:00,On Noisy Evaluation in Federated Hyperparameter Tuning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kevin Kuo'), arxiv.Result.Author('Pratiksha Thaker'), arxiv.Result.Author('Mikhail Khodak'), arxiv.Result.Author('John Ngyuen'), arxiv.Result.Author('Daniel Jiang'), arxiv.Result.Author('Ameet Talwalkar'), arxiv.Result.Author('Virginia Smith')]","Hyperparameter tuning is critical to the success of federated learning
applications. Unfortunately, appropriately selecting hyperparameters is
challenging in federated networks. Issues of scale, privacy, and heterogeneity
introduce noise in the tuning process and make it difficult to evaluate the
performance of various hyperparameters. In this work, we perform the first
systematic study on the effect of noisy evaluation in federated hyperparameter
tuning. We first identify and rigorously explore key sources of noise,
including client subsampling, data and systems heterogeneity, and data privacy.
Surprisingly, our results indicate that even small amounts of noise can
significantly impact tuning methods-reducing the performance of
state-of-the-art approaches to that of naive baselines. To address noisy
evaluation in such scenarios, we propose a simple and effective approach that
leverages public proxy data to boost the evaluation signal. Our work
establishes general challenges, baselines, and best practices for future work
in federated hyperparameter tuning."
15078,"However, it would be useful to further study this
area to develop tools for easily determining if/when proxy data is appropriate.","Finally, a key takeaway from our experiments is that proxy data (even seemingly
unrelated) can be useful when faced with high-noise evaluation.","Our work also suggests that public
data, used to improve private training of large models Li et al.",2022-12-17 18:51:00+00:00,On Noisy Evaluation in Federated Hyperparameter Tuning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kevin Kuo'), arxiv.Result.Author('Pratiksha Thaker'), arxiv.Result.Author('Mikhail Khodak'), arxiv.Result.Author('John Ngyuen'), arxiv.Result.Author('Daniel Jiang'), arxiv.Result.Author('Ameet Talwalkar'), arxiv.Result.Author('Virginia Smith')]","Hyperparameter tuning is critical to the success of federated learning
applications. Unfortunately, appropriately selecting hyperparameters is
challenging in federated networks. Issues of scale, privacy, and heterogeneity
introduce noise in the tuning process and make it difficult to evaluate the
performance of various hyperparameters. In this work, we perform the first
systematic study on the effect of noisy evaluation in federated hyperparameter
tuning. We first identify and rigorously explore key sources of noise,
including client subsampling, data and systems heterogeneity, and data privacy.
Surprisingly, our results indicate that even small amounts of noise can
significantly impact tuning methods-reducing the performance of
state-of-the-art approaches to that of naive baselines. To address noisy
evaluation in such scenarios, we propose a simple and effective approach that
leverages public proxy data to boost the evaluation signal. Our work
establishes general challenges, baselines, and best practices for future work
in federated hyperparameter tuning."
15079,"Our study provides insights into best practices for federated HP tuning and
suggests several directions for further study in this broad area.","In this work, we systematically study the use of
noisy evaluation in federated HP tuning.","Our results also lead us to propose simple baselines
that can help to mitigate the eﬀect of noisy evaluation in practical FL applications.",2022-12-17 18:51:00+00:00,On Noisy Evaluation in Federated Hyperparameter Tuning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kevin Kuo'), arxiv.Result.Author('Pratiksha Thaker'), arxiv.Result.Author('Mikhail Khodak'), arxiv.Result.Author('John Ngyuen'), arxiv.Result.Author('Daniel Jiang'), arxiv.Result.Author('Ameet Talwalkar'), arxiv.Result.Author('Virginia Smith')]","Hyperparameter tuning is critical to the success of federated learning
applications. Unfortunately, appropriately selecting hyperparameters is
challenging in federated networks. Issues of scale, privacy, and heterogeneity
introduce noise in the tuning process and make it difficult to evaluate the
performance of various hyperparameters. In this work, we perform the first
systematic study on the effect of noisy evaluation in federated hyperparameter
tuning. We first identify and rigorously explore key sources of noise,
including client subsampling, data and systems heterogeneity, and data privacy.
Surprisingly, our results indicate that even small amounts of noise can
significantly impact tuning methods-reducing the performance of
state-of-the-art approaches to that of naive baselines. To address noisy
evaluation in such scenarios, we propose a simple and effective approach that
leverages public proxy data to boost the evaluation signal. Our work
establishes general challenges, baselines, and best practices for future work
in federated hyperparameter tuning."
15080,"However, it would be useful to further study this
area to develop tools for easily determining if/when proxy data is appropriate.","Finally, a key takeaway from our experiments is that proxy data (even seemingly
unrelated) can be useful when faced with high-noise evaluation.","Our work also suggests that public
data, used to improve private training of large models [Li et al., 2021, Yu et al., 2021, De et al., 2022], may also be
useful to improve private evaluation in a similar way.",2022-12-17 18:51:00+00:00,On Noisy Evaluation in Federated Hyperparameter Tuning,cs.LG,['cs.LG'],"[arxiv.Result.Author('Kevin Kuo'), arxiv.Result.Author('Pratiksha Thaker'), arxiv.Result.Author('Mikhail Khodak'), arxiv.Result.Author('John Ngyuen'), arxiv.Result.Author('Daniel Jiang'), arxiv.Result.Author('Ameet Talwalkar'), arxiv.Result.Author('Virginia Smith')]","Hyperparameter tuning is critical to the success of federated learning
applications. Unfortunately, appropriately selecting hyperparameters is
challenging in federated networks. Issues of scale, privacy, and heterogeneity
introduce noise in the tuning process and make it difficult to evaluate the
performance of various hyperparameters. In this work, we perform the first
systematic study on the effect of noisy evaluation in federated hyperparameter
tuning. We first identify and rigorously explore key sources of noise,
including client subsampling, data and systems heterogeneity, and data privacy.
Surprisingly, our results indicate that even small amounts of noise can
significantly impact tuning methods-reducing the performance of
state-of-the-art approaches to that of naive baselines. To address noisy
evaluation in such scenarios, we propose a simple and effective approach that
leverages public proxy data to boost the evaluation signal. Our work
establishes general challenges, baselines, and best practices for future work
in federated hyperparameter tuning."
15092,"8
We hope that our work triggers further research into these areas and that it promotes the clinical
applicability of multimodal deep learning models.","Third,
domain-transferability of the model could not be tested due to the lack of suitable datasets that have
concordant labels and concordant data available for training.","Online Methods

Ethics Statement
All experiments were conducted in accordance with the Declaration of Helsinki and the International
Ethical Guidelines for Biomedical Research Involving Human Subjects by the Council for International
Organizations of Medical Sciences (CIOMS).",2022-12-18 20:43:37+00:00,Medical Diagnosis with Large Scale Multimodal Transformers -- Leveraging Diverse Data for More Accurate Diagnosis,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Firas Khader'), arxiv.Result.Author('Gustav Mueller-Franzes'), arxiv.Result.Author('Tianci Wang'), arxiv.Result.Author('Tianyu Han'), arxiv.Result.Author('Soroosh Tayebi Arasteh'), arxiv.Result.Author('Christoph Haarburger'), arxiv.Result.Author('Johannes Stegmaier'), arxiv.Result.Author('Keno Bressem'), arxiv.Result.Author('Christiane Kuhl'), arxiv.Result.Author('Sven Nebelung'), arxiv.Result.Author('Jakob Nikolas Kather'), arxiv.Result.Author('Daniel Truhn')]","Multimodal deep learning has been used to predict clinical endpoints and
diagnoses from clinical routine data. However, these models suffer from scaling
issues: they have to learn pairwise interactions between each piece of
information in each data type, thereby escalating model complexity beyond
manageable scales. This has so far precluded a widespread use of multimodal
deep learning. Here, we present a new technical approach of ""learnable
synergies"", in which the model only selects relevant interactions between data
modalities and keeps an ""internal memory"" of relevant data. Our approach is
easily scalable and naturally adapts to multimodal data inputs from clinical
routine. We demonstrate this approach on three large multimodal datasets from
radiology and ophthalmology and show that it outperforms state-of-the-art
models in clinically relevant diagnosis tasks. Our new approach is transferable
and will allow the application of multimodal deep learning to a broad set of
clinically relevant problems."
15093,"8
We hope that our work triggers further research into these areas and that it promotes the clinical
applicability of multimodal deep learning models.","Third,
domain-transferability of the model could not be tested due to the lack of suitable datasets that have
concordant labels and concordant data available for training.","Online Methods

Ethics Statement
All experiments were conducted in accordance with the Declaration of Helsinki and the International
Ethical Guidelines for Biomedical Research Involving Human Subjects by the Council for International
Organizations of Medical Sciences (CIOMS).",2022-12-18 20:43:37+00:00,Medical Diagnosis with Large Scale Multimodal Transformers: Leveraging Diverse Data for More Accurate Diagnosis,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Firas Khader'), arxiv.Result.Author('Gustav Mueller-Franzes'), arxiv.Result.Author('Tianci Wang'), arxiv.Result.Author('Tianyu Han'), arxiv.Result.Author('Soroosh Tayebi Arasteh'), arxiv.Result.Author('Christoph Haarburger'), arxiv.Result.Author('Johannes Stegmaier'), arxiv.Result.Author('Keno Bressem'), arxiv.Result.Author('Christiane Kuhl'), arxiv.Result.Author('Sven Nebelung'), arxiv.Result.Author('Jakob Nikolas Kather'), arxiv.Result.Author('Daniel Truhn')]","Multimodal deep learning has been used to predict clinical endpoints and
diagnoses from clinical routine data. However, these models suffer from scaling
issues: they have to learn pairwise interactions between each piece of
information in each data type, thereby escalating model complexity beyond
manageable scales. This has so far precluded a widespread use of multimodal
deep learning. Here, we present a new technical approach of ""learnable
synergies"", in which the model only selects relevant interactions between data
modalities and keeps an ""internal memory"" of relevant data. Our approach is
easily scalable and naturally adapts to multimodal data inputs from clinical
routine. We demonstrate this approach on three large multimodal datasets from
radiology and ophthalmology and show that it outperforms state-of-the-art
models in clinically relevant diagnosis tasks. Our new approach is transferable
and will allow the application of multimodal deep learning to a broad set of
clinically relevant problems."
15135,promising areas for further study.,Use a ﬂoating point or quantile quantization data type.,"In some cases, integer data types might be preferable
                                                                      to improve inference latency depending on the imple-
Quantization methods.",2022-12-19 18:48:33+00:00,The case for 4-bit precision: k-bit Inference Scaling Laws,cs.LG,"['cs.LG', 'cs.NE']","[arxiv.Result.Author('Tim Dettmers'), arxiv.Result.Author('Luke Zettlemoyer')]","Quantization methods reduce the number of bits required to represent each
parameter in a model, trading accuracy for smaller memory footprints and
inference latencies. However, the final model size depends on both the number
of parameters of the original model and the rate of compression. For example, a
30B 8-bit model and a 60B 4-bit model have the same number of bits but may have
very different zero-shot accuracies. In this work, we study this trade-off by
developing inference scaling laws of zero-shot performance in Large Language
Models (LLMs) to determine the bit-precision and model size that maximizes
zero-shot performance. We run more than 35,000 zero-shot experiments with
16-bit inputs and k-bit parameters to examine which quantization methods
improve scaling for 3 to 8-bit precision at scales of 19M to 66B parameters
across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is
challenging to improve the bit-level scaling trade-off, with the only
improvements being the use of a small block size -- splitting the parameters
into small independently quantized blocks -- and the quantization data type
being used (e.g., Int vs Float). Overall, our findings show that 4-bit
precision is almost universally optimal for total model bits and zero-shot
accuracy."
15141,"We further study
the different performances of NRF which is from dynamic sparse training and from random topology
in DSN method.","Intuitively, the DSN method with random topology can also cover various eNRF.",We conduct ablation studies to compare DSN with its static variant (i.e.,2022-12-19 20:32:27+00:00,"Dynamic Sparse Network for Time Series Classification: Learning What to ""see''",cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Qiao Xiao'), arxiv.Result.Author('Boqian Wu'), arxiv.Result.Author('Yu Zhang'), arxiv.Result.Author('Shiwei Liu'), arxiv.Result.Author('Mykola Pechenizkiy'), arxiv.Result.Author('Elena Mocanu'), arxiv.Result.Author('Decebal Constantin Mocanu')]","The receptive field (RF), which determines the region of time series to be
``seen'' and used, is critical to improve the performance for time series
classification (TSC). However, the variation of signal scales across and within
time series data, makes it challenging to decide on proper RF sizes for TSC. In
this paper, we propose a dynamic sparse network (DSN) with sparse connections
for TSC, which can learn to cover various RF without cumbersome
hyper-parameters tuning. The kernels in each sparse layer are sparse and can be
explored under the constraint regions by dynamic sparse training, which makes
it possible to reduce the resource cost. The experimental results show that the
proposed DSN model can achieve state-of-art performance on both univariate and
multivariate TSC datasets with less than 50\% computational cost compared with
recent baseline methods, opening the path towards more accurate resource-aware
methods for time series analyses. Our code is publicly available at:
https://github.com/QiaoXiao7282/DSN."
15186,"The broad idea in
these approaches is to use only a limited number of function      Section VI, we present the conclusions of this study, and also
                                                                  present some directions for further research.","The earliest such approach is due to        [14] and the simultaneous perturbation stochastic approxi-
                                        Kiefer and Wolfowitz [12] which was originally proposed            mation (SPSA) [19], [20] and it’s deterministic perturbation
                                                                                                           variant [4] are more efﬁcient than K-W.","measurements (often one or two) to get the estimates of all

the partial derivatives of the performance objective.",2022-12-20 17:50:36+00:00,Generalized Simultaneous Perturbation Stochastic Approximation with Reduced Estimator Bias,cs.LG,"['cs.LG', 'math.ST', 'stat.ML', 'stat.TH']","[arxiv.Result.Author('Shalabh Bhatnagar'), arxiv.Result.Author('Prashanth L. A')]","We present in this paper a family of generalized simultaneous perturbation
stochastic approximation (G-SPSA) estimators that estimate the gradient of the
objective using noisy function measurements, but where the number of function
measurements and the form of the gradient estimator is guided by the desired
estimator bias. In particular, estimators with more function measurements are
seen to result in lower bias. We provide an analysis of convergence of the
generalized SPSA algorithm, and point to possible future directions."
15207,"We outline a few
extensions of our approach and interesting directions for further research:

    • Our method for generating poison and camouﬂage points was based on the gradient-matching
       attack of Geiping et al.","This shows that as we introduce new functionality
to machine learning systems, we must be aware of novel threats that emerge.",(2021).,2022-12-21 01:52:17+00:00,Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,cs.LG,"['cs.LG', 'cs.AI', 'cs.CR', 'cs.CY']","[arxiv.Result.Author('Jimmy Z. Di'), arxiv.Result.Author('Jack Douglas'), arxiv.Result.Author('Jayadev Acharya'), arxiv.Result.Author('Gautam Kamath'), arxiv.Result.Author('Ayush Sekhari')]","We introduce camouflaged data poisoning attacks, a new attack vector that
arises in the context of machine unlearning and other settings when model
retraining may be induced. An adversary first adds a few carefully crafted
points to the training dataset such that the impact on the model's predictions
is minimal. The adversary subsequently triggers a request to remove a subset of
the introduced points at which point the attack is unleashed and the model's
predictions are negatively affected. In particular, we consider clean-label
targeted attacks (in which the goal is to cause the model to misclassify a
specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.
This attack is realized by constructing camouflage datapoints that mask the
effect of a poisoned dataset."
15220,"With these preliminary results we want to motivate further research both into how RL agents generalize
in the ﬁrst place and how we can more easily adapt their hyperparameters to support this goal.","Thus there is
a need for more insights into cRL in general and AutoRL for cRL in particular.","Our
results also clearly show the importance of using existing AutoRL methods when applying RL to a
task and the potential improvements gained.",2022-12-21 09:38:18+00:00,Hyperparameters in Contextual RL are Highly Situational,cs.LG,['cs.LG'],"[arxiv.Result.Author('Theresa Eimer'), arxiv.Result.Author('Carolin Benjamins'), arxiv.Result.Author('Marius Lindauer')]","Although Reinforcement Learning (RL) has shown impressive results in games
and simulation, real-world application of RL suffers from its instability under
changing environment conditions and hyperparameters. We give a first impression
of the extent of this instability by showing that the hyperparameters found by
automatic hyperparameter optimization (HPO) methods are not only dependent on
the problem at hand, but even on how well the state describes the environment
dynamics. Specifically, we show that agents in contextual RL require different
hyperparameters if they are shown how environmental factors change. In
addition, finding adequate hyperparameter configurations is not equally easy
for both settings, further highlighting the need for research into how
hyperparameters influence learning and generalization in RL."
15223,"Based on the hypothesis that conventional                       term dynamic lot size planning taking into account primary re-
metaheuristics can be improved in this way, further research                      quirements in subsequent periods is not possible.","Long-
problem instance.","This study

                                                                               9
considers an application and not an in-depth investigation of            Appendix A.",2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
15224,"However, further research should           OS:      CentOS 7.5.1804
be undertaken to examine the predictability of appropriate hy-           vCPU:    Intel Xeon E5-2630 V4 2.2 GHz with 28 cores assigned
perparameters for diﬀerent production environments consider-             RAM:     64 GB DDR4 2133 MHz
ing varying numbers of stations, workers or tasks.","This is not a disadvantage of this study,
as we were still able to conﬁrm that DRL can improve the re-             Solver:  IBM CPLEX V12
sults of a memetic algorithm.","Table A.6: GA and GASA hyperparameters
6.",2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
15225,"A relative time greater than 1 is an indicator of
some shortcomings that lead to further research questions.","However, the proposed framework has                            erage WIP per station.",The                   wait times and that work is not evenly distributed across resources.,2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
15226,"Based on the hypothesis that conventional                       term dynamic lot size planning taking into account primary re-
metaheuristics can be improved in this way, further research                      quirements in subsequent periods is not possible.","Long-
problem instance.","This study

                                                                               9
considers an application and not an in-depth investigation of            Appendix A.",2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
15227,"However, further research should           OS:      CentOS 7.5.1804
be undertaken to examine the predictability of appropriate hy-           vCPU:    Intel Xeon E5-2630 V4 2.2 GHz with 28 cores assigned
perparameters for diﬀerent production environments consider-             RAM:     64 GB DDR4 2133 MHz
ing varying numbers of stations, workers or tasks.","This is not a disadvantage of this study,
as we were still able to conﬁrm that DRL can improve the re-             Solver:  IBM CPLEX V12
sults of a memetic algorithm.","Table A.6: GA and GASA hyperparameters
6.",2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
15228,"A relative time greater than 1 is an indicator of
some shortcomings that lead to further research questions.","However, the proposed framework has                            erage WIP per station.",The                   wait times and that work is not evenly distributed across resources.,2022-12-21 11:24:32+00:00,A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Author('Felix Grumbach'), arxiv.Result.Author('Nour Eldin Alaa Badr'), arxiv.Result.Author('Pascal Reusch'), arxiv.Result.Author('Sebastian Trojahn')]","The following article presents a memetic algorithm with applying deep
reinforcement learning (DRL) for solving practically oriented dual resource
constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years,
there has been extensive research on DRL techniques, but without considering
realistic, flexible and human-centered shopfloors. A research gap can be
identified in the context of make-to-order oriented discontinuous manufacturing
as it is often represented in medium-size companies with high service levels.
From practical industry projects in this domain, we recognize requirements to
depict flexible machines, human workers and capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-depended setup times and
(partially) automated tasks. On the other hand, intensive research has been
done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of
suitable and generic scheduling methods that can be holistically applied in
sociotechnical production and assembly processes. In this paper, we first
formulate an extended DRC-FJSSP induced by the practical requirements
mentioned. Then we present our proposed hybrid framework with parallel
computing for multicriteria optimization. Through numerical experiments with
real-world data, we confirm that the framework generates feasible schedules
efficiently and reliably. Utilizing DRL instead of random operations leads to
better results and outperforms traditional approaches."
15231,"Since this chapter focuses on the impact of OI, further research on
incorporating these methods into OI is left open.","Carousel shaping ensures each stage receives
the same training amount.","𝜖-greedy and softmax are both commonly applied exploration techniques in reinforcement
learning.",2022-12-21 15:33:01+00:00,On Reinforcement Learning for the Game of 2048,cs.LG,"['cs.LG', 'cs.AI', 'I.2.6; I.2.8']",[arxiv.Result.Author('Hung Guei')],"2048 is a single-player stochastic puzzle game. This intriguing and addictive
game has been popular worldwide and has attracted researchers to develop
game-playing programs. Due to its simplicity and complexity, 2048 has become an
interesting and challenging platform for evaluating the effectiveness of
machine learning methods. This dissertation conducts comprehensive research on
reinforcement learning and computer game algorithms for 2048. First, this
dissertation proposes optimistic temporal difference learning, which
significantly improves the quality of learning by employing optimistic
initialization to encourage exploration for 2048. Furthermore, based on this
approach, a state-of-the-art program for 2048 is developed, which achieves the
highest performance among all learning-based programs, namely an average score
of 625377 points and a rate of 72% for reaching 32768-tiles. Second, this
dissertation investigates several techniques related to 2048, including the
n-tuple network ensemble learning, Monte Carlo tree search, and deep
reinforcement learning. These techniques are promising for further improving
the performance of the current state-of-the-art program. Finally, this
dissertation discusses pedagogical applications related to 2048 by proposing
course designs and summarizing the teaching experience. The proposed course
designs use 2048-like games as materials for beginners to learn reinforcement
learning and computer game algorithms. The courses have been successfully
applied to graduate-level students and received well by student feedback."
15236,"In summary, while more work is required to improve the memory eﬃciency
of the proposed approaches, the success of the linear combination methods suggests venues of research to
reduce memory consumption, while the performance advantages justify further research of masking methods
in LRL.","Increasing the threshold (currently set to 0), or applying top k-winners as in (Wortsman et al., 2020)
for supervised learning, may lead to a meta optimization process where signiﬁcantly smaller masks maintain
acceptable levels of performance.","8 Conclusion

This work introduces the use of modulating masks for lifelong reinforcement learning problems.",2022-12-21 15:49:20+00:00,Lifelong Reinforcement Learning with Modulating Masks,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Author('Eseoghene Ben-Iwhiwhu'), arxiv.Result.Author('Saptarshi Nath'), arxiv.Result.Author('Praveen K. Pilly'), arxiv.Result.Author('Soheil Kolouri'), arxiv.Result.Author('Andrea Soltoggio')]","Lifelong learning aims to create AI systems that continuously and
incrementally learn during a lifetime, similar to biological learning. Attempts
so far have met problems, including catastrophic forgetting, interference among
tasks, and the inability to exploit previous knowledge. While considerable
research has focused on learning multiple input distributions, typically in
classification, lifelong reinforcement learning (LRL) must also deal with
variations in the state and transition distributions, and in the reward
functions. Modulating masks, recently developed for classification, are
particularly suitable to deal with such a large spectrum of task variations. In
this paper, we adapted modulating masks to work with deep LRL, specifically PPO
and IMPALA agents. The comparison with LRL baselines in both discrete and
continuous RL tasks shows competitive performance. We further investigated the
use of a linear combination of previously learned masks to exploit previous
knowledge when learning new tasks: not only is learning faster, the algorithm
solves tasks that we could not otherwise solve from scratch due to extremely
sparse rewards. The results suggest that RL with modulating masks is a
promising approach to lifelong learning, to the composition of knowledge to
learn increasingly complex tasks, and to knowledge reuse for efficient and
faster learning."
15238,"We publish the code of balanced-split for further research,
                                                                  and it can be publicly accessed using the following URL: https:
Fig.",multiple splitting strategies.,5: Comparisons for random forest classiﬁer.,2022-12-17 10:36:39+00:00,Balanced Split: A new train-test data splitting strategy for imbalanced datasets,cs.LG,['cs.LG'],[arxiv.Result.Author('Azal Ahmad Khan')],"Classification data sets with skewed class proportions are called imbalanced.
Class imbalance is a problem since most machine learning classification
algorithms are built with an assumption of equal representation of all classes
in the training dataset. Therefore to counter the class imbalance problem, many
algorithm-level and data-level approaches have been developed. These mainly
include ensemble learning and data augmentation techniques. This paper shows a
new way to counter the class imbalance problem through a new data-splitting
strategy called balanced split. Data splitting can play an important role in
correctly classifying imbalanced datasets. We show that the commonly used
data-splitting strategies have some disadvantages, and our proposed balanced
split has solved those problems."
15249,"Furthermore,
further researchers can make a better comparison between the performance of different machine-
learning causal models.","In terms of our case study, our data is classified as panel data; however, we
did not account for the panel effect and this issue can be investigated in future work.","Moreover, in terms of pedestrian reaction in a collaborative environment
concept, analysis of pedestrian group-crossing behavior and the role of conformity can be a future
research direction.",2022-12-21 19:36:48+00:00,Debiased machine learning for estimating the causal effect of urban traffic on pedestrian crossing behaviour,cs.LG,"['cs.LG', 'stat.ME']","[arxiv.Result.Author('Kimia Kamal'), arxiv.Result.Author('Bilal Farooq')]","Before the transition of AVs to urban roads and subsequently unprecedented
changes in traffic conditions, evaluation of transportation policies and
futuristic road design related to pedestrian crossing behavior is of vital
importance. Recent studies analyzed the non-causal impact of various variables
on pedestrian waiting time in the presence of AVs. However, we mainly
investigate the causal effect of traffic density on pedestrian waiting time. We
develop a Double/Debiased Machine Learning (DML) model in which the impact of
confounders variable influencing both a policy and an outcome of interest is
addressed, resulting in unbiased policy evaluation. Furthermore, we try to
analyze the effect of traffic density by developing a copula-based joint model
of two main components of pedestrian crossing behavior, pedestrian stress level
and waiting time. The copula approach has been widely used in the literature,
for addressing self-selection problems, which can be classified as a causality
analysis in travel behavior modeling. The results obtained from copula approach
and DML are compared based on the effect of traffic density. In DML model
structure, the standard error term of density parameter is lower than copula
approach and the confidence interval is considerably more reliable. In
addition, despite the similar sign of effect, the copula approach estimates the
effect of traffic density lower than DML, due to the spurious effect of
confounders. In short, the DML model structure can flexibly adjust the impact
of confounders by using machine learning algorithms and is more reliable for
planning future policies."
15270,"First, we systematically evaluate the feasi-     ing in general and the effectiveness of AL in par-
bility, checking whether AL is practicable (achiev-     ticular, paving the way for further research into
able in realistic conditions) and effective (consis-    effective AL.","Even more importantly,
                                                        the results indicate that representation smoothness
   In this work, we address the practical challenges    analysis can be leveraged to improve model train-
of AL.",tently outperforms random selection).,2022-12-20 19:37:20+00:00,Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Josip Jukić'), arxiv.Result.Author('Jan Šnajder')]","Developed as a solution to a practical need, active learning (AL) methods aim
to reduce label complexity and the annotations costs in supervised learning.
While recent work has demonstrated the benefit of using AL in combination with
large pre-trained language models (PLMs), it has often overlooked the practical
challenges that hinder the feasibility of AL in realistic settings. We address
these challenges by leveraging representation smoothness analysis to improve
the effectiveness of AL. We develop an early stopping technique that does not
require a validation set -- often unavailable in realistic AL settings -- and
observe significant improvements across multiple datasets and AL methods.
Additionally, we find that task adaptation improves AL, whereas standard short
fine-tuning in AL does not provide improvements over random sampling. Our work
establishes the usefulness of representation smoothness analysis in AL and
presents an AL stopping criterion that reduces label complexity."
15271,"and representation smoothness is an exciting av-
enue for AL, and we hope our results will motivate        Liat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,
further research in that direction.","CoRR, abs/2002.06305.","Lena Dankin, Leshem Choshen, Marina Danilevsky,
                                                             Ranit Aharonov, Yoav Katz, and Noam Slonim.",2022-12-20 19:37:20+00:00,Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Author('Josip Jukić'), arxiv.Result.Author('Jan Šnajder')]","Developed as a solution to a practical need, active learning (AL) methods aim
to reduce label complexity and the annotations costs in supervised learning.
While recent work has demonstrated the benefit of using AL in combination with
large pre-trained language models (PLMs), it has often overlooked the practical
challenges that hinder the feasibility of AL in realistic settings. We address
these challenges by leveraging representation smoothness analysis to improve
the effectiveness of AL. We develop an early stopping technique that does not
require a validation set -- often unavailable in realistic AL settings -- and
observe significant improvements across multiple datasets and AL methods.
Additionally, we find that task adaptation improves AL, whereas standard short
fine-tuning in AL does not provide improvements over random sampling. Our work
establishes the usefulness of representation smoothness analysis in AL and
presents an AL stopping criterion that reduces label complexity."
15276,"This initiative is only
                                        are now commonplace in many research and application domains,         one of the various projects that suggest further research into the
                                        and they are frequently used in scenarios of complex and critical     ﬁeld of XAI, which—to a certain extent—addresses challenges re-
                                        decision-making [NGDM∗19, PWJ06, TKK18].",[KDS∗17].,"Medicine, for                lated to trust.",2022-12-22 14:29:43+00:00,The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations,cs.LG,"['cs.LG', 'cs.HC', 'stat.ML']","[arxiv.Result.Author('A. Chatzimparmpas'), arxiv.Result.Author('R. Martins'), arxiv.Result.Author('I. Jusufi'), arxiv.Result.Author('K. Kucher'), arxiv.Result.Author('Fabrice Rossi'), arxiv.Result.Author('A. Kerren')]","Machine learning (ML) models are nowadays used in complex applications in
various domains, such as medicine, bioinformatics, and other sciences. Due to
their black box nature, however, it may sometimes be hard to understand and
trust the results they provide. This has increased the demand for reliable
visualization tools related to enhancing trust in ML models, which has become a
prominent topic of research in the visualization community over the past
decades. To provide an overview and present the frontiers of current research
on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in
ML models with the use of interactive visualization. We define and describe the
background of the topic, introduce a categorization for visualization
techniques that aim to accomplish this goal, and discuss insights and
opportunities for future research directions. Among our contributions is a
categorization of trust against different facets of interactive ML, expanded
and improved from previous research. Our results are investigated from
different analytical perspectives: (a) providing a statistical overview, (b)
summarizing key findings, (c) performing topic analyses, and (d) exploring the
data sets used in the individual papers, all with the support of an interactive
web-based survey browser. We intend this survey to be beneficial for
visualization researchers whose interests involve making ML models more
trustworthy, as well as researchers and practitioners from other disciplines in
their search for effective visualization techniques suitable for solving their
tasks with confidence and conveying meaning to their data."
15277,"In
in a user-friendly way, but further research is required in this regard.","[JSS∗18] suggest that metaphor-      [AA97] ALIMOG˘ LU F., ALPAYDIN E.: Combining multiple represen-
ical narratives can explain the ML models to various target groups           tations and classiﬁers for pen-based handwritten digit recognition.","Proceedings of the Fourth International Conference on Document Anal-
                                                                             ysis and Recognition (1997), vol.",2022-12-22 14:29:43+00:00,The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations,cs.LG,"['cs.LG', 'cs.HC', 'stat.ML']","[arxiv.Result.Author('A. Chatzimparmpas'), arxiv.Result.Author('R. Martins'), arxiv.Result.Author('I. Jusufi'), arxiv.Result.Author('K. Kucher'), arxiv.Result.Author('Fabrice Rossi'), arxiv.Result.Author('A. Kerren')]","Machine learning (ML) models are nowadays used in complex applications in
various domains, such as medicine, bioinformatics, and other sciences. Due to
their black box nature, however, it may sometimes be hard to understand and
trust the results they provide. This has increased the demand for reliable
visualization tools related to enhancing trust in ML models, which has become a
prominent topic of research in the visualization community over the past
decades. To provide an overview and present the frontiers of current research
on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in
ML models with the use of interactive visualization. We define and describe the
background of the topic, introduce a categorization for visualization
techniques that aim to accomplish this goal, and discuss insights and
opportunities for future research directions. Among our contributions is a
categorization of trust against different facets of interactive ML, expanded
and improved from previous research. Our results are investigated from
different analytical perspectives: (a) providing a statistical overview, (b)
summarizing key findings, (c) performing topic analyses, and (d) exploring the
data sets used in the individual papers, all with the support of an interactive
web-based survey browser. We intend this survey to be beneficial for
visualization researchers whose interests involve making ML models more
trustworthy, as well as researchers and practitioners from other disciplines in
their search for effective visualization techniques suitable for solving their
tasks with confidence and conveying meaning to their data."
15299,"Most of the early works on
PCA problem, we further study the Grassmann manifold [14]           anomaly-based IDS in IoT networks have focused on central-
and propose an ADMM-based Grassmannian learning algo-               ized approaches ( [7], [21]–[25]) in which data is collected to
rithm, called FedPG.","based IDS plays a critical role in safeguarding networks
                                                                    against different malicious activities, as the revolutionary
Due to a special manifold structure of the constraint in Fed-       advances in machine learning.",Note that the Grassmann manifold has           a central server raising privacy concerns and cyber invasions.,2022-12-23 03:11:56+00:00,Federated PCA on Grassmann Manifold for Anomaly Detection in IoT Networks,cs.LG,['cs.LG'],"[arxiv.Result.Author('Tung-Anh Nguyen'), arxiv.Result.Author('Jiayu He'), arxiv.Result.Author('Long Tan Le'), arxiv.Result.Author('Wei Bao'), arxiv.Result.Author('Nguyen H. Tran')]","In the era of Internet of Things (IoT), network-wide anomaly detection is a
crucial part of monitoring IoT networks due to the inherent security
vulnerabilities of most IoT devices. Principal Components Analysis (PCA) has
been proposed to separate network traffics into two disjoint subspaces
corresponding to normal and malicious behaviors for anomaly detection. However,
the privacy concerns and limitations of devices' computing resources compromise
the practical effectiveness of PCA. We propose a federated PCA-based
Grassmannian optimization framework that coordinates IoT devices to aggregate a
joint profile of normal network behaviors for anomaly detection. First, we
introduce a privacy-preserving federated PCA framework to simultaneously
capture the profile of various IoT devices' traffic. Then, we investigate the
alternating direction method of multipliers gradient-based learning on the
Grassmann manifold to guarantee fast training and the absence of detecting
latency using limited computational resources. Empirical results on the NSL-KDD
dataset demonstrate that our method outperforms baseline approaches. Finally,
we show that the Grassmann manifold algorithm is highly adapted for IoT anomaly
detection, which permits drastically reducing the analysis time of the system.
To the best of our knowledge, this is the first federated PCA algorithm for
anomaly detection meeting the requirements of IoT networks."
15409,"We hope that our work leads to further research
in studying the effect of negative curvature in generalization as we show they are a practical issue for
class-imbalanced learning using deep neural networks.","We show
that combining SAM with state-of-the-art techniques for learning with imbalanced data leads to
signiﬁcant gains in performance on minority classes.","Acknowledgements: This work was supported in part by SERB-STAR Project
(Project:STR/2020/000128), Govt.",2022-12-28 14:00:44+00:00,Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Author('Harsh Rangwani'), arxiv.Result.Author('Sumukh K Aithal'), arxiv.Result.Author('Mayank Mishra'), arxiv.Result.Author('R. Venkatesh Babu')]","Real-world datasets exhibit imbalances of varying types and degrees. Several
techniques based on re-weighting and margin adjustment of loss are often used
to enhance the performance of neural networks, particularly on minority
classes. In this work, we analyze the class-imbalanced learning problem by
examining the loss landscape of neural networks trained with re-weighting and
margin-based techniques. Specifically, we examine the spectral density of
Hessian of class-wise loss, through which we observe that the network weights
converge to a saddle point in the loss landscapes of minority classes.
Following this observation, we also find that optimization methods designed to
escape from saddle points can be effectively used to improve generalization on
minority classes. We further theoretically and empirically demonstrate that
Sharpness-Aware Minimization (SAM), a recent technique that encourages
convergence to a flat minima, can be effectively used to escape saddle points
for minority classes. Using SAM results in a 6.2\% increase in accuracy on the
minority classes over the state-of-the-art Vector Scaling Loss, leading to an
overall average increase of 4\% across imbalanced datasets. The code is
available at: https://github.com/val-iisc/Saddle-LongTail."
15412,"We hope that this work will encourage further research into better model classes for deep
reinforcement learning algorithms, including and especially for reinforcement from image inputs.","To
remedy the pathology, we proposed the use of non-parametric behavioral reference policies, which
we showed can signiﬁcantly accelerate and improve online learning and yield online policies that
(often signiﬁcantly) outperform current state-of-the-art methods on challenging continuous control
tasks.","10
Acknowledgments and Disclosure of Funding

We thank Ashvin Nair for sharing his code and results, as well as for providing helpful insights about
the dexterous hand manipulation suite.",2022-12-28 16:29:09+00:00,On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations,cs.LG,"['cs.LG', 'cs.AI', 'stat.ME', 'stat.ML']","[arxiv.Result.Author('Tim G. J. Rudner'), arxiv.Result.Author('Cong Lu'), arxiv.Result.Author('Michael A. Osborne'), arxiv.Result.Author('Yarin Gal'), arxiv.Result.Author('Yee Whye Teh')]","KL-regularized reinforcement learning from expert demonstrations has proved
successful in improving the sample efficiency of deep reinforcement learning
algorithms, allowing them to be applied to challenging physical real-world
tasks. However, we show that KL-regularized reinforcement learning with
behavioral reference policies derived from expert demonstrations can suffer
from pathological training dynamics that can lead to slow, unstable, and
suboptimal online learning. We show empirically that the pathology occurs for
commonly chosen behavioral policy classes and demonstrate its impact on sample
efficiency and online policy performance. Finally, we show that the pathology
can be remedied by non-parametric behavioral reference policies and that this
allows KL-regularized reinforcement learning to significantly outperform
state-of-the-art approaches on a variety of challenging locomotion and
dexterous hand manipulation tasks."
